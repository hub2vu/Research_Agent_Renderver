{
  "filename": "115627_Alternating Gradient Flows_ A Theory of Feature Learning in Two-layer Neural Networks",
  "original_filename": "115627_Alternating Gradient Flows_ A Theory of Feature Learning in Two-layer Neural Networks",
  "references_detected": 130,
  "titles_extracted": 127,
  "titles": [
    "Zoom in: An introduction to circuits",
    "A mathematical framework for transformer circuits",
    "In-context learning and induction heads",
    "Sparse autoencoders find highly interpretable features in language models",
    "Towards monosemanticity: Decomposing language models with dictionary learning",
    "Mechanistic interpretability for ai safety–a review",
    "Emergent abilities of large language models",
    "A toy model of universality: Reverse engineering how networks learn group operations",
    "Open problems in mechanistic interpretability",
    "A closer look at memorization in deep networks",
    "Sgd on neural networks learns functions of increasing complexity",
    "Hidden progress in deep learning: Sgd learns parities near the computational limit",
    "The optimization landscape of sgd across the feature learning strength",
    "Saddle-tosaddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity",
    "A mathematical theory of semantic development in deep neural networks",
    "Incremental learning in diagonal linear networks",
    "Saddle-to-saddle dynamics in diagonal linear networks",
    "Implicit regularization of discrete gradient dynamics in linear neural networks",
    "Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning",
    "Understanding deflation process in over-parametrized tensor decomposition",
    "On the stepwise nature of self-supervised learning",
    "Solvable dynamics of self-supervised word embeddings and the emergence of analogical reasoning",
    "The inductive bias of relu networks on orthogonally separable data",
    "Gradient descent on two-layer nets: Margin maximization and simplicity bias",
    "Early stage convergence and global convergence of training mildly parameterized neural networks",
    "Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs",
    "Early neuron alignment in two-layer relu networks with small initialization",
    "Sgd finds then tunes features in two-layer neural networks with near-optimal sample complexity: A case study in the xor problem",
    "Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks",
    "Transformers learn through gradual rank increase",
    "Training dynamics of in-context learning in linear attention",
    "The staircase property: How hierarchical structure can guide deep learning",
    "The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks",
    "Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics",
    "On learning gaussian multi-index models with gradient flow",
    "Learning gaussian multi-index models with gradient flow: Time complexity and directional convergence",
    "The benefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents",
    "Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions",
    "A mean field view of the landscape of two-layer neural networks",
    "On the global convergence of gradient descent for overparameterized models using optimal transport",
    "Mean field analysis of neural networks: A law of large numbers",
    "Trainability and accuracy of artificial neural networks: An interacting particle system approach",
    "On-line learning in soft committee machines",
    "Exact solution for on-line learning in multilayer neural networks",
    "Dynamics of on-line gradient descent learning for multilayer neural networks",
    "Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup",
    "Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks",
    "Optimization and generalization of shallow neural networks with quadratic activation functions",
    "Learning curves of generic features maps for realistic datasets with a teacher-student model",
    "From highdimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks",
    "Neural tangent kernel: Convergence and generalization in neural networks",
    "On lazy training in differentiable programming",
    "Feature learning in infinite-width neural networks",
    "Noisy heteroclinic networks",
    "Gradient descent quantizes relu network features",
    "Neural networks as kernel learners: The silent alignment effect",
    "Directional convergence near small initializations and saddles in two-homogeneous neural networks",
    "Early directional convergence in deep homogeneous neural networks for small initializations",
    "Learning time-scales in two-layers neural networks",
    "Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning",
    "The implicit bias of depth: How incremental learning drives generalization",
    "Kernel and rich regimes in overparametrized models",
    "Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity",
    "s) gd over diagonal linear networks: Implicit regularisation, large stepsizes and edge of stability",
    "Leveraging continuous time to understand momentum when training diagonal linear networks",
    "The lasso problem and uniqueness",
    "Neural networks and principal component analysis: Learning from examples without local minima",
    "Effect of batch learning in multilayer neural networks",
    "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "An analytic theory of generalization dynamics and transfer learning in deep linear networks",
    "From lazy to rich: Exact learning dynamics in deep linear networks",
    "Features are fate: a theory of transfer learning in high-dimensional regression",
    "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction",
    "Implicit regularization in deep matrix factorization",
    "Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced",
    "Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics",
    "Reduced-rank regression for the multivariate linear model",
    "Language models are few-shot learners",
    "What can transformers learn in-context? a case study of simple function classes",
    "Transformers as algorithms: Generalization and stability in in-context learning",
    "Pretraining task diversity and the emergence of non-bayesian in-context learning for regression",
    "What learning algorithm is in-context learning? investigations with linear models",
    "edge of stability",
    "Transformers learn in-context by gradient descent",
    "Transformers learn to implement preconditioned gradient descent for in-context learning",
    "Trained transformers learn linear models in-context",
    "Grokking: Generalization beyond overfitting on small algorithmic datasets",
    "Grokking modular arithmetic",
    "The clock and the pizza: Two stories in mechanistic explanation of neural networks",
    "Pre-trained large language models use fourier features to compute addition",
    "Language models use trigonometry to do addition",
    "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product",
    "Harmonics of learning: Universal fourier features emerge in invariant networks",
    "Feature emergence via margin maximization: case studies in algebraic tasks",
    "Composing global optimizers to reasoning tasks via algebraic objects in neural nets",
    "Data-dependence of plateau phenomenon in learning with neural network—statistical mechanical analysis",
    "The quantization model of neural scaling",
    "An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem",
    "Emergence and scaling laws in sgd learning of shallow neural networks",
    "Learning quadratic neural networks in high dimensions: Sgd dynamics and scaling laws",
    "The neural race reduction: Dynamics of abstraction in gated networks",
    "Make haste slowly: A theory of emergent structured mixed selectivity in feature learning relu networks",
    "Transformer Circuits Thread",
    "Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer",
    "Self-consistent dynamical field theory of kernel evolution in wide neural networks",
    "The committee machine: Computational to statistical gaps in learning a two-layers neural network",
    "Optimal errors and phase transitions in high-dimensional generalized linear models",
    "Neural networks can learn representations with gradient descent",
    "Fundamental computational limits of weak learnability in high-dimensional multi-index models",
    "Optimal spectral transitions in high-dimensional multi-index models",
    "The computational advantage of depth: Learning high-dimensional hierarchical functions with gradient descent",
    "The large learning rate phase of deep learning: the catapult mechanism",
    "Catapults in sgd: spikes in the training loss and their impact on generalization through feature learning",
    "High-dimensional asymptotics of feature learning: How one gradient step improves the representation",
    "Asymptotics of feature learning in two-layer networks after one gradient-step",
    "A random matrix theory perspective on the spectrum of learned features and asymptotic generalization capabilities",
    "How two-layer neural networks learn, one (giant) step at a time",
    "Learning a neuron by a shallow relu network: Dynamics and implicit bias for correlated inputs",
    "Saddle-to-saddle dynamics in deep relu networks: Low-rank bias in the first saddle escape",
    "Optimization and dynamical systems",
    "catapult",
    "* fi(x; θi), y(x) − X j∈A fj(x; θj) +# −∇θiEx",
    "To formally prove that AGF converges to gradient flow in the limit of vanishing initialization one might consider using tools like Hartman–Grobman theorem to make this step rigorous",
    "When σ(·) is a homogeneous function of degree k, then the utility is a homogeneous function of degree κ = k + 1",
    "B.2 Deriving the jump time To compute τi∗, we use Equation (2) to obtain the time evolution of the norms of the dormant neurons",
    "Lower bound on jump time",
    "Here, we motivate and discuss this phenomenon"
  ],
  "diagnostics": {
    "has_references_header": true,
    "parsed_items_count": 130,
    "is_numbered": true,
    "id_normalized": false
  }
}