=== Page 1 ===
Alternating Gradient Flows: A Theory of
Feature Learning in Two-layer Neural Networks
Daniel Kunin†
Stanford University
Giovanni Luca Marchetti†
KTH
Feng Chen
Stanford University
Dhruva Karkada
UC Berkeley
James B. Simon
UC Berkeley and Imbue
Michael R. DeWeese
UC Berkeley
Surya Ganguli
Stanford University
Nina Miolane
UC Santa Barbara
Abstract
What features neural networks learn, and how, remains an open question. In this
paper, we introduce Alternating Gradient Flows (AGF), an algorithmic framework
that describes the dynamics of feature learning in two-layer networks trained from
small initialization. Prior works have shown that gradient flow in this regime
exhibits a staircase-like loss curve, alternating between plateaus where neurons
slowly align to useful directions and sharp drops where neurons rapidly grow
in norm. AGF approximates this behavior as an alternating two-step process:
maximizing a utility function over dormant neurons and minimizing a cost function
over active ones. AGF begins with all neurons dormant. At each iteration, a
dormant neuron activates, triggering the acquisition of a feature and a drop in the
loss. AGF quantifies the order, timing, and magnitude of these drops, matching
experiments across several commonly studied architectures. We show that AGF
unifies and extends existing saddle-to-saddle analyses in fully connected linear
networks and attention-only linear transformers, where the learned features are
singular modes and principal components, respectively. In diagonal linear networks,
we prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that networks
learn Fourier features in decreasing order of coefficient magnitude. Altogether,
AGF offers a promising step towards understanding feature learning in neural
networks.
1
Introduction
The impressive performance of artificial neural networks is often attributed to their capacity to learn
features from data. Yet, a precise understanding of what features they learn and how remains unclear.
A large body of recent work has sought to understand what features neural networks learn by reverse
engineering the computational mechanisms implemented by trained neural networks [1–6]. Known
as mechanistic interpretability (MI), this approach involves decomposing trained networks into
interpretable components to uncover their internal representations and algorithmic strategies [7]. MI
has achieved notable successes in understanding the emergent abilities of large language models
[8], including identifying induction heads that enable in-context learning [3] and revealing that
small transformers trained on algebraic tasks use Fourier features [6, 9]. Despite these discoveries,
mechanistic interpretability remains limited by its empirical nature, lacking a theoretical framework
to formally define features and predict when and how they emerge [10].
† Correspondence to: kunin@berkeley.edu and glma@kth.se.
39th Conference on Neural Information Processing Systems (NeurIPS 2025).


=== Page 2 ===
Diagonal 
Linear Networks
Fully-Connected 
Linear Networks
Attention-Only 
Linear Transformers
  Utility
Maximization
  Cost 
Minimization
...
+
...
...
...
...
+
...
...
+
...
Loss
Steps
...
+
...
Two-layer
Nonlinear Networks
Quadratic Networks
for Modular Addition
+
Learns 
singular 
vectors
Learns 
regression
coefficient
Learns 
Fourier 
frequency
Learns 
principal 
component
...
...
AGF
GD
Figure 1: A unified theory of feature learning in two-layer networks. Left: Alternating Gradient
Flows (AGF) models feature learning as a two-step process alternating between utility maximization
(blue plateaus) and cost minimization (red drops), where each drop reflects learning a new feature
(see Section 2). Middle: AGF unifies prior analyses of saddle-to-saddle dynamics (see Sections 3
and 4). Right: AGF enables new analysis of empirical phenomena (see Section 5).
A different line of work, rooted in deep learning theory, has sought to understand how features are
learned by directly studying the dynamics of the network during training. Empirical studies suggest
that neural networks learn simple functions first, progressively capturing more complex features
during training [11–14]. Termed incremental, stepwise, or saddle-to-saddle, this learning process is
marked by long plateaus of minimal loss change followed by rapid drops. It is conjectured to arise
from networks, initialized at small-scale, jumping between saddle points of the loss landscape [15],
with each drop corresponding to the acquisition of a new feature [16]. This saddle-to-saddle process
has been explored across a range of simple settings, including diagonal linear networks [17, 18],
fully connected linear networks [19, 15], tensor decomposition [20, 21], self-supervised learning
[22, 23], shallow ReLU networks [24–30], attention-only transformers [31, 32], and multi-index
models [33–39]. However, these analyses often rely on different simplifying assumptions about the
architecture (e.g., linear activations), data (e.g., orthogonal inputs), or optimizer (e.g., layer-wise
learning), making it difficult to establish a unified, scalable understanding of feature learning.
In this work, we introduce a theoretical framework that unifies existing analyses of saddle-to-saddle
dynamics in two-layer networks under the vanishing initialization limit, precisely predicting the order,
timing, and magnitude of loss drops, while extending beyond classical settings to explain empirically
observed patterns of feature emergence. See Figure 1 for a visual overview of the paper. Our main
contributions are:
1. We introduce Alternating Gradient Flows (AGF), a two-step algorithm that approximates gra-
dient flow in two-layer networks with small initialization by alternating between maximizing
a utility over dormant neurons and minimizing a cost over active ones (Section 2).
2. We prove that AGF converges to the dynamics of gradient flow for diagonal linear networks
in the vanishing initialization limit (Section 3).
3. We show how AGF unifies and generalizes existing theories of saddle-to-saddle dynamics
across fully connected linear networks and attention-only linear transformers (Section 4).
4. We use AGF to predict the emergence of Fourier features in modular arithmetic tasks,
providing the first theoretical account of both the order in which frequencies appear and the
dynamics that drive it (Section 5).
Our work takes a step toward unifying deep learning theory and mechanistic interpretability, suggest-
ing that what features networks learn, and how, can be understood through optimization dynamics.
See Appendix A for further discussion of related theoretical approaches to studying feature learning
and saddle-to-saddle dynamics in two-layer networks, including mean-field analysis [40–43] and
teacher–student frameworks [44–51].
2
Deriving a Two-step Algorithm that Approximates Gradient Flow
Setup and notations.
We consider a two-layer neural network with H hidden neurons of the form
f(x; Θ) = PH
i=1 aiσ(w⊺
i gi(x)), where each hidden neuron i has learnable input weights wi ∈Rd
and output weights ai ∈Rc, and processes a potentially neuron-specific input representation gi(x).
2


=== Page 3 ===
The activation σ: R →R is origin-passing, i.e., σ(0) = 0, as satisfied by common functions (e.g.,
linear, ReLU, square, tanh). Let fi(x; θi) = aiσ(w⊺
i gi(x)) denote the output of the ith neuron, where
θi = (wi, ai). The parameters Θ = (θ1, . . . , θH) ∈RH×(d+c) for the entire network evolve under
gradient flow (GF), ˙Θ = −η∇ΘL(Θ), to minimize the MSE loss L(Θ) = Ex
 1
2∥y(x) −f(x; Θ)∥2
,
where y: Rd →Rc is the ground truth function and the expectation is taken over the input distribution.
The learning rate η > 0 rescales time without affecting the trajectory. The parameter vectors for each
neuron are initialized ai ∼N(0, α2
2c Ic), wi ∼N(0, α2
2d Id). In the limit α →∞, the gradient flow
dynamics enter the kernel regime [52, 53]. When α = 1, the dynamics correspond to a mean-field or
maximal update parameterization [40, 54]. We study the small-scale initialization regime α ≪1,
where the dynamics are conjectured to follow a saddle-to-saddle trajectory as α →0 [15, 14].
2.1
The behavior of gradient flow at small-scale initialization
Bakhtin [55] showed that, under a suitable time rescaling, the trajectory of a smooth dynamical
system initialized at a critical point and perturbed by vanishing noise converges to a piecewise
constant process that jumps instantaneously between critical points. These results suggest that, in the
vanishing initialization limit α →0, gradient flow converges to a similar piecewise constant process,
jumping between saddle points of the loss before reaching a minimum. To approximate this behavior,
we first examine the structure of critical points in the loss landscape of the two-layer networks we
study, and then analyze the dynamics of gradient flow near them. These dynamics reveals two distinct
phases, separated by timescales, which in turn motivates the construction of AGF.
Critical points are structured by partitions of neurons into dormant and active sets.
As α →0,
the initialization converges to the origin Θ = 0, which is a critical point of the loss. The origin is
one of potentially an exponential number of distinct (up to symmetry) critical points, that can be
structured according to partitions of the H hidden neurons. Formally, split neurons into two disjoint
sets: dormant neurons D ⊆[H] and active neurons A = [H]\D, with parameters ΘD = (θi)i∈D
and ΘA = (θi)i∈A, respectively. Due to the two-layer structure of our model and the origin-passing
activation function, setting the parameters of the dormant neurons to zero ΘD = 0 restricts the loss
to a function L(ΘA) of only the active neurons. Now any critical point Θ∗
A of the restricted loss
yields a critical point Θ = (0, Θ∗
A) of the original loss. Thus the 2H partitions of dormant and active
neurons structure critical ponts of the loss landscape, with the origin corresponding to the special
critical point with all neurons dormant.
Dynamics near a saddle point.
Consider the dynamics near a saddle point defined by a dormant
and active set, where we assume the active neurons are at a local minimum of their restricted loss
L(ΘA). By construction, neurons in our two-layer network only interact during training through the
residual r(x; Θ) = y(x) −f(x; Θ) ∈Rc, which quantifies the difference between the predicted and
target values at each data point x. Near a critical point, the residual primarily depends on the active
neurons, as the contribution from the dormant neurons is effectively zero. Consequently, dormant
neurons evolve independently of each other, while active neurons remain collectively equilibrated.
To characterize the dynamics of the dormant neurons, which will determine how the dynamics escape
the saddle point, we define the utility function, Ui : Rd+c →R for each i ∈D as:
Ui(θ; r) = Ex [⟨fi(x; θ), r(x)⟩] ,
where r(x) = y(x) −f(x; ΘA).
(1)
Intuitively, the utility Ui quantifies how correlated the ith dormant neuron is with the residual r,
which itself is solely determined by the active neurons. Using this function we can approximate the
directional and radial dynamics for the parameters of each dormant neuron as
d
dt ¯θi = η∥θi∥κ−2P⊥
θi∇θUi(¯θi; r),
d
dt∥θi∥= ηκ∥θi∥κ−1Ui(¯θi; r),
(2)
where ¯θi = θi/∥θi∥are the normalized parameters, P⊥
θi =
 Id −¯θi¯θ⊺
i

is an orthogonal projection
matrix, and κ ≥2 is the leading order of the Taylor expansion of the utility from the origin. Note
that the directional dynamics scale as ∥θi∥κ−2, while the radial dynamics scale as ∥θi∥κ−1. Because
∥θi∥≪1 for dormant neurons, the directional dynamics evolves significantly faster than the radial
dynamics. Consequently, the contribution of a dormant neuron to the residual is suppressed, keeping
the residual effectively constant, and this approximation to the dynamics stays valid. This separation
of timescales between directional and radial components has been utilized in several previous studies
of gradient descent dynamics near the origin [56–61].
3


=== Page 4 ===
Algorithm 1: AGF
Initialize: D ←[H], A ←∅, S ←0 ∈RH
while ∇L(ΘD) ̸= 0 do
while ∀i ∈D
Si ≤ci do
for i ∈D do
d¯
θi
dt = ηα∥θi∥κ−2P⊥
θi ∇U(¯θi, r)
dSi
dt
= ηακU(¯θi, r)
Activate neuron: D, A ←D \ {i∗}, A ∪{i∗}
while ∇L(ΘA) ̸= 0 do
for j ∈A do
dθj
dt
= −∇θj L(ΘA)
Remove collapsed neurons: D, A ←D ∪C, A \ C
Conceptual Illustration: AGF
Neuron 1
Neuron 2
Neuron 3
Activates
Dormant neurons 
work individually at small scales 
to maximize their utility
Steps
Loss
Active neurons 
work collectively at large scales
to minimize the cost
Figure 2: Alternating Gradient Flows (AGF). AGF alternates between utility maximization (blue)
over dormant neurons and cost minimization (red) over active neurons, predicting saddle-to-saddle
dynamics in training. Utility maximization: dormant neurons evolve independently driven by
projected gradient flow to maximize their utility. We keep track of the accumulated utility for each
dormant neuron to determine the first neuron to transition to active. Cost minimization: Active
neurons work collectively driven by gradient flow to minimize the loss until convergence. After
convergence, active neurons compute a new residual that determines the utility at the next iteration. It
is possible in this step for active neurons to become dormant again (see Appendix B).
Estimating the jump time from dormant to active.
The dynamical approximation in Equation (2)
breaks down if a dormant neuron crosses the threshold ∥θi∥= O(1), becoming active by influencing
the residual. We denote by i∗∈D the first dormant neuron to reach this threshold and by τi∗the
jump time at which this dormant to active transition occurs. To determine τi∗, we compute for each
dormant neuron the earliest time τi when ∥θi(τi)∥> 1, such that i∗= arg mini∈D τi. This time τi is
defined implicitly in terms of the accumulated utility, the path integral Si(t) =
R t
0 κUi(¯θi(s); r) ds:
τi = inf
n
t > 0
 Si(t) > ci
η
o
,
where ci =
−log (∥θi(0)∥)
if κ = 2,
−
1
2−κ
 ∥θi(0)∥2−κ −1

if κ > 2.
(3)
In the vanishing initialization limit as α →0, the jump time defined in Equation (3) diverges
as gradient flow exhibits extreme slowing near the critical point at the origin. Thus, to capture
meaningful dynamics in the limit, we accelerate time by setting the learning rate η = EΘ0 [ci] where
ci is the threshold defined in Equation (3) and the expectation is taken over the initialization.
2.2
Alternating Gradient Flow (AGF) approximates GF at small-scale initialization
Based on the behaviors of gradient flow (GF) at small-scale initialization, we introduce Alternating
Gradient Flows (AGF) (Algorithm 1) as an approximation of the dynamics. AGF characterizes phases
of constant loss as periods of utility maximization and sudden loss drops as cost minimization steps,
accurately predicting both the loss levels and the timing of loss drops during training. At initialization,
all neurons are dormant. At each iteration, a neuron leaves the dormant set and enters the active set.
The dormant neurons work individually to maximize their utility, and the active neurons collectively
work to minimize a cost (Figure 2). Specifically, each iteration in AGF is divided into two steps.
Step 1: Utility maximization over dormant neurons.
In this step, only dormant neurons update
their parameters. They independently maximize (locally) their utility function over the unit sphere by
following the projected gradient flow (left of Equation (2)). We keep track of the accumulated utility
for each neuron to determine the first neuron to transition and the jump time using Equation (3). At
the jump time, we record the norms and directions of the dormant neurons that have not activated, as
they will serve as the initialization for the utility maximization phase of the next iteration.
Step 2: Cost minimization over active neurons.
In this step, all active neurons interact to minimize
(locally) the loss, by following the negative gradient flow of L(ΘA). For previously active neurons,
the initialization is determined by the previous cost minimization step. For the newly activated neuron,
the initialization comes from the utility maximization step. It is possible in this step for active neurons
to become dormant again if the optimization trajectory brings an active neuron near the origin (see
Appendix B). After convergence, we compute the new residual r(x) = y(x) −f(x; ΘA), which
defines a new utility for the remaining dormant neurons at the next iteration.
4


=== Page 5 ===
Termination.
We repeat both steps, recording the corresponding sequence of jump times and loss
levels, until there are either no remaining dormant neurons or we are at a local minimum of the
loss. Through this process, we have generated a precise sequence of saddle points and jump times to
describe how gradient flow leaves the origin through a saddle-to-saddle process.
While AGF and GF exhibit similar behaviors at small-scale initialization (α ≪1), a natural question
is whether their trajectories converge to each other as α →0. While a general proof remains open (see
Section 6), in the next section we present an illustrative setting where convergence can be established.
3
A Setting Where AGF and GF Provably Converge to Each Other
Steps
Coefficient
Steps
Loss
Figure 3: AGF = GF as α →0 in diagonal linear networks.
Training loss curves for a diagonal linear network under the
setup described in Section 3 for various initialization values
α. As α →0, the trajectory predicted by AGF and the
empirics of gradient flow converge. To ensure a meaningful
comparison between experiments we set η = −log(α).
Diagonal linear networks are simple
yet insightful models for analyzing
learning [62–65, 17, 66]. Central to
their analysis is an interpretation of
gradient flow in function space as
mirror descent with an initialization-
dependent potential that promotes
sparsity when α is small. Using this
perspective, Pesme and Flammarion
[18] characterized the sequence of
saddles and jump times for gradient
flow in the limit α →0. We show that,
in this limit, AGF converges to the ex-
act same sequence, establishing a set-
ting where AGF provably converges
to gradient flow (see Figure 3).
We consider a two-layer diagonal linear network trained via gradient flow to minimize the MSE
loss L(β) =
1
2n∥y −β⊺X∥2, where the regression coefficients β ∈Rd are parameterized as the
element-wise product β = u ⊙v with u, v ∈Rd, and (X, y) ∈Rd×N × RN are the input, output
data. This setup fits naturally within AGF, where the ith neuron corresponds to βi with parameters
θi = (ui, vi), data representation gi(x) = xi, and κ = 2. Following Pesme and Flammarion [18],
we initialize θi(0) = (
√
2α, 0) such that βi(0) = 0, and assume the inputs are in general position, a
standard technical condition in Lasso literature [67, 18] to rule out unexpected linear dependencies.
Utility maximization.
The utility function for the ith neuron is Ui = −uivi∇βiL(βA), which is
maximized on the unit sphere u2
i + v2
i = 1 when sgn(uivi) = −sgn(∇βiL(βA)) and |ui| = |vi|,
yielding a maximal utility of ¯U∗
i =
1
2|∇βiL(βA)|. What makes this setting special is that not
only can the maximal utility be computed in closed form, but every quantity involved in utility
maximization—namely the accumulated utility, jump times, and directional dynamics—admits an
exact analytical expression. The key insight is that the normalized utility ¯Ui(t) for each dormant
neuron i evolves according to a separable Riccati ODE, interpolating from its initial value ¯Ui(0) to its
maximum ¯U∗
i . As a result, we can derive an explicit formula for the normalized utility ¯Ui(t), whose
integral yields the accumulated utility Si(t), evolving as:
Si(τ (k) + t) =
1
2ηα log cosh

2ηα

2U∗
i t ±
1
2ηα cosh−1 exp
 2ηαSi
 τ (k)
,
(4)
where ηα = −log(
√
2α) is the learning rate and the unspecified sign is chosen based on sgn(uivi)
and sgn(∇βiL(βA)), as explained in Appendix C. This expression allows us to determine the next
neuron to activate, as the first i ∈D for which Si = 1, from which the jump time can be computed.
Cost minimization.
Active neurons represent non-zero regression coefficients of β (see Figure 3
right). During the cost minimization step, the active neurons work to collectively minimize the loss.
When a neuron activates, it does so with a certain sign sgn(uivi) = −sgn(∇βiL(βA)). If, during
the cost minimization phase, a neuron changes sign, then it can do so only by returning to dormancy
first. This is due to the fact that throughout the gradient flow dynamics, the quantity u2
i −v2
i = 2α2
is conserved and thus, in order to flip the sign of the product uivi, the parameters must pass through
their initialization. As a result, the critical point reached during the cost minimization step is the
5


=== Page 6 ===
unique solution to the constrained optimization problem,
βA = arg min
β∈Rd
L(β)
subject to
βi = 0
if i /∈A,
βi · sgn(uivi) ≥0
if i ∈A.
(5)
All coordinates where (βA)i = 0 are dormant at the next step of AGF.
AGF in action: sparse regression.
We now connect AGF to the algorithm proposed by Pesme
and Flammarion [18], which captures the limiting behavior of gradient flow under vanishing
initialization.
Their algorithm tracks, for each coefficient of β, an integral of the gradient,
Si(t) = −
R t
0 ∇βiL(β(˜tα(s))) ds, with a time rescaling ˜tα(s) = −log(α)s. They show that in
the limit α →0, this quantity is piecewise linear and remains bounded in [−1, 1]. Using these
properties, each step of their algorithm determines a new coordinate to activate (the first for which
Si(t) = ±1), adds it to an active set, then solves a constrained optimization over this set, iterating
until convergence. Despite differing formulations, this process is identical to AGF in the limit α →0:
Theorem 3.1. Let (βAGF, tAGF) and (βPF, tPF) be the sequences produced by AGF and Algorithm 1
of Pesme and Flammarion [18], respectively. Then, (βAGF, tAGF) →(βPF, tPF) pointwise as α →0.
The key connection lies in the asymptotic identity log cosh(x) →|x| as |x| →∞, which implies
that the accumulated utility Si(t) in AGF converges to the absolute value of the integral Si(t). The
unspecified sign in AGF corresponds to the sign of the boundary conditions in their algorithm. Thus,
AGF converges to the same saddle-to-saddle trajectory as the algorithm of Pesme and Flammarion
[18], and therefore to gradient flow. See Appendix C for a full derivation.
4
AGF Unifies Existing Analysis of Saddle-to-Saddle Dynamics
(a) Commuting
(b) Non-commuting
Singular Value
GD
Steps
Conj. 4.1
Figure 4: Stepwise singular value decomposition. Training
a two-layer fully connected linear network on Gaussian in-
puts with a power-law covariance Σxx and labels y(x) = Bx
generated from a random B. We show the dynamics of the
singular values of the network’s map AW when Σxx com-
mutes with Σ⊺
yxΣyx (a) and when it does not (b). Conjec-
ture 4.1 (black dashed lines) predicts the dynamics well.
Fully connected linear network.
Linear networks have long served as
an analytically tractable setting for
studying neural network learning dy-
namics [68–73]. Such linear networks
exhibit highly nonlinear learning dy-
namics. Saxe et al. [70] demonstrated
that gradient flow from a task-aligned
initialization learns a sequential singu-
lar value decomposition of the input-
output cross-covariance. This behav-
ior persists in the vanishing initial-
ization limit without task alignment
[57, 74, 19, 20, 15], and is amplified
by depth [75, 62]. Here, we show how
AGF naturally recovers such greedy
low-rank learning.
We consider a fully connected two-layer linear network, f(x; θ) = AWx, with parameters W ∈
RH×d and A ∈Rc×H. The network is trained to minimize the MSE loss with data generated
from the linear map y(x) = Bx, where B ∈Rc×d is an unknown matrix. The inputs are drawn
independently from a Gaussian x ∼N(0, Σxx), where Σxx ∈Rd×d is the input covariance, and
Σyx = BΣxx ∈Rc×d is the input-output cross-covariance, which we assume are full rank with
distinct singular values. A subtlety in applying AGF to this setting is that the standard notion of
a neuron used in Section 2 is misaligned with the geometry of the loss landscape. Due to the
network’s invariance under (W, A) 7→(GW, AG−1) for any G ∈GLH(R), critical points form
manifolds entangling hidden neurons, and conserved quantities under gradient flow couple their
dynamics [76, 77]. To resolve this, we can reinterpret AGF in terms of evolving dormant and active
orthogonal basis vectors instead of neurons. Each basis vector ( ˜ai, ˜wi) ∈Rc+d forms a rank-1 map
˜ai ˜w⊺
i ∈Rc×d such that the function computed by the network is the sum over these rank-1 maps,
f(x; Θ) = P
i∈[ ˜
H] ˜ai ˜w⊺
i x, where ˜H = min(c, H, d). From this perspective, at each iteration of AGF
the dormant set loses one basis vector, while the active set gains one. See Appendix D for details.
Utility maximization.
Let βA = P
i∈A ˜ai ˜w⊺
i be the function computed by the active basis vectors
and m = |D|. The total utility over the dormant basis vectors is U = Pm
i=1 ˜a⊺
i ∇βL(βA) ˜wi.
6


=== Page 7 ===
Maximizing this sum while maintaining orthonormality between the basis vectors yields a Rayleigh
quotient problem, whose solution aligns the dormant basis (˜ai, ˜wi)m
i=1 with the top m singular modes
(ui, vi)m
i=1 of ∇βL(βA). Each aligned pair attains a maximum utility of U∗
i = σi/2, where σi is the
corresponding singular value. The basis vector aligned with the top singular mode activates first,
exiting the dormant set and joining the active one.
Cost minimization.
The cost minimization step of AGF can be recast as a reduced rank regression
problem, minimizing L(β) over β ∈Rc×d subject to rank(β) = k, where k = |A|. As first shown in
Izenman [78], the global minimum for this problem is an orthogonal projection of the OLS solution
β∗
A = PUkΣyxΣ−1
xx , where PUk = UkU ⊺
k is the projection onto Uk ∈Rc×k, the top k eigenvectors
of ΣyxΣ−1
xx Σ⊺
yx. Using this solution, we can show that the next step of utility maximization will be
computed with the matrix ∇βL(β∗
A) = P⊥
UkΣyx where P⊥
Uk = Ic −PUk.
AGF in action: greedy low-rank learning.
In the vanishing initialization limit, AGF reduces to
an iterative procedure that selects the top singular mode of ∇βL(β∗
A), transfers this vector from the
dormant to the active basis, then minimizes the loss with the active basis to update β∗
A. This procedure
is identical to the Greedy Low-Rank Learning (GLRL) algorithm by Li et al. [20] that characterizes
the gradient flow dynamics of two-layer matrix factorization problems with infinitesimal initialization.
Encouraged by this connection, we make the following conjecture:
Conjecture 4.1. In the initialization limit α →0, a two-layer fully connected linear network trained
by gradient flow, with η = −log(α), learns one rank at a time leading to the sequence
f (k)(x) = P
i≤k PuiΣyxΣ−1
xx x,
ℓ(k) = 1
2
P
i>k µi,
τ (k) = P
i≤k ∆τ (i),
(6)
where 0 ≤k ≤min(d, H, c), (ui, µi) are the eigenvectors and eigenvalues of ΣyxΣ−1
xx Σ⊺
yx, σ(k)
i
is
the ith singular value of P⊥
UkΣyx, and ∆τ (i) =

1 −Pi−2
j=0 σ(j)
i−j∆τ (j+1)
/σ(i−1)
1
with ∆τ (0) = 0.
When Σxx commutes with Σ⊺
yxΣyx, Conjecture 4.1 recovers the sequence originally proposed by
Gidel et al. [19]. Figure 4 empirically supports this conjecture. See Appendix D for details.
GD
AGF
Steps
Singular Value
Figure 5: Stepwise principal component regres-
sion. Training a linear transformer to learn linear
regression in context. We show the evolution of
singular values of PH
i=1 ViKiQ⊺
i . Horizontal lines
show theoretical Ak and vertical dashed lines show
lower bounds for the jump time from Equation (7)
with l = k −1. Dashed black lines are numerical
AGF predictions.
Attention-only
linear
transformer.
Pre-
trained large language models can learn new
tasks from only a few examples in context,
without explicit fine-tuning [79]. To understand
the emergence of this in-context learning ability,
previous empirical [80–82] and theoretical
works [83–88] have examined how transformers
learn to perform linear regression in context.
Notably, Zhang et al. [32] showed that an
attention-only linear transformer learns to
implement principal component regression
sequentially, with each learned component
corresponding to a drop in the loss. We show
that AGF recovers their analysis (see Figure 5).
We consider a attention-only linear transformer
f(X; Θ) = X+PH
i=1 WV,iXX⊺WK,iW ⊺
Q,iX,
where X is the input sequence, WV
∈
RH×(d+1)×(d+1), WK, WQ ∈RH×(d+1)×R
represent the value, key, and query matrices
respectively, and H denotes the number of at-
tention heads. While this network uses linear
activations, it is cubic in its input and parameters. Also, when the rank R = 1, each attention head
behaves like a homogeneous neuron with κ = 3, making it compatible with the AGF framework.
We consider the linear regression task with input Xi = ( xi
yi ) for i ≤N, where xi ∼N(0, Σxx),
and yi = β⊺xi with β ∼N(0, Id). The input covariance Σxx ∈Rd×d has eigenvalues λ1 ≥· · · ≥
λd > 0 with corresponding eigenvectors v1, . . . , vd. The final input token is XN+1 = ( xN+1
0
). The
network is trained by MSE loss to predict yN+1 given the entire sequence X, where the model
prediction is taken to be ˆyN+1 = f(X; Θ)d+1,N+1. Following Zhang et al. [32], we initialize all
7


=== Page 8 ===
parameters to zero except for some slices, which we denote by V ∈RH, Q, K ∈RH×d, which are
initialized from N(0, α). The parameters initialized at zero will remain at zero throughout training
[32], and the model prediction reduces to ˆyN+1 = PH
h=1 Vh
PN
n=1 ynx⊺
nKhQ⊺
hxN+1. We defer
derivations to Appendix E, and briefly outline how AGF predicts the saddle-to-saddle dynamics.
Utility maximization.
At the kth iteration of AGF, the utility of a dormant attention head is
Ui = NViQ⊺
i (Σ2
xx −Pk−1
h=1 λ2
hvhv⊺
h)Ki which is maximized over normalized parameters when
¯Vi = ±1/
√
3 and ¯Qi, ¯Ki = ±vk/
√
3, where the sign is chosen such that the maximum utility is
¯U∗= Nλ2
k/3
√
3. In other words, utility maximization encourages dormant attention heads to align
their key and query vectors with the dominant principal component of the input covariance not yet
captured by any active head. This creates a race condition among dormant heads, where the first to
reach the activation threshold, measured by their accumulated utility, becomes active and learns the
corresponding component. Assuming instantaneous alignment of the key and query vectors, we can
lower bound the jump time for the next head to activate, as shown in Equation (7).
Cost minimization.
During cost minimization, because v1, . . . , vd form an orthonormal basis, the
updates for each attention head are decoupled. Thus, we only need to focus on how the magnitude
of the newly active head changes. Specifically, we determine the magnitude Ak of the newly
learned function component, given by fk(X; θk)d+1,N+1 = Ak
PN
n=1 ynx⊺
nvkv⊺
kxN+1. Solving
∂L(Ak)
∂Ak
= 0, we find that the optimal magnitude is Ak =
1
trΣxx+(N+1)λk , from which we can derive
the expression for the prediction and loss level after the kth iteration of AGF, as shown in Equation (7).
AGF in action: principal component regression.
At each iteration, the network projects the input
onto a newly selected principal component of Σxx and fits a linear regressor along that direction. Let
µ(l) = maxi∈D ∥θi(τ (l))∥for l < k and ηα = α−1. This process yields the sequence,
ˆy(k)
N+1 = Pk,N
i,n=1
ynx⊺
nviv⊺
i xN+1
trΣxx+(N+1)λi , ℓ(k) = trΣxx
2
−Pk
i=1
Nλi/2
trΣxx
λi
+N+1, τ (k) ≳τ (l) + η−1
α
µ(l)
√
3
Nλ2
k ,
(7)
which recovers the results derived in Zhang et al. [32] and provides an excellent approximation to the
gradient flow dynamics (see Figure 5).
5
AGF Predicts the Emergence of Fourier Features in Modular Addition
In previous sections, we showed how AGF unifies prior analyses of feature learning in linear networks.
We now consider a novel nonlinear setting: a two-layer quadratic network trained on modular addition.
Originally proposed as a minimal setting to explore emergent behavior [89], modular addition has
since become a foundational setup for mechanistic interpretability. Prior work has shown that
networks trained on this task develop internal Fourier representations and use trigonometric identities
to implement addition as rotations on the circle [6, 90, 91]. Similar Fourier features have been
observed in networks trained on group composition tasks [9], and in large pre-trained language
models performing arithmetic [92, 93]. Despite extensive empirical evidence for the universality
of Fourier features in deep learning, a precise theoretical explanation of their emergence remains
open. Recent work has linked this phenomenon to the average gradient outer product framework
[94], the relationship between symmetry and irreducible representations [95], implicit maximum
margin biases of gradient descent [96], and the algebraic structure of the solution space coupled
with a simplicity bias [97]. Here, we leverage AGF to unveil saddle-to-saddle dynamics, where each
saddle corresponds to the emergence of a Fourier feature in the network (see Figure 6).
We consider a setting similar to [90, 96, 97], but with more general input encodings. Given p ∈N, the
ground-truth function is y: Z/p×Z/p →Z/p, (a, b) 7→a+b mod p, where Z/p = {0, . . . , p−1} is
the (additive) Abelian group of integers modulo p. Given a vector x ∈Rp, we consider the encoding
Z/p →Rp, a 7→a · x, where · denotes the action of the cyclic permutation group of order p on
x (which cyclically permutes the components of x). The modular addition task with this encoding
consists of mapping (a · x, b · x) to (a + b) · x. For x = e0, our encoding coincides with the one-hot
encoding a 7→ea studied in prior work. We consider a two-layer neural network with quadratic
activation function σ(z) = z2, where each neuron is parameterized by θi = (ui, vi, wi) ∈R3p and
computes the function fi(a · x, b · x; θi) = (⟨ui, a · x⟩+ ⟨vi, b · x⟩)2wi. The network is composed of
the sum of H neurons and is trained over the entire dataset of p2 pairs (a, b). We make some technical
assumptions in order to simplify the analysis. First, since the network contains no bias term, we
assume that the data is centered, i.e., we subtract (⟨x, 1⟩/p)1 from x. Second, in order to encourage
8


=== Page 9 ===
(b) Predictions
(a) Power
(c) Weights
(d) Frequency+Phases
Steps
AGF
Frequency
Figure 6: Stepwise Fourier decomposition. We train a two-layer quadratic network on a modular
addition task with p = 20, using a template vector x ∈Rp composed of three cosine waves:
ˆx[1] = 10, ˆx[3] = 5, and ˆx[5] = 2.5. (a) Output power spectrum over time. The network learns the
task by sequentially decomposing x into its Fourier components, acquiring dominant frequencies
first. Colored solid lines are gradient descent, black dashed line is AGF run numerically from the
same initialization. (b) Model outputs on selected inputs at four training steps, showing progressively
accurate reconstructions of the template. (c) Output weight vector wi for all H = 18 neurons and (d)
their frequency spectra and dominant phase. Neurons are color-coded by dominant frequency. As
predicted by the theory, the neurons group by frequency, while distributing their phase shifts.
sequential learning of frequencies, we assume that all the non-conjugate Fourier coefficients of x
have distinct magnitude: |ˆx[k]| ̸= |ˆx[k′]| for k ̸= ±k′ (mod p). Third, in order to avoid dealing
with the Nyquist frequency k = p/2, we assume that p is odd. We now describe how AGF applies to
this setting (see Appendix F for proofs). Here,ˆ· denotes the Discrete Fourier Transform (DFT).
Utility maximization.
First, we show how the initial utility can be expressed entirely in the
frequency domain of the parameters ˆΘ. For a dormant neuron parameterized by θ = (u, v, w), the
initial utility is U = (2/p3) P
k∈[p]\{0} |ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k]. We then argue that the unit vectors
maximizing U align with the dominant harmonic of ˆx:
Theorem 5.1. Let ξ be the frequency that maximizes |ˆx[k]|, k = 1, . . . , p −1, and denote by sx the
phase of ˆx[ξ]. Then the unit vectors θ∗= (u∗, v∗, w∗) maximizing the utility function U are:
u∗[a] = Ap cos (ωξa + su) ,
v∗[b] = Ap cos (ωξb + sv) ,
w∗[c] = Ap cos (ωξc + sw) ,
(8)
where a, b, c ∈[p] are indices, su, sv, sw ∈R are phase shifts satisfying su + sv ≡sw + sx
(mod 2π), Ap =
p
2/(3p) is the amplitude, and ωξ = 2πξ/p is the frequency. Moreover, U has no
other local maxima and achieves a maximal value of ¯U∗=
p
2/(27p3)|ˆx[ξ]|3.
Therefore, after utility minimization, neurons specialize to unique frequencies. Now that we know
the maximal utility, we can estimate the jump time by assuming instantaneous alignment as done in
Section 4, resulting in the lower bound shown in Equation (9).
Cost minimization.
To study cost minimization, we consider a regime in which a group A of
N ≤H neurons activates simultaneously, each aligned to the harmonic of frequency ξ. While this is
a technical simplification of AGF, which activates a single neuron per iteration, it allows us to analyze
the collective behavior more directly. We additionally assume that once aligned, the neurons in A
remain aligned under the gradient flow (see Appendix F.2.1 for a discussion of possible “resonant”
escape directions). We then analyze cost minimization for a configuration ΘA = (ui, vi, wi)N
i=1 of
aligned neurons with arbitrary amplitudes and phase shifts, and prove the following result.
Theorem 5.2. The loss function satisfies the lower bound L(ΘA) ≥∥x∥2/2 −⟨x, 1⟩2/(2p) −
|ˆx[ξ]|2/p, which is tight for N ≥6. When the bound is achieved, the network learns the function f(a·
x, b·x; ΘA) = (2|ˆx[ξ]|/p) (a+b)·χξ, where χξ[c] = cos (2πξc/p + sx), which leads to the new util-
ity function for the remaining dormant neurons: U = (2/p3) P
k∈[p]\{0,±ξ} |ˆx[k]|2ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ].
Put simply, the updated utility function after the first iteration of AGF has the same form as the old
one, but with the dominant frequency ξ of x removed (together with its conjugate frequency −ξ).
Therefore, at the second iteration of AGF, another group of neurons aligns with the harmonic of
the second dominant frequency of x. Lastly, we argue via orthogonality that the groups of neurons
aligned with different harmonics optimize their loss functions independently, implying that at each
iteration of AGF, another group of neurons learns a new Fourier feature of x.
9


=== Page 10 ===
AGF in action: greedy Fourier decomposition.
Taken together, we have shown that a two-layer
quadratic network with hidden dimension H ≥3p trained to perform modular addition with a
centered encoding vector x ∈Rp sequentially learns frequencies ξ1, ξ2, . . . , ordered by decreasing
|ˆx[ξ]|. After learning k ≥0 frequencies, the function, level, and next jump time are:
f (k)(a·x, b·x) = Pk
i=1
2|ˆx[ξi]|
p
(a+b)·χξi, ℓ(k) = P
i>k
|ˆx[ξi]|2
p
, τ (k) ≳τ (l)+ η−1
α
µ(l)
√
3p
3
2
√
2|ˆx[ξk+1]|3 , (9)
where µ(l) = maxi∈D ∥θi(τ (l))∥and learning rate ηα = α−1. Simulating AGF numerically, we find
this sequence closely approximates the gradient flow dynamics (see Figure 6).
Extensions to other algebraic tasks.
Our analysis of modular addition can naturally extend to
a broader class of algebraic problems. First, one can consider modular addition over multiple
summands, defined by the map y: (Z/p)k →Z/p, (ai)k
i=1 7→a1 + · · · + ak (mod p). Using a
higher-degree activation function σ(x) = xk, the arguments for utility maximization should carry
over, while the cost minimization step might be more subtle. Second, one can replace modular
integers with an arbitrary finite group and study a network trained to learn the group multiplication
map. Non-commutative groups introduce technical challenges due to the involvement of higher-
dimensional unitary representations in their Fourier analysis. We leave a detailed analysis of these
extensions to future work.
6
Discussion, Limitations, and Future Work
Steps
Optimal Linear Predictor
Loss
Width = 1
Width = 2
Width = 4
Width = 8
Width = 16
Width = 32
Figure 7: From a staircase to a slide in a two-
layer ReLU network. Training loss for a two-
layer ReLU network on a subset of CIFAR-10
under varying hidden widths from an extremely
small initialization. Solid lines show gradient de-
scent dynamics and dotted lines show the sequence
produced by a numerical implementation of AGF
from the same initialization. At small widths, gra-
dient descent loss curves are stepwise and AGF
tracks the jumps closely. As width increases, cost-
minimization phases overlap and multiple dormant
neurons activate in close succession, producing a
smoother slide-like loss curve; accordingly, the
correspondence with AGF weakens. All experi-
mental details are available at a Github code base.
In this work we introduced Alternating Gradi-
ent Flows (AGF), a framework modeling feature
learning in two-layer neural networks as an al-
ternating two-step process: maximizing a utility
function over dormant neurons and minimizing
a cost function over active ones. We showed
how AGF converges to gradient flow in diag-
onal linear networks, recovers prior saddle-to-
saddle analyses in linear networks, and extends
to quadratic networks trained on modular ad-
dition. While these findings highlight AGF’s
utility as an ansatz for feature learning, it re-
mains open whether its correspondence to gra-
dient flow always holds in the vanishing ini-
tialization limit. Proving such a conjecture is
theoretically challenging. Empirical validation
is also difficult because it requires taking both
the initialization scale and learning rate to zero.
Moreover, this conjecture may simply fail to
hold in more general settings. On natural data
tasks, loss curves are often not visibly stepwise
even at very small initialization scales (see Fig-
ure 7), suggesting there may be limitations to
AGF. That said, if many dormant neurons reach
their activation thresholds in close succession,
their cost-minimization phases could interleave,
causing the aggregate loss to appear smooth. Re-
cent works reconciling the emergent capabilities
of large language models with their neural scal-
ing laws have made similar suggestions [98–102]. However, there are feature learning regimes in
two-layer networks—such as those studied in Saad and Solla [46], Goldt et al. [47], Arnaboldi et al.
[51]—that display saddle-to-saddle behavior due to population-level transitions not captured by AGF.
Connecting AGF to these multi-index models and teacher–student settings (see Appendix A for a
review) remains a key direction for future work. Lastly, the central limitation of the framework is
its focus on two-layer networks, leaving open how it might generalize to deeper and more realistic
architectures. Possible extensions include leveraging recent analyses of modularity in deep networks
[103–105] and adapting insights from the early alignment dynamics of deep networks near the
origin [59, 106]. All together, our results suggest that AGF offers a promising step towards a deeper
understanding of what features neural networks learn and how.
10


=== Page 11 ===
Acknowledgments and Disclosure of Funding
We thank Clémentine Dominé, Jim Halverson, Boris Hanin, Christopher Hillar, Alex Infanger, Arthur
Jacot, Mason Kamb, David Klindt, Florent Krzakala, Zhiyuan Li, Sophia Sanborn, Nati Srebro,
and Yedi Zhang for helpful discussions. Daniel thanks the Open Philanthropy AI Fellowship for
support. Giovanni is partially supported by the Wallenberg AI, Autonomous Systems and Software
Program (WASP) funded by the Knut and Alice Wallenberg Foundation. Surya and Feng are partially
supported by NSF grant 1845166. Surya thanks the Simons Foundation, NTT Research, an NSF
CAREER Award, and a Schmidt Science Polymath award for support. Nina is partially supported by
NSF grant 2313150 and the NSF grant 240158. This work was supported in part by the U.S. Army
Research Laboratory and the U.S. Army Research Office under Contract No. W911NF-20-1-0151.
Author Contributions
Daniel, Nina, Giovanni, and James are primarily responsible for developing the AGF framework in
Section 2. Daniel is primarily responsible for the analysis of diagonal and fully-connected linear
networks in Sections 3 and 4. Feng is primarily responsible for the analysis of the attention-only
linear transformer in Section 4. Daniel and Giovanni are primarily responsible for the analysis of the
modular addition task in Section 5. Dhruva is primarily responsible for an implementation of AGF
used in the empirics. All authors contributed to the writing of the manuscript.
References
[1] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.
Zoom in: An introduction to circuits. Distill, 5(3):e00024–001, 2020.
[2] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for
transformer circuits. Transformer Circuits Thread, 1(1):12, 2021.
[3] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and
induction heads. arXiv preprint arXiv:2209.11895, 2022.
[4] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse
autoencoders find highly interpretable features in language models.
arXiv preprint
arXiv:2309.08600, 2023.
[5] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-
erly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,
Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex
Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter,
Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language
models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-
circuits.pub/2023/monosemantic-features/index.html.
[6] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress
measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217,
2023.
[7] Leonard Bereska and Efstratios Gavves. Mechanistic interpretability for ai safety–a review.
arXiv preprint arXiv:2404.14082, 2024.
[8] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
language models. arXiv preprint arXiv:2206.07682, 2022.
[9] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse
engineering how networks learn group operations. In International Conference on Machine
Learning, pages 6243–6267. PMLR, 2023.
[10] Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas
Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, et al. Open problems
in mechanistic interpretability. arXiv preprint arXiv:2501.16496, 2025.
11


=== Page 12 ===
[11] Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,
Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.
A closer look at memorization in deep networks. In International conference on machine
learning, pages 233–242. PMLR, 2017.
[12] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz
Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity.
Advances in neural information processing systems, 32, 2019.
[13] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances
in Neural Information Processing Systems, 35:21750–21764, 2022.
[14] Alexander Atanasov, Alexandru Meterez, James B Simon, and Cengiz Pehlevan. The optimiza-
tion landscape of sgd across the feature learning strength. arXiv preprint arXiv:2410.04642,
2024.
[15] Arthur Jacot, François Ged, Berfin ¸Sim¸sek, Clément Hongler, and Franck Gabriel. Saddle-to-
saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity.
arXiv preprint arXiv:2106.15933, 2021.
[16] Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proceedings of the National Academy of Sciences, 116
(23):11537–11546, 2019.
[17] Raphaël Berthier. Incremental learning in diagonal linear networks. Journal of Machine
Learning Research, 24(171):1–26, 2023.
[18] Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks.
Advances in Neural Information Processing Systems, 36:7475–7505, 2023.
[19] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete
gradient dynamics in linear neural networks. Advances in Neural Information Processing
Systems, 32, 2019.
[20] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient
descent for matrix factorization: Greedy low-rank learning. arXiv preprint arXiv:2012.09839,
2020.
[21] Rong Ge, Yunwei Ren, Xiang Wang, and Mo Zhou. Understanding deflation process in
over-parametrized tensor decomposition. Advances in Neural Information Processing Systems,
34:1299–1311, 2021.
[22] James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and Joshua
Albrecht. On the stepwise nature of self-supervised learning. In International Conference on
Machine Learning, pages 31852–31876. PMLR, 2023.
[23] Dhruva Karkada, James B Simon, Yasaman Bahri, and Michael R DeWeese. Solvable dynamics
of self-supervised word embeddings and the emergence of analogical reasoning. arXiv preprint
arXiv:2502.09863, 2025.
[24] Mary Phuong and Christoph H Lampert. The inductive bias of relu networks on orthogonally
separable data. In International Conference on Learning Representations, 2020.
[25] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer
nets: Margin maximization and simplicity bias. Advances in Neural Information Processing
Systems, 34, 2021.
[26] Mingze Wang and Chao Ma. Early stage convergence and global convergence of training
mildly parameterized neural networks. Advances in Neural Information Processing Systems,
35:743–756, 2022.
[27] Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of
shallow relu networks for square loss and orthogonal inputs. Advances in Neural Information
Processing Systems, 35:20105–20118, 2022.
12


=== Page 13 ===
[28] Hancheng Min, René Vidal, and Enrique Mallada. Early neuron alignment in two-layer relu
networks with small initialization. arXiv preprint arXiv:2307.12851, 2023.
[29] Margalit Glasgow. Sgd finds then tunes features in two-layer neural networks with near-optimal
sample complexity: A case study in the xor problem. arXiv preprint arXiv:2309.15111, 2023.
[30] Mingze Wang and Chao Ma. Understanding multi-phase optimization dynamics and rich
nonlinear behaviors of relu networks. Advances in Neural Information Processing Systems, 36,
2024.
[31] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind.
Transformers learn through gradual rank increase. Advances in Neural Information Processing
Systems, 36:24519–24551, 2023.
[32] Yedi Zhang, Aaditya K Singh, Peter E Latham, and Andrew Saxe. Training dynamics of
in-context learning in linear attention. arXiv preprint arXiv:2501.16265, 2025.
[33] Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj.
The staircase property: How hierarchical structure can guide deep learning. Advances in
Neural Information Processing Systems, 34:26989–27002, 2021.
[34] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase
property: a necessary and nearly sufficient condition for sgd learning of sparse functions on
two-layer neural networks. In Conference on Learning Theory, pages 4782–4887. PMLR,
2022.
[35] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural
networks: leap complexity and saddle-to-saddle dynamics.
In The Thirty Sixth Annual
Conference on Learning Theory, pages 2552–2623. PMLR, 2023.
[36] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index
models with gradient flow. arXiv preprint arXiv:2310.19793, 2023.
[37] Berfin Simsek, Amire Bendjeddou, and Daniel Hsu. Learning gaussian multi-index mod-
els with gradient flow: Time complexity and directional convergence.
arXiv preprint
arXiv:2411.08798, 2024.
[38] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborová, and Florent
Krzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking
the curse of information and leap exponents. arXiv preprint arXiv:2402.03220, 2024.
[39] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita
iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions. arXiv
preprint arXiv:2405.15459, 2024.
[40] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape
of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):
E7665–E7671, 2018.
[41] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. Advances in neural information processing
systems, 31, 2018.
[42] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A
law of large numbers. SIAM Journal on Applied Mathematics, 80(2):725–752, 2020.
[43] Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural net-
works: An interacting particle system approach. Communications on Pure and Applied
Mathematics, 75(9):1889–1935, 2022.
[44] David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review
E, 52(4):4225, 1995.
[45] David Saad and Sara A Solla. Exact solution for on-line learning in multilayer neural networks.
Physical Review Letters, 74(21):4337, 1995.
13


=== Page 14 ===
[46] David Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer
neural networks. Advances in neural information processing systems, 8, 1995.
[47] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborová.
Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student
setup. Advances in neural information processing systems, 32, 2019.
[48] Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová.
Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks.
Advances in Neural Information Processing Systems, 35:23244–23255, 2022.
[49] Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborová. Optimization and
generalization of shallow neural networks with quadratic activation functions. Advances in
Neural Information Processing Systems, 33:13445–13455, 2020.
[50] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,
and Lenka Zdeborová. Learning curves of generic features maps for realistic datasets with a
teacher-student model. Advances in Neural Information Processing Systems, 34:18137–18151,
2021.
[51] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro.
From high-
dimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd
in two-layers networks. In The Thirty Sixth Annual Conference on Learning Theory, pages
1199–1227. PMLR, 2023.
[52] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems, 31,
2018.
[53] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-
ming. Advances in neural information processing systems, 32, 2019.
[54] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
arXiv:2011.14522, 2020.
[55] Yuri Bakhtin. Noisy heteroclinic networks. Probability theory and related fields, 150:1–42,
2011.
[56] Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu
network features. arXiv preprint arXiv:1803.08367, 2018.
[57] Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners:
The silent alignment effect. In International Conference on Learning Representations, 2021.
[58] Akshay Kumar and Jarvis Haupt. Directional convergence near small initializations and
saddles in two-homogeneous neural networks. arXiv preprint arXiv:2402.09226, 2024.
[59] Akshay Kumar and Jarvis Haupt. Early directional convergence in deep homogeneous neural
networks for small initializations. arXiv preprint arXiv:2403.08121, 2024.
[60] Raphaël Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers
neural networks. Foundations of Computational Mathematics, pages 1–84, 2024.
[61] Daniel Kunin, Allan Raventós, Clémentine Dominé, Feng Chen, David Klindt, Andrew Saxe,
and Surya Ganguli. Get rich quick: exact solutions reveal how unbalanced initializations
promote rapid feature learning. arXiv preprint arXiv:2406.06158, 2024.
[62] Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How
incremental learning drives generalization. arXiv preprint arXiv:1909.12051, 2019.
[63] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized
models. In Conference on Learning Theory, pages 3635–3673. PMLR, 2020.
14


=== Page 15 ===
[64] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal
linear networks: a provable benefit of stochasticity. Advances in Neural Information Processing
Systems, 34:29218–29230, 2021.
[65] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (s) gd over diagonal
linear networks: Implicit regularisation, large stepsizes and edge of stability. arXiv preprint
arXiv:2302.08982, 2023.
[66] Hristo Papazov, Scott Pesme, and Nicolas Flammarion. Leveraging continuous time to
understand momentum when training diagonal linear networks. In International Conference
on Artificial Intelligence and Statistics, pages 3556–3564. PMLR, 2024.
[67] Ryan J Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 2013.
[68] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning
from examples without local minima. Neural networks, 2(1):53–58, 1989.
[69] Kenji Fukumizu. Effect of batch learning in multilayer neural networks. Gen, 1(04):1E–03,
1998.
[70] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
[71] Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and
transfer learning in deep linear networks. arXiv preprint arXiv:1809.10374, 2018.
[72] Clémentine CJ Dominé, Nicolas Anguita, Alexandra M Proca, Lukas Braun, Daniel Kunin,
Pedro AM Mediano, and Andrew M Saxe. From lazy to rich: Exact learning dynamics in deep
linear networks. arXiv preprint arXiv:2409.14623, 2024.
[73] Javan Tahir, Surya Ganguli, and Grant M Rotskoff. Features are fate: a theory of transfer
learning in high-dimensional regression. arXiv preprint arXiv:2410.08194, 2024.
[74] Dominik Stöger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral
learning: Optimization and generalization guarantees for overparameterized low-rank matrix
reconstruction. Advances in Neural Information Processing Systems, 34:23831–23843, 2021.
[75] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Advances in Neural Information Processing Systems, 32, 2019.
[76] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homoge-
neous models: Layers are automatically balanced. Advances in Neural Information Processing
Systems, 31, 2018.
[77] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.
Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv
preprint arXiv:2012.04728, 2020.
[78] Alan Julian Izenman. Reduced-rank regression for the multivariate linear model. Journal of
multivariate analysis, 5(2):248–264, 1975.
[79] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. Advances in neural information processing systems, 33:1877–
1901, 2020.
[80] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers
learn in-context? a case study of simple function classes. Advances in Neural Information
Processing Systems, 35:30583–30598, 2022.
[81] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Trans-
formers as algorithms: Generalization and stability in in-context learning. In International
conference on machine learning, pages 19565–19594. PMLR, 2023.
15


=== Page 16 ===
[82] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity
and the emergence of non-bayesian in-context learning for regression. Advances in neural
information processing systems, 36:14228–14246, 2023.
[83] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What
learning algorithm is in-context learning? investigations with linear models. arXiv preprint
arXiv:2211.15661, 2022.
[84] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L
Bartlett. How many pretraining tasks are needed for in-context learning of linear regression?
arXiv preprint arXiv:2310.08391, 2023.
[85] Kwangjun Ahn, Sébastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang.
Learning threshold neurons via the" edge of stability". arXiv preprint arXiv:2212.07469, 2022.
[86] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander
Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by
gradient descent. In International Conference on Machine Learning, pages 35151–35174.
PMLR, 2023.
[87] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to
implement preconditioned gradient descent for in-context learning. Advances in Neural
Information Processing Systems, 36:45614–45650, 2023.
[88] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models
in-context. Journal of Machine Learning Research, 25(49):1–55, 2024.
[89] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint
arXiv:2201.02177, 2022.
[90] Andrey Gromov. Grokking modular arithmetic. arXiv preprint arXiv:2301.02679, 2023.
[91] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza:
Two stories in mechanistic explanation of neural networks. Advances in Neural Information
Processing Systems, 36, 2024.
[92] Tianyi Zhou, Deqing Fu, Vatsal Sharan, and Robin Jia. Pre-trained large language models use
fourier features to compute addition. arXiv preprint arXiv:2406.03445, 2024.
[93] Subhash Kantamneni and Max Tegmark. Language models use trigonometry to do addition.
arXiv preprint arXiv:2502.00873, 2025.
[94] Neil Mallinar, Daniel Beaglehole, Libin Zhu, Adityanarayanan Radhakrishnan, Parthe Pandit,
and Mikhail Belkin. Emergence in non-neural models: grokking modular arithmetic via
average gradient outer product. arXiv preprint arXiv:2407.20199, 2024.
[95] Giovanni Luca Marchetti, Christopher J Hillar, Danica Kragic, and Sophia Sanborn. Harmonics
of learning: Universal fourier features emerge in invariant networks. In The Thirty Seventh
Annual Conference on Learning Theory, pages 3775–3797. PMLR, 2024.
[96] Depen Morwani, Benjamin L Edelman, Costin-Andrei Oncescu, Rosie Zhao, and Sham M
Kakade. Feature emergence via margin maximization: case studies in algebraic tasks. In The
Twelfth International Conference on Learning Representations, 2023.
[97] Yuandong Tian. Composing global optimizers to reasoning tasks via algebraic objects in
neural nets. arXiv preprint arXiv:2410.01779, 2024.
[98] Yuki Yoshida and Masato Okada. Data-dependence of plateau phenomenon in learning with
neural network—statistical mechanical analysis. Advances in Neural Information Processing
Systems, 32, 2019.
[99] Eric Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural
scaling. Advances in Neural Information Processing Systems, 36:28699–28722, 2023.
16


=== Page 17 ===
[100] Yoonsoo Nam, Nayara Fonseca, Seok Hyeong Lee, Chris Mingard, and Ard A Louis. An
exactly solvable model for emergence and scaling laws in the multitask sparse parity problem.
arXiv preprint arXiv:2404.17563, 2024.
[101] Yunwei Ren, Eshaan Nichani, Denny Wu, and Jason D Lee. Emergence and scaling laws in
sgd learning of shallow neural networks. arXiv preprint arXiv:2504.19983, 2025.
[102] Gérard Ben Arous, Murat A Erdogdu, N Mert Vural, and Denny Wu. Learning quadratic
neural networks in high dimensions: Sgd dynamics and scaling laws.
arXiv preprint
arXiv:2508.03688, 2025.
[103] Andrew Saxe, Shagun Sodhani, and Sam Jay Lewallen. The neural race reduction: Dynamics
of abstraction in gated networks. In International Conference on Machine Learning, pages
19287–19309. PMLR, 2022.
[104] Devon Jarvis, Richard Klein, Benjamin Rosman, and Andrew M Saxe. Make haste slowly:
A theory of emergent structured mixed selectivity in feature learning relu networks. arXiv
preprint arXiv:2503.06181, 2025.
[105] Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian
Chen, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael
Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas
Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam
Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. Circuit tracing:
Revealing computational graphs in language models. Transformer Circuits Thread, 2025. URL
https://transformer-circuits.pub/2025/attribution-graphs/methods.html.
[106] Akshay Kumar and Jarvis Haupt. Towards understanding gradient flow dynamics of homoge-
neous neural networks beyond the origin. arXiv preprint arXiv:2502.15952, 2025.
[107] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick
Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large
neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466,
2022.
[108] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evo-
lution in wide neural networks. Advances in Neural Information Processing Systems, 35:
32240–32256, 2022.
[109] Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, Lenka Zdeborová, et al.
The committee machine: Computational to statistical gaps in learning a two-layers neural
network. Advances in Neural Information Processing Systems, 31, 2018.
[110] Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal
errors and phase transitions in high-dimensional generalized linear models. Proceedings of the
National Academy of Sciences, 116(12):5451–5460, 2019.
[111] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn represen-
tations with gradient descent. In Conference on Learning Theory, pages 5413–5452. PMLR,
2022.
[112] Emanuele Troiani, Yatin Dandi, Leonardo Defilippis, Lenka Zdeborová, Bruno Loureiro, and
Florent Krzakala. Fundamental computational limits of weak learnability in high-dimensional
multi-index models. arXiv preprint arXiv:2405.15480, 2024.
[113] Leonardo Defilippis, Yatin Dandi, Pierre Mergny, Florent Krzakala, and Bruno Loureiro.
Optimal spectral transitions in high-dimensional multi-index models.
arXiv preprint
arXiv:2502.02545, 2025.
[114] Yatin Dandi, Luca Pesce, Lenka Zdeborová, and Florent Krzakala. The computational advan-
tage of depth: Learning high-dimensional hierarchical functions with gradient descent. arXiv
preprint arXiv:2502.13961, 2025.
17


=== Page 18 ===
[115] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari.
The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint
arXiv:2003.02218, 2020.
[116] Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin. Catapults in
sgd: spikes in the training loss and their impact on generalization through feature learning.
arXiv preprint arXiv:2306.04815, 2023.
[117] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.
High-dimensional asymptotics of feature learning: How one gradient step improves the
representation. Advances in Neural Information Processing Systems, 35:37932–37946, 2022.
[118] Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue M Lu, Lenka Zdeborová, and
Bruno Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step.
arXiv preprint arXiv:2402.04980, 2024.
[119] Yatin Dandi, Luca Pesce, Hugo Cui, Florent Krzakala, Yue M Lu, and Bruno Loureiro.
A random matrix theory perspective on the spectrum of learned features and asymptotic
generalization capabilities. arXiv preprint arXiv:2410.18938, 2024.
[120] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How
two-layer neural networks learn, one (giant) step at a time. Journal of Machine Learning
Research, 25(349):1–65, 2024.
[121] Dmitry Chistikov, Matthias Englert, and Ranko Lazic. Learning a neuron by a shallow relu
network: Dynamics and implicit bias for correlated inputs. Advances in Neural Information
Processing Systems, 36:23748–23760, 2023.
[122] Ioannis Bantzis, James B Simon, and Arthur Jacot. Saddle-to-saddle dynamics in deep relu
networks: Low-rank bias in the first saddle escape. arXiv preprint arXiv:2505.21722, 2025.
[123] Uwe Helmke and John B Moore. Optimization and dynamical systems. Springer Science &
Business Media, 2012.
[124] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii.
SIAM Journal on Numerical Analysis, 7(1):1–46, 1970.
18


=== Page 19 ===
A
Additional Related Work
A.1
What is a feature?
Here, we briefly review definitions of a feature used in the mechanistic interpretability and deep
learning theory literature:
• In mechanistic interpretability, terms such as feature, circuit, and motif are used to describe
network components—such as directions in activation space, subnetworks of weights, or
recurring activation motifs—that correspond to human-interpretable concepts or functions
within the model’s computation [10]. While this literature has led to many insights into
interpretability, these definitions generally lack precise mathematical formalization.
• In deep learning theory, features and feature learning are defined in terms of the neural
tangent kernel (NTK). For a network f(x; θ), the NTK is given by the kernel K(x, x′; θ) =
⟨∇θf(x; θ), ∇θf(x′; θ)⟩, formed by the Jacobian feature map ∇θf(x; θ). Feature learn-
ing—also referred to as rich learning—occurs when this kernel evolves during training.
While this definition is mathematically precise, it offers limited interpretability.
In this work, we implicitly adopt the notion of features from the deep learning theory perspective, but
consider learning settings often studied in mechanistic interpretability.
A.2
Analysis of feature learning in two-layer networks
Here, we discuss related theoretical approaches to studying feature learning in two-layer networks.
These approaches operate in distinct regimes—mean-field/infinite-width, teacher–student at high-
dimensional (thermodynamic) limits, and single-step analyses—whereas AGF focuses on the vanish-
ing initialization limit for two-layer networks. Our aim is to position AGF as complementary to, not
a replacement for, these approaches.
Mean-field analysis. An early line of work in the study of feature learning analyzes the population
dynamics of two-layer neural networks under the mean-field parameterization, yielding analytic
characterizations of training dynamics in the infinite-width limit [40–43]. These ideas have been
extended to deep networks via the tensor program framework, culminating in the maximal update
parametrization (µP) [54, 107]. Related analyses have also been obtained through self-consistent
dynamical mean-field theory [108]. Although feature learning occurs in this regime, these analyses
primarily address how to preserve feature learning when changing network depth and width, rather
than elucidating what specific features are learned or how they emerge.
Teacher-student setup. The two-layer teacher–student framework provides a precise setting for
studying feature learning by analyzing how a student network trained on supervised examples
generated from a fixed teacher recovers the structure of the teacher. Foundational work by Saad
and Solla [44, 45, 46] provided closed-form dynamics for online gradient descent in soft committee
machines trained on i.i.d. Gaussian inputs in the high-dimensional limit, deriving deterministic
differential equations governing a set of order parameters. Goldt et al. [47], Veiga et al. [48] extended
this dynamical theory to study the evolution of generalization error of one-pass SGD under an
arbitrary learning rate, and a range of hidden layer widths. Further developments have analyzed
conditions for the absence of spurious minima [49], extended the framework to capture more realistic
data settings [50], and provided a unified perspective connecting to the infinite hidden width limit of
the mean-field regime [51]. Much of this analysis use techniques from statistical physics—including
the replica method and approximate message passing—to characterize the generalization error and
uncover sharp phase transitions and statistical-to-computational gaps in teacher–student models,
discussed further in Aubin et al. [109], Barbier et al. [110].
Multi-index models. Similar to the teacher–student setting, multi-index models provide a structured
framework in which a neural network is trained on data whose labels depend on a nonlinear function of
low-dimensional projections of high-dimensional inputs. Feature learning in this setting is governed
by the network’s ability to align with the relevant low-dimensional subspace. This setup often
gives rise to characteristic staircase-like training dynamics [111, 36, 37, 60, 33–35]. Prior analyses
have primarily focused on characterizing generalization and sample complexity, and recent work has
established precise theoretical and computational thresholds for weak learnability in high-dimensional
multi-index problems [112, 113]. Other studies have demonstrated that reusing batches in gradient
descent can help networks overcome information bottlenecks, accelerating the learning of relevant
19


=== Page 20 ===
projections [38, 39]. Additionally, incorporating depth into these models has been shown to provide
computational advantages for learning hierarchical multi-index functions [114].
Single gradient step. A recent line of work has focused on understanding feature learning in neural
networks after just a single gradient step. Empirically, it has been observed that with large initial
learning rates, models can undergo “catapult” dynamics, where sudden spikes in the training loss
accelerate feature learning and improve generalization [115, 116]. Ba et al. [117] initiated the precise
analysis of feature learning after a single gradient step by showing, in a high-dimensional single-
index setting, how the first update to the first-layer weights can dramatically improve prediction
performance beyond random features, depending on the learning rate scaling. This single-step
analysis has been extended to characterize the asymptotic behavior of the generalization error [118],
establish an equivalence between the updated features and an isotropic spiked random feature model
[119], and investigate the effects of multiple gradient steps [120]. Collectively, these works highlight
that even a single gradient step can induce meaningful feature learning in two-layer networks.
Distinct phases of alignment and growth in ReLU networks. Maennel et al. [56] first observed a
striking phenomenon in two-layer ReLU networks trained from small initializations: the first-layer
weights concentrate along fixed directions determined by the training data, regardless of network
width. Subsequent studies of piecewise linear networks have refined this effect by imposing structural
constraints on the training data, such as orthogonally separable [24, 26, 28], linearly separable and
symmetric [25], pair-wise orthonormal [27], correlated [121], small angle [30], binary and sparse
[29]. Across these analyses, a consistent observation is that the learning dynamics involve distinct
learning phases separated by timescales: a fast alignment phase driven by directional movement and
a slow fitting phase driven by radial movement [60, 61]. Concurrently, Bantzis et al. [122] use a
closely related directional–radial decomposition to identify the optimal escape directions from the
saddle at the origin in deep ReLU networks.
20


=== Page 21 ===
B
Derivations in Alternating Gradient Flows Framework
In this section, we proivde details and motivations for the general framework of AGF. All code and ex-
perimental details are available at github.com/danielkunin/alternating-gradient-flows.
B.1
Deriving dormant dynamics near a saddle point in terms of the utility function
At any point in parameter space the gradient of the MSE loss with respect to the parameters of a
neuron can be expressed as
−∇θiL(Θ) = ∇θiEx [⟨fi(x; θi), r(x; Θ)⟩] ,
(10)
where r(x; Θ) = y(x) −P
i fi(x; θi) is a residual that depends on all neurons. Given a partition of
neurons into two sets, an active A and dormant D set, then the gradient for a dormant neuron i ∈D
can be decomposed into two terms:
−∇θiL(Θ) = ∇θiEx
"*
fi(x; θi), y(x) −
X
j∈A
fj(x; θj)
+#
−∇θiEx
"*
fi(x; θi),
X
j∈D
fj(x; θj)
+#
. (11)
The first term is the gradient of the utility ∇θUi as defined in Equation (1). The second term only
depends on the dormant neurons and is thus negligible when all the dormant neurons are small in
norm. Taken together, in the vicinity of a saddle point of the loss defined by an active and dormant
set, the dormant neurons are driven approximately by gradient flow maximizing their utility:
d
dtθi ≈η∇θUi.
(12)
This observation motivates the utility and our AGF ansatz for gradient flow. To formally prove that
AGF converges to gradient flow in the limit of vanishing initialization one might consider using tools
like Hartman–Grobman theorem to make this step rigorous.
Directional and radial expansion of the dynamics.
Using this approximation, we can further
decompose the dynamics of a dormant neuron near a saddle point into a directional and radial
component:
d
dt
θi
∥θi∥= η P⊥
θi
∥θi∥∇θUi,
d
dt∥θi∥= η
1
∥θi∥⟨θi, ∇θUi⟩,
(13)
where P⊥
θi =

Im −θiθ⊺
i
∥θi∥2

is a projection matrix. When σ(·) is a homogeneous function of degree
k, then the utility is a homogeneous function of degree κ = k + 1. Using Euler’s homogeneous
function theorem, Equation (13) then simplifies to Equation (2). When σ(·) is not homogeneous, we
can Taylor expand the utility around the origin, such that the utility coincides approximately with a
homogeneous function of degree κ, where κ is the leading order of the Taylor expansion.
Dynamics of normalized utility.
An interesting observation, that we will use in Appendix C, is
that the normalized utility function follows a Riccati-like differential equation
d
dt
¯Ui = ηκ2∥θi∥κ−2
∥∇θ ¯Ui∥2
κ2
−¯U2
i

.
(14)
In general, this ODE is coupled with the dynamics for both the direction and norm, except when
κ = 2 for which the dependency on the norm disappears.
B.2
Deriving the jump time
To compute τi∗, we use Equation (2) to obtain the time evolution of the norms of the dormant neurons.
For i ∈D, we obtain:
∥θi(t)∥=
(
∥θi(0)∥exp (Si(t))
if κ = 2,
 ∥θi(0)∥2−κ + (2 −κ)Si(t)

1
2−κ
if κ > 2,
(15)
where we have defined the accumulated utility as the path integral Si(t) =
R t
0 κ ¯Ui(s)ds of the
normalized utility. We find τi as the earliest time at which neuron i satisfies ∥θi(τi)∥> 1:
τi = inf {t > 0 | Si(t) > ci/η} ,
where ci =
(
−log ∥θi(0)∥
if κ = 2,
−∥θi(0)∥2−κ−1
2−κ
if κ > 2.
(16)
Note that the expression for ci is continuous at κ = 2.
21


=== Page 22 ===
Lower bound on jump time.
When applying this framework to analyze dynamics, computing the
exact jump time can be challenging due to the complexity of integrating ¯U. In such cases, we resort
on the following lower bound as a useful analytical approximation. Let U∗
i be the maximal value of
the utility, or at least an upper bound on it. Since Si(t) ≤κU∗
i t, we deduce:
τi ≥
(
−log ∥θi(0)∥
2U∗
i
if κ = 2,
−∥θi(0)∥2−κ
κ(2−κ)U∗
i
if κ > 2.
(17)
B.3
Active neurons can become dormant again
In the cost minimization phase of AGF, active neurons can become dormant. Here, we motivate and
discuss this phenomenon. Intuitively, this happens when the GF trajectory brings an active neuron
close to the origin. Due to the preserved quantities of GF for homogeneous activation functions,
the trajectory of active neurons is constrained. Thus, this phenomenon can occur only in specific
scenarios, as quantified by the following result.
Lemma B.1. Suppose that Θ evolves via the GF of L, and let c = (κ −1)∥ai(0)∥2 −∥wi(0)∥2. The
norm ∥θi(t)∥2 satisfies the lower bound,
∥θi(t)∥2 ≥max

−c,
c
κ −1

,
(18)
which holds with equality when either ∥wi(t)∥2 = 0 for c ≥0 or ∥ai(t)∥2 = 0 for c < 0.
Proof. For homogeneous activation functions of degree κ, it is well known that the quantity c =
(κ −1)∥ai(t)∥2 −∥wi(t)∥2 is preserved along trajectories of gradient flow. Since ∥θi(t)∥2 =
∥ai(t)∥2 + ∥wi(t)∥2, we have:
∥θi(t)∥2 = −c + κ∥ai(t)∥2 ≥−c,
∥θi(t)∥2 =
c
κ −1 +
κ
κ −1∥wi(t)∥2 ≥
c
κ −1.
(19)
The claim follows by combining the above inequalities.
Since at vanishing initialization (κ −1)∥ai∥2 ∼∥wi∥2, we have c ∼0. Therefore, an active neuron
can approach the origin during the cost minimization phase. If this happens, the neuron becomes
dormant. When a neuron becomes dormant again it does so with an accumulated utility Si(t) = ci
η .
See Figure 8 for an example of a diagonal linear network where this behavior is observable.
B.4
Instantaneous alignment in the vanishing initialization limit
In this section, we revisit the separation of the dynamics dictated by revisit Equation (2). In
particular, we wish to discuss the conditions under which the directional dynamics dominates the
radial one, resulting, at the vanishing initialization limit, in instantaneous alignment during the utility
maximization phase. To this end, we establish the following scaling symmetry with respect to the
initialization scale factor α.
Theorem B.2. Suppose θi(t) = fi(t) solves the initial value problem defined by Equation (2) with
initial condition θi(0) = θ0,i. Then for all α > 0, the scaled solution θi(t) = αfi(ακ−2t) solves the
initial value problem with initial condition θi(0) = αθ0,i.
Proof. First, we consider the angular dynamics:
d
dt
¯θi = d
dt
fi(ακ−2t)
∥fi(ακ−2t)∥
= ακ−2 d
ds
fi(s)
∥fi(s)∥

s=ακ−2t
= ακ−2∥fi(ακ−2t)∥κ−2P⊥
θi∇θUi(¯θi(ακ−2t); r)
= ∥αfi(ακ−2t)∥κ−2P⊥
θi∇θUi(¯θi(ακ−2t); r)
22


=== Page 23 ===
Therefore, θi(t) = αfi(ακ−2t) satisfies the first identity in Equation (2) above. Next, consider the
norm dynamics:
d
dt∥θi∥= d
dt∥αfi(ακ−2t)∥
= ακ−1 d
ds∥fi(s)∥

s=ακ−2t
= ακ−1κ∥fi(ακ−2t)∥κ−1Ui(¯θi(ακ−2t); r)
= κ∥αfi(ακ−2t)∥κ−1Ui(¯θi(ακ−2t); r)
Hence, θi(t) = αfi(tακ−2) also satisfies the second equation. Finally, verifying the initial conditions
gives, θi(0) = αfi(0) = αθ0,i. Therefore, θi(t) = αfi(tακ−2) satisfies the given initial condition.
Theorem B.2 establishes a transformation among the angular dynamics of different initialization
scales. Specifically, for a given point (s,
fi(s)
∥fi(s)∥) in the solution trajectory of the initial value problem
with initial condition θi(0) = θ0,i, the corresponding point in the initial value problem with scaled
initial condition θi(0) = αθ0,i is (sα2−κ,
fi(s)
∥fi(s)∥). This correspondence reveals that the angular
alignment process is effectively slowed down by a factor of ακ−2 with initialization scale α.
Next, we examine the alignment speed in accelerated time in the limit of α →0. In this regime, the
asymptotics of Equation (16) is given by,
cκ(α) ∼
−log α
if κ = 2,
α2−κ
if κ > 2,
(20)
Therefore, in the accelerated time, the alignment speed is effectively scaled by a scaling factor of:
γ(α) := cκ(α)
ακ−2 =
−log α
if κ = 2,
1
if κ > 2,
(21)
And,
lim
α→0 γ(α) =
+∞
if κ = 2,
1
if κ > 2,
(22)
The limiting behavior implies that the alignment is instantaneous in accelerated time for almost all
initialization θ0,i if and only if κ = 2.
B.5
A neuron-specific adaptive learning rate yields instantaneous alignment
As discussed above, instantaneous alignment of dormant neurons with their local utility-maximizing
directions occurs only when κ = 2. For higher-order activations (κ > 2), the directional dynamics
acquire a norm-dependent factor ∥θi∥κ−2 that slows their angular evolution. Consequently, directional
and radial dynamics no longer decouple, even in the vanishing initialization limit, and dormant
neurons rotate gradually rather than aligning instantaneously.
This dependence can be removed by introducing a neuron-specific adaptive learning rate
ηi = ∥θi∥2−κ η,
(23)
where η is a global base rate. When κ = 2, this scaling has no effect and all neurons evolve at
the same rate. For κ > 2, however, neurons with small norm (∥θi∥< 1) are accelerated, while
those with large norm (∥θi∥> 1) are slowed down. This rescaling effectively reparametrizes
time so that the directional dynamics become norm-independent while the radial dynamics remain
norm-dependent. Substituting this adaptive rate into Equation (2) yields an evolution equivalent
to that of the κ = 2 case, resulting in the decoupling between directional and radial dynamics in
the vanishing-initialization limit for all κ. In practice, this scaling acts analogously to a form of
neuron-wise adaptive optimization—resembling RMSProp or Adam—but derived directly from the
analytical structure of the κ-homogeneous gradient flow.
23


=== Page 24 ===
C
Complete Proofs for Diagonal Linear Networks
In this section, we derive the AGF dynamics for the two-layer diagonal linear network; see Section 3
for the problem setup and notation. Throughout this section, we assume the initialization θi(0) =
(
√
2α, 0), following the convention in Pesme and Flammarion [18].
C.1
Utility maximization
Lemma C.1. After the kth iteration of AGF, the utility for the ith neuron is
Ui

θi; r(k)
= −uivi∇βiL

β(k)
,
(24)
which is maximized on the unit sphere ∥θi∥= 1 when sgn(uivi) = −sgn
 ∇βiL
 β(k)
and
|ui| = |vi| resulting in a maximal utility value of ¯U∗
i = 1
2
∇βiL
 β(k).
Proof. Substituting the residual r(k)
j
= yj −
 X⊺β(k)
j into the definition of the utility function:
Ui(θi; r(k)) = 1
n
n
X
j=1
uiviXijrj = −uivi∇βiL

β(k)
.
(25)
Observe that the expression for Ui is linear in uivi. Therefore, under the normalization constraint
∥θi∥= 1, the utility is maximized when |ui| = |vi| and sgn(uivi) = −sgn
 ∇βiL
 β(k)
, yielding
the maximal value ¯U∗
i = 1
2
∇βiL
 β(k).
Lemma C.2. At any time t during AGF, the parameters of the ith neuron satisfy:
ui(t)2 −vi(t)2 = 2α,
2ui(t)vi(t) = ±
p
∥θi(t)∥2 −4α4
(26)
Proof. Both the utility and loss are invariant under the transformation (ui, vi) 7→(gui, g−1vi) for
any g ̸= 0. This continuous symmetry, together with the fact that each AGF step consists of gradient
flow in one of these functions, implies that the quantity ui(t)2 −vi(t)2 is conserved throughout
AGF. Plugging the initialization into this expression gives the first identity ui(t)2 −vi(t)2 = 2α.
See Kunin et al. [77] for a general connection between continuous symmetries and conserved
quantities in gradient flow. For the second identity, we solve for the intersection of the hyperbola
ui(t)2 −vi(t)2 = 2α and the circle ∥θi(t)∥2 = ui(t)2 + vi(t)2, which gives
ui(t) = ±
r
∥θi(t)∥2 + 2α2
2
,
vi(t) = ±
r
∥θi(t)∥2 −2α2
2
.
(27)
Multiplying these expressions together gives the identity for the product.
Lemma C.3. After the kth iteration of AGF, the normalized utility for the ith neuron is driven by a
Riccati ODE d
dt ¯Ui = 4ηα
  ¯U∗
i
2 −¯U2
i

with the unique solution,
¯Ui(τ (k) + t) = ¯U∗
i tanh

δ(k)
i
+ 4ηα ¯U∗
i t

,
where δ(k)
i
= tanh−1
 ¯Ui
 τ (k)
¯U∗
i
!
.
(28)
Proof. We begin by observing the gradient of the utility function takes the form
∇θUi

θi; r(k)
= −∇βiL

β(k) 
vi
ui

.
(29)
Evaluating this quantity at the normalized parameters ¯θi, and recalling the expression for the maximal
utility from Lemma C.1, we obtain,
∥∇θUi(¯θi; r(k))∥2 = 4
  ¯U∗
i
2 .
(30)
Substituting this into the normalized utility dynamics derived previously (see Equation (14)), we
obtain the Riccati equation. This is a standard ODE with a known solution, where the constant δ(k)
i
is
determined by the initial condition ¯Ui
 τ (k)
.
24


=== Page 25 ===
Theorem C.4. After the kth iteration of AGF, the accumulated utility for the ith neuron is
Si

τ (k) + t

=
1
2ηα
log cosh

2ηα

2U∗
i t + ζi
2ηα
cosh−1 exp

2ηαSi

τ (k)
,
(31)
where ηα = −log(
√
2α) is the learning rate and ζi = sgn
 −∇βiL
 β(k)
ρi
 τ (k)
. The quantity
ρi
 τ (k)
= sgn
 ui
 τ (k)
vi
 τ (k)
is determined by the recursive formula
ρi

τ (k) + t

= sgn
 
ρi
 τ (k)
2ηα
cosh−1 exp

2ηαSi

τ (k)
−∇βiL

β(k)
t
!
,
(32)
where Si(0) = 0 and ρi(0) = 0.
Proof. Using the expression for the normalized utility derived in Lemma C.3, we can derive an exact
expression for the integral of the normalized utility, i.e., the accumulated utility:
Si
 τ (k) + t

= Si
 τ (k)
+
R t
0 2 ¯Ui(s)ds = Si
 τ (k)
+
1
2ηα log

cosh

4ηα ¯U∗
i t+δ(k)
i

cosh

δ(k)
i


.
(33)
Using the hyperbolic identity tanh−1(x) = sgn(x) cosh−1 
1
√
1−x2

, we can express the constant
δ(k)
i
introduced in Lemma C.3:
δ(k)
i
= sgn

−∇βiL

β(k)
tanh−1
2ui(τ (k))vi(τ (k))
∥θi(τ (k))∥2

Lemma C.1
(34)
= sgn

−∇βiL

β(k)
ρi

τ (k)
cosh−1
∥θ(τ (k))∥2
2α2

Lemma C.2
(35)
= sgn

−∇βiL

β(k)
ρi

τ (k)
cosh−1 
exp

2ηαSi

τ (k)
,
(36)
where in the last equality we used the simplification ∥θ(τ (k))∥2 = 2α2 exp
 2ηαSi
 τ (k)
. Substi-
tuting this expression into Equation (33), notice that the denominator inside the logarithm simplifies
substantially, as the Si(τ (k)) term cancels out, yielding Equation (31). Finally, by Lemma C.2, ρi
changes sign only when Si = 0, yielding Equation (32).
Corollary C.5. After the kth iteration of AGF, the next dormant neuron to activate is i∗=
arg mini∈D ∆τi where
∆τi = cosh−1 exp (2ηα) −ζi cosh−1 exp
 2ηαSi(τ (k))

2ηα · 2 ¯U∗
i
,
(37)
ζi = sgn
 −∇βiL
 β(k)
ρi
 τ (k)
, and the next jump time τ (k+1) = τ (k) + ∆τi∗.
Proof. From Theorem C.4, we solve for the time t such that Si(τ (k) + t) = 1 by inverting the
expression for accumulated utility. This time is ∆τi.
C.2
Cost minimization
Active neurons represents non-zero regression coefficients of β. During the cost minimization step,
the active neurons work to collectively minimize the loss. When a neuron activates it does so with a
certain sign sgn(uivi) = −sgn(∇βiL(βA)). If during the cost minimization phase a neuron changes
sign, then it can do so only by returning to dormancy first. This is due to the fact that throughout
the gradient flow dynamics the quantity u2
i −v2
i = 2α2 is conserved and thus in order to flip the
sign of the product uivi, the parameters must return to their initialization. As a result, the critical
point reached during the cost minimization step is the unique solution to the constrained optimization
problem,
β∗
A = arg min
β∈Rd
L(β)
subject to
βi = 0
if i /∈A,
βi · sgn(uivi) ≥0
if i ∈A,
(38)
where uniqueness follows from the general position assumption. All coordinates where (β∗
A)i = 0
are dormant at the next step of AGF.
25


=== Page 26 ===
τ(1) τ(2)
τ(3)
ℓ(0)
ℓ(1)
ℓ(2)
α = 0.50
α = 0.10
α = 1e-02
α = 1e-04
α = 1e-06
α →0
(a) Loss L(t)
β1 = u1v1
β2 = u2v2
β (0)
β (1)
β (2)
β (3)
2
2
4
4
6
6
8
8
10
10
12
12
14
16
18
(b) Function β(t)
−π/4
0
π/4
10−6
10−4
10−2
100
θ1 = (u1, v1)
−π/4
0
π/4
10−6
10−4
10−2
100
θ2 = (u2, v2)
(c) Parameters θ(t)
0
1
˜S(t)
θ1
θ2
τ(1) τ(2)
τ(2) + τ(3)
2
τ(3)
−π/4
0
π/4
ˆθ(t)
τ(1) τ(2)
τ(2) + τ(3)
2
τ(3)
(d) Utility ˜S(t) and ˆθ(t)
Figure 8: AGF = GF as α →0 in diagonal linear networks. We consider a diagonal linear network
β = u ⊙v ∈R2, trained by gradient flow from initialization u =
√
2α1, v = 0, with varying α,
inspired by Figure 2 of Pesme and Flammarion [18]. This example is special as it demonstrates how
an active neuron, in this case β(1), can return to dormancy during the cost minimization phase, as
discussed theoretically in Appendix B. In (a) we plot the loss over accelerated time and in (b) the loss
landscape over β. For small α, trajectories evolve from β(0) to β(3), passing near intermediate saddle
points β(1) and β(2). Each saddle point is associated with a plateau in the loss. As α →0, gradient
flow spends all its time at these saddles, jumping instantaneously between them at τ (1), τ (2), τ (3),
matching the stepwise loss drops. In (c) we plot trajectories in parameter space and in (d) the same
dynamics visualized in terms of their accumulated utility and angle. The jump times between critical
points correspond to when the accumulated utility satisfies ˜Si(τ) = 1. When the first coordinate
returns to dormancy, the accumulated utility first touches zero, causing the angle to flip sign, before
reactivating with the opposite sign.
C.3
AGF in action
Combining the results of utility maximization (Appendix C.1) and cost minimization (Appendix C.2)
yields an explicit recursive expression for the sequence generated by AGF. For each neuron we
only need to track the accumulated utility Si(t) and the sign ρi(t), which at initialization are both
zero. We can now consider the relationship between the sequence produced by AGF in the vanishing
initialization limit α →0 and the sequence for gradient flow in the same initialization limit introduced
by Pesme and Flammarion [18].
Algorithm 2: Pesme and Flammarion [18]
Initialize: t ←0, β ←0 ∈Rd, S ←0 ∈Rd;
while ∇L(β) ̸= 0 do
D ←{j ∈[d] | ∇L(β)j ̸= 0}
τ ∗←inf {τi > 0 | ∃i ∈D, Si −τi∇L(β)i = ±1}
t ←t + τ ∗,
S ←S −τ ∗∇L(β)
β = arg min L(β) where β ∈





β ∈Rd

βi ≥0
if Si = +1,
βi ≤0
if Si = −1,
βi = 0
if Si ∈(−1, 1)





return Sequence of (β, t);
We adapt the original notation to fit our framework, highlighting
utility maximization (blue) and cost minimization (red) steps.
In Pesme and Flammarion [18], they
prove that in the vanishing initializa-
tion limit α →0, the trajectory of
gradient flow, under accelerated time
˜tα(t) = −log(α) · t, converges to-
wards a piecewise constant limiting
process corresponding to the sequence
of saddles produced by Algorithm 2.
This algorithm can be split into two
steps, which match exactly with the
two steps of AGF in the vanishing ini-
tialization limit.
As in AGF, at each iteration of their
algorithm a coefficient that was zero becomes non-zero. The new coefficient comes from the set
{i ∈[d] : ∇L(β)i ̸= 0}, which is equivalent to the set of dormant neurons with non-zero utility, as
all active neurons will be in equilibrium from the previous cost minimization step. The index for the
new active coefficient is determined by the following expression,
i∗= arg min{τi > 0 : ∃i ∈D, si −τi∇L(β)i = ±1}.
(39)
Because τi > 0 and si ∈(−1, 1) for all coefficients i ∈D, then this expression only makes sense if
we choose the boundary associated with sgn(∇L(β)i). Taking this into account, we get the following
simplified expression for the jump times proposed by Pesme and Flammarion [18],
τi = 1 + sgn(∇L(β)i)si
|∇L(β)i|
.
(40)
This expression is equivalent to the expression given in Corollary C.5 in the limit ηα →∞. To see
this, we use the asymptotic identity cosh−1(exp(x)) ∼x + log 2 as x →∞. Combined with the
26


=== Page 27 ===
simplification 2 ¯U∗
i = |∇βiL(β)|, this allows us to simplify the expression for the jump time and
show that ρiSi = si where si is the integral from Algorithm 2.
The second step of Algorithm 2, is that the new β is determined by the following constrained
minimization problem,
β∗= arg min
β∈Rd
L(β)
subject to



βi ≥0,
if si = 1
βi ≤0,
if si = −1
βi = 0,
if si ∈(−1, 1)
(41)
Using the correspondence ρi = sgn(si), and noting that neurons with si ∈(−1, 1) correspond to
the dormant set, this constrained optimization problem is equivalent to the cost minimization step of
AGF, presented in Appendix C.2.
27


=== Page 28 ===
D
Complete Proofs for Fully Connected Linear Networks
In this section, we provide the details behind the results around linear networks in Section 4.
We begin by clarifying the notion of ‘neuron’ in this context. As briefly explained in Section 4,
due to the symmetry of the parametrization of fully-connected linear networks, the usual notion of
a neuron does not define a canonical decomposition of f into rank-1 maps. Instead, the singular
value decomposition of the linear map AW computed by the network, defines such a canonical
decomposition, being the unique orthogonal one. Therefore, we will think of neurons as basis vectors
(˜ai, ˜wi) ∈Rc+d under orthogonality constraints. These vectors align with the singular vectors and
are partitioned into dormant and active sets, D and A, based on whether their corresponding singular
value is O(1).
D.1
Utility maximization
Differently from the usual setting of AGF, the orthogonality constraint on the dormant basis vectors
implies that the utility function is not decoupled. Instead, it is maximized over the space of |D|
orthonormal basis vectors, i.e., the Stiefel manifold.
Lemma D.1. After the kth iteration of AGF, the utility function of the ith dormant basis vector with
parameters θi = (˜ai, ˜wi) is:
Ui

θi; r(k)
= −˜a⊺
i ∇βL

β(k)
A

˜wi.
(42)
The total utility P
i∈D Ui
 θi; r(k)
is maximized on the Stiefel manifold when {˜ai, ˜wi}|D|
i=1 coincides
with the set of the top |D| = H−k left and right singular vectors of ∇βL(β(k)
A ), resulting in a maximal
utility value for the ith dormant basis vector of ¯U∗
i = σ(k)
i
/2, where σ(k)
i
is the corresponding singular
value. Moreover, Ui has no other local maxima.
Proof. After substituting r(k) = y −β(k)
A x, the computation of the utility is straightforward. Every-
thing else follows from the standard theory of Reyleigh quotients over Stiefel manifolds [123].
This means that the gradient flow of the utility function aligns the dormant basis vectors with the
singular vectors of −∇βL(β(k)
A ).
D.2
Cost minimization
As discussed in Section 4, the cost minimization phase is governed by the well-known Eckart-Young
theory of reduced-rank regression [78]. According to the latter, during cost minimization, the active
basis vectors converge to
β(k)
A
= PUkΣyxΣ−1
xx ,
(43)
where PUk is the projection onto the top k eigenvectors of ΣyxΣ−1
xx Σ⊺
yx. Plugging this solution into
the expression for the gradient, we find that the utility at the next iteration will be computed with the
projection P⊥
UkΣyx:
−∇βL(β(k)
A ) = Σyx −β(k)
A Σxx = (Ic −PUk)Σyx.
(44)
D.3
AGF in action
Algorithm 3: Greedy Low-Rank Learning Li et al. [20]
Initialize: r ←0, W0 ←0 ∈Rd×d, U0(∞) ∈Rd×0
while λ1(−∇L(Wr)) > 0 do
r ←r + 1;
ur ←unit top eigenvector of −∇L(Wr−1);
Ur(0) ←[Ur−1(∞)
√εur] ∈Rd×r;
for t = 0, 1, . . . , T do
Ur(t + 1) ←Ur(t) −η∇L(Ur(t));
Wr ←Ur(∞)U ⊤
r (∞)
return Sequence of Wr
Putting together the utility maximiza-
tion and cost minimization steps, AGF
progressively learns the projected
OLS solution (Equation (43)), coin-
ciding with the GLRL algorithm by
Li et al. [20], shown in Algorithm 3,
with notation from the original work.
At each stage, the GLRL algorithm
selects a top eigenvector (blue: utility
maximization) and performs gradient
descent in the span of selected direc-
tions (red: cost minimization).
28


=== Page 29 ===
We now discuss jump times, and motivate Conjecture 4.1. Recall from Section B.4 that in this setting,
because κ = 2, in the limit of vanishing initialization, the alignment in the utility maximization
phase occurs instantaneously. As a consequence, the individual utility functions remain, essentially,
constantly equal to their corresponding maximal value throughout each utility maximization phase.
From the definition of accumulated utility with ηα = −log(α), we immediately deduce the recursive
relation:
Si

τ (k) + t

= Si

τ (k−1)
+ σ(k)
i
t.
(45)
However, since −∇βL(β(k)
A ) = P⊥
UkΣyx and −∇βL(β(k−1)
A
) = P⊥
Uk−1Σyx have different singular
values and vectors, the relation between the values of Equation (45) as i ∈D varies is unclear, and it
is hard to determine the first dormant basis vector to accumulate a utility value of 1. Yet, suppose
that the dormant basis vectors align in such a way that the ordering of the corresponding singular
values is preserved across the utility maximization phases. In this case, the ordering of the cumulated
utilities in Equation (45) is also preserved for all iterations k and all times t. In particular, it follows
by induction that the basis vector with index i∗corresponding to the largest eigenvalue σ(k)
i∗jumps
after a time of:
∆τ (k) =
1
σ(k)
i∗

1 −Si∗

τ (k−1)
.
(46)
Once unrolled, the above recursion is equivalent to the statement on jump times in Conjecture 4.1.
When Σxx commutes with Σ⊺
yxΣyx, the left singular vectors of Σyx simultaneously diagonalize Σxx
and ΣyxΣ−1
xx Σ⊺
yx. In this case, the singular values of P⊥
UkΣyx correspond to the bottom ones of
Σyx, with the same associated singular vectors. Therefore, the dormant basis vectors are correctly
aligned already from the first iteration of AGF. By the discussion above, this confirms Conjecture
4.1 in this specific scenario. Moreover, the recursive expression from Equation (46) reduces to
∆τ (k) = 1/σk −1/σk−1, where σk is the kth largest singular value of Σyx. Therefore, the jump
times are simply:
τ (k) = 1
σk
.
(47)
These values, together with the loss values in Conjecture 4.1, coincide with the ones derived by Gidel
et al. [19] for GF at vanishing initialization. This means that under the commutativity assumption,
AGF and GF converge to the same limit, similarly to the setting of diagonal linear networks.
When Σxx and Σ⊺
yxΣyx do not commute, in order to prove Conjecture 4.1, it is necessary to understand
the relation between the SVD of P⊥
UkΣyx and of Σyx, as k varies. The Poincaré separation theorem
describes this relation for the singular values, stating that they are interlaced, i.e., they alternate
between each other when ordered. This is not sufficient, since the geometry of the singular vectors
plays a crucial role in the alignment phase. Even though there exist classical results from linear
algebra in this direction [124], we believe that Conjecture 4.1 remains open.
29


=== Page 30 ===
E
Complete Proofs for Attention-only Linear Transformer
Here, we provide additional details and derivations for the results around transformers (see Section 4
for the problem setup and notation). This analysis is based on the setting presented by Zhang et al.
[32], to which we refer the reader for further details.
Note that this scenario does not exactly fit the formalism of Section 2, since we are considering a
neural architecture different from a fully-connected network. Yet, due to our assumption on the rank
of the attention heads, each head behaves as a bottleneck, resembling a ‘cubical version’ of a neuron.
Therefore, we will interpret attention heads as ‘neurons’, and apply AGF to this context (with κ = 3).
We will show in the following sections that at the end of the kth iteration of AGF, the ith attention
head, h = 1, . . . , k, learns the function:
fh(X; θ∗
h)D+1,N+1 =
1
trΣxx + (N + 1)λh
N
X
n=1
ynx⊺
nvhv⊺
hxN+1,
(48)
where λh is the hth largest eigenvalue of Σxx, and vh is the corresponding eigenvector. We will
proceed by induction on k.
E.1
Utility maximization
We prove the following lemma, establishing, inductively, the utility function for the kth iteration of
AGF.
Lemma E.1. At the kth iteration of AGF, assume that the (k −1) active attention heads have the
function form as in Equation (48). Then, the utility function of a dormant head with parameters
θi = (Vi, Qi, Ki) is given by:
Ui

θi; r(k)
= NVi Q⊺
i
 
Σ2
xx −
k−1
X
h=1
λ2
hvhv⊺
h
!
Ki
(49)
Moreover,
the utility is maximized over normalized parameters by
( ¯V ∗
i , ¯Q∗
i , ¯K∗
i )
=
(±1/
√
3, ±vk/
√
3, ±vk/
√
3), where the sign is determined such that ¯U∗
i = Nλ2
k/(3
√
3). Moreover,
Ui has no other local maxima.
Proof. By assumption, we have:
Ui

θi; r(k)
= Ex1,...,xN+1,β
" 
yN+1 −
k−1
X
h=1
fh(X; θ∗
h)D+1,N+1
!
fi(X, θi)D+1,N+1
#
.
(50)
The first term, which coincides with the initial utility, can be computed as:
Ui (θi; y) = Ex1,...,xN+1,β [yN+1fi(X, θi)D+1,N+1]
= Ex1,...,xN+1,β
"
β⊺xN+1Vi
N
X
n=1
ynx⊺
nKiQ⊺
i xN+1
#
= Vi Q⊺
i ExN+1[xN+1x⊺
N+1]Eβ[ββ⊺]
N
X
n=1
Exn[xnx⊺
n]Ki
= NVi Q⊺
i Σ2
xxKi.
(51)
Denote Ah = (trΣxx + (N + 1)λk)−1. Then, the summand can be computed as:
Ex1,...,xN+1,β [f ∗
h(X, θh)D+1,N+1fi(X, θi)D+1,N+1]
= AhViEx1,...,xN+1,βtr
 
ββ⊺
N
X
n=1
xnx⊺
nKiQ⊺
i xN+1x⊺
N+1vhv⊺
h
N
X
n=1
xnx⊺
n
!
= AhVitr

KiQ⊺
i Σxxvhv⊺
hEx1,...,xN
 N
X
n=1
xnx⊺
n
!2

= AhVitr
 KiQ⊺
i λhvhv⊺
h
 NΣxxtrΣxx + N(N + 1)Σ2
xx

= NViQ⊺
i λ2
hvhv⊺
hKi,
(52)
30


=== Page 31 ===
where in the second equality, we have used the fact that
Ex1,...,xN


 N
X
n=1
xnx⊺
n
!2
= Ex1,...,xN


N
X
n=1
(xnx⊺
n)2 + 2
X
1≤i<j≤N
xix⊺
i xjx⊺
j


= NΣxxtrΣxx + N(N + 1)Σ2
xx.
(53)
This provides the desired expression for the utility, which corresponds to a Rayleigh quotient. Since
the largest eigenvalue of (Σ2
xx −Pk−1
h=1 λ2
hvhv⊺
h) is λ2
k, the rest of the statement follows from the
standard theory of Rayleigh quotients.
E.2
Cost minimization
After utility maximization, a new head aligns to the corresponding eigenvector and becomes active.
This implies that at the beginning of the cost minimization phase of the kth iteration of AGF, the k
active attention heads have parameters θh = (Vh, Qh, Kh), such that Qh and Kh coincide up to a
multiplicative scalar to vh for all 1 ≤h ≤k. Moreover, Equation (48) holds for 1 ≤h < k.
We wish to show that during the cost minimization phase, the newly-activated head with parameters
θk learns a function in the same form. First, we prove that the loss function is decoupled for each
active attention head.
Lemma E.2. Assume that the k active attention heads have the same function form, up to multiplica-
tive scalar, as in Equation (48), Then, the loss over active heads decoupled into sum of individual
losses, that is
L (ΘA) =
k
X
i=1
L (θi) .
(54)
Moreover, for all h = 1, . . . , k, the directional derivatives
∂L
∂Qh L(ΘA) and
∂L
∂Kh L(ΘA) are propor-
tional to vh throughout the cost minimization phase.
Proof. The first statement follows from a straightforward calculation using the orthogonality of
eigenvectors. As a result,
∂L
∂Qh
(ΘA) = ∂L
∂Qh
(θh) = Ex1,...,xN,β
 
Vh
N
X
n=1
ynx⊺
nKh
!2
ΣxxQh,
(55)
From this expression, we see that if Qh starts as an eigenvector of Σxx, then subsequent gradient
updates will be in the same direction. As a result, the direction of Qh remains unchanged in cost
minimization. A similar argument holds for Kh as well.
Since the active heads follow the GF of L, the decoupling of the losses implies that during the
cost minimization phase, each head actually follows the gradient of the loss of its own parameters,
disregarding the other heads. In particular, θh remains at equilibrium for h = 1, . . . , k −1, since
its loss has been optimized during the previous iterations of AGF. By Lemmas E.1 and E.2, the
newly-activated head remains aligned to its eigenvector vk, learning a function of the form
fk (X; θk)D+1,N+1 = Ak
N
X
n=1
ynx⊺
nvk+1v⊺
k+1xN+1,
(56)
where Ak ∈R is a parameter that is optimized in cost minimization. L(θk) is convex with respect to
Ak, and the corresponding minimization problem is easily seen to be solved by Ak = 1/(trΣxx +
(N + 1)λk). This concludes our inductive argument.
31


=== Page 32 ===
E.3
AGF in action
As we have seen above, AGF describes a recursive procedure, where each head sequentially learns a
principal component. We can easily compute the loss value at the end of each iteration:
ℓ(k) ≡Ex1,...,xN+1,β
1
2
 
yN+1 −
k
X
h=1
Ah
N
X
n=1
ynx⊺
nvhv⊺
hxN+1
!2
= 1
2
 D
X
i=1
λi −
k
X
h=1
Nλ2
h
trΣxx + (N + 1)λh
!
.
(57)
Furthermore, as an immediate consequence of Lemma E.1, we can lower bound the jump times. The
accumulated utility at the kth iteration of AGF is upper-bounded by Si(t) ≤3t ¯U∗
i = tNλ2
k/
√
3. The
jump time is given by the first head that reaches Si(τ (k) −τ (l)) = 1/∥θi(τ (l))∥, which yields
τ (k) −τ (l) ≥
√
3
Nλ2
kµl
,
(58)
where µk = maxi ∥θi(τ (l))∥and τ (0) = 0.
32


=== Page 33 ===
F
Complete Proofs for Generalized Modular Addition
In this section, we provide the proofs for the results summarized in Section 5. Instead of reasoning
inductively as in Section E, for simplicity of explanation we will derive in detail the steps of the
first iteration of AGF. As we will explain in Section F.3, all the derivations can be straightforwardly
extended to the successive iterations, completing the inductive argument.
F.1
Utility maximization
In order to compute the utility for a single neuron, we will heavily rely on the Discrete Fourier
Transform (DFT) and its properties. To this end, recall that for u ∈Rp, its DFT ˆu ∈Rp is defined as:
ˆu[k] =
p−1
X
a=0
u[a]e−2πika/p,
(59)
where i = √−1 is the imaginary unit. We start by proving a technical result.
Lemma F.1. Let x, u, v, w ∈Rp. Then:
p−1
X
a,b=0
⟨u, a · x⟩⟨v, b · x⟩⟨w, (a + b) · x⟩= 1
p
p−1
X
k=0
|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k].
(60)
Proof. Notice that the inner product ⟨u, a · x⟩can be expressed as
⟨u, a · x⟩=
p
X
k=0
x[k]u[k + a] = (x ⋆u)[a],
(61)
where x ⋆u is the cross-correlation between x and u. Thus, the left-hand side of Equation (60) can
be rewritten as:
p−1
X
a,b=0
(x ⋆u)[a] (x ⋆v)[b] (x ⋆w)[a + b] = ⟨x ⋆u, x ⋆v ⋆x ⋆w⟩
By using Plancharel’s theorem ⟨x, y⟩=
1
p⟨ˆx, ˆy⟩and the cross-correlation property [
x ⋆y[k] =
ˆx[k]ˆy[k], the above expression equals
1
p
p−1
X
k=0
ˆx[k]ˆu[k]ˆv[k] ˆw[k]ˆx[k]2,
(62)
which corresponds to the desired result via the simplification ˆx[k]ˆx[k] = |ˆx[k]|2.
Lemma F.2. Under the modular addition task with embedding vector x and mean centered labels,
the initial utility function for a single neuron parameterized by θ = (u, v, w) can be expressed as
U(θ; y) = 2
p3
p−1
X
k=1
|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k].
(63)
Proof. By definition, the utility is:
U(θ; y) = 1
p2
p−1
X
a,b=0

(⟨u, a · x⟩+ ⟨v, b · x⟩)2 w, (a + b) · x −⟨x, 1⟩
p
1

= 1
p2
p−1
X
a,b=0
 ⟨u, a · x⟩2 + ⟨v, b · x⟩2 + 2⟨u, a · x⟩⟨v, b · x⟩
 
⟨w, (a + b) · x⟩−⟨w, 1⟩⟨x, 1⟩
p

.
(64)
33


=== Page 34 ===
Due to the cyclic structure of ⟨w, (a+b)·x⟩, the contributions from the terms ⟨u, a·x⟩2 and ⟨v, b·x⟩2
terms vanish. This reduces the utility to
2
p2
p−1
X
a,b=0
⟨u, a · x⟩⟨v, b · x⟩⟨w, (a + b) · x⟩−2
p3
 p−1
X
a=0
⟨u, a · x⟩
!  p−1
X
b=0
⟨v, b · x⟩
!  p−1
X
c=0
⟨w, c · x⟩
!
.
(65)
The first summand in the above expression is provided by Lemma F.1. Moreover, since the mean of a
vector corresponds to the zero-frequency component k = 0 of its DFT, the second summand reduces
to:
2
p3
 p−1
X
a=0
⟨u, a · x⟩
!  p−1
X
b=0
⟨v, b · x⟩
!  p−1
X
c=0
⟨w, c · x⟩
!
= 2
p3 [
x ⋆u[0] [
x ⋆v[0] [
x ⋆w[0]
= 2
p3 |ˆx[0]|3 ˆu[0]ˆv[0] ˆw[0].
(66)
Since the latter coincides with the term k = 0 in Equation (61), we obtain the desired expression.
We now solve the constrained optimization problem involved in the utility maximization step of our
framework.
Theorem F.3. Let ξ be a frequency that maximizes |ˆx[k]|, k = 1, . . . , p −1, and denote by sx the
phase of ˆx[ξ]. Then the unit vectors θ∗= (u∗, v∗, w∗) that maximize the initial utility function U(θ; y)
take the form
u∗[a] =
r 2
3p cos

2π ξ
pa + su

v∗[b] =
r 2
3p cos

2π ξ
pb + sv

w∗[c] =
r 2
3p cos

2π ξ
pc + sw

,
(67)
where a, b, c ∈{0, . . . , p −1} are indices and su, sv, sw ∈R are phase shifts satisfying su + sv ≡
sw + sx (mod 2π). They achieve a maximal value of ¯U∗=
p
2/(27p3)|ˆx[ξ]|3. Moreover, the utility
function has no local maxima other than the ones described above.
Proof. By Plancharel’s theorem, the constraint ∥θ∥2 = ∥u∥2 + ∥v∥2 + ∥w∥2 = 1 is equivalent to a
constraint on the squared norm of its DFT, up to a scaling. Together with Lemma F.2, this allows us
to reformulate the original optimization problem in the frequency domain as:
maximize
2
p3
p−1
X
k=1
|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k]
subject to
∥ˆu∥2 + ∥ˆv∥2 + ∥ˆw∥2 = p.
(68)
To solve this optimization problem, consider the magnitudes µx, µu, µv, µw ∈Rp
+ and phases
Sx, Su, Sv, Sw ∈[0, 2π)p of the DFT coefficient of x, u, v, w, respectively. This means that ˆx[k] =
µx[k] exp(iSx[k]) for every k, and similarly for u, v, w.
Since u, v, and w are real-valued, their DFTs satisfy conjugate symmetry (i.e., ˆu[−k] = ˆu[k]). Since
p is odd, the periodicity of the DFT (i.e., ˆu[k] = ˆu[k + p]) allows us to simplify the objective by only
considering the first half of the frequencies. Substituting the magnitude and phase expressions we
obtain the equivalent optimization,
maximize
4
p3
p−1
2
X
k=1
µx[k]3µu[k]µv[k]µw[k] cos(Su[k] + Sv[k] −Sw[k] −Sx[k])
subject to
p−1
2
X
k=0
µu[k]2 +
p−1
2
X
k=0
µv[a]2 +
p−1
2
X
k=0
µw[a]2 = p
2.
(69)
34


=== Page 35 ===
The phase terms can be chosen independent of the constraints to maximize the cosine term. Since the
only local maximum of the cosine is 0 (mod 2π), the only local optimum of that term is achieved
by setting Su[k] + Sv[k] −Sw[k] −Sx[k] ≡0 (mod 2π). But then the optimization problem
reduces to maximizing P
k∈[(p−1)/2] µx[k]3µu[k]µv[k]µw[k], subject to the constraints. The only
local maximum of this problem is easily seen to be achieved by concentrating all the magnitude at
the dominant frequency ξ of x (and its conjugate), i.e.:
µu[k] = µv[k] = µw[k] =
p p
6
if k = ±ξ
(mod p)
0
otherwise.
(70)
This gives a maximal objective value of ¯U∗= (4/p3)µx[ξ]3(p/6)3/2 =
p
2/(27p3)|ˆx[ξ]|3. By
applying the inverse DFT with these choices for magnitudes and phases, while accounting for
conjugate symmetry, yields the desired result. Note that in the statement, sx, su, sv, sw denote
Sx[ξ], Su[ξ], Sv[ξ], Sw[ξ], respectively.
We highlight that Theorem F.3 and its proof is very similar to Theorem 7 and its proof in Morwani
et al. [96]. They derived this optimization problem by looking for a maximum margin solution,
suggesting there may be an interesting connection between utility maximization and maximum
margin biases of gradient descent.
F.2
Cost minimization
We now discuss cost minimization. As demonstrated in the previous section, during the utility
maximization step, the parameters of each neuron align with the harmonic of the dominant frequency
ξ of x, albeit with phase shifts si
u, si
v, si
w. The goal of this section is two-fold. First, in Appendix F.2.1
we discuss our assumption that during the cost minimization phase, the neurons remain aligned with
the same harmonic, possibly varying the amplitudes and phase shifts of their parameters. Second,
in Appendix F.2.2 we solve the cost minimization problem over such aligned neurons. Therefore,
throughout the section, we consider N neurons parametrized by Θ = (ui, vi, wi)N
i=1, where:
ui[a] = Ai
u
r 2
3p cos

2π ξ
pa + si
u

,
vi[b] = Ai
v
r 2
3p cos

2π ξ
pb + si
v

,
wi[c] = Ai
w
r 2
3p cos

2π ξ
pc + si
w

,
(71)
for some amplitudes Ai
u, Ai
v, Ai
w ∈R≥0 and some phase shifts si
u, si
v, si
w ∈R.
To begin with, note that loss splits as:
L(Θ) =
1
2p2
p−1
X
a,b=0

N
X
i=1
fi(a · x, b · x; θi) −(a + b) · x + ⟨x, 1⟩
p
1

2
= C(Θ) −U(Θ; y) + 1
2

∥x∥2 −⟨x, 1⟩2
p

,
(72)
where
C(Θ) =
1
2p2
N
X
i,j=1
⟨wi, wj⟩
p−1
X
a,b=0
 ⟨ui, a · x⟩+ ⟨vi, b · x⟩
2  ⟨uj, a · x⟩+ ⟨vj, b · x⟩
2
(73)
and U(Θ; y) = PN
i=1 U(θi; y) is the cumulated utility function of the N neurons. From Lemma F.2,
we know that:
U(θi; y) =
r
2
27p3 |ˆx[ξ]|3Ai
uAi
vAi
w cos(si
u + si
v −si
w −sx).
(74)
35


=== Page 36 ===
Throughout this section, we repeatedly use the following identity: for any p ∈N, k ∈Z, and s ∈R,
p−1
X
a=0
cos

s −2πka
p

=
(
p cos(s),
if k = 0
(mod p),
0,
otherwise.
(75)
This follows by writing cos(θ) = Re(eiθ) and noting that the geometric sum Pp−1
a=0 e−2πika/p
vanishes unless k = 0 (mod p).
F.2.1
Preservation of harmonic alignment
To analyze cost minimization, we restrict attention to a regime in which the newly activated neurons
do not change their aligned harmonic during this phase.
Assumption F.4. During cost minimization, the N newly activated neurons remain aligned to the
harmonic ξ.
This restriction allows us to solve cost minimization within the subspace spanned by these N neurons,
in close analogy with prior sections. However, first we characterize the conditions under which this
assumption is valid and clarify in what sense it provides a faithful description of the dynamics.
Theorem F.5. Let h ∈Rp be a vector in the form:
h[a] = A cos

2π ξ′
p a + s

,
(76)
for some A, s ∈R and ξ′ ̸= 0, ±ξ. If for all a, b ∈[p]
N
X
i=1
wi⟨ui, a · x⟩2 =
N
X
i=1
wi⟨vi, b · x⟩2 = 0,
(77)
then for all i = 1, . . . , N:

h, ∂L
∂ui (Θ)

=

h, ∂L
∂vi (Θ)

=

h, ∂L
∂wi (Θ)

= 0.
(78)
Proof. A direct calculation leads to the following expressions for the derivatives:

h, ∂L
∂ui (Θ)

= 4
p−1
X
a,b=0
⟨h, a · x⟩
 ⟨ui, a · x⟩+ ⟨vi, b · x⟩
 
wi, f(a · x, b · x; Θ) −(a + b) · x

,

h, ∂L
∂vi (Θ)

= 4
p−1
X
a,b=0
⟨h, b · x⟩
 ⟨ui, a · x⟩+ ⟨vi, b · x⟩
 
wi, f(a · x, b · x; Θ) −(a + b) · x

,

h, ∂L
∂wi (Θ)

= 2
p−1
X
a,b=0
 ⟨ui, a · x⟩+ ⟨vi, b · x⟩
2 ⟨h, f(a · x, b · x; Θ) −(a + b) · x⟩.
(79)
Via a tedious goniometric computation, the three expressions above can be expanded into a sum
of cosine terms. Since ξ ̸= ξ′, each of these terms depends on a or b, and therefore averages out.
We do not report the details of the computation here, but refer to the proof of Lemma F.6 for more
details.
The condition Equation (77) rules out square terms that could otherwise generate gradient components
at resonant frequencies ξ′ ̸= ±ξ. If the squared activations ⟨ui, a · x⟩2 or ⟨vi, b · x⟩2 have a nonzero
aggregate contribution, their trigonometric expansion produces cosine terms at doubled or mixed
frequencies that may survive the averaging over a or b via the cosine-sum identity Equation (75). In
this case, the gradient can acquire components outside the span of the aligned harmonic, allowing
neurons to escape alignment.
However, pairs of aligned neurons can easily satisfy Equation (77) by coordinating their amplitudes
and phases. For example, using the identity z1z2 = 1
4(z1 + z2)2 −1
4(z1 −z2)2, neurons can arrange
for square terms to cancel in aggregate while preserving their linear contributions. Such cancellations
correspond to directional adjustments that can occur rapidly during cost minimization.
36


=== Page 37 ===
F.2.2
Cost minimization over aligned neurons
Next, in order to solve the cost minimization problem over aligned neurons, we explicitly compute
the term C(Θ) in the loss function.
Lemma F.6. We have:
C(Θ) = |ˆx[ξ]|4
54p2
N
X
i,j=1
Ai
wAj
w cos(si
w −sj
w)
  (Ai
u)2 + (Ai
v)2  (Aj
u)2 + (Aj
v)2
+(Ai
uAj
u)2
2
cos(2(si
u −sj
u)) + (Ai
vAj
v)2
2
cos(2(si
v −sj
v))
+2Ai
uAi
vAj
uAj
v
 cos(si
u + si
v −sj
u −sj
v) + cos(si
u −si
v −sj
u + sj
v)
 
.
(80)
Proof. This follows from a rather tedious computation, which we summarize here. First, note that
⟨wi, wj⟩= 2Ai
wAj
w
3p
p−1
X
c=0
cos

si
w + 2π ξ
pc

cos

sj
w + 2π ξ
pc

= Ai
wAj
w
3
cos(si
w −sj
w).
(81)
Next, the shift-equivariance property of the Fourier transform and Plancharel’s theorem imply that
for all i and a we have:
⟨ui, a · x⟩= 1
p⟨bui, d
a · x⟩=
r 2
3p|ˆx[ξ]|Ai
u cos

sx −si
u −2πξ a
p

,
(82)
and similarly for vi.
We can plug the above expression into the quadratic term
 ⟨ui, a · x⟩+ ⟨vi, b · x⟩
2 from Equation (73) and expand it via the goniometric identity:
(A cos(α)+B cos(β))2 = 1
2(A2+B2+A2 cos(2α)+B2 cos(2β))+AB(cos(α+β)+cos(α−β)).
(83)
We similarly expand the term
 ⟨uj, a · x⟩+ ⟨vj, b · x⟩
2. By leveraging on the goniometric identity
2 cos(α) cos(β) = cos(α −β) + cos(α + β), the product of these quadratic terms for the ith and
jth neurons expands into 16 unique terms. Since p is odd, 2ξ, 4ξ ̸= 0 (mod p), and thus, by
Equation (75), only the terms independent from both a and b do not average out. This leaves four
terms, providing the desired result.
We now provide a lower bound for the (meaningful terms of the) loss function.
Theorem F.7. We have the following lower bound:
C(Θ) −U(Θ; y) ≥−|ˆx[ξ]|2
p
.
(84)
Moreover, equality holds if, and only if, we have that PN
i=1 Ci cos(αi) = PN
i=1 Ci sin(αi) = 0 for
any choice of (Ci, αi) among
(Ai
w((Ai
u)2 + (Ai
v)2), si
w),
(Ai
w(Ai
u)2, si
w ± 2si
u),
(Ai
w(Ai
v)2, si
w ± 2si
v),
(Ai
wAi
uAi
v, si
w ± (si
u −si
v)),
(Ai
wAi
uAi
v, si
w + si
u + si
v),
(85)
and, moreover, PN
i=1 Ai
wAi
uAi
v sin(si
w + sx −si
u −si
v) = 0 and PN
i=1 Ai
wAi
uAi
v cos(si
w + sx −
si
u −si
v) = √54p/|ˆx[ξ]|.
Proof. Given amplitudes C1, . . . , CN and angles α1, . . . , αN, consider the goniometric identity
N
X
i,j=1
CiCj cos(αi −αj) =
 N
X
i=1
Ci cos(αi)
!2
+
 N
X
i=1
Ci sin(αi)
!2
.
(86)
37


=== Page 38 ===
By expanding each product of two cosines in Equation (80) into a sum of two cosines, we can apply
the above identity to each summand by choosing αi ∈{si
w, si
w ±2si
u, si
w ±2si
v, si
w ±(si
u −si
v), si
w +
si
u + si
v, si
w + sx −si
u −si
v}. Since both the summands in the right-hand side of Equation (86) are
positive, we conclude that:
C(Θ) ≥|ˆx[ξ]|4
54p2
 N
X
i=1
Ai
wAi
uAi
v cos(si
w + sx −si
u −si
v)
!2
,
(87)
with equality holding only if Equation (85) is satisfied. Therefore, using Equation (74), we deduce:
C(Θ) −U(Θ; y) ≥|ˆx[ξ]|4
54p2
 N
X
i=1
Ai
wAi
uAi
v cos(si
w + sx −si
u −si
v)
!2
−
r
2
27p3 |ˆx[ξ]|3
N
X
i=1
Ai
wAi
uAi
v cos(si
w + sx −si
u −si
v)
=
 
|ˆx[ξ]|2
√
54p
N
X
i=1
Ai
wAi
uAi
v cos(si
w + sx −si
u −si
v) −|ˆx[ξ]|
√p
!2
−|ˆx[ξ]|2
p
≥−|ˆx[ξ]|2
p
,
(88)
with equality only when (|ˆx[ξ]|2/(
√
54p)) PN
i=1 Ai
wAi
uAi
v cos(si
w + sx −si
u −si
v) = |ˆx[ξ]|/√p,
which concludes the proof.
For N ≥6, the lower bound from Equation (84) is tight. For even N, minimizers can be constructed
by setting si
w + sx = si
u + si
v =
2πi
N , si
u −si
v ∈{0, π} alternating (depending on N), and
Ai
wAi
uAi
v = √54p/(N|ˆx[ξ]|). For N odd, a similar construction holds.
By combining Theorem F.7 and Equation (72), we deduce the following tight lower bound on the
loss function:
L(Θ) ≥1
2

∥x∥2 −⟨x, 1⟩2
p
−2|ˆx[ξ]|2
p

.
(89)
F.3
Utility update
Next, we consider the utility for a new neuron after some group of N ≥6 neurons has undergone
the previous steps AGF. Suppose that Θ∗= (ui
∗, vi
∗, wi
∗)N
i=1 are parameters minimizing the loss as in
Section F.2. We start by describing the function computed by the network with parameters Θ∗.
Lemma F.8. For all 0 ≤a, b < p, the network satisfies:
f(a · x, b · x; Θ∗) = 2|ˆx[ξ]|
p
(a + b) · χξ,
(90)
where χξ is defined as χξ[c] = cos

2π ξ
pc + sx

.
Proof. The N neurons compute the function
f(a · x, b · x; Θ∗)[c] =
N
X
i=1
 ⟨ui
∗, a · x⟩+ ⟨vi
∗, b · x⟩
2 wi
∗[c].
(91)
The equation above can be expanded via a computation analogous to the one in the proof of Lemma
F.6. The conditions on minimizers from Theorem F.7 imply that the only non-vanishing term is of the
38


=== Page 39 ===
form:
r
2
27p3 |ˆx[ξ]|2
N
X
i=1
Ai
wAi
uAi
v cos

2sx + si
w −si
u −si
v + 2π ξ
p(c −a −b)

=
r
2
27p3 cos

sx + 2π ξ
p(c −a −b)
 N
X
i=1
Ai
wAi
uAi
v cos(si
w + sx −si
u −si
v)
|
{z
}
√54p/|ˆx[ξ]|
=2|ˆx[ξ]|
p
cos

sx + 2π ξ
p(c −a −b)

,
(92)
where in the first identity we used the fact that PN
i=1 Ai
wAi
uAi
v sin(si
w + sx −si
u −si
v) = 0. This
concludes the proof.
Finally, we compute the utility function with the updated residual r(a · x, b · x) = (a + b) · x −
(1/p)⟨x, 1⟩1 −f(a · x, b · x; Θ∗), which is maximized by new neurons in the second iteration of
AGF.
Theorem F.9. The utility function with the updated residual for a neuron with parameters θ =
(u, v, w) is:
U(θ; r) = 2
p3
X
k∈[p]\{0,±ξ}
|ˆx[k]|2ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ].
(93)
Proof. The new utility function is
U(θ; r) = U(θ; y) −1
p2
p−1
X
a,b=0
D
(⟨u, a · x⟩+ ⟨v, b · x⟩)2 w, f(a · x, b · x; Θ∗)
E
|
{z
}
∆(θ)
(94)
We wish to express ∆in the frequency domain. By plugging in Equation (90), and due to the cyclic
structure of χξ, the squared terms average out:
∆(θ) =
p−1
X
a,b=0
 ⟨u, a · x⟩2 + 2⟨u, a · x⟩⟨v, b · x⟩+ ⟨v, b · x⟩2
⟨w, f(a · x, b · x; Θ∗)⟩
(95)
= 2
p−1
X
a,b=0
⟨u, a · x⟩⟨v, b · x⟩⟨w, f(a · x, b · x; Θ∗)⟩.
(96)
Now, by reasoning analogously to the proof of Lemma F.1, we obtain:
∆(θ) = 4|ˆx[ξ]|
p
p−1
X
a,b=0
(x ⋆u)[a] (x ⋆y)[b] (χξ ⋆w)[a + b]
= 4|ˆx[ξ]|
p2
p−1
X
k=0
\
(x ⋆u)[k] \
(x ⋆v)[k]
\
(χξ ⋆w)[k]
= 2|ˆx[ξ]|
p

ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ] + ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ]

,
(97)
where in the last equality we used the fact that c
χξ[k] = p
2e±isx if k = ±ξ and 0 otherwise. By
subtracting the above expression to the expression for the initial utility from Lemma F.2, we obtain
the desired result.
In summary, after an iteration of AGF, the utility for a new neuron has the same form as the
initial one, but with the summand corresponding to the dominant (conjugate) frequencies ±ξ of x
removed. Consequently, by the same reasoning as in Section F.1, we conclude that during the utility
39


=== Page 40 ===
maximization phase of the second iteration of AGF, some new group of dormant neurons aligns with
the harmonic whose frequency has the second largest magnitude in ˆx.
A subtlety arises during the cost minimization phase of the second iteration, as the neurons that
aligned during the first phase are still involved in the optimization process. However, note that in
the loss component C(Θ) (Equation (73)), the terms of the form ⟨wi, wj⟩vanish when i and j are
indices of neurons aligned with harmonics of different frequencies. Therefore, during the second cost
minimization phase, the loss L(Θ) splits into the sum of two losses, corresponding to the neurons
aligned during the first and second iteration of AGF, respectively. The neurons from the first phase are
already at a critical point of their respective loss term, thus the second group of neurons is optimized
independently, via the same arguments as in Section F.1. This scenario is analogous to the one
discussed in Section E.2 for linear transformers (cf. Lemma E.2). In conclusion, after the second
iteration of AGF, the new utility will again have the same form as the initial one, but with the two
(conjugate pairs of) frequencies removed. This argument iterates recursively, until either all the
frequencies are exhausted, or all the H neurons have become active. The quantities in Equation (9)
can be derived for the successive iterations of AGF analogously to the previous sections. Lastly, the
estimate on jump times is obtained by the same argument as in Section E.3.
40


=== Page 41 ===
NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract describes the contributions on the high level, while the introduction
contains a precise statement of contributions, with references to the sections where such
contributions are discussed.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The main limitation of our work is that our theory is developed for two-layer
neural networks. We highlight this limitation in the title, abstract, and introduction. When
introducing our framework in Section 2 we clearly explain the assumptions used to develop
our algorithm. At the end of the paper in Section 6 we discuss additional limitations of our
work and how future work could address them.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
41


=== Page 42 ===
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory assumptions and proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All theorems and formal statements are stated either in the main body, or in
the appendix, with the full set of assumptions and clearly defined notation. All the formal
proofs are provided in the appendix with intuition discussed in the main.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental result reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide a detailed description of our algorithm AGF in Section 2.2 and
provide pseudocode in Figure 2. In each setting where we applied AGF we fully describe the
experimental setup, including necessary assumptions on initialization and hyperparameters,
such that a reader could faithfully reconstruct our experiments. Every figure clearly discusses
what quantities from the experiments are being visualized.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
42


=== Page 43 ===
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: In our supplementary material we enclose a set of notebooks that can be run to
generate all figures and experiments discussed in the paper. In these notebooks we have also
implemented a version of our main algorithm AGF such that others could adapt our code to
run in settings not discussed in this work.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental setting/details
43


=== Page 44 ===
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The primary purpose of our experiment is to describe the feature learning
dynamics of gradient flow. Thus, our experiments do not discuss train or test splits, rather
they show faithful reconstruction of gradient flow dynamics from our proposed theory of
AGF. As mentioned above, we provide all the necessary hyperparameters (learning rate and
initialization scheme) to reconstruct the figures introduced in this work.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment statistical significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The experiments we introduce are for gradient flow with complete access to
the training data. Thus, in the figures we present there should be no variability between runs
with the same initialization as each run is essentially a solution to an ODE up to numerical
precision. Our provided code sets a random seed for each experiment such that a reader can
exactly reproduce each figure introduced in our work.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments compute resources
44


=== Page 45 ===
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: All experiments in this work involve shallow networks with small constructed
datasets that can be run locally. Our provided code describe the necessary packages needed
to run our code, but specialized computer resources are not needed.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code of ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We have reviewed the Code of Ethics, and believe that the research in the
paper conforms to it in every aspect (e.g., complete anonymity).
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper discusses general theoretical properties of neural networks. As
such, we do not see any potential societal impact of the research presented here.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
45


=== Page 46 ===
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not involve any model or dataset with a high risk for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use any existing asset.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
46


=== Page 47 ===
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not introduce any new asset.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and research with human subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional review board (IRB) approvals or equivalent for research with human
subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
47


=== Page 48 ===
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
16. Declaration of LLM usage
Question: Does the paper describe the usage of LLMs if it is an important, original, or
non-standard component of the core methods in this research? Note that if the LLM is used
only for writing, editing, or formatting purposes and does not impact the core methodology,
scientific rigorousness, or originality of the research, declaration is not required.
Answer: [NA]
Justification: No LLM was used in the development of the core results of the paper. LLMs
were only used for writing and editing purposes.
Guidelines:
• The answer NA means that the core method development in this research does not
involve LLMs as any important, original, or non-standard components.
• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)
for what should or should not be described.
48


