{
  "filename": "115627_Alternating Gradient Flows_ A Theory of Feature Learning in Two-layer Neural Networks.pdf",
  "total_pages": 48,
  "full_text": "Alternating Gradient Flows: A Theory of\nFeature Learning in Two-layer Neural Networks\nDaniel Kunin†\nStanford University\nGiovanni Luca Marchetti†\nKTH\nFeng Chen\nStanford University\nDhruva Karkada\nUC Berkeley\nJames B. Simon\nUC Berkeley and Imbue\nMichael R. DeWeese\nUC Berkeley\nSurya Ganguli\nStanford University\nNina Miolane\nUC Santa Barbara\nAbstract\nWhat features neural networks learn, and how, remains an open question. In this\npaper, we introduce Alternating Gradient Flows (AGF), an algorithmic framework\nthat describes the dynamics of feature learning in two-layer networks trained from\nsmall initialization. Prior works have shown that gradient flow in this regime\nexhibits a staircase-like loss curve, alternating between plateaus where neurons\nslowly align to useful directions and sharp drops where neurons rapidly grow\nin norm. AGF approximates this behavior as an alternating two-step process:\nmaximizing a utility function over dormant neurons and minimizing a cost function\nover active ones. AGF begins with all neurons dormant. At each iteration, a\ndormant neuron activates, triggering the acquisition of a feature and a drop in the\nloss. AGF quantifies the order, timing, and magnitude of these drops, matching\nexperiments across several commonly studied architectures. We show that AGF\nunifies and extends existing saddle-to-saddle analyses in fully connected linear\nnetworks and attention-only linear transformers, where the learned features are\nsingular modes and principal components, respectively. In diagonal linear networks,\nwe prove AGF converges to gradient flow in the limit of vanishing initialization.\nApplying AGF to quadratic networks trained to perform modular addition, we give\nthe first complete characterization of the training dynamics, revealing that networks\nlearn Fourier features in decreasing order of coefficient magnitude. Altogether,\nAGF offers a promising step towards understanding feature learning in neural\nnetworks.\n1\nIntroduction\nThe impressive performance of artificial neural networks is often attributed to their capacity to learn\nfeatures from data. Yet, a precise understanding of what features they learn and how remains unclear.\nA large body of recent work has sought to understand what features neural networks learn by reverse\nengineering the computational mechanisms implemented by trained neural networks [1–6]. Known\nas mechanistic interpretability (MI), this approach involves decomposing trained networks into\ninterpretable components to uncover their internal representations and algorithmic strategies [7]. MI\nhas achieved notable successes in understanding the emergent abilities of large language models\n[8], including identifying induction heads that enable in-context learning [3] and revealing that\nsmall transformers trained on algebraic tasks use Fourier features [6, 9]. Despite these discoveries,\nmechanistic interpretability remains limited by its empirical nature, lacking a theoretical framework\nto formally define features and predict when and how they emerge [10].\n† Correspondence to: kunin@berkeley.edu and glma@kth.se.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nDiagonal \nLinear Networks\nFully-Connected \nLinear Networks\nAttention-Only \nLinear Transformers\n  Utility\nMaximization\n  Cost \nMinimization\n...\n+\n...\n...\n...\n...\n+\n...\n...\n+\n...\nLoss\nSteps\n...\n+\n...\nTwo-layer\nNonlinear Networks\nQuadratic Networks\nfor Modular Addition\n+\nLearns \nsingular \nvectors\nLearns \nregression\ncoefficient\nLearns \nFourier \nfrequency\nLearns \nprincipal \ncomponent\n...\n...\nAGF\nGD\nFigure 1: A unified theory of feature learning in two-layer networks. Left: Alternating Gradient\nFlows (AGF) models feature learning as a two-step process alternating between utility maximization\n(blue plateaus) and cost minimization (red drops), where each drop reflects learning a new feature\n(see Section 2). Middle: AGF unifies prior analyses of saddle-to-saddle dynamics (see Sections 3\nand 4). Right: AGF enables new analysis of empirical phenomena (see Section 5).\nA different line of work, rooted in deep learning theory, has sought to understand how features are\nlearned by directly studying the dynamics of the network during training. Empirical studies suggest\nthat neural networks learn simple functions first, progressively capturing more complex features\nduring training [11–14]. Termed incremental, stepwise, or saddle-to-saddle, this learning process is\nmarked by long plateaus of minimal loss change followed by rapid drops. It is conjectured to arise\nfrom networks, initialized at small-scale, jumping between saddle points of the loss landscape [15],\nwith each drop corresponding to the acquisition of a new feature [16]. This saddle-to-saddle process\nhas been explored across a range of simple settings, including diagonal linear networks [17, 18],\nfully connected linear networks [19, 15], tensor decomposition [20, 21], self-supervised learning\n[22, 23], shallow ReLU networks [24–30], attention-only transformers [31, 32], and multi-index\nmodels [33–39]. However, these analyses often rely on different simplifying assumptions about the\narchitecture (e.g., linear activations), data (e.g., orthogonal inputs), or optimizer (e.g., layer-wise\nlearning), making it difficult to establish a unified, scalable understanding of feature learning.\nIn this work, we introduce a theoretical framework that unifies existing analyses of saddle-to-saddle\ndynamics in two-layer networks under the vanishing initialization limit, precisely predicting the order,\ntiming, and magnitude of loss drops, while extending beyond classical settings to explain empirically\nobserved patterns of feature emergence. See Figure 1 for a visual overview of the paper. Our main\ncontributions are:\n1. We introduce Alternating Gradient Flows (AGF), a two-step algorithm that approximates gra-\ndient flow in two-layer networks with small initialization by alternating between maximizing\na utility over dormant neurons and minimizing a cost over active ones (Section 2).\n2. We prove that AGF converges to the dynamics of gradient flow for diagonal linear networks\nin the vanishing initialization limit (Section 3).\n3. We show how AGF unifies and generalizes existing theories of saddle-to-saddle dynamics\nacross fully connected linear networks and attention-only linear transformers (Section 4).\n4. We use AGF to predict the emergence of Fourier features in modular arithmetic tasks,\nproviding the first theoretical account of both the order in which frequencies appear and the\ndynamics that drive it (Section 5).\nOur work takes a step toward unifying deep learning theory and mechanistic interpretability, suggest-\ning that what features networks learn, and how, can be understood through optimization dynamics.\nSee Appendix A for further discussion of related theoretical approaches to studying feature learning\nand saddle-to-saddle dynamics in two-layer networks, including mean-field analysis [40–43] and\nteacher–student frameworks [44–51].\n2\nDeriving a Two-step Algorithm that Approximates Gradient Flow\nSetup and notations.\nWe consider a two-layer neural network with H hidden neurons of the form\nf(x; Θ) = PH\ni=1 aiσ(w⊺\ni gi(x)), where each hidden neuron i has learnable input weights wi ∈Rd\nand output weights ai ∈Rc, and processes a potentially neuron-specific input representation gi(x).\n2\nThe activation σ: R →R is origin-passing, i.e., σ(0) = 0, as satisfied by common functions (e.g.,\nlinear, ReLU, square, tanh). Let fi(x; θi) = aiσ(w⊺\ni gi(x)) denote the output of the ith neuron, where\nθi = (wi, ai). The parameters Θ = (θ1, . . . , θH) ∈RH×(d+c) for the entire network evolve under\ngradient flow (GF), ˙Θ = −η∇ΘL(Θ), to minimize the MSE loss L(Θ) = Ex\n\u0002 1\n2∥y(x) −f(x; Θ)∥2\u0003\n,\nwhere y: Rd →Rc is the ground truth function and the expectation is taken over the input distribution.\nThe learning rate η > 0 rescales time without affecting the trajectory. The parameter vectors for each\nneuron are initialized ai ∼N(0, α2\n2c Ic), wi ∼N(0, α2\n2d Id). In the limit α →∞, the gradient flow\ndynamics enter the kernel regime [52, 53]. When α = 1, the dynamics correspond to a mean-field or\nmaximal update parameterization [40, 54]. We study the small-scale initialization regime α ≪1,\nwhere the dynamics are conjectured to follow a saddle-to-saddle trajectory as α →0 [15, 14].\n2.1\nThe behavior of gradient flow at small-scale initialization\nBakhtin [55] showed that, under a suitable time rescaling, the trajectory of a smooth dynamical\nsystem initialized at a critical point and perturbed by vanishing noise converges to a piecewise\nconstant process that jumps instantaneously between critical points. These results suggest that, in the\nvanishing initialization limit α →0, gradient flow converges to a similar piecewise constant process,\njumping between saddle points of the loss before reaching a minimum. To approximate this behavior,\nwe first examine the structure of critical points in the loss landscape of the two-layer networks we\nstudy, and then analyze the dynamics of gradient flow near them. These dynamics reveals two distinct\nphases, separated by timescales, which in turn motivates the construction of AGF.\nCritical points are structured by partitions of neurons into dormant and active sets.\nAs α →0,\nthe initialization converges to the origin Θ = 0, which is a critical point of the loss. The origin is\none of potentially an exponential number of distinct (up to symmetry) critical points, that can be\nstructured according to partitions of the H hidden neurons. Formally, split neurons into two disjoint\nsets: dormant neurons D ⊆[H] and active neurons A = [H]\\D, with parameters ΘD = (θi)i∈D\nand ΘA = (θi)i∈A, respectively. Due to the two-layer structure of our model and the origin-passing\nactivation function, setting the parameters of the dormant neurons to zero ΘD = 0 restricts the loss\nto a function L(ΘA) of only the active neurons. Now any critical point Θ∗\nA of the restricted loss\nyields a critical point Θ = (0, Θ∗\nA) of the original loss. Thus the 2H partitions of dormant and active\nneurons structure critical ponts of the loss landscape, with the origin corresponding to the special\ncritical point with all neurons dormant.\nDynamics near a saddle point.\nConsider the dynamics near a saddle point defined by a dormant\nand active set, where we assume the active neurons are at a local minimum of their restricted loss\nL(ΘA). By construction, neurons in our two-layer network only interact during training through the\nresidual r(x; Θ) = y(x) −f(x; Θ) ∈Rc, which quantifies the difference between the predicted and\ntarget values at each data point x. Near a critical point, the residual primarily depends on the active\nneurons, as the contribution from the dormant neurons is effectively zero. Consequently, dormant\nneurons evolve independently of each other, while active neurons remain collectively equilibrated.\nTo characterize the dynamics of the dormant neurons, which will determine how the dynamics escape\nthe saddle point, we define the utility function, Ui : Rd+c →R for each i ∈D as:\nUi(θ; r) = Ex [⟨fi(x; θ), r(x)⟩] ,\nwhere r(x) = y(x) −f(x; ΘA).\n(1)\nIntuitively, the utility Ui quantifies how correlated the ith dormant neuron is with the residual r,\nwhich itself is solely determined by the active neurons. Using this function we can approximate the\ndirectional and radial dynamics for the parameters of each dormant neuron as\nd\ndt ¯θi = η∥θi∥κ−2P⊥\nθi∇θUi(¯θi; r),\nd\ndt∥θi∥= ηκ∥θi∥κ−1Ui(¯θi; r),\n(2)\nwhere ¯θi = θi/∥θi∥are the normalized parameters, P⊥\nθi =\n\u0000Id −¯θi¯θ⊺\ni\n\u0001\nis an orthogonal projection\nmatrix, and κ ≥2 is the leading order of the Taylor expansion of the utility from the origin. Note\nthat the directional dynamics scale as ∥θi∥κ−2, while the radial dynamics scale as ∥θi∥κ−1. Because\n∥θi∥≪1 for dormant neurons, the directional dynamics evolves significantly faster than the radial\ndynamics. Consequently, the contribution of a dormant neuron to the residual is suppressed, keeping\nthe residual effectively constant, and this approximation to the dynamics stays valid. This separation\nof timescales between directional and radial components has been utilized in several previous studies\nof gradient descent dynamics near the origin [56–61].\n3\nAlgorithm 1: AGF\nInitialize: D ←[H], A ←∅, S ←0 ∈RH\nwhile ∇L(ΘD) ̸= 0 do\nwhile ∀i ∈D\nSi ≤ci do\nfor i ∈D do\nd¯\nθi\ndt = ηα∥θi∥κ−2P⊥\nθi ∇U(¯θi, r)\ndSi\ndt\n= ηακU(¯θi, r)\nActivate neuron: D, A ←D \\ {i∗}, A ∪{i∗}\nwhile ∇L(ΘA) ̸= 0 do\nfor j ∈A do\ndθj\ndt\n= −∇θj L(ΘA)\nRemove collapsed neurons: D, A ←D ∪C, A \\ C\nConceptual Illustration: AGF\nNeuron 1\nNeuron 2\nNeuron 3\nActivates\nDormant neurons \nwork individually at small scales \nto maximize their utility\nSteps\nLoss\nActive neurons \nwork collectively at large scales\nto minimize the cost\nFigure 2: Alternating Gradient Flows (AGF). AGF alternates between utility maximization (blue)\nover dormant neurons and cost minimization (red) over active neurons, predicting saddle-to-saddle\ndynamics in training. Utility maximization: dormant neurons evolve independently driven by\nprojected gradient flow to maximize their utility. We keep track of the accumulated utility for each\ndormant neuron to determine the first neuron to transition to active. Cost minimization: Active\nneurons work collectively driven by gradient flow to minimize the loss until convergence. After\nconvergence, active neurons compute a new residual that determines the utility at the next iteration. It\nis possible in this step for active neurons to become dormant again (see Appendix B).\nEstimating the jump time from dormant to active.\nThe dynamical approximation in Equation (2)\nbreaks down if a dormant neuron crosses the threshold ∥θi∥= O(1), becoming active by influencing\nthe residual. We denote by i∗∈D the first dormant neuron to reach this threshold and by τi∗the\njump time at which this dormant to active transition occurs. To determine τi∗, we compute for each\ndormant neuron the earliest time τi when ∥θi(τi)∥> 1, such that i∗= arg mini∈D τi. This time τi is\ndefined implicitly in terms of the accumulated utility, the path integral Si(t) =\nR t\n0 κUi(¯θi(s); r) ds:\nτi = inf\nn\nt > 0\n\f\f\f Si(t) > ci\nη\no\n,\nwhere ci =\n\u001a−log (∥θi(0)∥)\nif κ = 2,\n−\n1\n2−κ\n\u0000∥θi(0)∥2−κ −1\n\u0001\nif κ > 2.\n(3)\nIn the vanishing initialization limit as α →0, the jump time defined in Equation (3) diverges\nas gradient flow exhibits extreme slowing near the critical point at the origin. Thus, to capture\nmeaningful dynamics in the limit, we accelerate time by setting the learning rate η = EΘ0 [ci] where\nci is the threshold defined in Equation (3) and the expectation is taken over the initialization.\n2.2\nAlternating Gradient Flow (AGF) approximates GF at small-scale initialization\nBased on the behaviors of gradient flow (GF) at small-scale initialization, we introduce Alternating\nGradient Flows (AGF) (Algorithm 1) as an approximation of the dynamics. AGF characterizes phases\nof constant loss as periods of utility maximization and sudden loss drops as cost minimization steps,\naccurately predicting both the loss levels and the timing of loss drops during training. At initialization,\nall neurons are dormant. At each iteration, a neuron leaves the dormant set and enters the active set.\nThe dormant neurons work individually to maximize their utility, and the active neurons collectively\nwork to minimize a cost (Figure 2). Specifically, each iteration in AGF is divided into two steps.\nStep 1: Utility maximization over dormant neurons.\nIn this step, only dormant neurons update\ntheir parameters. They independently maximize (locally) their utility function over the unit sphere by\nfollowing the projected gradient flow (left of Equation (2)). We keep track of the accumulated utility\nfor each neuron to determine the first neuron to transition and the jump time using Equation (3). At\nthe jump time, we record the norms and directions of the dormant neurons that have not activated, as\nthey will serve as the initialization for the utility maximization phase of the next iteration.\nStep 2: Cost minimization over active neurons.\nIn this step, all active neurons interact to minimize\n(locally) the loss, by following the negative gradient flow of L(ΘA). For previously active neurons,\nthe initialization is determined by the previous cost minimization step. For the newly activated neuron,\nthe initialization comes from the utility maximization step. It is possible in this step for active neurons\nto become dormant again if the optimization trajectory brings an active neuron near the origin (see\nAppendix B). After convergence, we compute the new residual r(x) = y(x) −f(x; ΘA), which\ndefines a new utility for the remaining dormant neurons at the next iteration.\n4\nTermination.\nWe repeat both steps, recording the corresponding sequence of jump times and loss\nlevels, until there are either no remaining dormant neurons or we are at a local minimum of the\nloss. Through this process, we have generated a precise sequence of saddle points and jump times to\ndescribe how gradient flow leaves the origin through a saddle-to-saddle process.\nWhile AGF and GF exhibit similar behaviors at small-scale initialization (α ≪1), a natural question\nis whether their trajectories converge to each other as α →0. While a general proof remains open (see\nSection 6), in the next section we present an illustrative setting where convergence can be established.\n3\nA Setting Where AGF and GF Provably Converge to Each Other\nSteps\nCoefficient\nSteps\nLoss\nFigure 3: AGF = GF as α →0 in diagonal linear networks.\nTraining loss curves for a diagonal linear network under the\nsetup described in Section 3 for various initialization values\nα. As α →0, the trajectory predicted by AGF and the\nempirics of gradient flow converge. To ensure a meaningful\ncomparison between experiments we set η = −log(α).\nDiagonal linear networks are simple\nyet insightful models for analyzing\nlearning [62–65, 17, 66]. Central to\ntheir analysis is an interpretation of\ngradient flow in function space as\nmirror descent with an initialization-\ndependent potential that promotes\nsparsity when α is small. Using this\nperspective, Pesme and Flammarion\n[18] characterized the sequence of\nsaddles and jump times for gradient\nflow in the limit α →0. We show that,\nin this limit, AGF converges to the ex-\nact same sequence, establishing a set-\nting where AGF provably converges\nto gradient flow (see Figure 3).\nWe consider a two-layer diagonal linear network trained via gradient flow to minimize the MSE\nloss L(β) =\n1\n2n∥y −β⊺X∥2, where the regression coefficients β ∈Rd are parameterized as the\nelement-wise product β = u ⊙v with u, v ∈Rd, and (X, y) ∈Rd×N × RN are the input, output\ndata. This setup fits naturally within AGF, where the ith neuron corresponds to βi with parameters\nθi = (ui, vi), data representation gi(x) = xi, and κ = 2. Following Pesme and Flammarion [18],\nwe initialize θi(0) = (\n√\n2α, 0) such that βi(0) = 0, and assume the inputs are in general position, a\nstandard technical condition in Lasso literature [67, 18] to rule out unexpected linear dependencies.\nUtility maximization.\nThe utility function for the ith neuron is Ui = −uivi∇βiL(βA), which is\nmaximized on the unit sphere u2\ni + v2\ni = 1 when sgn(uivi) = −sgn(∇βiL(βA)) and |ui| = |vi|,\nyielding a maximal utility of ¯U∗\ni =\n1\n2|∇βiL(βA)|. What makes this setting special is that not\nonly can the maximal utility be computed in closed form, but every quantity involved in utility\nmaximization—namely the accumulated utility, jump times, and directional dynamics—admits an\nexact analytical expression. The key insight is that the normalized utility ¯Ui(t) for each dormant\nneuron i evolves according to a separable Riccati ODE, interpolating from its initial value ¯Ui(0) to its\nmaximum ¯U∗\ni . As a result, we can derive an explicit formula for the normalized utility ¯Ui(t), whose\nintegral yields the accumulated utility Si(t), evolving as:\nSi(τ (k) + t) =\n1\n2ηα log cosh\n\u0010\n2ηα\n\u0010\n2U∗\ni t ±\n1\n2ηα cosh−1 exp\n\u00002ηαSi\n\u0000τ (k)\u0001\u0001\u0011\u0011\n,\n(4)\nwhere ηα = −log(\n√\n2α) is the learning rate and the unspecified sign is chosen based on sgn(uivi)\nand sgn(∇βiL(βA)), as explained in Appendix C. This expression allows us to determine the next\nneuron to activate, as the first i ∈D for which Si = 1, from which the jump time can be computed.\nCost minimization.\nActive neurons represent non-zero regression coefficients of β (see Figure 3\nright). During the cost minimization step, the active neurons work to collectively minimize the loss.\nWhen a neuron activates, it does so with a certain sign sgn(uivi) = −sgn(∇βiL(βA)). If, during\nthe cost minimization phase, a neuron changes sign, then it can do so only by returning to dormancy\nfirst. This is due to the fact that throughout the gradient flow dynamics, the quantity u2\ni −v2\ni = 2α2\nis conserved and thus, in order to flip the sign of the product uivi, the parameters must pass through\ntheir initialization. As a result, the critical point reached during the cost minimization step is the\n5\nunique solution to the constrained optimization problem,\nβA = arg min\nβ∈Rd\nL(β)\nsubject to\n\u001aβi = 0\nif i /∈A,\nβi · sgn(uivi) ≥0\nif i ∈A.\n(5)\nAll coordinates where (βA)i = 0 are dormant at the next step of AGF.\nAGF in action: sparse regression.\nWe now connect AGF to the algorithm proposed by Pesme\nand Flammarion [18], which captures the limiting behavior of gradient flow under vanishing\ninitialization.\nTheir algorithm tracks, for each coefficient of β, an integral of the gradient,\nSi(t) = −\nR t\n0 ∇βiL(β(˜tα(s))) ds, with a time rescaling ˜tα(s) = −log(α)s. They show that in\nthe limit α →0, this quantity is piecewise linear and remains bounded in [−1, 1]. Using these\nproperties, each step of their algorithm determines a new coordinate to activate (the first for which\nSi(t) = ±1), adds it to an active set, then solves a constrained optimization over this set, iterating\nuntil convergence. Despite differing formulations, this process is identical to AGF in the limit α →0:\nTheorem 3.1. Let (βAGF, tAGF) and (βPF, tPF) be the sequences produced by AGF and Algorithm 1\nof Pesme and Flammarion [18], respectively. Then, (βAGF, tAGF) →(βPF, tPF) pointwise as α →0.\nThe key connection lies in the asymptotic identity log cosh(x) →|x| as |x| →∞, which implies\nthat the accumulated utility Si(t) in AGF converges to the absolute value of the integral Si(t). The\nunspecified sign in AGF corresponds to the sign of the boundary conditions in their algorithm. Thus,\nAGF converges to the same saddle-to-saddle trajectory as the algorithm of Pesme and Flammarion\n[18], and therefore to gradient flow. See Appendix C for a full derivation.\n4\nAGF Unifies Existing Analysis of Saddle-to-Saddle Dynamics\n(a) Commuting\n(b) Non-commuting\nSingular Value\nGD\nSteps\nConj. 4.1\nFigure 4: Stepwise singular value decomposition. Training\na two-layer fully connected linear network on Gaussian in-\nputs with a power-law covariance Σxx and labels y(x) = Bx\ngenerated from a random B. We show the dynamics of the\nsingular values of the network’s map AW when Σxx com-\nmutes with Σ⊺\nyxΣyx (a) and when it does not (b). Conjec-\nture 4.1 (black dashed lines) predicts the dynamics well.\nFully connected linear network.\nLinear networks have long served as\nan analytically tractable setting for\nstudying neural network learning dy-\nnamics [68–73]. Such linear networks\nexhibit highly nonlinear learning dy-\nnamics. Saxe et al. [70] demonstrated\nthat gradient flow from a task-aligned\ninitialization learns a sequential singu-\nlar value decomposition of the input-\noutput cross-covariance. This behav-\nior persists in the vanishing initial-\nization limit without task alignment\n[57, 74, 19, 20, 15], and is amplified\nby depth [75, 62]. Here, we show how\nAGF naturally recovers such greedy\nlow-rank learning.\nWe consider a fully connected two-layer linear network, f(x; θ) = AWx, with parameters W ∈\nRH×d and A ∈Rc×H. The network is trained to minimize the MSE loss with data generated\nfrom the linear map y(x) = Bx, where B ∈Rc×d is an unknown matrix. The inputs are drawn\nindependently from a Gaussian x ∼N(0, Σxx), where Σxx ∈Rd×d is the input covariance, and\nΣyx = BΣxx ∈Rc×d is the input-output cross-covariance, which we assume are full rank with\ndistinct singular values. A subtlety in applying AGF to this setting is that the standard notion of\na neuron used in Section 2 is misaligned with the geometry of the loss landscape. Due to the\nnetwork’s invariance under (W, A) 7→(GW, AG−1) for any G ∈GLH(R), critical points form\nmanifolds entangling hidden neurons, and conserved quantities under gradient flow couple their\ndynamics [76, 77]. To resolve this, we can reinterpret AGF in terms of evolving dormant and active\northogonal basis vectors instead of neurons. Each basis vector ( ˜ai, ˜wi) ∈Rc+d forms a rank-1 map\n˜ai ˜w⊺\ni ∈Rc×d such that the function computed by the network is the sum over these rank-1 maps,\nf(x; Θ) = P\ni∈[ ˜\nH] ˜ai ˜w⊺\ni x, where ˜H = min(c, H, d). From this perspective, at each iteration of AGF\nthe dormant set loses one basis vector, while the active set gains one. See Appendix D for details.\nUtility maximization.\nLet βA = P\ni∈A ˜ai ˜w⊺\ni be the function computed by the active basis vectors\nand m = |D|. The total utility over the dormant basis vectors is U = Pm\ni=1 ˜a⊺\ni ∇βL(βA) ˜wi.\n6\nMaximizing this sum while maintaining orthonormality between the basis vectors yields a Rayleigh\nquotient problem, whose solution aligns the dormant basis (˜ai, ˜wi)m\ni=1 with the top m singular modes\n(ui, vi)m\ni=1 of ∇βL(βA). Each aligned pair attains a maximum utility of U∗\ni = σi/2, where σi is the\ncorresponding singular value. The basis vector aligned with the top singular mode activates first,\nexiting the dormant set and joining the active one.\nCost minimization.\nThe cost minimization step of AGF can be recast as a reduced rank regression\nproblem, minimizing L(β) over β ∈Rc×d subject to rank(β) = k, where k = |A|. As first shown in\nIzenman [78], the global minimum for this problem is an orthogonal projection of the OLS solution\nβ∗\nA = PUkΣyxΣ−1\nxx , where PUk = UkU ⊺\nk is the projection onto Uk ∈Rc×k, the top k eigenvectors\nof ΣyxΣ−1\nxx Σ⊺\nyx. Using this solution, we can show that the next step of utility maximization will be\ncomputed with the matrix ∇βL(β∗\nA) = P⊥\nUkΣyx where P⊥\nUk = Ic −PUk.\nAGF in action: greedy low-rank learning.\nIn the vanishing initialization limit, AGF reduces to\nan iterative procedure that selects the top singular mode of ∇βL(β∗\nA), transfers this vector from the\ndormant to the active basis, then minimizes the loss with the active basis to update β∗\nA. This procedure\nis identical to the Greedy Low-Rank Learning (GLRL) algorithm by Li et al. [20] that characterizes\nthe gradient flow dynamics of two-layer matrix factorization problems with infinitesimal initialization.\nEncouraged by this connection, we make the following conjecture:\nConjecture 4.1. In the initialization limit α →0, a two-layer fully connected linear network trained\nby gradient flow, with η = −log(α), learns one rank at a time leading to the sequence\nf (k)(x) = P\ni≤k PuiΣyxΣ−1\nxx x,\nℓ(k) = 1\n2\nP\ni>k µi,\nτ (k) = P\ni≤k ∆τ (i),\n(6)\nwhere 0 ≤k ≤min(d, H, c), (ui, µi) are the eigenvectors and eigenvalues of ΣyxΣ−1\nxx Σ⊺\nyx, σ(k)\ni\nis\nthe ith singular value of P⊥\nUkΣyx, and ∆τ (i) =\n\u0010\n1 −Pi−2\nj=0 σ(j)\ni−j∆τ (j+1)\u0011\n/σ(i−1)\n1\nwith ∆τ (0) = 0.\nWhen Σxx commutes with Σ⊺\nyxΣyx, Conjecture 4.1 recovers the sequence originally proposed by\nGidel et al. [19]. Figure 4 empirically supports this conjecture. See Appendix D for details.\nGD\nAGF\nSteps\nSingular Value\nFigure 5: Stepwise principal component regres-\nsion. Training a linear transformer to learn linear\nregression in context. We show the evolution of\nsingular values of PH\ni=1 ViKiQ⊺\ni . Horizontal lines\nshow theoretical Ak and vertical dashed lines show\nlower bounds for the jump time from Equation (7)\nwith l = k −1. Dashed black lines are numerical\nAGF predictions.\nAttention-only\nlinear\ntransformer.\nPre-\ntrained large language models can learn new\ntasks from only a few examples in context,\nwithout explicit fine-tuning [79]. To understand\nthe emergence of this in-context learning ability,\nprevious empirical [80–82] and theoretical\nworks [83–88] have examined how transformers\nlearn to perform linear regression in context.\nNotably, Zhang et al. [32] showed that an\nattention-only linear transformer learns to\nimplement principal component regression\nsequentially, with each learned component\ncorresponding to a drop in the loss. We show\nthat AGF recovers their analysis (see Figure 5).\nWe consider a attention-only linear transformer\nf(X; Θ) = X+PH\ni=1 WV,iXX⊺WK,iW ⊺\nQ,iX,\nwhere X is the input sequence, WV\n∈\nRH×(d+1)×(d+1), WK, WQ ∈RH×(d+1)×R\nrepresent the value, key, and query matrices\nrespectively, and H denotes the number of at-\ntention heads. While this network uses linear\nactivations, it is cubic in its input and parameters. Also, when the rank R = 1, each attention head\nbehaves like a homogeneous neuron with κ = 3, making it compatible with the AGF framework.\nWe consider the linear regression task with input Xi = ( xi\nyi ) for i ≤N, where xi ∼N(0, Σxx),\nand yi = β⊺xi with β ∼N(0, Id). The input covariance Σxx ∈Rd×d has eigenvalues λ1 ≥· · · ≥\nλd > 0 with corresponding eigenvectors v1, . . . , vd. The final input token is XN+1 = ( xN+1\n0\n). The\nnetwork is trained by MSE loss to predict yN+1 given the entire sequence X, where the model\nprediction is taken to be ˆyN+1 = f(X; Θ)d+1,N+1. Following Zhang et al. [32], we initialize all\n7\nparameters to zero except for some slices, which we denote by V ∈RH, Q, K ∈RH×d, which are\ninitialized from N(0, α). The parameters initialized at zero will remain at zero throughout training\n[32], and the model prediction reduces to ˆyN+1 = PH\nh=1 Vh\nPN\nn=1 ynx⊺\nnKhQ⊺\nhxN+1. We defer\nderivations to Appendix E, and briefly outline how AGF predicts the saddle-to-saddle dynamics.\nUtility maximization.\nAt the kth iteration of AGF, the utility of a dormant attention head is\nUi = NViQ⊺\ni (Σ2\nxx −Pk−1\nh=1 λ2\nhvhv⊺\nh)Ki which is maximized over normalized parameters when\n¯Vi = ±1/\n√\n3 and ¯Qi, ¯Ki = ±vk/\n√\n3, where the sign is chosen such that the maximum utility is\n¯U∗= Nλ2\nk/3\n√\n3. In other words, utility maximization encourages dormant attention heads to align\ntheir key and query vectors with the dominant principal component of the input covariance not yet\ncaptured by any active head. This creates a race condition among dormant heads, where the first to\nreach the activation threshold, measured by their accumulated utility, becomes active and learns the\ncorresponding component. Assuming instantaneous alignment of the key and query vectors, we can\nlower bound the jump time for the next head to activate, as shown in Equation (7).\nCost minimization.\nDuring cost minimization, because v1, . . . , vd form an orthonormal basis, the\nupdates for each attention head are decoupled. Thus, we only need to focus on how the magnitude\nof the newly active head changes. Specifically, we determine the magnitude Ak of the newly\nlearned function component, given by fk(X; θk)d+1,N+1 = Ak\nPN\nn=1 ynx⊺\nnvkv⊺\nkxN+1. Solving\n∂L(Ak)\n∂Ak\n= 0, we find that the optimal magnitude is Ak =\n1\ntrΣxx+(N+1)λk , from which we can derive\nthe expression for the prediction and loss level after the kth iteration of AGF, as shown in Equation (7).\nAGF in action: principal component regression.\nAt each iteration, the network projects the input\nonto a newly selected principal component of Σxx and fits a linear regressor along that direction. Let\nµ(l) = maxi∈D ∥θi(τ (l))∥for l < k and ηα = α−1. This process yields the sequence,\nˆy(k)\nN+1 = Pk,N\ni,n=1\nynx⊺\nnviv⊺\ni xN+1\ntrΣxx+(N+1)λi , ℓ(k) = trΣxx\n2\n−Pk\ni=1\nNλi/2\ntrΣxx\nλi\n+N+1, τ (k) ≳τ (l) + η−1\nα\nµ(l)\n√\n3\nNλ2\nk ,\n(7)\nwhich recovers the results derived in Zhang et al. [32] and provides an excellent approximation to the\ngradient flow dynamics (see Figure 5).\n5\nAGF Predicts the Emergence of Fourier Features in Modular Addition\nIn previous sections, we showed how AGF unifies prior analyses of feature learning in linear networks.\nWe now consider a novel nonlinear setting: a two-layer quadratic network trained on modular addition.\nOriginally proposed as a minimal setting to explore emergent behavior [89], modular addition has\nsince become a foundational setup for mechanistic interpretability. Prior work has shown that\nnetworks trained on this task develop internal Fourier representations and use trigonometric identities\nto implement addition as rotations on the circle [6, 90, 91]. Similar Fourier features have been\nobserved in networks trained on group composition tasks [9], and in large pre-trained language\nmodels performing arithmetic [92, 93]. Despite extensive empirical evidence for the universality\nof Fourier features in deep learning, a precise theoretical explanation of their emergence remains\nopen. Recent work has linked this phenomenon to the average gradient outer product framework\n[94], the relationship between symmetry and irreducible representations [95], implicit maximum\nmargin biases of gradient descent [96], and the algebraic structure of the solution space coupled\nwith a simplicity bias [97]. Here, we leverage AGF to unveil saddle-to-saddle dynamics, where each\nsaddle corresponds to the emergence of a Fourier feature in the network (see Figure 6).\nWe consider a setting similar to [90, 96, 97], but with more general input encodings. Given p ∈N, the\nground-truth function is y: Z/p×Z/p →Z/p, (a, b) 7→a+b mod p, where Z/p = {0, . . . , p−1} is\nthe (additive) Abelian group of integers modulo p. Given a vector x ∈Rp, we consider the encoding\nZ/p →Rp, a 7→a · x, where · denotes the action of the cyclic permutation group of order p on\nx (which cyclically permutes the components of x). The modular addition task with this encoding\nconsists of mapping (a · x, b · x) to (a + b) · x. For x = e0, our encoding coincides with the one-hot\nencoding a 7→ea studied in prior work. We consider a two-layer neural network with quadratic\nactivation function σ(z) = z2, where each neuron is parameterized by θi = (ui, vi, wi) ∈R3p and\ncomputes the function fi(a · x, b · x; θi) = (⟨ui, a · x⟩+ ⟨vi, b · x⟩)2wi. The network is composed of\nthe sum of H neurons and is trained over the entire dataset of p2 pairs (a, b). We make some technical\nassumptions in order to simplify the analysis. First, since the network contains no bias term, we\nassume that the data is centered, i.e., we subtract (⟨x, 1⟩/p)1 from x. Second, in order to encourage\n8\n(b) Predictions\n(a) Power\n(c) Weights\n(d) Frequency+Phases\nSteps\nAGF\nFrequency\nFigure 6: Stepwise Fourier decomposition. We train a two-layer quadratic network on a modular\naddition task with p = 20, using a template vector x ∈Rp composed of three cosine waves:\nˆx[1] = 10, ˆx[3] = 5, and ˆx[5] = 2.5. (a) Output power spectrum over time. The network learns the\ntask by sequentially decomposing x into its Fourier components, acquiring dominant frequencies\nfirst. Colored solid lines are gradient descent, black dashed line is AGF run numerically from the\nsame initialization. (b) Model outputs on selected inputs at four training steps, showing progressively\naccurate reconstructions of the template. (c) Output weight vector wi for all H = 18 neurons and (d)\ntheir frequency spectra and dominant phase. Neurons are color-coded by dominant frequency. As\npredicted by the theory, the neurons group by frequency, while distributing their phase shifts.\nsequential learning of frequencies, we assume that all the non-conjugate Fourier coefficients of x\nhave distinct magnitude: |ˆx[k]| ̸= |ˆx[k′]| for k ̸= ±k′ (mod p). Third, in order to avoid dealing\nwith the Nyquist frequency k = p/2, we assume that p is odd. We now describe how AGF applies to\nthis setting (see Appendix F for proofs). Here,ˆ· denotes the Discrete Fourier Transform (DFT).\nUtility maximization.\nFirst, we show how the initial utility can be expressed entirely in the\nfrequency domain of the parameters ˆΘ. For a dormant neuron parameterized by θ = (u, v, w), the\ninitial utility is U = (2/p3) P\nk∈[p]\\{0} |ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k]. We then argue that the unit vectors\nmaximizing U align with the dominant harmonic of ˆx:\nTheorem 5.1. Let ξ be the frequency that maximizes |ˆx[k]|, k = 1, . . . , p −1, and denote by sx the\nphase of ˆx[ξ]. Then the unit vectors θ∗= (u∗, v∗, w∗) maximizing the utility function U are:\nu∗[a] = Ap cos (ωξa + su) ,\nv∗[b] = Ap cos (ωξb + sv) ,\nw∗[c] = Ap cos (ωξc + sw) ,\n(8)\nwhere a, b, c ∈[p] are indices, su, sv, sw ∈R are phase shifts satisfying su + sv ≡sw + sx\n(mod 2π), Ap =\np\n2/(3p) is the amplitude, and ωξ = 2πξ/p is the frequency. Moreover, U has no\nother local maxima and achieves a maximal value of ¯U∗=\np\n2/(27p3)|ˆx[ξ]|3.\nTherefore, after utility minimization, neurons specialize to unique frequencies. Now that we know\nthe maximal utility, we can estimate the jump time by assuming instantaneous alignment as done in\nSection 4, resulting in the lower bound shown in Equation (9).\nCost minimization.\nTo study cost minimization, we consider a regime in which a group A of\nN ≤H neurons activates simultaneously, each aligned to the harmonic of frequency ξ. While this is\na technical simplification of AGF, which activates a single neuron per iteration, it allows us to analyze\nthe collective behavior more directly. We additionally assume that once aligned, the neurons in A\nremain aligned under the gradient flow (see Appendix F.2.1 for a discussion of possible “resonant”\nescape directions). We then analyze cost minimization for a configuration ΘA = (ui, vi, wi)N\ni=1 of\naligned neurons with arbitrary amplitudes and phase shifts, and prove the following result.\nTheorem 5.2. The loss function satisfies the lower bound L(ΘA) ≥∥x∥2/2 −⟨x, 1⟩2/(2p) −\n|ˆx[ξ]|2/p, which is tight for N ≥6. When the bound is achieved, the network learns the function f(a·\nx, b·x; ΘA) = (2|ˆx[ξ]|/p) (a+b)·χξ, where χξ[c] = cos (2πξc/p + sx), which leads to the new util-\nity function for the remaining dormant neurons: U = (2/p3) P\nk∈[p]\\{0,±ξ} |ˆx[k]|2ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ].\nPut simply, the updated utility function after the first iteration of AGF has the same form as the old\none, but with the dominant frequency ξ of x removed (together with its conjugate frequency −ξ).\nTherefore, at the second iteration of AGF, another group of neurons aligns with the harmonic of\nthe second dominant frequency of x. Lastly, we argue via orthogonality that the groups of neurons\naligned with different harmonics optimize their loss functions independently, implying that at each\niteration of AGF, another group of neurons learns a new Fourier feature of x.\n9\nAGF in action: greedy Fourier decomposition.\nTaken together, we have shown that a two-layer\nquadratic network with hidden dimension H ≥3p trained to perform modular addition with a\ncentered encoding vector x ∈Rp sequentially learns frequencies ξ1, ξ2, . . . , ordered by decreasing\n|ˆx[ξ]|. After learning k ≥0 frequencies, the function, level, and next jump time are:\nf (k)(a·x, b·x) = Pk\ni=1\n2|ˆx[ξi]|\np\n(a+b)·χξi, ℓ(k) = P\ni>k\n|ˆx[ξi]|2\np\n, τ (k) ≳τ (l)+ η−1\nα\nµ(l)\n√\n3p\n3\n2\n√\n2|ˆx[ξk+1]|3 , (9)\nwhere µ(l) = maxi∈D ∥θi(τ (l))∥and learning rate ηα = α−1. Simulating AGF numerically, we find\nthis sequence closely approximates the gradient flow dynamics (see Figure 6).\nExtensions to other algebraic tasks.\nOur analysis of modular addition can naturally extend to\na broader class of algebraic problems. First, one can consider modular addition over multiple\nsummands, defined by the map y: (Z/p)k →Z/p, (ai)k\ni=1 7→a1 + · · · + ak (mod p). Using a\nhigher-degree activation function σ(x) = xk, the arguments for utility maximization should carry\nover, while the cost minimization step might be more subtle. Second, one can replace modular\nintegers with an arbitrary finite group and study a network trained to learn the group multiplication\nmap. Non-commutative groups introduce technical challenges due to the involvement of higher-\ndimensional unitary representations in their Fourier analysis. We leave a detailed analysis of these\nextensions to future work.\n6\nDiscussion, Limitations, and Future Work\nSteps\nOptimal Linear Predictor\nLoss\nWidth = 1\nWidth = 2\nWidth = 4\nWidth = 8\nWidth = 16\nWidth = 32\nFigure 7: From a staircase to a slide in a two-\nlayer ReLU network. Training loss for a two-\nlayer ReLU network on a subset of CIFAR-10\nunder varying hidden widths from an extremely\nsmall initialization. Solid lines show gradient de-\nscent dynamics and dotted lines show the sequence\nproduced by a numerical implementation of AGF\nfrom the same initialization. At small widths, gra-\ndient descent loss curves are stepwise and AGF\ntracks the jumps closely. As width increases, cost-\nminimization phases overlap and multiple dormant\nneurons activate in close succession, producing a\nsmoother slide-like loss curve; accordingly, the\ncorrespondence with AGF weakens. All experi-\nmental details are available at a Github code base.\nIn this work we introduced Alternating Gradi-\nent Flows (AGF), a framework modeling feature\nlearning in two-layer neural networks as an al-\nternating two-step process: maximizing a utility\nfunction over dormant neurons and minimizing\na cost function over active ones. We showed\nhow AGF converges to gradient flow in diag-\nonal linear networks, recovers prior saddle-to-\nsaddle analyses in linear networks, and extends\nto quadratic networks trained on modular ad-\ndition. While these findings highlight AGF’s\nutility as an ansatz for feature learning, it re-\nmains open whether its correspondence to gra-\ndient flow always holds in the vanishing ini-\ntialization limit. Proving such a conjecture is\ntheoretically challenging. Empirical validation\nis also difficult because it requires taking both\nthe initialization scale and learning rate to zero.\nMoreover, this conjecture may simply fail to\nhold in more general settings. On natural data\ntasks, loss curves are often not visibly stepwise\neven at very small initialization scales (see Fig-\nure 7), suggesting there may be limitations to\nAGF. That said, if many dormant neurons reach\ntheir activation thresholds in close succession,\ntheir cost-minimization phases could interleave,\ncausing the aggregate loss to appear smooth. Re-\ncent works reconciling the emergent capabilities\nof large language models with their neural scal-\ning laws have made similar suggestions [98–102]. However, there are feature learning regimes in\ntwo-layer networks—such as those studied in Saad and Solla [46], Goldt et al. [47], Arnaboldi et al.\n[51]—that display saddle-to-saddle behavior due to population-level transitions not captured by AGF.\nConnecting AGF to these multi-index models and teacher–student settings (see Appendix A for a\nreview) remains a key direction for future work. Lastly, the central limitation of the framework is\nits focus on two-layer networks, leaving open how it might generalize to deeper and more realistic\narchitectures. Possible extensions include leveraging recent analyses of modularity in deep networks\n[103–105] and adapting insights from the early alignment dynamics of deep networks near the\norigin [59, 106]. All together, our results suggest that AGF offers a promising step towards a deeper\nunderstanding of what features neural networks learn and how.\n10\nAcknowledgments and Disclosure of Funding\nWe thank Clémentine Dominé, Jim Halverson, Boris Hanin, Christopher Hillar, Alex Infanger, Arthur\nJacot, Mason Kamb, David Klindt, Florent Krzakala, Zhiyuan Li, Sophia Sanborn, Nati Srebro,\nand Yedi Zhang for helpful discussions. Daniel thanks the Open Philanthropy AI Fellowship for\nsupport. Giovanni is partially supported by the Wallenberg AI, Autonomous Systems and Software\nProgram (WASP) funded by the Knut and Alice Wallenberg Foundation. Surya and Feng are partially\nsupported by NSF grant 1845166. Surya thanks the Simons Foundation, NTT Research, an NSF\nCAREER Award, and a Schmidt Science Polymath award for support. Nina is partially supported by\nNSF grant 2313150 and the NSF grant 240158. This work was supported in part by the U.S. Army\nResearch Laboratory and the U.S. Army Research Office under Contract No. W911NF-20-1-0151.\nAuthor Contributions\nDaniel, Nina, Giovanni, and James are primarily responsible for developing the AGF framework in\nSection 2. Daniel is primarily responsible for the analysis of diagonal and fully-connected linear\nnetworks in Sections 3 and 4. Feng is primarily responsible for the analysis of the attention-only\nlinear transformer in Section 4. Daniel and Giovanni are primarily responsible for the analysis of the\nmodular addition task in Section 5. Dhruva is primarily responsible for an implementation of AGF\nused in the empirics. All authors contributed to the writing of the manuscript.\nReferences\n[1] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\nZoom in: An introduction to circuits. Distill, 5(3):e00024–001, 2020.\n[2] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for\ntransformer circuits. Transformer Circuits Thread, 1(1):12, 2021.\n[3] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and\ninduction heads. arXiv preprint arXiv:2209.11895, 2022.\n[4] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse\nautoencoders find highly interpretable features in language models.\narXiv preprint\narXiv:2309.08600, 2023.\n[5] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-\nerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,\nShauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex\nTamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter,\nTom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language\nmodels with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-\ncircuits.pub/2023/monosemantic-features/index.html.\n[6] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress\nmeasures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217,\n2023.\n[7] Leonard Bereska and Efstratios Gavves. Mechanistic interpretability for ai safety–a review.\narXiv preprint arXiv:2404.14082, 2024.\n[8] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682, 2022.\n[9] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse\nengineering how networks learn group operations. In International Conference on Machine\nLearning, pages 6243–6267. PMLR, 2023.\n[10] Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas\nGoldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, et al. Open problems\nin mechanistic interpretability. arXiv preprint arXiv:2501.16496, 2025.\n11\n[11] Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,\nMaxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.\nA closer look at memorization in deep networks. In International conference on machine\nlearning, pages 233–242. PMLR, 2017.\n[12] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz\nBarak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity.\nAdvances in neural information processing systems, 32, 2019.\n[13] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.\nHidden progress in deep learning: Sgd learns parities near the computational limit. Advances\nin Neural Information Processing Systems, 35:21750–21764, 2022.\n[14] Alexander Atanasov, Alexandru Meterez, James B Simon, and Cengiz Pehlevan. The optimiza-\ntion landscape of sgd across the feature learning strength. arXiv preprint arXiv:2410.04642,\n2024.\n[15] Arthur Jacot, François Ged, Berfin ¸Sim¸sek, Clément Hongler, and Franck Gabriel. Saddle-to-\nsaddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity.\narXiv preprint arXiv:2106.15933, 2021.\n[16] Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic\ndevelopment in deep neural networks. Proceedings of the National Academy of Sciences, 116\n(23):11537–11546, 2019.\n[17] Raphaël Berthier. Incremental learning in diagonal linear networks. Journal of Machine\nLearning Research, 24(171):1–26, 2023.\n[18] Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks.\nAdvances in Neural Information Processing Systems, 36:7475–7505, 2023.\n[19] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete\ngradient dynamics in linear neural networks. Advances in Neural Information Processing\nSystems, 32, 2019.\n[20] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient\ndescent for matrix factorization: Greedy low-rank learning. arXiv preprint arXiv:2012.09839,\n2020.\n[21] Rong Ge, Yunwei Ren, Xiang Wang, and Mo Zhou. Understanding deflation process in\nover-parametrized tensor decomposition. Advances in Neural Information Processing Systems,\n34:1299–1311, 2021.\n[22] James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and Joshua\nAlbrecht. On the stepwise nature of self-supervised learning. In International Conference on\nMachine Learning, pages 31852–31876. PMLR, 2023.\n[23] Dhruva Karkada, James B Simon, Yasaman Bahri, and Michael R DeWeese. Solvable dynamics\nof self-supervised word embeddings and the emergence of analogical reasoning. arXiv preprint\narXiv:2502.09863, 2025.\n[24] Mary Phuong and Christoph H Lampert. The inductive bias of relu networks on orthogonally\nseparable data. In International Conference on Learning Representations, 2020.\n[25] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer\nnets: Margin maximization and simplicity bias. Advances in Neural Information Processing\nSystems, 34, 2021.\n[26] Mingze Wang and Chao Ma. Early stage convergence and global convergence of training\nmildly parameterized neural networks. Advances in Neural Information Processing Systems,\n35:743–756, 2022.\n[27] Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of\nshallow relu networks for square loss and orthogonal inputs. Advances in Neural Information\nProcessing Systems, 35:20105–20118, 2022.\n12\n[28] Hancheng Min, René Vidal, and Enrique Mallada. Early neuron alignment in two-layer relu\nnetworks with small initialization. arXiv preprint arXiv:2307.12851, 2023.\n[29] Margalit Glasgow. Sgd finds then tunes features in two-layer neural networks with near-optimal\nsample complexity: A case study in the xor problem. arXiv preprint arXiv:2309.15111, 2023.\n[30] Mingze Wang and Chao Ma. Understanding multi-phase optimization dynamics and rich\nnonlinear behaviors of relu networks. Advances in Neural Information Processing Systems, 36,\n2024.\n[31] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind.\nTransformers learn through gradual rank increase. Advances in Neural Information Processing\nSystems, 36:24519–24551, 2023.\n[32] Yedi Zhang, Aaditya K Singh, Peter E Latham, and Andrew Saxe. Training dynamics of\nin-context learning in linear attention. arXiv preprint arXiv:2501.16265, 2025.\n[33] Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj.\nThe staircase property: How hierarchical structure can guide deep learning. Advances in\nNeural Information Processing Systems, 34:26989–27002, 2021.\n[34] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase\nproperty: a necessary and nearly sufficient condition for sgd learning of sparse functions on\ntwo-layer neural networks. In Conference on Learning Theory, pages 4782–4887. PMLR,\n2022.\n[35] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural\nnetworks: leap complexity and saddle-to-saddle dynamics.\nIn The Thirty Sixth Annual\nConference on Learning Theory, pages 2552–2623. PMLR, 2023.\n[36] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index\nmodels with gradient flow. arXiv preprint arXiv:2310.19793, 2023.\n[37] Berfin Simsek, Amire Bendjeddou, and Daniel Hsu. Learning gaussian multi-index mod-\nels with gradient flow: Time complexity and directional convergence.\narXiv preprint\narXiv:2411.08798, 2024.\n[38] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborová, and Florent\nKrzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking\nthe curse of information and leap exponents. arXiv preprint arXiv:2402.03220, 2024.\n[39] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita\niuvant: Data repetition allows sgd to learn high-dimensional multi-index functions. arXiv\npreprint arXiv:2405.15459, 2024.\n[40] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape\nof two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):\nE7665–E7671, 2018.\n[41] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-\nparameterized models using optimal transport. Advances in neural information processing\nsystems, 31, 2018.\n[42] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A\nlaw of large numbers. SIAM Journal on Applied Mathematics, 80(2):725–752, 2020.\n[43] Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural net-\nworks: An interacting particle system approach. Communications on Pure and Applied\nMathematics, 75(9):1889–1935, 2022.\n[44] David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review\nE, 52(4):4225, 1995.\n[45] David Saad and Sara A Solla. Exact solution for on-line learning in multilayer neural networks.\nPhysical Review Letters, 74(21):4337, 1995.\n13\n[46] David Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer\nneural networks. Advances in neural information processing systems, 8, 1995.\n[47] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborová.\nDynamics of stochastic gradient descent for two-layer neural networks in the teacher-student\nsetup. Advances in neural information processing systems, 32, 2019.\n[48] Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová.\nPhase diagram of stochastic gradient descent in high-dimensional two-layer neural networks.\nAdvances in Neural Information Processing Systems, 35:23244–23255, 2022.\n[49] Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborová. Optimization and\ngeneralization of shallow neural networks with quadratic activation functions. Advances in\nNeural Information Processing Systems, 33:13445–13455, 2020.\n[50] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,\nand Lenka Zdeborová. Learning curves of generic features maps for realistic datasets with a\nteacher-student model. Advances in Neural Information Processing Systems, 34:18137–18151,\n2021.\n[51] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro.\nFrom high-\ndimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd\nin two-layers networks. In The Thirty Sixth Annual Conference on Learning Theory, pages\n1199–1227. PMLR, 2023.\n[52] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. Advances in neural information processing systems, 31,\n2018.\n[53] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-\nming. Advances in neural information processing systems, 32, 2019.\n[54] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\narXiv:2011.14522, 2020.\n[55] Yuri Bakhtin. Noisy heteroclinic networks. Probability theory and related fields, 150:1–42,\n2011.\n[56] Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu\nnetwork features. arXiv preprint arXiv:1803.08367, 2018.\n[57] Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners:\nThe silent alignment effect. In International Conference on Learning Representations, 2021.\n[58] Akshay Kumar and Jarvis Haupt. Directional convergence near small initializations and\nsaddles in two-homogeneous neural networks. arXiv preprint arXiv:2402.09226, 2024.\n[59] Akshay Kumar and Jarvis Haupt. Early directional convergence in deep homogeneous neural\nnetworks for small initializations. arXiv preprint arXiv:2403.08121, 2024.\n[60] Raphaël Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers\nneural networks. Foundations of Computational Mathematics, pages 1–84, 2024.\n[61] Daniel Kunin, Allan Raventós, Clémentine Dominé, Feng Chen, David Klindt, Andrew Saxe,\nand Surya Ganguli. Get rich quick: exact solutions reveal how unbalanced initializations\npromote rapid feature learning. arXiv preprint arXiv:2406.06158, 2024.\n[62] Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How\nincremental learning drives generalization. arXiv preprint arXiv:1909.12051, 2019.\n[63] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay\nGolan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized\nmodels. In Conference on Learning Theory, pages 3635–3673. PMLR, 2020.\n14\n[64] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal\nlinear networks: a provable benefit of stochasticity. Advances in Neural Information Processing\nSystems, 34:29218–29230, 2021.\n[65] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (s) gd over diagonal\nlinear networks: Implicit regularisation, large stepsizes and edge of stability. arXiv preprint\narXiv:2302.08982, 2023.\n[66] Hristo Papazov, Scott Pesme, and Nicolas Flammarion. Leveraging continuous time to\nunderstand momentum when training diagonal linear networks. In International Conference\non Artificial Intelligence and Statistics, pages 3556–3564. PMLR, 2024.\n[67] Ryan J Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 2013.\n[68] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning\nfrom examples without local minima. Neural networks, 2(1):53–58, 1989.\n[69] Kenji Fukumizu. Effect of batch learning in multilayer neural networks. Gen, 1(04):1E–03,\n1998.\n[70] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear\ndynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.\n[71] Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and\ntransfer learning in deep linear networks. arXiv preprint arXiv:1809.10374, 2018.\n[72] Clémentine CJ Dominé, Nicolas Anguita, Alexandra M Proca, Lukas Braun, Daniel Kunin,\nPedro AM Mediano, and Andrew M Saxe. From lazy to rich: Exact learning dynamics in deep\nlinear networks. arXiv preprint arXiv:2409.14623, 2024.\n[73] Javan Tahir, Surya Ganguli, and Grant M Rotskoff. Features are fate: a theory of transfer\nlearning in high-dimensional regression. arXiv preprint arXiv:2410.08194, 2024.\n[74] Dominik Stöger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral\nlearning: Optimization and generalization guarantees for overparameterized low-rank matrix\nreconstruction. Advances in Neural Information Processing Systems, 34:23831–23843, 2021.\n[75] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix\nfactorization. Advances in Neural Information Processing Systems, 32, 2019.\n[76] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homoge-\nneous models: Layers are automatically balanced. Advances in Neural Information Processing\nSystems, 31, 2018.\n[77] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.\nNeural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv\npreprint arXiv:2012.04728, 2020.\n[78] Alan Julian Izenman. Reduced-rank regression for the multivariate linear model. Journal of\nmultivariate analysis, 5(2):248–264, 1975.\n[79] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information processing systems, 33:1877–\n1901, 2020.\n[80] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers\nlearn in-context? a case study of simple function classes. Advances in Neural Information\nProcessing Systems, 35:30583–30598, 2022.\n[81] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Trans-\nformers as algorithms: Generalization and stability in in-context learning. In International\nconference on machine learning, pages 19565–19594. PMLR, 2023.\n15\n[82] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity\nand the emergence of non-bayesian in-context learning for regression. Advances in neural\ninformation processing systems, 36:14228–14246, 2023.\n[83] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What\nlearning algorithm is in-context learning? investigations with linear models. arXiv preprint\narXiv:2211.15661, 2022.\n[84] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L\nBartlett. How many pretraining tasks are needed for in-context learning of linear regression?\narXiv preprint arXiv:2310.08391, 2023.\n[85] Kwangjun Ahn, Sébastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang.\nLearning threshold neurons via the\" edge of stability\". arXiv preprint arXiv:2212.07469, 2022.\n[86] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander\nMordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by\ngradient descent. In International Conference on Machine Learning, pages 35151–35174.\nPMLR, 2023.\n[87] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to\nimplement preconditioned gradient descent for in-context learning. Advances in Neural\nInformation Processing Systems, 36:45614–45650, 2023.\n[88] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models\nin-context. Journal of Machine Learning Research, 25(49):1–55, 2024.\n[89] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.\nGrokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint\narXiv:2201.02177, 2022.\n[90] Andrey Gromov. Grokking modular arithmetic. arXiv preprint arXiv:2301.02679, 2023.\n[91] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza:\nTwo stories in mechanistic explanation of neural networks. Advances in Neural Information\nProcessing Systems, 36, 2024.\n[92] Tianyi Zhou, Deqing Fu, Vatsal Sharan, and Robin Jia. Pre-trained large language models use\nfourier features to compute addition. arXiv preprint arXiv:2406.03445, 2024.\n[93] Subhash Kantamneni and Max Tegmark. Language models use trigonometry to do addition.\narXiv preprint arXiv:2502.00873, 2025.\n[94] Neil Mallinar, Daniel Beaglehole, Libin Zhu, Adityanarayanan Radhakrishnan, Parthe Pandit,\nand Mikhail Belkin. Emergence in non-neural models: grokking modular arithmetic via\naverage gradient outer product. arXiv preprint arXiv:2407.20199, 2024.\n[95] Giovanni Luca Marchetti, Christopher J Hillar, Danica Kragic, and Sophia Sanborn. Harmonics\nof learning: Universal fourier features emerge in invariant networks. In The Thirty Seventh\nAnnual Conference on Learning Theory, pages 3775–3797. PMLR, 2024.\n[96] Depen Morwani, Benjamin L Edelman, Costin-Andrei Oncescu, Rosie Zhao, and Sham M\nKakade. Feature emergence via margin maximization: case studies in algebraic tasks. In The\nTwelfth International Conference on Learning Representations, 2023.\n[97] Yuandong Tian. Composing global optimizers to reasoning tasks via algebraic objects in\nneural nets. arXiv preprint arXiv:2410.01779, 2024.\n[98] Yuki Yoshida and Masato Okada. Data-dependence of plateau phenomenon in learning with\nneural network—statistical mechanical analysis. Advances in Neural Information Processing\nSystems, 32, 2019.\n[99] Eric Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural\nscaling. Advances in Neural Information Processing Systems, 36:28699–28722, 2023.\n16\n[100] Yoonsoo Nam, Nayara Fonseca, Seok Hyeong Lee, Chris Mingard, and Ard A Louis. An\nexactly solvable model for emergence and scaling laws in the multitask sparse parity problem.\narXiv preprint arXiv:2404.17563, 2024.\n[101] Yunwei Ren, Eshaan Nichani, Denny Wu, and Jason D Lee. Emergence and scaling laws in\nsgd learning of shallow neural networks. arXiv preprint arXiv:2504.19983, 2025.\n[102] Gérard Ben Arous, Murat A Erdogdu, N Mert Vural, and Denny Wu. Learning quadratic\nneural networks in high dimensions: Sgd dynamics and scaling laws.\narXiv preprint\narXiv:2508.03688, 2025.\n[103] Andrew Saxe, Shagun Sodhani, and Sam Jay Lewallen. The neural race reduction: Dynamics\nof abstraction in gated networks. In International Conference on Machine Learning, pages\n19287–19309. PMLR, 2022.\n[104] Devon Jarvis, Richard Klein, Benjamin Rosman, and Andrew M Saxe. Make haste slowly:\nA theory of emergent structured mixed selectivity in feature learning relu networks. arXiv\npreprint arXiv:2503.06181, 2025.\n[105] Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian\nChen, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael\nSklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas\nHenighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam\nZimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. Circuit tracing:\nRevealing computational graphs in language models. Transformer Circuits Thread, 2025. URL\nhttps://transformer-circuits.pub/2025/attribution-graphs/methods.html.\n[106] Akshay Kumar and Jarvis Haupt. Towards understanding gradient flow dynamics of homoge-\nneous neural networks beyond the origin. arXiv preprint arXiv:2502.15952, 2025.\n[107] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick\nRyder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466,\n2022.\n[108] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evo-\nlution in wide neural networks. Advances in Neural Information Processing Systems, 35:\n32240–32256, 2022.\n[109] Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, Lenka Zdeborová, et al.\nThe committee machine: Computational to statistical gaps in learning a two-layers neural\nnetwork. Advances in Neural Information Processing Systems, 31, 2018.\n[110] Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal\nerrors and phase transitions in high-dimensional generalized linear models. Proceedings of the\nNational Academy of Sciences, 116(12):5451–5460, 2019.\n[111] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn represen-\ntations with gradient descent. In Conference on Learning Theory, pages 5413–5452. PMLR,\n2022.\n[112] Emanuele Troiani, Yatin Dandi, Leonardo Defilippis, Lenka Zdeborová, Bruno Loureiro, and\nFlorent Krzakala. Fundamental computational limits of weak learnability in high-dimensional\nmulti-index models. arXiv preprint arXiv:2405.15480, 2024.\n[113] Leonardo Defilippis, Yatin Dandi, Pierre Mergny, Florent Krzakala, and Bruno Loureiro.\nOptimal spectral transitions in high-dimensional multi-index models.\narXiv preprint\narXiv:2502.02545, 2025.\n[114] Yatin Dandi, Luca Pesce, Lenka Zdeborová, and Florent Krzakala. The computational advan-\ntage of depth: Learning high-dimensional hierarchical functions with gradient descent. arXiv\npreprint arXiv:2502.13961, 2025.\n17\n[115] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari.\nThe large learning rate phase of deep learning: the catapult mechanism. arXiv preprint\narXiv:2003.02218, 2020.\n[116] Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin. Catapults in\nsgd: spikes in the training loss and their impact on generalization through feature learning.\narXiv preprint arXiv:2306.04815, 2023.\n[117] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.\nHigh-dimensional asymptotics of feature learning: How one gradient step improves the\nrepresentation. Advances in Neural Information Processing Systems, 35:37932–37946, 2022.\n[118] Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue M Lu, Lenka Zdeborová, and\nBruno Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step.\narXiv preprint arXiv:2402.04980, 2024.\n[119] Yatin Dandi, Luca Pesce, Hugo Cui, Florent Krzakala, Yue M Lu, and Bruno Loureiro.\nA random matrix theory perspective on the spectrum of learned features and asymptotic\ngeneralization capabilities. arXiv preprint arXiv:2410.18938, 2024.\n[120] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How\ntwo-layer neural networks learn, one (giant) step at a time. Journal of Machine Learning\nResearch, 25(349):1–65, 2024.\n[121] Dmitry Chistikov, Matthias Englert, and Ranko Lazic. Learning a neuron by a shallow relu\nnetwork: Dynamics and implicit bias for correlated inputs. Advances in Neural Information\nProcessing Systems, 36:23748–23760, 2023.\n[122] Ioannis Bantzis, James B Simon, and Arthur Jacot. Saddle-to-saddle dynamics in deep relu\nnetworks: Low-rank bias in the first saddle escape. arXiv preprint arXiv:2505.21722, 2025.\n[123] Uwe Helmke and John B Moore. Optimization and dynamical systems. Springer Science &\nBusiness Media, 2012.\n[124] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii.\nSIAM Journal on Numerical Analysis, 7(1):1–46, 1970.\n18\nA\nAdditional Related Work\nA.1\nWhat is a feature?\nHere, we briefly review definitions of a feature used in the mechanistic interpretability and deep\nlearning theory literature:\n• In mechanistic interpretability, terms such as feature, circuit, and motif are used to describe\nnetwork components—such as directions in activation space, subnetworks of weights, or\nrecurring activation motifs—that correspond to human-interpretable concepts or functions\nwithin the model’s computation [10]. While this literature has led to many insights into\ninterpretability, these definitions generally lack precise mathematical formalization.\n• In deep learning theory, features and feature learning are defined in terms of the neural\ntangent kernel (NTK). For a network f(x; θ), the NTK is given by the kernel K(x, x′; θ) =\n⟨∇θf(x; θ), ∇θf(x′; θ)⟩, formed by the Jacobian feature map ∇θf(x; θ). Feature learn-\ning—also referred to as rich learning—occurs when this kernel evolves during training.\nWhile this definition is mathematically precise, it offers limited interpretability.\nIn this work, we implicitly adopt the notion of features from the deep learning theory perspective, but\nconsider learning settings often studied in mechanistic interpretability.\nA.2\nAnalysis of feature learning in two-layer networks\nHere, we discuss related theoretical approaches to studying feature learning in two-layer networks.\nThese approaches operate in distinct regimes—mean-field/infinite-width, teacher–student at high-\ndimensional (thermodynamic) limits, and single-step analyses—whereas AGF focuses on the vanish-\ning initialization limit for two-layer networks. Our aim is to position AGF as complementary to, not\na replacement for, these approaches.\nMean-field analysis. An early line of work in the study of feature learning analyzes the population\ndynamics of two-layer neural networks under the mean-field parameterization, yielding analytic\ncharacterizations of training dynamics in the infinite-width limit [40–43]. These ideas have been\nextended to deep networks via the tensor program framework, culminating in the maximal update\nparametrization (µP) [54, 107]. Related analyses have also been obtained through self-consistent\ndynamical mean-field theory [108]. Although feature learning occurs in this regime, these analyses\nprimarily address how to preserve feature learning when changing network depth and width, rather\nthan elucidating what specific features are learned or how they emerge.\nTeacher-student setup. The two-layer teacher–student framework provides a precise setting for\nstudying feature learning by analyzing how a student network trained on supervised examples\ngenerated from a fixed teacher recovers the structure of the teacher. Foundational work by Saad\nand Solla [44, 45, 46] provided closed-form dynamics for online gradient descent in soft committee\nmachines trained on i.i.d. Gaussian inputs in the high-dimensional limit, deriving deterministic\ndifferential equations governing a set of order parameters. Goldt et al. [47], Veiga et al. [48] extended\nthis dynamical theory to study the evolution of generalization error of one-pass SGD under an\narbitrary learning rate, and a range of hidden layer widths. Further developments have analyzed\nconditions for the absence of spurious minima [49], extended the framework to capture more realistic\ndata settings [50], and provided a unified perspective connecting to the infinite hidden width limit of\nthe mean-field regime [51]. Much of this analysis use techniques from statistical physics—including\nthe replica method and approximate message passing—to characterize the generalization error and\nuncover sharp phase transitions and statistical-to-computational gaps in teacher–student models,\ndiscussed further in Aubin et al. [109], Barbier et al. [110].\nMulti-index models. Similar to the teacher–student setting, multi-index models provide a structured\nframework in which a neural network is trained on data whose labels depend on a nonlinear function of\nlow-dimensional projections of high-dimensional inputs. Feature learning in this setting is governed\nby the network’s ability to align with the relevant low-dimensional subspace. This setup often\ngives rise to characteristic staircase-like training dynamics [111, 36, 37, 60, 33–35]. Prior analyses\nhave primarily focused on characterizing generalization and sample complexity, and recent work has\nestablished precise theoretical and computational thresholds for weak learnability in high-dimensional\nmulti-index problems [112, 113]. Other studies have demonstrated that reusing batches in gradient\ndescent can help networks overcome information bottlenecks, accelerating the learning of relevant\n19\nprojections [38, 39]. Additionally, incorporating depth into these models has been shown to provide\ncomputational advantages for learning hierarchical multi-index functions [114].\nSingle gradient step. A recent line of work has focused on understanding feature learning in neural\nnetworks after just a single gradient step. Empirically, it has been observed that with large initial\nlearning rates, models can undergo “catapult” dynamics, where sudden spikes in the training loss\naccelerate feature learning and improve generalization [115, 116]. Ba et al. [117] initiated the precise\nanalysis of feature learning after a single gradient step by showing, in a high-dimensional single-\nindex setting, how the first update to the first-layer weights can dramatically improve prediction\nperformance beyond random features, depending on the learning rate scaling. This single-step\nanalysis has been extended to characterize the asymptotic behavior of the generalization error [118],\nestablish an equivalence between the updated features and an isotropic spiked random feature model\n[119], and investigate the effects of multiple gradient steps [120]. Collectively, these works highlight\nthat even a single gradient step can induce meaningful feature learning in two-layer networks.\nDistinct phases of alignment and growth in ReLU networks. Maennel et al. [56] first observed a\nstriking phenomenon in two-layer ReLU networks trained from small initializations: the first-layer\nweights concentrate along fixed directions determined by the training data, regardless of network\nwidth. Subsequent studies of piecewise linear networks have refined this effect by imposing structural\nconstraints on the training data, such as orthogonally separable [24, 26, 28], linearly separable and\nsymmetric [25], pair-wise orthonormal [27], correlated [121], small angle [30], binary and sparse\n[29]. Across these analyses, a consistent observation is that the learning dynamics involve distinct\nlearning phases separated by timescales: a fast alignment phase driven by directional movement and\na slow fitting phase driven by radial movement [60, 61]. Concurrently, Bantzis et al. [122] use a\nclosely related directional–radial decomposition to identify the optimal escape directions from the\nsaddle at the origin in deep ReLU networks.\n20\nB\nDerivations in Alternating Gradient Flows Framework\nIn this section, we proivde details and motivations for the general framework of AGF. All code and ex-\nperimental details are available at github.com/danielkunin/alternating-gradient-flows.\nB.1\nDeriving dormant dynamics near a saddle point in terms of the utility function\nAt any point in parameter space the gradient of the MSE loss with respect to the parameters of a\nneuron can be expressed as\n−∇θiL(Θ) = ∇θiEx [⟨fi(x; θi), r(x; Θ)⟩] ,\n(10)\nwhere r(x; Θ) = y(x) −P\ni fi(x; θi) is a residual that depends on all neurons. Given a partition of\nneurons into two sets, an active A and dormant D set, then the gradient for a dormant neuron i ∈D\ncan be decomposed into two terms:\n−∇θiL(Θ) = ∇θiEx\n\"*\nfi(x; θi), y(x) −\nX\nj∈A\nfj(x; θj)\n+#\n−∇θiEx\n\"*\nfi(x; θi),\nX\nj∈D\nfj(x; θj)\n+#\n. (11)\nThe first term is the gradient of the utility ∇θUi as defined in Equation (1). The second term only\ndepends on the dormant neurons and is thus negligible when all the dormant neurons are small in\nnorm. Taken together, in the vicinity of a saddle point of the loss defined by an active and dormant\nset, the dormant neurons are driven approximately by gradient flow maximizing their utility:\nd\ndtθi ≈η∇θUi.\n(12)\nThis observation motivates the utility and our AGF ansatz for gradient flow. To formally prove that\nAGF converges to gradient flow in the limit of vanishing initialization one might consider using tools\nlike Hartman–Grobman theorem to make this step rigorous.\nDirectional and radial expansion of the dynamics.\nUsing this approximation, we can further\ndecompose the dynamics of a dormant neuron near a saddle point into a directional and radial\ncomponent:\nd\ndt\nθi\n∥θi∥= η P⊥\nθi\n∥θi∥∇θUi,\nd\ndt∥θi∥= η\n1\n∥θi∥⟨θi, ∇θUi⟩,\n(13)\nwhere P⊥\nθi =\n\u0010\nIm −θiθ⊺\ni\n∥θi∥2\n\u0011\nis a projection matrix. When σ(·) is a homogeneous function of degree\nk, then the utility is a homogeneous function of degree κ = k + 1. Using Euler’s homogeneous\nfunction theorem, Equation (13) then simplifies to Equation (2). When σ(·) is not homogeneous, we\ncan Taylor expand the utility around the origin, such that the utility coincides approximately with a\nhomogeneous function of degree κ, where κ is the leading order of the Taylor expansion.\nDynamics of normalized utility.\nAn interesting observation, that we will use in Appendix C, is\nthat the normalized utility function follows a Riccati-like differential equation\nd\ndt\n¯Ui = ηκ2∥θi∥κ−2\n\u0012∥∇θ ¯Ui∥2\nκ2\n−¯U2\ni\n\u0013\n.\n(14)\nIn general, this ODE is coupled with the dynamics for both the direction and norm, except when\nκ = 2 for which the dependency on the norm disappears.\nB.2\nDeriving the jump time\nTo compute τi∗, we use Equation (2) to obtain the time evolution of the norms of the dormant neurons.\nFor i ∈D, we obtain:\n∥θi(t)∥=\n(\n∥θi(0)∥exp (Si(t))\nif κ = 2,\n\u0000∥θi(0)∥2−κ + (2 −κ)Si(t)\n\u0001\n1\n2−κ\nif κ > 2,\n(15)\nwhere we have defined the accumulated utility as the path integral Si(t) =\nR t\n0 κ ¯Ui(s)ds of the\nnormalized utility. We find τi as the earliest time at which neuron i satisfies ∥θi(τi)∥> 1:\nτi = inf {t > 0 | Si(t) > ci/η} ,\nwhere ci =\n(\n−log ∥θi(0)∥\nif κ = 2,\n−∥θi(0)∥2−κ−1\n2−κ\nif κ > 2.\n(16)\nNote that the expression for ci is continuous at κ = 2.\n21\nLower bound on jump time.\nWhen applying this framework to analyze dynamics, computing the\nexact jump time can be challenging due to the complexity of integrating ¯U. In such cases, we resort\non the following lower bound as a useful analytical approximation. Let U∗\ni be the maximal value of\nthe utility, or at least an upper bound on it. Since Si(t) ≤κU∗\ni t, we deduce:\nτi ≥\n(\n−log ∥θi(0)∥\n2U∗\ni\nif κ = 2,\n−∥θi(0)∥2−κ\nκ(2−κ)U∗\ni\nif κ > 2.\n(17)\nB.3\nActive neurons can become dormant again\nIn the cost minimization phase of AGF, active neurons can become dormant. Here, we motivate and\ndiscuss this phenomenon. Intuitively, this happens when the GF trajectory brings an active neuron\nclose to the origin. Due to the preserved quantities of GF for homogeneous activation functions,\nthe trajectory of active neurons is constrained. Thus, this phenomenon can occur only in specific\nscenarios, as quantified by the following result.\nLemma B.1. Suppose that Θ evolves via the GF of L, and let c = (κ −1)∥ai(0)∥2 −∥wi(0)∥2. The\nnorm ∥θi(t)∥2 satisfies the lower bound,\n∥θi(t)∥2 ≥max\n\u0012\n−c,\nc\nκ −1\n\u0013\n,\n(18)\nwhich holds with equality when either ∥wi(t)∥2 = 0 for c ≥0 or ∥ai(t)∥2 = 0 for c < 0.\nProof. For homogeneous activation functions of degree κ, it is well known that the quantity c =\n(κ −1)∥ai(t)∥2 −∥wi(t)∥2 is preserved along trajectories of gradient flow. Since ∥θi(t)∥2 =\n∥ai(t)∥2 + ∥wi(t)∥2, we have:\n∥θi(t)∥2 = −c + κ∥ai(t)∥2 ≥−c,\n∥θi(t)∥2 =\nc\nκ −1 +\nκ\nκ −1∥wi(t)∥2 ≥\nc\nκ −1.\n(19)\nThe claim follows by combining the above inequalities.\nSince at vanishing initialization (κ −1)∥ai∥2 ∼∥wi∥2, we have c ∼0. Therefore, an active neuron\ncan approach the origin during the cost minimization phase. If this happens, the neuron becomes\ndormant. When a neuron becomes dormant again it does so with an accumulated utility Si(t) = ci\nη .\nSee Figure 8 for an example of a diagonal linear network where this behavior is observable.\nB.4\nInstantaneous alignment in the vanishing initialization limit\nIn this section, we revisit the separation of the dynamics dictated by revisit Equation (2). In\nparticular, we wish to discuss the conditions under which the directional dynamics dominates the\nradial one, resulting, at the vanishing initialization limit, in instantaneous alignment during the utility\nmaximization phase. To this end, we establish the following scaling symmetry with respect to the\ninitialization scale factor α.\nTheorem B.2. Suppose θi(t) = fi(t) solves the initial value problem defined by Equation (2) with\ninitial condition θi(0) = θ0,i. Then for all α > 0, the scaled solution θi(t) = αfi(ακ−2t) solves the\ninitial value problem with initial condition θi(0) = αθ0,i.\nProof. First, we consider the angular dynamics:\nd\ndt\n¯θi = d\ndt\nfi(ακ−2t)\n∥fi(ακ−2t)∥\n= ακ−2 d\nds\nfi(s)\n∥fi(s)∥\n\f\f\f\f\ns=ακ−2t\n= ακ−2∥fi(ακ−2t)∥κ−2P⊥\nθi∇θUi(¯θi(ακ−2t); r)\n= ∥αfi(ακ−2t)∥κ−2P⊥\nθi∇θUi(¯θi(ακ−2t); r)\n22\nTherefore, θi(t) = αfi(ακ−2t) satisfies the first identity in Equation (2) above. Next, consider the\nnorm dynamics:\nd\ndt∥θi∥= d\ndt∥αfi(ακ−2t)∥\n= ακ−1 d\nds∥fi(s)∥\n\f\f\f\f\ns=ακ−2t\n= ακ−1κ∥fi(ακ−2t)∥κ−1Ui(¯θi(ακ−2t); r)\n= κ∥αfi(ακ−2t)∥κ−1Ui(¯θi(ακ−2t); r)\nHence, θi(t) = αfi(tακ−2) also satisfies the second equation. Finally, verifying the initial conditions\ngives, θi(0) = αfi(0) = αθ0,i. Therefore, θi(t) = αfi(tακ−2) satisfies the given initial condition.\nTheorem B.2 establishes a transformation among the angular dynamics of different initialization\nscales. Specifically, for a given point (s,\nfi(s)\n∥fi(s)∥) in the solution trajectory of the initial value problem\nwith initial condition θi(0) = θ0,i, the corresponding point in the initial value problem with scaled\ninitial condition θi(0) = αθ0,i is (sα2−κ,\nfi(s)\n∥fi(s)∥). This correspondence reveals that the angular\nalignment process is effectively slowed down by a factor of ακ−2 with initialization scale α.\nNext, we examine the alignment speed in accelerated time in the limit of α →0. In this regime, the\nasymptotics of Equation (16) is given by,\ncκ(α) ∼\n\u001a−log α\nif κ = 2,\nα2−κ\nif κ > 2,\n(20)\nTherefore, in the accelerated time, the alignment speed is effectively scaled by a scaling factor of:\nγ(α) := cκ(α)\nακ−2 =\n\u001a−log α\nif κ = 2,\n1\nif κ > 2,\n(21)\nAnd,\nlim\nα→0 γ(α) =\n\u001a+∞\nif κ = 2,\n1\nif κ > 2,\n(22)\nThe limiting behavior implies that the alignment is instantaneous in accelerated time for almost all\ninitialization θ0,i if and only if κ = 2.\nB.5\nA neuron-specific adaptive learning rate yields instantaneous alignment\nAs discussed above, instantaneous alignment of dormant neurons with their local utility-maximizing\ndirections occurs only when κ = 2. For higher-order activations (κ > 2), the directional dynamics\nacquire a norm-dependent factor ∥θi∥κ−2 that slows their angular evolution. Consequently, directional\nand radial dynamics no longer decouple, even in the vanishing initialization limit, and dormant\nneurons rotate gradually rather than aligning instantaneously.\nThis dependence can be removed by introducing a neuron-specific adaptive learning rate\nηi = ∥θi∥2−κ η,\n(23)\nwhere η is a global base rate. When κ = 2, this scaling has no effect and all neurons evolve at\nthe same rate. For κ > 2, however, neurons with small norm (∥θi∥< 1) are accelerated, while\nthose with large norm (∥θi∥> 1) are slowed down. This rescaling effectively reparametrizes\ntime so that the directional dynamics become norm-independent while the radial dynamics remain\nnorm-dependent. Substituting this adaptive rate into Equation (2) yields an evolution equivalent\nto that of the κ = 2 case, resulting in the decoupling between directional and radial dynamics in\nthe vanishing-initialization limit for all κ. In practice, this scaling acts analogously to a form of\nneuron-wise adaptive optimization—resembling RMSProp or Adam—but derived directly from the\nanalytical structure of the κ-homogeneous gradient flow.\n23\nC\nComplete Proofs for Diagonal Linear Networks\nIn this section, we derive the AGF dynamics for the two-layer diagonal linear network; see Section 3\nfor the problem setup and notation. Throughout this section, we assume the initialization θi(0) =\n(\n√\n2α, 0), following the convention in Pesme and Flammarion [18].\nC.1\nUtility maximization\nLemma C.1. After the kth iteration of AGF, the utility for the ith neuron is\nUi\n\u0010\nθi; r(k)\u0011\n= −uivi∇βiL\n\u0010\nβ(k)\u0011\n,\n(24)\nwhich is maximized on the unit sphere ∥θi∥= 1 when sgn(uivi) = −sgn\n\u0000∇βiL\n\u0000β(k)\u0001\u0001\nand\n|ui| = |vi| resulting in a maximal utility value of ¯U∗\ni = 1\n2\n\f\f∇βiL\n\u0000β(k)\u0001\f\f.\nProof. Substituting the residual r(k)\nj\n= yj −\n\u0000X⊺β(k)\u0001\nj into the definition of the utility function:\nUi(θi; r(k)) = 1\nn\nn\nX\nj=1\nuiviXijrj = −uivi∇βiL\n\u0010\nβ(k)\u0011\n.\n(25)\nObserve that the expression for Ui is linear in uivi. Therefore, under the normalization constraint\n∥θi∥= 1, the utility is maximized when |ui| = |vi| and sgn(uivi) = −sgn\n\u0000∇βiL\n\u0000β(k)\u0001\u0001\n, yielding\nthe maximal value ¯U∗\ni = 1\n2\n\f\f∇βiL\n\u0000β(k)\u0001\f\f.\nLemma C.2. At any time t during AGF, the parameters of the ith neuron satisfy:\nui(t)2 −vi(t)2 = 2α,\n2ui(t)vi(t) = ±\np\n∥θi(t)∥2 −4α4\n(26)\nProof. Both the utility and loss are invariant under the transformation (ui, vi) 7→(gui, g−1vi) for\nany g ̸= 0. This continuous symmetry, together with the fact that each AGF step consists of gradient\nflow in one of these functions, implies that the quantity ui(t)2 −vi(t)2 is conserved throughout\nAGF. Plugging the initialization into this expression gives the first identity ui(t)2 −vi(t)2 = 2α.\nSee Kunin et al. [77] for a general connection between continuous symmetries and conserved\nquantities in gradient flow. For the second identity, we solve for the intersection of the hyperbola\nui(t)2 −vi(t)2 = 2α and the circle ∥θi(t)∥2 = ui(t)2 + vi(t)2, which gives\nui(t) = ±\nr\n∥θi(t)∥2 + 2α2\n2\n,\nvi(t) = ±\nr\n∥θi(t)∥2 −2α2\n2\n.\n(27)\nMultiplying these expressions together gives the identity for the product.\nLemma C.3. After the kth iteration of AGF, the normalized utility for the ith neuron is driven by a\nRiccati ODE d\ndt ¯Ui = 4ηα\n\u0010\u0000 ¯U∗\ni\n\u00012 −¯U2\ni\n\u0011\nwith the unique solution,\n¯Ui(τ (k) + t) = ¯U∗\ni tanh\n\u0010\nδ(k)\ni\n+ 4ηα ¯U∗\ni t\n\u0011\n,\nwhere δ(k)\ni\n= tanh−1\n ¯Ui\n\u0000τ (k)\u0001\n¯U∗\ni\n!\n.\n(28)\nProof. We begin by observing the gradient of the utility function takes the form\n∇θUi\n\u0010\nθi; r(k)\u0011\n= −∇βiL\n\u0010\nβ(k)\u0011 \u0014\nvi\nui\n\u0015\n.\n(29)\nEvaluating this quantity at the normalized parameters ¯θi, and recalling the expression for the maximal\nutility from Lemma C.1, we obtain,\n∥∇θUi(¯θi; r(k))∥2 = 4\n\u0000 ¯U∗\ni\n\u00012 .\n(30)\nSubstituting this into the normalized utility dynamics derived previously (see Equation (14)), we\nobtain the Riccati equation. This is a standard ODE with a known solution, where the constant δ(k)\ni\nis\ndetermined by the initial condition ¯Ui\n\u0000τ (k)\u0001\n.\n24\nTheorem C.4. After the kth iteration of AGF, the accumulated utility for the ith neuron is\nSi\n\u0010\nτ (k) + t\n\u0011\n=\n1\n2ηα\nlog cosh\n\u0012\n2ηα\n\u0012\n2U∗\ni t + ζi\n2ηα\ncosh−1 exp\n\u0010\n2ηαSi\n\u0010\nτ (k)\u0011\u0011\u0013\u0013\n,\n(31)\nwhere ηα = −log(\n√\n2α) is the learning rate and ζi = sgn\n\u0000−∇βiL\n\u0000β(k)\u0001\u0001\nρi\n\u0000τ (k)\u0001\n. The quantity\nρi\n\u0000τ (k)\u0001\n= sgn\n\u0000ui\n\u0000τ (k)\u0001\nvi\n\u0000τ (k)\u0001\u0001\nis determined by the recursive formula\nρi\n\u0010\nτ (k) + t\n\u0011\n= sgn\n \nρi\n\u0000τ (k)\u0001\n2ηα\ncosh−1 exp\n\u0010\n2ηαSi\n\u0010\nτ (k)\u0011\u0011\n−∇βiL\n\u0010\nβ(k)\u0011\nt\n!\n,\n(32)\nwhere Si(0) = 0 and ρi(0) = 0.\nProof. Using the expression for the normalized utility derived in Lemma C.3, we can derive an exact\nexpression for the integral of the normalized utility, i.e., the accumulated utility:\nSi\n\u0000τ (k) + t\n\u0001\n= Si\n\u0000τ (k)\u0001\n+\nR t\n0 2 ¯Ui(s)ds = Si\n\u0000τ (k)\u0001\n+\n1\n2ηα log\n\u0012\ncosh\n\u0010\n4ηα ¯U∗\ni t+δ(k)\ni\n\u0011\ncosh\n\u0010\nδ(k)\ni\n\u0011\n\u0013\n.\n(33)\nUsing the hyperbolic identity tanh−1(x) = sgn(x) cosh−1 \u0010\n1\n√\n1−x2\n\u0011\n, we can express the constant\nδ(k)\ni\nintroduced in Lemma C.3:\nδ(k)\ni\n= sgn\n\u0010\n−∇βiL\n\u0010\nβ(k)\u0011\u0011\ntanh−1\n\u00122ui(τ (k))vi(τ (k))\n∥θi(τ (k))∥2\n\u0013\nLemma C.1\n(34)\n= sgn\n\u0010\n−∇βiL\n\u0010\nβ(k)\u0011\u0011\nρi\n\u0010\nτ (k)\u0011\ncosh−1\n\u0012∥θ(τ (k))∥2\n2α2\n\u0013\nLemma C.2\n(35)\n= sgn\n\u0010\n−∇βiL\n\u0010\nβ(k)\u0011\u0011\nρi\n\u0010\nτ (k)\u0011\ncosh−1 \u0010\nexp\n\u0010\n2ηαSi\n\u0010\nτ (k)\u0011\u0011\u0011\n,\n(36)\nwhere in the last equality we used the simplification ∥θ(τ (k))∥2 = 2α2 exp\n\u00002ηαSi\n\u0000τ (k)\u0001\u0001\n. Substi-\ntuting this expression into Equation (33), notice that the denominator inside the logarithm simplifies\nsubstantially, as the Si(τ (k)) term cancels out, yielding Equation (31). Finally, by Lemma C.2, ρi\nchanges sign only when Si = 0, yielding Equation (32).\nCorollary C.5. After the kth iteration of AGF, the next dormant neuron to activate is i∗=\narg mini∈D ∆τi where\n∆τi = cosh−1 exp (2ηα) −ζi cosh−1 exp\n\u00002ηαSi(τ (k))\n\u0001\n2ηα · 2 ¯U∗\ni\n,\n(37)\nζi = sgn\n\u0000−∇βiL\n\u0000β(k)\u0001\u0001\nρi\n\u0000τ (k)\u0001\n, and the next jump time τ (k+1) = τ (k) + ∆τi∗.\nProof. From Theorem C.4, we solve for the time t such that Si(τ (k) + t) = 1 by inverting the\nexpression for accumulated utility. This time is ∆τi.\nC.2\nCost minimization\nActive neurons represents non-zero regression coefficients of β. During the cost minimization step,\nthe active neurons work to collectively minimize the loss. When a neuron activates it does so with a\ncertain sign sgn(uivi) = −sgn(∇βiL(βA)). If during the cost minimization phase a neuron changes\nsign, then it can do so only by returning to dormancy first. This is due to the fact that throughout\nthe gradient flow dynamics the quantity u2\ni −v2\ni = 2α2 is conserved and thus in order to flip the\nsign of the product uivi, the parameters must return to their initialization. As a result, the critical\npoint reached during the cost minimization step is the unique solution to the constrained optimization\nproblem,\nβ∗\nA = arg min\nβ∈Rd\nL(β)\nsubject to\n\u001aβi = 0\nif i /∈A,\nβi · sgn(uivi) ≥0\nif i ∈A,\n(38)\nwhere uniqueness follows from the general position assumption. All coordinates where (β∗\nA)i = 0\nare dormant at the next step of AGF.\n25\nτ(1) τ(2)\nτ(3)\nℓ(0)\nℓ(1)\nℓ(2)\nα = 0.50\nα = 0.10\nα = 1e-02\nα = 1e-04\nα = 1e-06\nα →0\n(a) Loss L(t)\nβ1 = u1v1\nβ2 = u2v2\nβ (0)\nβ (1)\nβ (2)\nβ (3)\n2\n2\n4\n4\n6\n6\n8\n8\n10\n10\n12\n12\n14\n16\n18\n(b) Function β(t)\n−π/4\n0\nπ/4\n10−6\n10−4\n10−2\n100\nθ1 = (u1, v1)\n−π/4\n0\nπ/4\n10−6\n10−4\n10−2\n100\nθ2 = (u2, v2)\n(c) Parameters θ(t)\n0\n1\n˜S(t)\nθ1\nθ2\nτ(1) τ(2)\nτ(2) + τ(3)\n2\nτ(3)\n−π/4\n0\nπ/4\nˆθ(t)\nτ(1) τ(2)\nτ(2) + τ(3)\n2\nτ(3)\n(d) Utility ˜S(t) and ˆθ(t)\nFigure 8: AGF = GF as α →0 in diagonal linear networks. We consider a diagonal linear network\nβ = u ⊙v ∈R2, trained by gradient flow from initialization u =\n√\n2α1, v = 0, with varying α,\ninspired by Figure 2 of Pesme and Flammarion [18]. This example is special as it demonstrates how\nan active neuron, in this case β(1), can return to dormancy during the cost minimization phase, as\ndiscussed theoretically in Appendix B. In (a) we plot the loss over accelerated time and in (b) the loss\nlandscape over β. For small α, trajectories evolve from β(0) to β(3), passing near intermediate saddle\npoints β(1) and β(2). Each saddle point is associated with a plateau in the loss. As α →0, gradient\nflow spends all its time at these saddles, jumping instantaneously between them at τ (1), τ (2), τ (3),\nmatching the stepwise loss drops. In (c) we plot trajectories in parameter space and in (d) the same\ndynamics visualized in terms of their accumulated utility and angle. The jump times between critical\npoints correspond to when the accumulated utility satisfies ˜Si(τ) = 1. When the first coordinate\nreturns to dormancy, the accumulated utility first touches zero, causing the angle to flip sign, before\nreactivating with the opposite sign.\nC.3\nAGF in action\nCombining the results of utility maximization (Appendix C.1) and cost minimization (Appendix C.2)\nyields an explicit recursive expression for the sequence generated by AGF. For each neuron we\nonly need to track the accumulated utility Si(t) and the sign ρi(t), which at initialization are both\nzero. We can now consider the relationship between the sequence produced by AGF in the vanishing\ninitialization limit α →0 and the sequence for gradient flow in the same initialization limit introduced\nby Pesme and Flammarion [18].\nAlgorithm 2: Pesme and Flammarion [18]\nInitialize: t ←0, β ←0 ∈Rd, S ←0 ∈Rd;\nwhile ∇L(β) ̸= 0 do\nD ←{j ∈[d] | ∇L(β)j ̸= 0}\nτ ∗←inf {τi > 0 | ∃i ∈D, Si −τi∇L(β)i = ±1}\nt ←t + τ ∗,\nS ←S −τ ∗∇L(β)\nβ = arg min L(β) where β ∈\n\n\n\n\n\nβ ∈Rd\n\f\f\f\f\f\f\f\nβi ≥0\nif Si = +1,\nβi ≤0\nif Si = −1,\nβi = 0\nif Si ∈(−1, 1)\n\n\n\n\n\nreturn Sequence of (β, t);\nWe adapt the original notation to fit our framework, highlighting\nutility maximization (blue) and cost minimization (red) steps.\nIn Pesme and Flammarion [18], they\nprove that in the vanishing initializa-\ntion limit α →0, the trajectory of\ngradient flow, under accelerated time\n˜tα(t) = −log(α) · t, converges to-\nwards a piecewise constant limiting\nprocess corresponding to the sequence\nof saddles produced by Algorithm 2.\nThis algorithm can be split into two\nsteps, which match exactly with the\ntwo steps of AGF in the vanishing ini-\ntialization limit.\nAs in AGF, at each iteration of their\nalgorithm a coefficient that was zero becomes non-zero. The new coefficient comes from the set\n{i ∈[d] : ∇L(β)i ̸= 0}, which is equivalent to the set of dormant neurons with non-zero utility, as\nall active neurons will be in equilibrium from the previous cost minimization step. The index for the\nnew active coefficient is determined by the following expression,\ni∗= arg min{τi > 0 : ∃i ∈D, si −τi∇L(β)i = ±1}.\n(39)\nBecause τi > 0 and si ∈(−1, 1) for all coefficients i ∈D, then this expression only makes sense if\nwe choose the boundary associated with sgn(∇L(β)i). Taking this into account, we get the following\nsimplified expression for the jump times proposed by Pesme and Flammarion [18],\nτi = 1 + sgn(∇L(β)i)si\n|∇L(β)i|\n.\n(40)\nThis expression is equivalent to the expression given in Corollary C.5 in the limit ηα →∞. To see\nthis, we use the asymptotic identity cosh−1(exp(x)) ∼x + log 2 as x →∞. Combined with the\n26\nsimplification 2 ¯U∗\ni = |∇βiL(β)|, this allows us to simplify the expression for the jump time and\nshow that ρiSi = si where si is the integral from Algorithm 2.\nThe second step of Algorithm 2, is that the new β is determined by the following constrained\nminimization problem,\nβ∗= arg min\nβ∈Rd\nL(β)\nsubject to\n\n\n\nβi ≥0,\nif si = 1\nβi ≤0,\nif si = −1\nβi = 0,\nif si ∈(−1, 1)\n(41)\nUsing the correspondence ρi = sgn(si), and noting that neurons with si ∈(−1, 1) correspond to\nthe dormant set, this constrained optimization problem is equivalent to the cost minimization step of\nAGF, presented in Appendix C.2.\n27\nD\nComplete Proofs for Fully Connected Linear Networks\nIn this section, we provide the details behind the results around linear networks in Section 4.\nWe begin by clarifying the notion of ‘neuron’ in this context. As briefly explained in Section 4,\ndue to the symmetry of the parametrization of fully-connected linear networks, the usual notion of\na neuron does not define a canonical decomposition of f into rank-1 maps. Instead, the singular\nvalue decomposition of the linear map AW computed by the network, defines such a canonical\ndecomposition, being the unique orthogonal one. Therefore, we will think of neurons as basis vectors\n(˜ai, ˜wi) ∈Rc+d under orthogonality constraints. These vectors align with the singular vectors and\nare partitioned into dormant and active sets, D and A, based on whether their corresponding singular\nvalue is O(1).\nD.1\nUtility maximization\nDifferently from the usual setting of AGF, the orthogonality constraint on the dormant basis vectors\nimplies that the utility function is not decoupled. Instead, it is maximized over the space of |D|\northonormal basis vectors, i.e., the Stiefel manifold.\nLemma D.1. After the kth iteration of AGF, the utility function of the ith dormant basis vector with\nparameters θi = (˜ai, ˜wi) is:\nUi\n\u0010\nθi; r(k)\u0011\n= −˜a⊺\ni ∇βL\n\u0010\nβ(k)\nA\n\u0011\n˜wi.\n(42)\nThe total utility P\ni∈D Ui\n\u0000θi; r(k)\u0001\nis maximized on the Stiefel manifold when {˜ai, ˜wi}|D|\ni=1 coincides\nwith the set of the top |D| = H−k left and right singular vectors of ∇βL(β(k)\nA ), resulting in a maximal\nutility value for the ith dormant basis vector of ¯U∗\ni = σ(k)\ni\n/2, where σ(k)\ni\nis the corresponding singular\nvalue. Moreover, Ui has no other local maxima.\nProof. After substituting r(k) = y −β(k)\nA x, the computation of the utility is straightforward. Every-\nthing else follows from the standard theory of Reyleigh quotients over Stiefel manifolds [123].\nThis means that the gradient flow of the utility function aligns the dormant basis vectors with the\nsingular vectors of −∇βL(β(k)\nA ).\nD.2\nCost minimization\nAs discussed in Section 4, the cost minimization phase is governed by the well-known Eckart-Young\ntheory of reduced-rank regression [78]. According to the latter, during cost minimization, the active\nbasis vectors converge to\nβ(k)\nA\n= PUkΣyxΣ−1\nxx ,\n(43)\nwhere PUk is the projection onto the top k eigenvectors of ΣyxΣ−1\nxx Σ⊺\nyx. Plugging this solution into\nthe expression for the gradient, we find that the utility at the next iteration will be computed with the\nprojection P⊥\nUkΣyx:\n−∇βL(β(k)\nA ) = Σyx −β(k)\nA Σxx = (Ic −PUk)Σyx.\n(44)\nD.3\nAGF in action\nAlgorithm 3: Greedy Low-Rank Learning Li et al. [20]\nInitialize: r ←0, W0 ←0 ∈Rd×d, U0(∞) ∈Rd×0\nwhile λ1(−∇L(Wr)) > 0 do\nr ←r + 1;\nur ←unit top eigenvector of −∇L(Wr−1);\nUr(0) ←[Ur−1(∞)\n√εur] ∈Rd×r;\nfor t = 0, 1, . . . , T do\nUr(t + 1) ←Ur(t) −η∇L(Ur(t));\nWr ←Ur(∞)U ⊤\nr (∞)\nreturn Sequence of Wr\nPutting together the utility maximiza-\ntion and cost minimization steps, AGF\nprogressively learns the projected\nOLS solution (Equation (43)), coin-\nciding with the GLRL algorithm by\nLi et al. [20], shown in Algorithm 3,\nwith notation from the original work.\nAt each stage, the GLRL algorithm\nselects a top eigenvector (blue: utility\nmaximization) and performs gradient\ndescent in the span of selected direc-\ntions (red: cost minimization).\n28\nWe now discuss jump times, and motivate Conjecture 4.1. Recall from Section B.4 that in this setting,\nbecause κ = 2, in the limit of vanishing initialization, the alignment in the utility maximization\nphase occurs instantaneously. As a consequence, the individual utility functions remain, essentially,\nconstantly equal to their corresponding maximal value throughout each utility maximization phase.\nFrom the definition of accumulated utility with ηα = −log(α), we immediately deduce the recursive\nrelation:\nSi\n\u0010\nτ (k) + t\n\u0011\n= Si\n\u0010\nτ (k−1)\u0011\n+ σ(k)\ni\nt.\n(45)\nHowever, since −∇βL(β(k)\nA ) = P⊥\nUkΣyx and −∇βL(β(k−1)\nA\n) = P⊥\nUk−1Σyx have different singular\nvalues and vectors, the relation between the values of Equation (45) as i ∈D varies is unclear, and it\nis hard to determine the first dormant basis vector to accumulate a utility value of 1. Yet, suppose\nthat the dormant basis vectors align in such a way that the ordering of the corresponding singular\nvalues is preserved across the utility maximization phases. In this case, the ordering of the cumulated\nutilities in Equation (45) is also preserved for all iterations k and all times t. In particular, it follows\nby induction that the basis vector with index i∗corresponding to the largest eigenvalue σ(k)\ni∗jumps\nafter a time of:\n∆τ (k) =\n1\nσ(k)\ni∗\n\u0010\n1 −Si∗\n\u0010\nτ (k−1)\u0011\u0011\n.\n(46)\nOnce unrolled, the above recursion is equivalent to the statement on jump times in Conjecture 4.1.\nWhen Σxx commutes with Σ⊺\nyxΣyx, the left singular vectors of Σyx simultaneously diagonalize Σxx\nand ΣyxΣ−1\nxx Σ⊺\nyx. In this case, the singular values of P⊥\nUkΣyx correspond to the bottom ones of\nΣyx, with the same associated singular vectors. Therefore, the dormant basis vectors are correctly\naligned already from the first iteration of AGF. By the discussion above, this confirms Conjecture\n4.1 in this specific scenario. Moreover, the recursive expression from Equation (46) reduces to\n∆τ (k) = 1/σk −1/σk−1, where σk is the kth largest singular value of Σyx. Therefore, the jump\ntimes are simply:\nτ (k) = 1\nσk\n.\n(47)\nThese values, together with the loss values in Conjecture 4.1, coincide with the ones derived by Gidel\net al. [19] for GF at vanishing initialization. This means that under the commutativity assumption,\nAGF and GF converge to the same limit, similarly to the setting of diagonal linear networks.\nWhen Σxx and Σ⊺\nyxΣyx do not commute, in order to prove Conjecture 4.1, it is necessary to understand\nthe relation between the SVD of P⊥\nUkΣyx and of Σyx, as k varies. The Poincaré separation theorem\ndescribes this relation for the singular values, stating that they are interlaced, i.e., they alternate\nbetween each other when ordered. This is not sufficient, since the geometry of the singular vectors\nplays a crucial role in the alignment phase. Even though there exist classical results from linear\nalgebra in this direction [124], we believe that Conjecture 4.1 remains open.\n29\nE\nComplete Proofs for Attention-only Linear Transformer\nHere, we provide additional details and derivations for the results around transformers (see Section 4\nfor the problem setup and notation). This analysis is based on the setting presented by Zhang et al.\n[32], to which we refer the reader for further details.\nNote that this scenario does not exactly fit the formalism of Section 2, since we are considering a\nneural architecture different from a fully-connected network. Yet, due to our assumption on the rank\nof the attention heads, each head behaves as a bottleneck, resembling a ‘cubical version’ of a neuron.\nTherefore, we will interpret attention heads as ‘neurons’, and apply AGF to this context (with κ = 3).\nWe will show in the following sections that at the end of the kth iteration of AGF, the ith attention\nhead, h = 1, . . . , k, learns the function:\nfh(X; θ∗\nh)D+1,N+1 =\n1\ntrΣxx + (N + 1)λh\nN\nX\nn=1\nynx⊺\nnvhv⊺\nhxN+1,\n(48)\nwhere λh is the hth largest eigenvalue of Σxx, and vh is the corresponding eigenvector. We will\nproceed by induction on k.\nE.1\nUtility maximization\nWe prove the following lemma, establishing, inductively, the utility function for the kth iteration of\nAGF.\nLemma E.1. At the kth iteration of AGF, assume that the (k −1) active attention heads have the\nfunction form as in Equation (48). Then, the utility function of a dormant head with parameters\nθi = (Vi, Qi, Ki) is given by:\nUi\n\u0010\nθi; r(k)\u0011\n= NVi Q⊺\ni\n \nΣ2\nxx −\nk−1\nX\nh=1\nλ2\nhvhv⊺\nh\n!\nKi\n(49)\nMoreover,\nthe utility is maximized over normalized parameters by\n( ¯V ∗\ni , ¯Q∗\ni , ¯K∗\ni )\n=\n(±1/\n√\n3, ±vk/\n√\n3, ±vk/\n√\n3), where the sign is determined such that ¯U∗\ni = Nλ2\nk/(3\n√\n3). Moreover,\nUi has no other local maxima.\nProof. By assumption, we have:\nUi\n\u0010\nθi; r(k)\u0011\n= Ex1,...,xN+1,β\n\" \nyN+1 −\nk−1\nX\nh=1\nfh(X; θ∗\nh)D+1,N+1\n!\nfi(X, θi)D+1,N+1\n#\n.\n(50)\nThe first term, which coincides with the initial utility, can be computed as:\nUi (θi; y) = Ex1,...,xN+1,β [yN+1fi(X, θi)D+1,N+1]\n= Ex1,...,xN+1,β\n\"\nβ⊺xN+1Vi\nN\nX\nn=1\nynx⊺\nnKiQ⊺\ni xN+1\n#\n= Vi Q⊺\ni ExN+1[xN+1x⊺\nN+1]Eβ[ββ⊺]\nN\nX\nn=1\nExn[xnx⊺\nn]Ki\n= NVi Q⊺\ni Σ2\nxxKi.\n(51)\nDenote Ah = (trΣxx + (N + 1)λk)−1. Then, the summand can be computed as:\nEx1,...,xN+1,β [f ∗\nh(X, θh)D+1,N+1fi(X, θi)D+1,N+1]\n= AhViEx1,...,xN+1,βtr\n \nββ⊺\nN\nX\nn=1\nxnx⊺\nnKiQ⊺\ni xN+1x⊺\nN+1vhv⊺\nh\nN\nX\nn=1\nxnx⊺\nn\n!\n= AhVitr\n\nKiQ⊺\ni Σxxvhv⊺\nhEx1,...,xN\n N\nX\nn=1\nxnx⊺\nn\n!2\n\n= AhVitr\n\u0000KiQ⊺\ni λhvhv⊺\nh\n\u0000NΣxxtrΣxx + N(N + 1)Σ2\nxx\n\u0001\u0001\n= NViQ⊺\ni λ2\nhvhv⊺\nhKi,\n(52)\n30\nwhere in the second equality, we have used the fact that\nEx1,...,xN\n\n\n N\nX\nn=1\nxnx⊺\nn\n!2\n= Ex1,...,xN\n\n\nN\nX\nn=1\n(xnx⊺\nn)2 + 2\nX\n1≤i<j≤N\nxix⊺\ni xjx⊺\nj\n\n\n= NΣxxtrΣxx + N(N + 1)Σ2\nxx.\n(53)\nThis provides the desired expression for the utility, which corresponds to a Rayleigh quotient. Since\nthe largest eigenvalue of (Σ2\nxx −Pk−1\nh=1 λ2\nhvhv⊺\nh) is λ2\nk, the rest of the statement follows from the\nstandard theory of Rayleigh quotients.\nE.2\nCost minimization\nAfter utility maximization, a new head aligns to the corresponding eigenvector and becomes active.\nThis implies that at the beginning of the cost minimization phase of the kth iteration of AGF, the k\nactive attention heads have parameters θh = (Vh, Qh, Kh), such that Qh and Kh coincide up to a\nmultiplicative scalar to vh for all 1 ≤h ≤k. Moreover, Equation (48) holds for 1 ≤h < k.\nWe wish to show that during the cost minimization phase, the newly-activated head with parameters\nθk learns a function in the same form. First, we prove that the loss function is decoupled for each\nactive attention head.\nLemma E.2. Assume that the k active attention heads have the same function form, up to multiplica-\ntive scalar, as in Equation (48), Then, the loss over active heads decoupled into sum of individual\nlosses, that is\nL (ΘA) =\nk\nX\ni=1\nL (θi) .\n(54)\nMoreover, for all h = 1, . . . , k, the directional derivatives\n∂L\n∂Qh L(ΘA) and\n∂L\n∂Kh L(ΘA) are propor-\ntional to vh throughout the cost minimization phase.\nProof. The first statement follows from a straightforward calculation using the orthogonality of\neigenvectors. As a result,\n∂L\n∂Qh\n(ΘA) = ∂L\n∂Qh\n(θh) = Ex1,...,xN,β\n \nVh\nN\nX\nn=1\nynx⊺\nnKh\n!2\nΣxxQh,\n(55)\nFrom this expression, we see that if Qh starts as an eigenvector of Σxx, then subsequent gradient\nupdates will be in the same direction. As a result, the direction of Qh remains unchanged in cost\nminimization. A similar argument holds for Kh as well.\nSince the active heads follow the GF of L, the decoupling of the losses implies that during the\ncost minimization phase, each head actually follows the gradient of the loss of its own parameters,\ndisregarding the other heads. In particular, θh remains at equilibrium for h = 1, . . . , k −1, since\nits loss has been optimized during the previous iterations of AGF. By Lemmas E.1 and E.2, the\nnewly-activated head remains aligned to its eigenvector vk, learning a function of the form\nfk (X; θk)D+1,N+1 = Ak\nN\nX\nn=1\nynx⊺\nnvk+1v⊺\nk+1xN+1,\n(56)\nwhere Ak ∈R is a parameter that is optimized in cost minimization. L(θk) is convex with respect to\nAk, and the corresponding minimization problem is easily seen to be solved by Ak = 1/(trΣxx +\n(N + 1)λk). This concludes our inductive argument.\n31\nE.3\nAGF in action\nAs we have seen above, AGF describes a recursive procedure, where each head sequentially learns a\nprincipal component. We can easily compute the loss value at the end of each iteration:\nℓ(k) ≡Ex1,...,xN+1,β\n1\n2\n \nyN+1 −\nk\nX\nh=1\nAh\nN\nX\nn=1\nynx⊺\nnvhv⊺\nhxN+1\n!2\n= 1\n2\n D\nX\ni=1\nλi −\nk\nX\nh=1\nNλ2\nh\ntrΣxx + (N + 1)λh\n!\n.\n(57)\nFurthermore, as an immediate consequence of Lemma E.1, we can lower bound the jump times. The\naccumulated utility at the kth iteration of AGF is upper-bounded by Si(t) ≤3t ¯U∗\ni = tNλ2\nk/\n√\n3. The\njump time is given by the first head that reaches Si(τ (k) −τ (l)) = 1/∥θi(τ (l))∥, which yields\nτ (k) −τ (l) ≥\n√\n3\nNλ2\nkµl\n,\n(58)\nwhere µk = maxi ∥θi(τ (l))∥and τ (0) = 0.\n32\nF\nComplete Proofs for Generalized Modular Addition\nIn this section, we provide the proofs for the results summarized in Section 5. Instead of reasoning\ninductively as in Section E, for simplicity of explanation we will derive in detail the steps of the\nfirst iteration of AGF. As we will explain in Section F.3, all the derivations can be straightforwardly\nextended to the successive iterations, completing the inductive argument.\nF.1\nUtility maximization\nIn order to compute the utility for a single neuron, we will heavily rely on the Discrete Fourier\nTransform (DFT) and its properties. To this end, recall that for u ∈Rp, its DFT ˆu ∈Rp is defined as:\nˆu[k] =\np−1\nX\na=0\nu[a]e−2πika/p,\n(59)\nwhere i = √−1 is the imaginary unit. We start by proving a technical result.\nLemma F.1. Let x, u, v, w ∈Rp. Then:\np−1\nX\na,b=0\n⟨u, a · x⟩⟨v, b · x⟩⟨w, (a + b) · x⟩= 1\np\np−1\nX\nk=0\n|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k].\n(60)\nProof. Notice that the inner product ⟨u, a · x⟩can be expressed as\n⟨u, a · x⟩=\np\nX\nk=0\nx[k]u[k + a] = (x ⋆u)[a],\n(61)\nwhere x ⋆u is the cross-correlation between x and u. Thus, the left-hand side of Equation (60) can\nbe rewritten as:\np−1\nX\na,b=0\n(x ⋆u)[a] (x ⋆v)[b] (x ⋆w)[a + b] = ⟨x ⋆u, x ⋆v ⋆x ⋆w⟩\nBy using Plancharel’s theorem ⟨x, y⟩=\n1\np⟨ˆx, ˆy⟩and the cross-correlation property [\nx ⋆y[k] =\nˆx[k]ˆy[k], the above expression equals\n1\np\np−1\nX\nk=0\nˆx[k]ˆu[k]ˆv[k] ˆw[k]ˆx[k]2,\n(62)\nwhich corresponds to the desired result via the simplification ˆx[k]ˆx[k] = |ˆx[k]|2.\nLemma F.2. Under the modular addition task with embedding vector x and mean centered labels,\nthe initial utility function for a single neuron parameterized by θ = (u, v, w) can be expressed as\nU(θ; y) = 2\np3\np−1\nX\nk=1\n|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k].\n(63)\nProof. By definition, the utility is:\nU(θ; y) = 1\np2\np−1\nX\na,b=0\n\u001c\n(⟨u, a · x⟩+ ⟨v, b · x⟩)2 w, (a + b) · x −⟨x, 1⟩\np\n1\n\u001d\n= 1\np2\np−1\nX\na,b=0\n\u0000⟨u, a · x⟩2 + ⟨v, b · x⟩2 + 2⟨u, a · x⟩⟨v, b · x⟩\n\u0001 \u0012\n⟨w, (a + b) · x⟩−⟨w, 1⟩⟨x, 1⟩\np\n\u0013\n.\n(64)\n33\nDue to the cyclic structure of ⟨w, (a+b)·x⟩, the contributions from the terms ⟨u, a·x⟩2 and ⟨v, b·x⟩2\nterms vanish. This reduces the utility to\n2\np2\np−1\nX\na,b=0\n⟨u, a · x⟩⟨v, b · x⟩⟨w, (a + b) · x⟩−2\np3\n p−1\nX\na=0\n⟨u, a · x⟩\n!  p−1\nX\nb=0\n⟨v, b · x⟩\n!  p−1\nX\nc=0\n⟨w, c · x⟩\n!\n.\n(65)\nThe first summand in the above expression is provided by Lemma F.1. Moreover, since the mean of a\nvector corresponds to the zero-frequency component k = 0 of its DFT, the second summand reduces\nto:\n2\np3\n p−1\nX\na=0\n⟨u, a · x⟩\n!  p−1\nX\nb=0\n⟨v, b · x⟩\n!  p−1\nX\nc=0\n⟨w, c · x⟩\n!\n= 2\np3 [\nx ⋆u[0] [\nx ⋆v[0] [\nx ⋆w[0]\n= 2\np3 |ˆx[0]|3 ˆu[0]ˆv[0] ˆw[0].\n(66)\nSince the latter coincides with the term k = 0 in Equation (61), we obtain the desired expression.\nWe now solve the constrained optimization problem involved in the utility maximization step of our\nframework.\nTheorem F.3. Let ξ be a frequency that maximizes |ˆx[k]|, k = 1, . . . , p −1, and denote by sx the\nphase of ˆx[ξ]. Then the unit vectors θ∗= (u∗, v∗, w∗) that maximize the initial utility function U(θ; y)\ntake the form\nu∗[a] =\nr 2\n3p cos\n\u0012\n2π ξ\npa + su\n\u0013\nv∗[b] =\nr 2\n3p cos\n\u0012\n2π ξ\npb + sv\n\u0013\nw∗[c] =\nr 2\n3p cos\n\u0012\n2π ξ\npc + sw\n\u0013\n,\n(67)\nwhere a, b, c ∈{0, . . . , p −1} are indices and su, sv, sw ∈R are phase shifts satisfying su + sv ≡\nsw + sx (mod 2π). They achieve a maximal value of ¯U∗=\np\n2/(27p3)|ˆx[ξ]|3. Moreover, the utility\nfunction has no local maxima other than the ones described above.\nProof. By Plancharel’s theorem, the constraint ∥θ∥2 = ∥u∥2 + ∥v∥2 + ∥w∥2 = 1 is equivalent to a\nconstraint on the squared norm of its DFT, up to a scaling. Together with Lemma F.2, this allows us\nto reformulate the original optimization problem in the frequency domain as:\nmaximize\n2\np3\np−1\nX\nk=1\n|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k]\nsubject to\n∥ˆu∥2 + ∥ˆv∥2 + ∥ˆw∥2 = p.\n(68)\nTo solve this optimization problem, consider the magnitudes µx, µu, µv, µw ∈Rp\n+ and phases\nSx, Su, Sv, Sw ∈[0, 2π)p of the DFT coefficient of x, u, v, w, respectively. This means that ˆx[k] =\nµx[k] exp(iSx[k]) for every k, and similarly for u, v, w.\nSince u, v, and w are real-valued, their DFTs satisfy conjugate symmetry (i.e., ˆu[−k] = ˆu[k]). Since\np is odd, the periodicity of the DFT (i.e., ˆu[k] = ˆu[k + p]) allows us to simplify the objective by only\nconsidering the first half of the frequencies. Substituting the magnitude and phase expressions we\nobtain the equivalent optimization,\nmaximize\n4\np3\np−1\n2\nX\nk=1\nµx[k]3µu[k]µv[k]µw[k] cos(Su[k] + Sv[k] −Sw[k] −Sx[k])\nsubject to\np−1\n2\nX\nk=0\nµu[k]2 +\np−1\n2\nX\nk=0\nµv[a]2 +\np−1\n2\nX\nk=0\nµw[a]2 = p\n2.\n(69)\n34\nThe phase terms can be chosen independent of the constraints to maximize the cosine term. Since the\nonly local maximum of the cosine is 0 (mod 2π), the only local optimum of that term is achieved\nby setting Su[k] + Sv[k] −Sw[k] −Sx[k] ≡0 (mod 2π). But then the optimization problem\nreduces to maximizing P\nk∈[(p−1)/2] µx[k]3µu[k]µv[k]µw[k], subject to the constraints. The only\nlocal maximum of this problem is easily seen to be achieved by concentrating all the magnitude at\nthe dominant frequency ξ of x (and its conjugate), i.e.:\nµu[k] = µv[k] = µw[k] =\n\u001ap p\n6\nif k = ±ξ\n(mod p)\n0\notherwise.\n(70)\nThis gives a maximal objective value of ¯U∗= (4/p3)µx[ξ]3(p/6)3/2 =\np\n2/(27p3)|ˆx[ξ]|3. By\napplying the inverse DFT with these choices for magnitudes and phases, while accounting for\nconjugate symmetry, yields the desired result. Note that in the statement, sx, su, sv, sw denote\nSx[ξ], Su[ξ], Sv[ξ], Sw[ξ], respectively.\nWe highlight that Theorem F.3 and its proof is very similar to Theorem 7 and its proof in Morwani\net al. [96]. They derived this optimization problem by looking for a maximum margin solution,\nsuggesting there may be an interesting connection between utility maximization and maximum\nmargin biases of gradient descent.\nF.2\nCost minimization\nWe now discuss cost minimization. As demonstrated in the previous section, during the utility\nmaximization step, the parameters of each neuron align with the harmonic of the dominant frequency\nξ of x, albeit with phase shifts si\nu, si\nv, si\nw. The goal of this section is two-fold. First, in Appendix F.2.1\nwe discuss our assumption that during the cost minimization phase, the neurons remain aligned with\nthe same harmonic, possibly varying the amplitudes and phase shifts of their parameters. Second,\nin Appendix F.2.2 we solve the cost minimization problem over such aligned neurons. Therefore,\nthroughout the section, we consider N neurons parametrized by Θ = (ui, vi, wi)N\ni=1, where:\nui[a] = Ai\nu\nr 2\n3p cos\n\u0012\n2π ξ\npa + si\nu\n\u0013\n,\nvi[b] = Ai\nv\nr 2\n3p cos\n\u0012\n2π ξ\npb + si\nv\n\u0013\n,\nwi[c] = Ai\nw\nr 2\n3p cos\n\u0012\n2π ξ\npc + si\nw\n\u0013\n,\n(71)\nfor some amplitudes Ai\nu, Ai\nv, Ai\nw ∈R≥0 and some phase shifts si\nu, si\nv, si\nw ∈R.\nTo begin with, note that loss splits as:\nL(Θ) =\n1\n2p2\np−1\nX\na,b=0\n\r\r\r\r\r\nN\nX\ni=1\nfi(a · x, b · x; θi) −(a + b) · x + ⟨x, 1⟩\np\n1\n\r\r\r\r\r\n2\n= C(Θ) −U(Θ; y) + 1\n2\n\u0012\n∥x∥2 −⟨x, 1⟩2\np\n\u0013\n,\n(72)\nwhere\nC(Θ) =\n1\n2p2\nN\nX\ni,j=1\n⟨wi, wj⟩\np−1\nX\na,b=0\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u00012 \u0000⟨uj, a · x⟩+ ⟨vj, b · x⟩\n\u00012\n(73)\nand U(Θ; y) = PN\ni=1 U(θi; y) is the cumulated utility function of the N neurons. From Lemma F.2,\nwe know that:\nU(θi; y) =\nr\n2\n27p3 |ˆx[ξ]|3Ai\nuAi\nvAi\nw cos(si\nu + si\nv −si\nw −sx).\n(74)\n35\nThroughout this section, we repeatedly use the following identity: for any p ∈N, k ∈Z, and s ∈R,\np−1\nX\na=0\ncos\n\u0010\ns −2πka\np\n\u0011\n=\n(\np cos(s),\nif k = 0\n(mod p),\n0,\notherwise.\n(75)\nThis follows by writing cos(θ) = Re(eiθ) and noting that the geometric sum Pp−1\na=0 e−2πika/p\nvanishes unless k = 0 (mod p).\nF.2.1\nPreservation of harmonic alignment\nTo analyze cost minimization, we restrict attention to a regime in which the newly activated neurons\ndo not change their aligned harmonic during this phase.\nAssumption F.4. During cost minimization, the N newly activated neurons remain aligned to the\nharmonic ξ.\nThis restriction allows us to solve cost minimization within the subspace spanned by these N neurons,\nin close analogy with prior sections. However, first we characterize the conditions under which this\nassumption is valid and clarify in what sense it provides a faithful description of the dynamics.\nTheorem F.5. Let h ∈Rp be a vector in the form:\nh[a] = A cos\n\u0012\n2π ξ′\np a + s\n\u0013\n,\n(76)\nfor some A, s ∈R and ξ′ ̸= 0, ±ξ. If for all a, b ∈[p]\nN\nX\ni=1\nwi⟨ui, a · x⟩2 =\nN\nX\ni=1\nwi⟨vi, b · x⟩2 = 0,\n(77)\nthen for all i = 1, . . . , N:\n\u001c\nh, ∂L\n∂ui (Θ)\n\u001d\n=\n\u001c\nh, ∂L\n∂vi (Θ)\n\u001d\n=\n\u001c\nh, ∂L\n∂wi (Θ)\n\u001d\n= 0.\n(78)\nProof. A direct calculation leads to the following expressions for the derivatives:\n\u001c\nh, ∂L\n∂ui (Θ)\n\u001d\n= 4\np−1\nX\na,b=0\n⟨h, a · x⟩\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u0001 \nwi, f(a · x, b · x; Θ) −(a + b) · x\n\u000b\n,\n\u001c\nh, ∂L\n∂vi (Θ)\n\u001d\n= 4\np−1\nX\na,b=0\n⟨h, b · x⟩\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u0001 \nwi, f(a · x, b · x; Θ) −(a + b) · x\n\u000b\n,\n\u001c\nh, ∂L\n∂wi (Θ)\n\u001d\n= 2\np−1\nX\na,b=0\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u00012 ⟨h, f(a · x, b · x; Θ) −(a + b) · x⟩.\n(79)\nVia a tedious goniometric computation, the three expressions above can be expanded into a sum\nof cosine terms. Since ξ ̸= ξ′, each of these terms depends on a or b, and therefore averages out.\nWe do not report the details of the computation here, but refer to the proof of Lemma F.6 for more\ndetails.\nThe condition Equation (77) rules out square terms that could otherwise generate gradient components\nat resonant frequencies ξ′ ̸= ±ξ. If the squared activations ⟨ui, a · x⟩2 or ⟨vi, b · x⟩2 have a nonzero\naggregate contribution, their trigonometric expansion produces cosine terms at doubled or mixed\nfrequencies that may survive the averaging over a or b via the cosine-sum identity Equation (75). In\nthis case, the gradient can acquire components outside the span of the aligned harmonic, allowing\nneurons to escape alignment.\nHowever, pairs of aligned neurons can easily satisfy Equation (77) by coordinating their amplitudes\nand phases. For example, using the identity z1z2 = 1\n4(z1 + z2)2 −1\n4(z1 −z2)2, neurons can arrange\nfor square terms to cancel in aggregate while preserving their linear contributions. Such cancellations\ncorrespond to directional adjustments that can occur rapidly during cost minimization.\n36\nF.2.2\nCost minimization over aligned neurons\nNext, in order to solve the cost minimization problem over aligned neurons, we explicitly compute\nthe term C(Θ) in the loss function.\nLemma F.6. We have:\nC(Θ) = |ˆx[ξ]|4\n54p2\nN\nX\ni,j=1\nAi\nwAj\nw cos(si\nw −sj\nw)\n\u0010 \u0000(Ai\nu)2 + (Ai\nv)2\u0001 \u0000(Aj\nu)2 + (Aj\nv)2\u0001\n+(Ai\nuAj\nu)2\n2\ncos(2(si\nu −sj\nu)) + (Ai\nvAj\nv)2\n2\ncos(2(si\nv −sj\nv))\n+2Ai\nuAi\nvAj\nuAj\nv\n\u0000cos(si\nu + si\nv −sj\nu −sj\nv) + cos(si\nu −si\nv −sj\nu + sj\nv)\n\u0001 \u0011\n.\n(80)\nProof. This follows from a rather tedious computation, which we summarize here. First, note that\n⟨wi, wj⟩= 2Ai\nwAj\nw\n3p\np−1\nX\nc=0\ncos\n\u0012\nsi\nw + 2π ξ\npc\n\u0013\ncos\n\u0012\nsj\nw + 2π ξ\npc\n\u0013\n= Ai\nwAj\nw\n3\ncos(si\nw −sj\nw).\n(81)\nNext, the shift-equivariance property of the Fourier transform and Plancharel’s theorem imply that\nfor all i and a we have:\n⟨ui, a · x⟩= 1\np⟨bui, d\na · x⟩=\nr 2\n3p|ˆx[ξ]|Ai\nu cos\n\u0012\nsx −si\nu −2πξ a\np\n\u0013\n,\n(82)\nand similarly for vi.\nWe can plug the above expression into the quadratic term\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u00012 from Equation (73) and expand it via the goniometric identity:\n(A cos(α)+B cos(β))2 = 1\n2(A2+B2+A2 cos(2α)+B2 cos(2β))+AB(cos(α+β)+cos(α−β)).\n(83)\nWe similarly expand the term\n\u0000⟨uj, a · x⟩+ ⟨vj, b · x⟩\n\u00012. By leveraging on the goniometric identity\n2 cos(α) cos(β) = cos(α −β) + cos(α + β), the product of these quadratic terms for the ith and\njth neurons expands into 16 unique terms. Since p is odd, 2ξ, 4ξ ̸= 0 (mod p), and thus, by\nEquation (75), only the terms independent from both a and b do not average out. This leaves four\nterms, providing the desired result.\nWe now provide a lower bound for the (meaningful terms of the) loss function.\nTheorem F.7. We have the following lower bound:\nC(Θ) −U(Θ; y) ≥−|ˆx[ξ]|2\np\n.\n(84)\nMoreover, equality holds if, and only if, we have that PN\ni=1 Ci cos(αi) = PN\ni=1 Ci sin(αi) = 0 for\nany choice of (Ci, αi) among\n(Ai\nw((Ai\nu)2 + (Ai\nv)2), si\nw),\n(Ai\nw(Ai\nu)2, si\nw ± 2si\nu),\n(Ai\nw(Ai\nv)2, si\nw ± 2si\nv),\n(Ai\nwAi\nuAi\nv, si\nw ± (si\nu −si\nv)),\n(Ai\nwAi\nuAi\nv, si\nw + si\nu + si\nv),\n(85)\nand, moreover, PN\ni=1 Ai\nwAi\nuAi\nv sin(si\nw + sx −si\nu −si\nv) = 0 and PN\ni=1 Ai\nwAi\nuAi\nv cos(si\nw + sx −\nsi\nu −si\nv) = √54p/|ˆx[ξ]|.\nProof. Given amplitudes C1, . . . , CN and angles α1, . . . , αN, consider the goniometric identity\nN\nX\ni,j=1\nCiCj cos(αi −αj) =\n N\nX\ni=1\nCi cos(αi)\n!2\n+\n N\nX\ni=1\nCi sin(αi)\n!2\n.\n(86)\n37\nBy expanding each product of two cosines in Equation (80) into a sum of two cosines, we can apply\nthe above identity to each summand by choosing αi ∈{si\nw, si\nw ±2si\nu, si\nw ±2si\nv, si\nw ±(si\nu −si\nv), si\nw +\nsi\nu + si\nv, si\nw + sx −si\nu −si\nv}. Since both the summands in the right-hand side of Equation (86) are\npositive, we conclude that:\nC(Θ) ≥|ˆx[ξ]|4\n54p2\n N\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv)\n!2\n,\n(87)\nwith equality holding only if Equation (85) is satisfied. Therefore, using Equation (74), we deduce:\nC(Θ) −U(Θ; y) ≥|ˆx[ξ]|4\n54p2\n N\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv)\n!2\n−\nr\n2\n27p3 |ˆx[ξ]|3\nN\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv)\n=\n \n|ˆx[ξ]|2\n√\n54p\nN\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv) −|ˆx[ξ]|\n√p\n!2\n−|ˆx[ξ]|2\np\n≥−|ˆx[ξ]|2\np\n,\n(88)\nwith equality only when (|ˆx[ξ]|2/(\n√\n54p)) PN\ni=1 Ai\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv) = |ˆx[ξ]|/√p,\nwhich concludes the proof.\nFor N ≥6, the lower bound from Equation (84) is tight. For even N, minimizers can be constructed\nby setting si\nw + sx = si\nu + si\nv =\n2πi\nN , si\nu −si\nv ∈{0, π} alternating (depending on N), and\nAi\nwAi\nuAi\nv = √54p/(N|ˆx[ξ]|). For N odd, a similar construction holds.\nBy combining Theorem F.7 and Equation (72), we deduce the following tight lower bound on the\nloss function:\nL(Θ) ≥1\n2\n\u0012\n∥x∥2 −⟨x, 1⟩2\np\n−2|ˆx[ξ]|2\np\n\u0013\n.\n(89)\nF.3\nUtility update\nNext, we consider the utility for a new neuron after some group of N ≥6 neurons has undergone\nthe previous steps AGF. Suppose that Θ∗= (ui\n∗, vi\n∗, wi\n∗)N\ni=1 are parameters minimizing the loss as in\nSection F.2. We start by describing the function computed by the network with parameters Θ∗.\nLemma F.8. For all 0 ≤a, b < p, the network satisfies:\nf(a · x, b · x; Θ∗) = 2|ˆx[ξ]|\np\n(a + b) · χξ,\n(90)\nwhere χξ is defined as χξ[c] = cos\n\u0010\n2π ξ\npc + sx\n\u0011\n.\nProof. The N neurons compute the function\nf(a · x, b · x; Θ∗)[c] =\nN\nX\ni=1\n\u0000⟨ui\n∗, a · x⟩+ ⟨vi\n∗, b · x⟩\n\u00012 wi\n∗[c].\n(91)\nThe equation above can be expanded via a computation analogous to the one in the proof of Lemma\nF.6. The conditions on minimizers from Theorem F.7 imply that the only non-vanishing term is of the\n38\nform:\nr\n2\n27p3 |ˆx[ξ]|2\nN\nX\ni=1\nAi\nwAi\nuAi\nv cos\n\u0012\n2sx + si\nw −si\nu −si\nv + 2π ξ\np(c −a −b)\n\u0013\n=\nr\n2\n27p3 cos\n\u0012\nsx + 2π ξ\np(c −a −b)\n\u0013 N\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv)\n|\n{z\n}\n√54p/|ˆx[ξ]|\n=2|ˆx[ξ]|\np\ncos\n\u0012\nsx + 2π ξ\np(c −a −b)\n\u0013\n,\n(92)\nwhere in the first identity we used the fact that PN\ni=1 Ai\nwAi\nuAi\nv sin(si\nw + sx −si\nu −si\nv) = 0. This\nconcludes the proof.\nFinally, we compute the utility function with the updated residual r(a · x, b · x) = (a + b) · x −\n(1/p)⟨x, 1⟩1 −f(a · x, b · x; Θ∗), which is maximized by new neurons in the second iteration of\nAGF.\nTheorem F.9. The utility function with the updated residual for a neuron with parameters θ =\n(u, v, w) is:\nU(θ; r) = 2\np3\nX\nk∈[p]\\{0,±ξ}\n|ˆx[k]|2ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ].\n(93)\nProof. The new utility function is\nU(θ; r) = U(θ; y) −1\np2\np−1\nX\na,b=0\nD\n(⟨u, a · x⟩+ ⟨v, b · x⟩)2 w, f(a · x, b · x; Θ∗)\nE\n|\n{z\n}\n∆(θ)\n(94)\nWe wish to express ∆in the frequency domain. By plugging in Equation (90), and due to the cyclic\nstructure of χξ, the squared terms average out:\n∆(θ) =\np−1\nX\na,b=0\n\u0000⟨u, a · x⟩2 + 2⟨u, a · x⟩⟨v, b · x⟩+ ⟨v, b · x⟩2\u0001\n⟨w, f(a · x, b · x; Θ∗)⟩\n(95)\n= 2\np−1\nX\na,b=0\n⟨u, a · x⟩⟨v, b · x⟩⟨w, f(a · x, b · x; Θ∗)⟩.\n(96)\nNow, by reasoning analogously to the proof of Lemma F.1, we obtain:\n∆(θ) = 4|ˆx[ξ]|\np\np−1\nX\na,b=0\n(x ⋆u)[a] (x ⋆y)[b] (χξ ⋆w)[a + b]\n= 4|ˆx[ξ]|\np2\np−1\nX\nk=0\n\\\n(x ⋆u)[k] \\\n(x ⋆v)[k]\n\\\n(χξ ⋆w)[k]\n= 2|ˆx[ξ]|\np\n\u0010\nˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ] + ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ]\n\u0011\n,\n(97)\nwhere in the last equality we used the fact that c\nχξ[k] = p\n2e±isx if k = ±ξ and 0 otherwise. By\nsubtracting the above expression to the expression for the initial utility from Lemma F.2, we obtain\nthe desired result.\nIn summary, after an iteration of AGF, the utility for a new neuron has the same form as the\ninitial one, but with the summand corresponding to the dominant (conjugate) frequencies ±ξ of x\nremoved. Consequently, by the same reasoning as in Section F.1, we conclude that during the utility\n39\nmaximization phase of the second iteration of AGF, some new group of dormant neurons aligns with\nthe harmonic whose frequency has the second largest magnitude in ˆx.\nA subtlety arises during the cost minimization phase of the second iteration, as the neurons that\naligned during the first phase are still involved in the optimization process. However, note that in\nthe loss component C(Θ) (Equation (73)), the terms of the form ⟨wi, wj⟩vanish when i and j are\nindices of neurons aligned with harmonics of different frequencies. Therefore, during the second cost\nminimization phase, the loss L(Θ) splits into the sum of two losses, corresponding to the neurons\naligned during the first and second iteration of AGF, respectively. The neurons from the first phase are\nalready at a critical point of their respective loss term, thus the second group of neurons is optimized\nindependently, via the same arguments as in Section F.1. This scenario is analogous to the one\ndiscussed in Section E.2 for linear transformers (cf. Lemma E.2). In conclusion, after the second\niteration of AGF, the new utility will again have the same form as the initial one, but with the two\n(conjugate pairs of) frequencies removed. This argument iterates recursively, until either all the\nfrequencies are exhausted, or all the H neurons have become active. The quantities in Equation (9)\ncan be derived for the successive iterations of AGF analogously to the previous sections. Lastly, the\nestimate on jump times is obtained by the same argument as in Section E.3.\n40\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: The abstract describes the contributions on the high level, while the introduction\ncontains a precise statement of contributions, with references to the sections where such\ncontributions are discussed.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The main limitation of our work is that our theory is developed for two-layer\nneural networks. We highlight this limitation in the title, abstract, and introduction. When\nintroducing our framework in Section 2 we clearly explain the assumptions used to develop\nour algorithm. At the end of the paper in Section 6 we discuss additional limitations of our\nwork and how future work could address them.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n41\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: All theorems and formal statements are stated either in the main body, or in\nthe appendix, with the full set of assumptions and clearly defined notation. All the formal\nproofs are provided in the appendix with intuition discussed in the main.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide a detailed description of our algorithm AGF in Section 2.2 and\nprovide pseudocode in Figure 2. In each setting where we applied AGF we fully describe the\nexperimental setup, including necessary assumptions on initialization and hyperparameters,\nsuch that a reader could faithfully reconstruct our experiments. Every figure clearly discusses\nwhat quantities from the experiments are being visualized.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\n42\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: In our supplementary material we enclose a set of notebooks that can be run to\ngenerate all figures and experiments discussed in the paper. In these notebooks we have also\nimplemented a version of our main algorithm AGF such that others could adapt our code to\nrun in settings not discussed in this work.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\n43\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: The primary purpose of our experiment is to describe the feature learning\ndynamics of gradient flow. Thus, our experiments do not discuss train or test splits, rather\nthey show faithful reconstruction of gradient flow dynamics from our proposed theory of\nAGF. As mentioned above, we provide all the necessary hyperparameters (learning rate and\ninitialization scheme) to reconstruct the figures introduced in this work.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [NA]\nJustification: The experiments we introduce are for gradient flow with complete access to\nthe training data. Thus, in the figures we present there should be no variability between runs\nwith the same initialization as each run is essentially a solution to an ODE up to numerical\nprecision. Our provided code sets a random seed for each experiment such that a reader can\nexactly reproduce each figure introduced in our work.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\n44\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: All experiments in this work involve shallow networks with small constructed\ndatasets that can be run locally. Our provided code describe the necessary packages needed\nto run our code, but specialized computer resources are not needed.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We have reviewed the Code of Ethics, and believe that the research in the\npaper conforms to it in every aspect (e.g., complete anonymity).\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: This paper discusses general theoretical properties of neural networks. As\nsuch, we do not see any potential societal impact of the research presented here.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\n45\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper does not involve any model or dataset with a high risk for misuse.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: The paper does not use any existing asset.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n46\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: The paper does not introduce any new asset.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n47\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: No LLM was used in the development of the core results of the paper. LLMs\nwere only used for writing and editing purposes.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n48\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Alternating Gradient Flows: A Theory of\nFeature Learning in Two-layer Neural Networks\nDaniel Kunin†\nStanford University\nGiovanni Luca Marchetti†\nKTH\nFeng Chen\nStanford University\nDhruva Karkada\nUC Berkeley\nJames B. Simon\nUC Berkeley and Imbue\nMichael R. DeWeese\nUC Berkeley\nSurya Ganguli\nStanford University\nNina Miolane\nUC Santa Barbara\nAbstract\nWhat features neural networks learn, and how, remains an open question. In this\npaper, we introduce Alternating Gradient Flows (AGF), an algorithmic framework\nthat describes the dynamics of feature learning in two-layer networks trained from\nsmall initialization. Prior works have shown that gradient flow in this regime\nexhibits a staircase-like loss curve, alternating between plateaus where neurons\nslowly align to useful directions and sharp drops where neurons rapidly grow\nin norm. AGF approximates this behavior as an alternating two-step process:\nmaximizing a utility function over dormant neurons and minimizing a cost function\nover active ones. AGF begins with all neurons dormant. At each iteration, a\ndormant neuron activates, triggering the acquisition of a feature and a drop in the\nloss. AGF quantifies the order, timing, and magnitude of these drops, matching\nexperiments across several commonly studied architectures. We show that AGF\nunifies and extends existing saddle-to-saddle analyses in fully connected linear\nnetworks and attention-only linear transformers, where the learned features are\nsingular modes and principal components, respectively. In diagonal linear networks,\nwe prove AGF converges to gradient flow in the limit of vanishing initialization.\nApplying AGF to quadratic networks trained to perform modular addition, we give\nthe first complete characterization of the training dynamics, revealing that networks\nlearn Fourier features in decreasing order of coefficient magnitude. Altogether,\nAGF offers a promising step towards understanding feature learning in neural\nnetworks.\n1\nIntroduction\nThe impressive performance of artificial neural networks is often attributed to their capacity to learn\nfeatures from data. Yet, a precise understanding of what features they learn and how remains unclear.\nA large body of recent work has sought to understand what features neural networks learn by reverse\nengineering the computational mechanisms implemented by trained neural networks [1–6]. Known\nas mechanistic interpretability (MI), this approach involves decomposing trained networks into\ninterpretable components to uncover their internal representations and algorithmic strategies [7]. MI\nhas achieved notable successes in understanding the emergent abilities of large language models\n[8], including identifying induction heads that enable in-context learning [3] and revealing that\nsmall transformers trained on algebraic tasks use Fourier features [6, 9]. Despite these discoveries,\nmechanistic interpretability remains limited by its empirical nature, lacking a theoretical framework\nto formally define features and predict when and how they emerge [10].\n† Correspondence to: kunin@berkeley.edu and glma@kth.se.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n"
    },
    {
      "page_number": 2,
      "text": "Diagonal \nLinear Networks\nFully-Connected \nLinear Networks\nAttention-Only \nLinear Transformers\n  Utility\nMaximization\n  Cost \nMinimization\n...\n+\n...\n...\n...\n...\n+\n...\n...\n+\n...\nLoss\nSteps\n...\n+\n...\nTwo-layer\nNonlinear Networks\nQuadratic Networks\nfor Modular Addition\n+\nLearns \nsingular \nvectors\nLearns \nregression\ncoefficient\nLearns \nFourier \nfrequency\nLearns \nprincipal \ncomponent\n...\n...\nAGF\nGD\nFigure 1: A unified theory of feature learning in two-layer networks. Left: Alternating Gradient\nFlows (AGF) models feature learning as a two-step process alternating between utility maximization\n(blue plateaus) and cost minimization (red drops), where each drop reflects learning a new feature\n(see Section 2). Middle: AGF unifies prior analyses of saddle-to-saddle dynamics (see Sections 3\nand 4). Right: AGF enables new analysis of empirical phenomena (see Section 5).\nA different line of work, rooted in deep learning theory, has sought to understand how features are\nlearned by directly studying the dynamics of the network during training. Empirical studies suggest\nthat neural networks learn simple functions first, progressively capturing more complex features\nduring training [11–14]. Termed incremental, stepwise, or saddle-to-saddle, this learning process is\nmarked by long plateaus of minimal loss change followed by rapid drops. It is conjectured to arise\nfrom networks, initialized at small-scale, jumping between saddle points of the loss landscape [15],\nwith each drop corresponding to the acquisition of a new feature [16]. This saddle-to-saddle process\nhas been explored across a range of simple settings, including diagonal linear networks [17, 18],\nfully connected linear networks [19, 15], tensor decomposition [20, 21], self-supervised learning\n[22, 23], shallow ReLU networks [24–30], attention-only transformers [31, 32], and multi-index\nmodels [33–39]. However, these analyses often rely on different simplifying assumptions about the\narchitecture (e.g., linear activations), data (e.g., orthogonal inputs), or optimizer (e.g., layer-wise\nlearning), making it difficult to establish a unified, scalable understanding of feature learning.\nIn this work, we introduce a theoretical framework that unifies existing analyses of saddle-to-saddle\ndynamics in two-layer networks under the vanishing initialization limit, precisely predicting the order,\ntiming, and magnitude of loss drops, while extending beyond classical settings to explain empirically\nobserved patterns of feature emergence. See Figure 1 for a visual overview of the paper. Our main\ncontributions are:\n1. We introduce Alternating Gradient Flows (AGF), a two-step algorithm that approximates gra-\ndient flow in two-layer networks with small initialization by alternating between maximizing\na utility over dormant neurons and minimizing a cost over active ones (Section 2).\n2. We prove that AGF converges to the dynamics of gradient flow for diagonal linear networks\nin the vanishing initialization limit (Section 3).\n3. We show how AGF unifies and generalizes existing theories of saddle-to-saddle dynamics\nacross fully connected linear networks and attention-only linear transformers (Section 4).\n4. We use AGF to predict the emergence of Fourier features in modular arithmetic tasks,\nproviding the first theoretical account of both the order in which frequencies appear and the\ndynamics that drive it (Section 5).\nOur work takes a step toward unifying deep learning theory and mechanistic interpretability, suggest-\ning that what features networks learn, and how, can be understood through optimization dynamics.\nSee Appendix A for further discussion of related theoretical approaches to studying feature learning\nand saddle-to-saddle dynamics in two-layer networks, including mean-field analysis [40–43] and\nteacher–student frameworks [44–51].\n2\nDeriving a Two-step Algorithm that Approximates Gradient Flow\nSetup and notations.\nWe consider a two-layer neural network with H hidden neurons of the form\nf(x; Θ) = PH\ni=1 aiσ(w⊺\ni gi(x)), where each hidden neuron i has learnable input weights wi ∈Rd\nand output weights ai ∈Rc, and processes a potentially neuron-specific input representation gi(x).\n2\n"
    },
    {
      "page_number": 3,
      "text": "The activation σ: R →R is origin-passing, i.e., σ(0) = 0, as satisfied by common functions (e.g.,\nlinear, ReLU, square, tanh). Let fi(x; θi) = aiσ(w⊺\ni gi(x)) denote the output of the ith neuron, where\nθi = (wi, ai). The parameters Θ = (θ1, . . . , θH) ∈RH×(d+c) for the entire network evolve under\ngradient flow (GF), ˙Θ = −η∇ΘL(Θ), to minimize the MSE loss L(Θ) = Ex\n\u0002 1\n2∥y(x) −f(x; Θ)∥2\u0003\n,\nwhere y: Rd →Rc is the ground truth function and the expectation is taken over the input distribution.\nThe learning rate η > 0 rescales time without affecting the trajectory. The parameter vectors for each\nneuron are initialized ai ∼N(0, α2\n2c Ic), wi ∼N(0, α2\n2d Id). In the limit α →∞, the gradient flow\ndynamics enter the kernel regime [52, 53]. When α = 1, the dynamics correspond to a mean-field or\nmaximal update parameterization [40, 54]. We study the small-scale initialization regime α ≪1,\nwhere the dynamics are conjectured to follow a saddle-to-saddle trajectory as α →0 [15, 14].\n2.1\nThe behavior of gradient flow at small-scale initialization\nBakhtin [55] showed that, under a suitable time rescaling, the trajectory of a smooth dynamical\nsystem initialized at a critical point and perturbed by vanishing noise converges to a piecewise\nconstant process that jumps instantaneously between critical points. These results suggest that, in the\nvanishing initialization limit α →0, gradient flow converges to a similar piecewise constant process,\njumping between saddle points of the loss before reaching a minimum. To approximate this behavior,\nwe first examine the structure of critical points in the loss landscape of the two-layer networks we\nstudy, and then analyze the dynamics of gradient flow near them. These dynamics reveals two distinct\nphases, separated by timescales, which in turn motivates the construction of AGF.\nCritical points are structured by partitions of neurons into dormant and active sets.\nAs α →0,\nthe initialization converges to the origin Θ = 0, which is a critical point of the loss. The origin is\none of potentially an exponential number of distinct (up to symmetry) critical points, that can be\nstructured according to partitions of the H hidden neurons. Formally, split neurons into two disjoint\nsets: dormant neurons D ⊆[H] and active neurons A = [H]\\D, with parameters ΘD = (θi)i∈D\nand ΘA = (θi)i∈A, respectively. Due to the two-layer structure of our model and the origin-passing\nactivation function, setting the parameters of the dormant neurons to zero ΘD = 0 restricts the loss\nto a function L(ΘA) of only the active neurons. Now any critical point Θ∗\nA of the restricted loss\nyields a critical point Θ = (0, Θ∗\nA) of the original loss. Thus the 2H partitions of dormant and active\nneurons structure critical ponts of the loss landscape, with the origin corresponding to the special\ncritical point with all neurons dormant.\nDynamics near a saddle point.\nConsider the dynamics near a saddle point defined by a dormant\nand active set, where we assume the active neurons are at a local minimum of their restricted loss\nL(ΘA). By construction, neurons in our two-layer network only interact during training through the\nresidual r(x; Θ) = y(x) −f(x; Θ) ∈Rc, which quantifies the difference between the predicted and\ntarget values at each data point x. Near a critical point, the residual primarily depends on the active\nneurons, as the contribution from the dormant neurons is effectively zero. Consequently, dormant\nneurons evolve independently of each other, while active neurons remain collectively equilibrated.\nTo characterize the dynamics of the dormant neurons, which will determine how the dynamics escape\nthe saddle point, we define the utility function, Ui : Rd+c →R for each i ∈D as:\nUi(θ; r) = Ex [⟨fi(x; θ), r(x)⟩] ,\nwhere r(x) = y(x) −f(x; ΘA).\n(1)\nIntuitively, the utility Ui quantifies how correlated the ith dormant neuron is with the residual r,\nwhich itself is solely determined by the active neurons. Using this function we can approximate the\ndirectional and radial dynamics for the parameters of each dormant neuron as\nd\ndt ¯θi = η∥θi∥κ−2P⊥\nθi∇θUi(¯θi; r),\nd\ndt∥θi∥= ηκ∥θi∥κ−1Ui(¯θi; r),\n(2)\nwhere ¯θi = θi/∥θi∥are the normalized parameters, P⊥\nθi =\n\u0000Id −¯θi¯θ⊺\ni\n\u0001\nis an orthogonal projection\nmatrix, and κ ≥2 is the leading order of the Taylor expansion of the utility from the origin. Note\nthat the directional dynamics scale as ∥θi∥κ−2, while the radial dynamics scale as ∥θi∥κ−1. Because\n∥θi∥≪1 for dormant neurons, the directional dynamics evolves significantly faster than the radial\ndynamics. Consequently, the contribution of a dormant neuron to the residual is suppressed, keeping\nthe residual effectively constant, and this approximation to the dynamics stays valid. This separation\nof timescales between directional and radial components has been utilized in several previous studies\nof gradient descent dynamics near the origin [56–61].\n3\n"
    },
    {
      "page_number": 4,
      "text": "Algorithm 1: AGF\nInitialize: D ←[H], A ←∅, S ←0 ∈RH\nwhile ∇L(ΘD) ̸= 0 do\nwhile ∀i ∈D\nSi ≤ci do\nfor i ∈D do\nd¯\nθi\ndt = ηα∥θi∥κ−2P⊥\nθi ∇U(¯θi, r)\ndSi\ndt\n= ηακU(¯θi, r)\nActivate neuron: D, A ←D \\ {i∗}, A ∪{i∗}\nwhile ∇L(ΘA) ̸= 0 do\nfor j ∈A do\ndθj\ndt\n= −∇θj L(ΘA)\nRemove collapsed neurons: D, A ←D ∪C, A \\ C\nConceptual Illustration: AGF\nNeuron 1\nNeuron 2\nNeuron 3\nActivates\nDormant neurons \nwork individually at small scales \nto maximize their utility\nSteps\nLoss\nActive neurons \nwork collectively at large scales\nto minimize the cost\nFigure 2: Alternating Gradient Flows (AGF). AGF alternates between utility maximization (blue)\nover dormant neurons and cost minimization (red) over active neurons, predicting saddle-to-saddle\ndynamics in training. Utility maximization: dormant neurons evolve independently driven by\nprojected gradient flow to maximize their utility. We keep track of the accumulated utility for each\ndormant neuron to determine the first neuron to transition to active. Cost minimization: Active\nneurons work collectively driven by gradient flow to minimize the loss until convergence. After\nconvergence, active neurons compute a new residual that determines the utility at the next iteration. It\nis possible in this step for active neurons to become dormant again (see Appendix B).\nEstimating the jump time from dormant to active.\nThe dynamical approximation in Equation (2)\nbreaks down if a dormant neuron crosses the threshold ∥θi∥= O(1), becoming active by influencing\nthe residual. We denote by i∗∈D the first dormant neuron to reach this threshold and by τi∗the\njump time at which this dormant to active transition occurs. To determine τi∗, we compute for each\ndormant neuron the earliest time τi when ∥θi(τi)∥> 1, such that i∗= arg mini∈D τi. This time τi is\ndefined implicitly in terms of the accumulated utility, the path integral Si(t) =\nR t\n0 κUi(¯θi(s); r) ds:\nτi = inf\nn\nt > 0\n\f\f\f Si(t) > ci\nη\no\n,\nwhere ci =\n\u001a−log (∥θi(0)∥)\nif κ = 2,\n−\n1\n2−κ\n\u0000∥θi(0)∥2−κ −1\n\u0001\nif κ > 2.\n(3)\nIn the vanishing initialization limit as α →0, the jump time defined in Equation (3) diverges\nas gradient flow exhibits extreme slowing near the critical point at the origin. Thus, to capture\nmeaningful dynamics in the limit, we accelerate time by setting the learning rate η = EΘ0 [ci] where\nci is the threshold defined in Equation (3) and the expectation is taken over the initialization.\n2.2\nAlternating Gradient Flow (AGF) approximates GF at small-scale initialization\nBased on the behaviors of gradient flow (GF) at small-scale initialization, we introduce Alternating\nGradient Flows (AGF) (Algorithm 1) as an approximation of the dynamics. AGF characterizes phases\nof constant loss as periods of utility maximization and sudden loss drops as cost minimization steps,\naccurately predicting both the loss levels and the timing of loss drops during training. At initialization,\nall neurons are dormant. At each iteration, a neuron leaves the dormant set and enters the active set.\nThe dormant neurons work individually to maximize their utility, and the active neurons collectively\nwork to minimize a cost (Figure 2). Specifically, each iteration in AGF is divided into two steps.\nStep 1: Utility maximization over dormant neurons.\nIn this step, only dormant neurons update\ntheir parameters. They independently maximize (locally) their utility function over the unit sphere by\nfollowing the projected gradient flow (left of Equation (2)). We keep track of the accumulated utility\nfor each neuron to determine the first neuron to transition and the jump time using Equation (3). At\nthe jump time, we record the norms and directions of the dormant neurons that have not activated, as\nthey will serve as the initialization for the utility maximization phase of the next iteration.\nStep 2: Cost minimization over active neurons.\nIn this step, all active neurons interact to minimize\n(locally) the loss, by following the negative gradient flow of L(ΘA). For previously active neurons,\nthe initialization is determined by the previous cost minimization step. For the newly activated neuron,\nthe initialization comes from the utility maximization step. It is possible in this step for active neurons\nto become dormant again if the optimization trajectory brings an active neuron near the origin (see\nAppendix B). After convergence, we compute the new residual r(x) = y(x) −f(x; ΘA), which\ndefines a new utility for the remaining dormant neurons at the next iteration.\n4\n"
    },
    {
      "page_number": 5,
      "text": "Termination.\nWe repeat both steps, recording the corresponding sequence of jump times and loss\nlevels, until there are either no remaining dormant neurons or we are at a local minimum of the\nloss. Through this process, we have generated a precise sequence of saddle points and jump times to\ndescribe how gradient flow leaves the origin through a saddle-to-saddle process.\nWhile AGF and GF exhibit similar behaviors at small-scale initialization (α ≪1), a natural question\nis whether their trajectories converge to each other as α →0. While a general proof remains open (see\nSection 6), in the next section we present an illustrative setting where convergence can be established.\n3\nA Setting Where AGF and GF Provably Converge to Each Other\nSteps\nCoefficient\nSteps\nLoss\nFigure 3: AGF = GF as α →0 in diagonal linear networks.\nTraining loss curves for a diagonal linear network under the\nsetup described in Section 3 for various initialization values\nα. As α →0, the trajectory predicted by AGF and the\nempirics of gradient flow converge. To ensure a meaningful\ncomparison between experiments we set η = −log(α).\nDiagonal linear networks are simple\nyet insightful models for analyzing\nlearning [62–65, 17, 66]. Central to\ntheir analysis is an interpretation of\ngradient flow in function space as\nmirror descent with an initialization-\ndependent potential that promotes\nsparsity when α is small. Using this\nperspective, Pesme and Flammarion\n[18] characterized the sequence of\nsaddles and jump times for gradient\nflow in the limit α →0. We show that,\nin this limit, AGF converges to the ex-\nact same sequence, establishing a set-\nting where AGF provably converges\nto gradient flow (see Figure 3).\nWe consider a two-layer diagonal linear network trained via gradient flow to minimize the MSE\nloss L(β) =\n1\n2n∥y −β⊺X∥2, where the regression coefficients β ∈Rd are parameterized as the\nelement-wise product β = u ⊙v with u, v ∈Rd, and (X, y) ∈Rd×N × RN are the input, output\ndata. This setup fits naturally within AGF, where the ith neuron corresponds to βi with parameters\nθi = (ui, vi), data representation gi(x) = xi, and κ = 2. Following Pesme and Flammarion [18],\nwe initialize θi(0) = (\n√\n2α, 0) such that βi(0) = 0, and assume the inputs are in general position, a\nstandard technical condition in Lasso literature [67, 18] to rule out unexpected linear dependencies.\nUtility maximization.\nThe utility function for the ith neuron is Ui = −uivi∇βiL(βA), which is\nmaximized on the unit sphere u2\ni + v2\ni = 1 when sgn(uivi) = −sgn(∇βiL(βA)) and |ui| = |vi|,\nyielding a maximal utility of ¯U∗\ni =\n1\n2|∇βiL(βA)|. What makes this setting special is that not\nonly can the maximal utility be computed in closed form, but every quantity involved in utility\nmaximization—namely the accumulated utility, jump times, and directional dynamics—admits an\nexact analytical expression. The key insight is that the normalized utility ¯Ui(t) for each dormant\nneuron i evolves according to a separable Riccati ODE, interpolating from its initial value ¯Ui(0) to its\nmaximum ¯U∗\ni . As a result, we can derive an explicit formula for the normalized utility ¯Ui(t), whose\nintegral yields the accumulated utility Si(t), evolving as:\nSi(τ (k) + t) =\n1\n2ηα log cosh\n\u0010\n2ηα\n\u0010\n2U∗\ni t ±\n1\n2ηα cosh−1 exp\n\u00002ηαSi\n\u0000τ (k)\u0001\u0001\u0011\u0011\n,\n(4)\nwhere ηα = −log(\n√\n2α) is the learning rate and the unspecified sign is chosen based on sgn(uivi)\nand sgn(∇βiL(βA)), as explained in Appendix C. This expression allows us to determine the next\nneuron to activate, as the first i ∈D for which Si = 1, from which the jump time can be computed.\nCost minimization.\nActive neurons represent non-zero regression coefficients of β (see Figure 3\nright). During the cost minimization step, the active neurons work to collectively minimize the loss.\nWhen a neuron activates, it does so with a certain sign sgn(uivi) = −sgn(∇βiL(βA)). If, during\nthe cost minimization phase, a neuron changes sign, then it can do so only by returning to dormancy\nfirst. This is due to the fact that throughout the gradient flow dynamics, the quantity u2\ni −v2\ni = 2α2\nis conserved and thus, in order to flip the sign of the product uivi, the parameters must pass through\ntheir initialization. As a result, the critical point reached during the cost minimization step is the\n5\n"
    },
    {
      "page_number": 6,
      "text": "unique solution to the constrained optimization problem,\nβA = arg min\nβ∈Rd\nL(β)\nsubject to\n\u001aβi = 0\nif i /∈A,\nβi · sgn(uivi) ≥0\nif i ∈A.\n(5)\nAll coordinates where (βA)i = 0 are dormant at the next step of AGF.\nAGF in action: sparse regression.\nWe now connect AGF to the algorithm proposed by Pesme\nand Flammarion [18], which captures the limiting behavior of gradient flow under vanishing\ninitialization.\nTheir algorithm tracks, for each coefficient of β, an integral of the gradient,\nSi(t) = −\nR t\n0 ∇βiL(β(˜tα(s))) ds, with a time rescaling ˜tα(s) = −log(α)s. They show that in\nthe limit α →0, this quantity is piecewise linear and remains bounded in [−1, 1]. Using these\nproperties, each step of their algorithm determines a new coordinate to activate (the first for which\nSi(t) = ±1), adds it to an active set, then solves a constrained optimization over this set, iterating\nuntil convergence. Despite differing formulations, this process is identical to AGF in the limit α →0:\nTheorem 3.1. Let (βAGF, tAGF) and (βPF, tPF) be the sequences produced by AGF and Algorithm 1\nof Pesme and Flammarion [18], respectively. Then, (βAGF, tAGF) →(βPF, tPF) pointwise as α →0.\nThe key connection lies in the asymptotic identity log cosh(x) →|x| as |x| →∞, which implies\nthat the accumulated utility Si(t) in AGF converges to the absolute value of the integral Si(t). The\nunspecified sign in AGF corresponds to the sign of the boundary conditions in their algorithm. Thus,\nAGF converges to the same saddle-to-saddle trajectory as the algorithm of Pesme and Flammarion\n[18], and therefore to gradient flow. See Appendix C for a full derivation.\n4\nAGF Unifies Existing Analysis of Saddle-to-Saddle Dynamics\n(a) Commuting\n(b) Non-commuting\nSingular Value\nGD\nSteps\nConj. 4.1\nFigure 4: Stepwise singular value decomposition. Training\na two-layer fully connected linear network on Gaussian in-\nputs with a power-law covariance Σxx and labels y(x) = Bx\ngenerated from a random B. We show the dynamics of the\nsingular values of the network’s map AW when Σxx com-\nmutes with Σ⊺\nyxΣyx (a) and when it does not (b). Conjec-\nture 4.1 (black dashed lines) predicts the dynamics well.\nFully connected linear network.\nLinear networks have long served as\nan analytically tractable setting for\nstudying neural network learning dy-\nnamics [68–73]. Such linear networks\nexhibit highly nonlinear learning dy-\nnamics. Saxe et al. [70] demonstrated\nthat gradient flow from a task-aligned\ninitialization learns a sequential singu-\nlar value decomposition of the input-\noutput cross-covariance. This behav-\nior persists in the vanishing initial-\nization limit without task alignment\n[57, 74, 19, 20, 15], and is amplified\nby depth [75, 62]. Here, we show how\nAGF naturally recovers such greedy\nlow-rank learning.\nWe consider a fully connected two-layer linear network, f(x; θ) = AWx, with parameters W ∈\nRH×d and A ∈Rc×H. The network is trained to minimize the MSE loss with data generated\nfrom the linear map y(x) = Bx, where B ∈Rc×d is an unknown matrix. The inputs are drawn\nindependently from a Gaussian x ∼N(0, Σxx), where Σxx ∈Rd×d is the input covariance, and\nΣyx = BΣxx ∈Rc×d is the input-output cross-covariance, which we assume are full rank with\ndistinct singular values. A subtlety in applying AGF to this setting is that the standard notion of\na neuron used in Section 2 is misaligned with the geometry of the loss landscape. Due to the\nnetwork’s invariance under (W, A) 7→(GW, AG−1) for any G ∈GLH(R), critical points form\nmanifolds entangling hidden neurons, and conserved quantities under gradient flow couple their\ndynamics [76, 77]. To resolve this, we can reinterpret AGF in terms of evolving dormant and active\northogonal basis vectors instead of neurons. Each basis vector ( ˜ai, ˜wi) ∈Rc+d forms a rank-1 map\n˜ai ˜w⊺\ni ∈Rc×d such that the function computed by the network is the sum over these rank-1 maps,\nf(x; Θ) = P\ni∈[ ˜\nH] ˜ai ˜w⊺\ni x, where ˜H = min(c, H, d). From this perspective, at each iteration of AGF\nthe dormant set loses one basis vector, while the active set gains one. See Appendix D for details.\nUtility maximization.\nLet βA = P\ni∈A ˜ai ˜w⊺\ni be the function computed by the active basis vectors\nand m = |D|. The total utility over the dormant basis vectors is U = Pm\ni=1 ˜a⊺\ni ∇βL(βA) ˜wi.\n6\n"
    },
    {
      "page_number": 7,
      "text": "Maximizing this sum while maintaining orthonormality between the basis vectors yields a Rayleigh\nquotient problem, whose solution aligns the dormant basis (˜ai, ˜wi)m\ni=1 with the top m singular modes\n(ui, vi)m\ni=1 of ∇βL(βA). Each aligned pair attains a maximum utility of U∗\ni = σi/2, where σi is the\ncorresponding singular value. The basis vector aligned with the top singular mode activates first,\nexiting the dormant set and joining the active one.\nCost minimization.\nThe cost minimization step of AGF can be recast as a reduced rank regression\nproblem, minimizing L(β) over β ∈Rc×d subject to rank(β) = k, where k = |A|. As first shown in\nIzenman [78], the global minimum for this problem is an orthogonal projection of the OLS solution\nβ∗\nA = PUkΣyxΣ−1\nxx , where PUk = UkU ⊺\nk is the projection onto Uk ∈Rc×k, the top k eigenvectors\nof ΣyxΣ−1\nxx Σ⊺\nyx. Using this solution, we can show that the next step of utility maximization will be\ncomputed with the matrix ∇βL(β∗\nA) = P⊥\nUkΣyx where P⊥\nUk = Ic −PUk.\nAGF in action: greedy low-rank learning.\nIn the vanishing initialization limit, AGF reduces to\nan iterative procedure that selects the top singular mode of ∇βL(β∗\nA), transfers this vector from the\ndormant to the active basis, then minimizes the loss with the active basis to update β∗\nA. This procedure\nis identical to the Greedy Low-Rank Learning (GLRL) algorithm by Li et al. [20] that characterizes\nthe gradient flow dynamics of two-layer matrix factorization problems with infinitesimal initialization.\nEncouraged by this connection, we make the following conjecture:\nConjecture 4.1. In the initialization limit α →0, a two-layer fully connected linear network trained\nby gradient flow, with η = −log(α), learns one rank at a time leading to the sequence\nf (k)(x) = P\ni≤k PuiΣyxΣ−1\nxx x,\nℓ(k) = 1\n2\nP\ni>k µi,\nτ (k) = P\ni≤k ∆τ (i),\n(6)\nwhere 0 ≤k ≤min(d, H, c), (ui, µi) are the eigenvectors and eigenvalues of ΣyxΣ−1\nxx Σ⊺\nyx, σ(k)\ni\nis\nthe ith singular value of P⊥\nUkΣyx, and ∆τ (i) =\n\u0010\n1 −Pi−2\nj=0 σ(j)\ni−j∆τ (j+1)\u0011\n/σ(i−1)\n1\nwith ∆τ (0) = 0.\nWhen Σxx commutes with Σ⊺\nyxΣyx, Conjecture 4.1 recovers the sequence originally proposed by\nGidel et al. [19]. Figure 4 empirically supports this conjecture. See Appendix D for details.\nGD\nAGF\nSteps\nSingular Value\nFigure 5: Stepwise principal component regres-\nsion. Training a linear transformer to learn linear\nregression in context. We show the evolution of\nsingular values of PH\ni=1 ViKiQ⊺\ni . Horizontal lines\nshow theoretical Ak and vertical dashed lines show\nlower bounds for the jump time from Equation (7)\nwith l = k −1. Dashed black lines are numerical\nAGF predictions.\nAttention-only\nlinear\ntransformer.\nPre-\ntrained large language models can learn new\ntasks from only a few examples in context,\nwithout explicit fine-tuning [79]. To understand\nthe emergence of this in-context learning ability,\nprevious empirical [80–82] and theoretical\nworks [83–88] have examined how transformers\nlearn to perform linear regression in context.\nNotably, Zhang et al. [32] showed that an\nattention-only linear transformer learns to\nimplement principal component regression\nsequentially, with each learned component\ncorresponding to a drop in the loss. We show\nthat AGF recovers their analysis (see Figure 5).\nWe consider a attention-only linear transformer\nf(X; Θ) = X+PH\ni=1 WV,iXX⊺WK,iW ⊺\nQ,iX,\nwhere X is the input sequence, WV\n∈\nRH×(d+1)×(d+1), WK, WQ ∈RH×(d+1)×R\nrepresent the value, key, and query matrices\nrespectively, and H denotes the number of at-\ntention heads. While this network uses linear\nactivations, it is cubic in its input and parameters. Also, when the rank R = 1, each attention head\nbehaves like a homogeneous neuron with κ = 3, making it compatible with the AGF framework.\nWe consider the linear regression task with input Xi = ( xi\nyi ) for i ≤N, where xi ∼N(0, Σxx),\nand yi = β⊺xi with β ∼N(0, Id). The input covariance Σxx ∈Rd×d has eigenvalues λ1 ≥· · · ≥\nλd > 0 with corresponding eigenvectors v1, . . . , vd. The final input token is XN+1 = ( xN+1\n0\n). The\nnetwork is trained by MSE loss to predict yN+1 given the entire sequence X, where the model\nprediction is taken to be ˆyN+1 = f(X; Θ)d+1,N+1. Following Zhang et al. [32], we initialize all\n7\n"
    },
    {
      "page_number": 8,
      "text": "parameters to zero except for some slices, which we denote by V ∈RH, Q, K ∈RH×d, which are\ninitialized from N(0, α). The parameters initialized at zero will remain at zero throughout training\n[32], and the model prediction reduces to ˆyN+1 = PH\nh=1 Vh\nPN\nn=1 ynx⊺\nnKhQ⊺\nhxN+1. We defer\nderivations to Appendix E, and briefly outline how AGF predicts the saddle-to-saddle dynamics.\nUtility maximization.\nAt the kth iteration of AGF, the utility of a dormant attention head is\nUi = NViQ⊺\ni (Σ2\nxx −Pk−1\nh=1 λ2\nhvhv⊺\nh)Ki which is maximized over normalized parameters when\n¯Vi = ±1/\n√\n3 and ¯Qi, ¯Ki = ±vk/\n√\n3, where the sign is chosen such that the maximum utility is\n¯U∗= Nλ2\nk/3\n√\n3. In other words, utility maximization encourages dormant attention heads to align\ntheir key and query vectors with the dominant principal component of the input covariance not yet\ncaptured by any active head. This creates a race condition among dormant heads, where the first to\nreach the activation threshold, measured by their accumulated utility, becomes active and learns the\ncorresponding component. Assuming instantaneous alignment of the key and query vectors, we can\nlower bound the jump time for the next head to activate, as shown in Equation (7).\nCost minimization.\nDuring cost minimization, because v1, . . . , vd form an orthonormal basis, the\nupdates for each attention head are decoupled. Thus, we only need to focus on how the magnitude\nof the newly active head changes. Specifically, we determine the magnitude Ak of the newly\nlearned function component, given by fk(X; θk)d+1,N+1 = Ak\nPN\nn=1 ynx⊺\nnvkv⊺\nkxN+1. Solving\n∂L(Ak)\n∂Ak\n= 0, we find that the optimal magnitude is Ak =\n1\ntrΣxx+(N+1)λk , from which we can derive\nthe expression for the prediction and loss level after the kth iteration of AGF, as shown in Equation (7).\nAGF in action: principal component regression.\nAt each iteration, the network projects the input\nonto a newly selected principal component of Σxx and fits a linear regressor along that direction. Let\nµ(l) = maxi∈D ∥θi(τ (l))∥for l < k and ηα = α−1. This process yields the sequence,\nˆy(k)\nN+1 = Pk,N\ni,n=1\nynx⊺\nnviv⊺\ni xN+1\ntrΣxx+(N+1)λi , ℓ(k) = trΣxx\n2\n−Pk\ni=1\nNλi/2\ntrΣxx\nλi\n+N+1, τ (k) ≳τ (l) + η−1\nα\nµ(l)\n√\n3\nNλ2\nk ,\n(7)\nwhich recovers the results derived in Zhang et al. [32] and provides an excellent approximation to the\ngradient flow dynamics (see Figure 5).\n5\nAGF Predicts the Emergence of Fourier Features in Modular Addition\nIn previous sections, we showed how AGF unifies prior analyses of feature learning in linear networks.\nWe now consider a novel nonlinear setting: a two-layer quadratic network trained on modular addition.\nOriginally proposed as a minimal setting to explore emergent behavior [89], modular addition has\nsince become a foundational setup for mechanistic interpretability. Prior work has shown that\nnetworks trained on this task develop internal Fourier representations and use trigonometric identities\nto implement addition as rotations on the circle [6, 90, 91]. Similar Fourier features have been\nobserved in networks trained on group composition tasks [9], and in large pre-trained language\nmodels performing arithmetic [92, 93]. Despite extensive empirical evidence for the universality\nof Fourier features in deep learning, a precise theoretical explanation of their emergence remains\nopen. Recent work has linked this phenomenon to the average gradient outer product framework\n[94], the relationship between symmetry and irreducible representations [95], implicit maximum\nmargin biases of gradient descent [96], and the algebraic structure of the solution space coupled\nwith a simplicity bias [97]. Here, we leverage AGF to unveil saddle-to-saddle dynamics, where each\nsaddle corresponds to the emergence of a Fourier feature in the network (see Figure 6).\nWe consider a setting similar to [90, 96, 97], but with more general input encodings. Given p ∈N, the\nground-truth function is y: Z/p×Z/p →Z/p, (a, b) 7→a+b mod p, where Z/p = {0, . . . , p−1} is\nthe (additive) Abelian group of integers modulo p. Given a vector x ∈Rp, we consider the encoding\nZ/p →Rp, a 7→a · x, where · denotes the action of the cyclic permutation group of order p on\nx (which cyclically permutes the components of x). The modular addition task with this encoding\nconsists of mapping (a · x, b · x) to (a + b) · x. For x = e0, our encoding coincides with the one-hot\nencoding a 7→ea studied in prior work. We consider a two-layer neural network with quadratic\nactivation function σ(z) = z2, where each neuron is parameterized by θi = (ui, vi, wi) ∈R3p and\ncomputes the function fi(a · x, b · x; θi) = (⟨ui, a · x⟩+ ⟨vi, b · x⟩)2wi. The network is composed of\nthe sum of H neurons and is trained over the entire dataset of p2 pairs (a, b). We make some technical\nassumptions in order to simplify the analysis. First, since the network contains no bias term, we\nassume that the data is centered, i.e., we subtract (⟨x, 1⟩/p)1 from x. Second, in order to encourage\n8\n"
    },
    {
      "page_number": 9,
      "text": "(b) Predictions\n(a) Power\n(c) Weights\n(d) Frequency+Phases\nSteps\nAGF\nFrequency\nFigure 6: Stepwise Fourier decomposition. We train a two-layer quadratic network on a modular\naddition task with p = 20, using a template vector x ∈Rp composed of three cosine waves:\nˆx[1] = 10, ˆx[3] = 5, and ˆx[5] = 2.5. (a) Output power spectrum over time. The network learns the\ntask by sequentially decomposing x into its Fourier components, acquiring dominant frequencies\nfirst. Colored solid lines are gradient descent, black dashed line is AGF run numerically from the\nsame initialization. (b) Model outputs on selected inputs at four training steps, showing progressively\naccurate reconstructions of the template. (c) Output weight vector wi for all H = 18 neurons and (d)\ntheir frequency spectra and dominant phase. Neurons are color-coded by dominant frequency. As\npredicted by the theory, the neurons group by frequency, while distributing their phase shifts.\nsequential learning of frequencies, we assume that all the non-conjugate Fourier coefficients of x\nhave distinct magnitude: |ˆx[k]| ̸= |ˆx[k′]| for k ̸= ±k′ (mod p). Third, in order to avoid dealing\nwith the Nyquist frequency k = p/2, we assume that p is odd. We now describe how AGF applies to\nthis setting (see Appendix F for proofs). Here,ˆ· denotes the Discrete Fourier Transform (DFT).\nUtility maximization.\nFirst, we show how the initial utility can be expressed entirely in the\nfrequency domain of the parameters ˆΘ. For a dormant neuron parameterized by θ = (u, v, w), the\ninitial utility is U = (2/p3) P\nk∈[p]\\{0} |ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k]. We then argue that the unit vectors\nmaximizing U align with the dominant harmonic of ˆx:\nTheorem 5.1. Let ξ be the frequency that maximizes |ˆx[k]|, k = 1, . . . , p −1, and denote by sx the\nphase of ˆx[ξ]. Then the unit vectors θ∗= (u∗, v∗, w∗) maximizing the utility function U are:\nu∗[a] = Ap cos (ωξa + su) ,\nv∗[b] = Ap cos (ωξb + sv) ,\nw∗[c] = Ap cos (ωξc + sw) ,\n(8)\nwhere a, b, c ∈[p] are indices, su, sv, sw ∈R are phase shifts satisfying su + sv ≡sw + sx\n(mod 2π), Ap =\np\n2/(3p) is the amplitude, and ωξ = 2πξ/p is the frequency. Moreover, U has no\nother local maxima and achieves a maximal value of ¯U∗=\np\n2/(27p3)|ˆx[ξ]|3.\nTherefore, after utility minimization, neurons specialize to unique frequencies. Now that we know\nthe maximal utility, we can estimate the jump time by assuming instantaneous alignment as done in\nSection 4, resulting in the lower bound shown in Equation (9).\nCost minimization.\nTo study cost minimization, we consider a regime in which a group A of\nN ≤H neurons activates simultaneously, each aligned to the harmonic of frequency ξ. While this is\na technical simplification of AGF, which activates a single neuron per iteration, it allows us to analyze\nthe collective behavior more directly. We additionally assume that once aligned, the neurons in A\nremain aligned under the gradient flow (see Appendix F.2.1 for a discussion of possible “resonant”\nescape directions). We then analyze cost minimization for a configuration ΘA = (ui, vi, wi)N\ni=1 of\naligned neurons with arbitrary amplitudes and phase shifts, and prove the following result.\nTheorem 5.2. The loss function satisfies the lower bound L(ΘA) ≥∥x∥2/2 −⟨x, 1⟩2/(2p) −\n|ˆx[ξ]|2/p, which is tight for N ≥6. When the bound is achieved, the network learns the function f(a·\nx, b·x; ΘA) = (2|ˆx[ξ]|/p) (a+b)·χξ, where χξ[c] = cos (2πξc/p + sx), which leads to the new util-\nity function for the remaining dormant neurons: U = (2/p3) P\nk∈[p]\\{0,±ξ} |ˆx[k]|2ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ].\nPut simply, the updated utility function after the first iteration of AGF has the same form as the old\none, but with the dominant frequency ξ of x removed (together with its conjugate frequency −ξ).\nTherefore, at the second iteration of AGF, another group of neurons aligns with the harmonic of\nthe second dominant frequency of x. Lastly, we argue via orthogonality that the groups of neurons\naligned with different harmonics optimize their loss functions independently, implying that at each\niteration of AGF, another group of neurons learns a new Fourier feature of x.\n9\n"
    },
    {
      "page_number": 10,
      "text": "AGF in action: greedy Fourier decomposition.\nTaken together, we have shown that a two-layer\nquadratic network with hidden dimension H ≥3p trained to perform modular addition with a\ncentered encoding vector x ∈Rp sequentially learns frequencies ξ1, ξ2, . . . , ordered by decreasing\n|ˆx[ξ]|. After learning k ≥0 frequencies, the function, level, and next jump time are:\nf (k)(a·x, b·x) = Pk\ni=1\n2|ˆx[ξi]|\np\n(a+b)·χξi, ℓ(k) = P\ni>k\n|ˆx[ξi]|2\np\n, τ (k) ≳τ (l)+ η−1\nα\nµ(l)\n√\n3p\n3\n2\n√\n2|ˆx[ξk+1]|3 , (9)\nwhere µ(l) = maxi∈D ∥θi(τ (l))∥and learning rate ηα = α−1. Simulating AGF numerically, we find\nthis sequence closely approximates the gradient flow dynamics (see Figure 6).\nExtensions to other algebraic tasks.\nOur analysis of modular addition can naturally extend to\na broader class of algebraic problems. First, one can consider modular addition over multiple\nsummands, defined by the map y: (Z/p)k →Z/p, (ai)k\ni=1 7→a1 + · · · + ak (mod p). Using a\nhigher-degree activation function σ(x) = xk, the arguments for utility maximization should carry\nover, while the cost minimization step might be more subtle. Second, one can replace modular\nintegers with an arbitrary finite group and study a network trained to learn the group multiplication\nmap. Non-commutative groups introduce technical challenges due to the involvement of higher-\ndimensional unitary representations in their Fourier analysis. We leave a detailed analysis of these\nextensions to future work.\n6\nDiscussion, Limitations, and Future Work\nSteps\nOptimal Linear Predictor\nLoss\nWidth = 1\nWidth = 2\nWidth = 4\nWidth = 8\nWidth = 16\nWidth = 32\nFigure 7: From a staircase to a slide in a two-\nlayer ReLU network. Training loss for a two-\nlayer ReLU network on a subset of CIFAR-10\nunder varying hidden widths from an extremely\nsmall initialization. Solid lines show gradient de-\nscent dynamics and dotted lines show the sequence\nproduced by a numerical implementation of AGF\nfrom the same initialization. At small widths, gra-\ndient descent loss curves are stepwise and AGF\ntracks the jumps closely. As width increases, cost-\nminimization phases overlap and multiple dormant\nneurons activate in close succession, producing a\nsmoother slide-like loss curve; accordingly, the\ncorrespondence with AGF weakens. All experi-\nmental details are available at a Github code base.\nIn this work we introduced Alternating Gradi-\nent Flows (AGF), a framework modeling feature\nlearning in two-layer neural networks as an al-\nternating two-step process: maximizing a utility\nfunction over dormant neurons and minimizing\na cost function over active ones. We showed\nhow AGF converges to gradient flow in diag-\nonal linear networks, recovers prior saddle-to-\nsaddle analyses in linear networks, and extends\nto quadratic networks trained on modular ad-\ndition. While these findings highlight AGF’s\nutility as an ansatz for feature learning, it re-\nmains open whether its correspondence to gra-\ndient flow always holds in the vanishing ini-\ntialization limit. Proving such a conjecture is\ntheoretically challenging. Empirical validation\nis also difficult because it requires taking both\nthe initialization scale and learning rate to zero.\nMoreover, this conjecture may simply fail to\nhold in more general settings. On natural data\ntasks, loss curves are often not visibly stepwise\neven at very small initialization scales (see Fig-\nure 7), suggesting there may be limitations to\nAGF. That said, if many dormant neurons reach\ntheir activation thresholds in close succession,\ntheir cost-minimization phases could interleave,\ncausing the aggregate loss to appear smooth. Re-\ncent works reconciling the emergent capabilities\nof large language models with their neural scal-\ning laws have made similar suggestions [98–102]. However, there are feature learning regimes in\ntwo-layer networks—such as those studied in Saad and Solla [46], Goldt et al. [47], Arnaboldi et al.\n[51]—that display saddle-to-saddle behavior due to population-level transitions not captured by AGF.\nConnecting AGF to these multi-index models and teacher–student settings (see Appendix A for a\nreview) remains a key direction for future work. Lastly, the central limitation of the framework is\nits focus on two-layer networks, leaving open how it might generalize to deeper and more realistic\narchitectures. Possible extensions include leveraging recent analyses of modularity in deep networks\n[103–105] and adapting insights from the early alignment dynamics of deep networks near the\norigin [59, 106]. All together, our results suggest that AGF offers a promising step towards a deeper\nunderstanding of what features neural networks learn and how.\n10\n"
    },
    {
      "page_number": 11,
      "text": "Acknowledgments and Disclosure of Funding\nWe thank Clémentine Dominé, Jim Halverson, Boris Hanin, Christopher Hillar, Alex Infanger, Arthur\nJacot, Mason Kamb, David Klindt, Florent Krzakala, Zhiyuan Li, Sophia Sanborn, Nati Srebro,\nand Yedi Zhang for helpful discussions. Daniel thanks the Open Philanthropy AI Fellowship for\nsupport. Giovanni is partially supported by the Wallenberg AI, Autonomous Systems and Software\nProgram (WASP) funded by the Knut and Alice Wallenberg Foundation. Surya and Feng are partially\nsupported by NSF grant 1845166. Surya thanks the Simons Foundation, NTT Research, an NSF\nCAREER Award, and a Schmidt Science Polymath award for support. Nina is partially supported by\nNSF grant 2313150 and the NSF grant 240158. This work was supported in part by the U.S. Army\nResearch Laboratory and the U.S. Army Research Office under Contract No. W911NF-20-1-0151.\nAuthor Contributions\nDaniel, Nina, Giovanni, and James are primarily responsible for developing the AGF framework in\nSection 2. Daniel is primarily responsible for the analysis of diagonal and fully-connected linear\nnetworks in Sections 3 and 4. Feng is primarily responsible for the analysis of the attention-only\nlinear transformer in Section 4. Daniel and Giovanni are primarily responsible for the analysis of the\nmodular addition task in Section 5. Dhruva is primarily responsible for an implementation of AGF\nused in the empirics. All authors contributed to the writing of the manuscript.\nReferences\n[1] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\nZoom in: An introduction to circuits. Distill, 5(3):e00024–001, 2020.\n[2] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for\ntransformer circuits. Transformer Circuits Thread, 1(1):12, 2021.\n[3] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and\ninduction heads. arXiv preprint arXiv:2209.11895, 2022.\n[4] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse\nautoencoders find highly interpretable features in language models.\narXiv preprint\narXiv:2309.08600, 2023.\n[5] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-\nerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,\nShauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex\nTamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter,\nTom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language\nmodels with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-\ncircuits.pub/2023/monosemantic-features/index.html.\n[6] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress\nmeasures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217,\n2023.\n[7] Leonard Bereska and Efstratios Gavves. Mechanistic interpretability for ai safety–a review.\narXiv preprint arXiv:2404.14082, 2024.\n[8] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682, 2022.\n[9] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse\nengineering how networks learn group operations. In International Conference on Machine\nLearning, pages 6243–6267. PMLR, 2023.\n[10] Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas\nGoldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, et al. Open problems\nin mechanistic interpretability. arXiv preprint arXiv:2501.16496, 2025.\n11\n"
    },
    {
      "page_number": 12,
      "text": "[11] Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,\nMaxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.\nA closer look at memorization in deep networks. In International conference on machine\nlearning, pages 233–242. PMLR, 2017.\n[12] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz\nBarak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity.\nAdvances in neural information processing systems, 32, 2019.\n[13] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.\nHidden progress in deep learning: Sgd learns parities near the computational limit. Advances\nin Neural Information Processing Systems, 35:21750–21764, 2022.\n[14] Alexander Atanasov, Alexandru Meterez, James B Simon, and Cengiz Pehlevan. The optimiza-\ntion landscape of sgd across the feature learning strength. arXiv preprint arXiv:2410.04642,\n2024.\n[15] Arthur Jacot, François Ged, Berfin ¸Sim¸sek, Clément Hongler, and Franck Gabriel. Saddle-to-\nsaddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity.\narXiv preprint arXiv:2106.15933, 2021.\n[16] Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic\ndevelopment in deep neural networks. Proceedings of the National Academy of Sciences, 116\n(23):11537–11546, 2019.\n[17] Raphaël Berthier. Incremental learning in diagonal linear networks. Journal of Machine\nLearning Research, 24(171):1–26, 2023.\n[18] Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks.\nAdvances in Neural Information Processing Systems, 36:7475–7505, 2023.\n[19] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete\ngradient dynamics in linear neural networks. Advances in Neural Information Processing\nSystems, 32, 2019.\n[20] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient\ndescent for matrix factorization: Greedy low-rank learning. arXiv preprint arXiv:2012.09839,\n2020.\n[21] Rong Ge, Yunwei Ren, Xiang Wang, and Mo Zhou. Understanding deflation process in\nover-parametrized tensor decomposition. Advances in Neural Information Processing Systems,\n34:1299–1311, 2021.\n[22] James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and Joshua\nAlbrecht. On the stepwise nature of self-supervised learning. In International Conference on\nMachine Learning, pages 31852–31876. PMLR, 2023.\n[23] Dhruva Karkada, James B Simon, Yasaman Bahri, and Michael R DeWeese. Solvable dynamics\nof self-supervised word embeddings and the emergence of analogical reasoning. arXiv preprint\narXiv:2502.09863, 2025.\n[24] Mary Phuong and Christoph H Lampert. The inductive bias of relu networks on orthogonally\nseparable data. In International Conference on Learning Representations, 2020.\n[25] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer\nnets: Margin maximization and simplicity bias. Advances in Neural Information Processing\nSystems, 34, 2021.\n[26] Mingze Wang and Chao Ma. Early stage convergence and global convergence of training\nmildly parameterized neural networks. Advances in Neural Information Processing Systems,\n35:743–756, 2022.\n[27] Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of\nshallow relu networks for square loss and orthogonal inputs. Advances in Neural Information\nProcessing Systems, 35:20105–20118, 2022.\n12\n"
    },
    {
      "page_number": 13,
      "text": "[28] Hancheng Min, René Vidal, and Enrique Mallada. Early neuron alignment in two-layer relu\nnetworks with small initialization. arXiv preprint arXiv:2307.12851, 2023.\n[29] Margalit Glasgow. Sgd finds then tunes features in two-layer neural networks with near-optimal\nsample complexity: A case study in the xor problem. arXiv preprint arXiv:2309.15111, 2023.\n[30] Mingze Wang and Chao Ma. Understanding multi-phase optimization dynamics and rich\nnonlinear behaviors of relu networks. Advances in Neural Information Processing Systems, 36,\n2024.\n[31] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind.\nTransformers learn through gradual rank increase. Advances in Neural Information Processing\nSystems, 36:24519–24551, 2023.\n[32] Yedi Zhang, Aaditya K Singh, Peter E Latham, and Andrew Saxe. Training dynamics of\nin-context learning in linear attention. arXiv preprint arXiv:2501.16265, 2025.\n[33] Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj.\nThe staircase property: How hierarchical structure can guide deep learning. Advances in\nNeural Information Processing Systems, 34:26989–27002, 2021.\n[34] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase\nproperty: a necessary and nearly sufficient condition for sgd learning of sparse functions on\ntwo-layer neural networks. In Conference on Learning Theory, pages 4782–4887. PMLR,\n2022.\n[35] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural\nnetworks: leap complexity and saddle-to-saddle dynamics.\nIn The Thirty Sixth Annual\nConference on Learning Theory, pages 2552–2623. PMLR, 2023.\n[36] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index\nmodels with gradient flow. arXiv preprint arXiv:2310.19793, 2023.\n[37] Berfin Simsek, Amire Bendjeddou, and Daniel Hsu. Learning gaussian multi-index mod-\nels with gradient flow: Time complexity and directional convergence.\narXiv preprint\narXiv:2411.08798, 2024.\n[38] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborová, and Florent\nKrzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking\nthe curse of information and leap exponents. arXiv preprint arXiv:2402.03220, 2024.\n[39] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita\niuvant: Data repetition allows sgd to learn high-dimensional multi-index functions. arXiv\npreprint arXiv:2405.15459, 2024.\n[40] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape\nof two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):\nE7665–E7671, 2018.\n[41] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-\nparameterized models using optimal transport. Advances in neural information processing\nsystems, 31, 2018.\n[42] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A\nlaw of large numbers. SIAM Journal on Applied Mathematics, 80(2):725–752, 2020.\n[43] Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural net-\nworks: An interacting particle system approach. Communications on Pure and Applied\nMathematics, 75(9):1889–1935, 2022.\n[44] David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review\nE, 52(4):4225, 1995.\n[45] David Saad and Sara A Solla. Exact solution for on-line learning in multilayer neural networks.\nPhysical Review Letters, 74(21):4337, 1995.\n13\n"
    },
    {
      "page_number": 14,
      "text": "[46] David Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer\nneural networks. Advances in neural information processing systems, 8, 1995.\n[47] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborová.\nDynamics of stochastic gradient descent for two-layer neural networks in the teacher-student\nsetup. Advances in neural information processing systems, 32, 2019.\n[48] Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová.\nPhase diagram of stochastic gradient descent in high-dimensional two-layer neural networks.\nAdvances in Neural Information Processing Systems, 35:23244–23255, 2022.\n[49] Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborová. Optimization and\ngeneralization of shallow neural networks with quadratic activation functions. Advances in\nNeural Information Processing Systems, 33:13445–13455, 2020.\n[50] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,\nand Lenka Zdeborová. Learning curves of generic features maps for realistic datasets with a\nteacher-student model. Advances in Neural Information Processing Systems, 34:18137–18151,\n2021.\n[51] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro.\nFrom high-\ndimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd\nin two-layers networks. In The Thirty Sixth Annual Conference on Learning Theory, pages\n1199–1227. PMLR, 2023.\n[52] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. Advances in neural information processing systems, 31,\n2018.\n[53] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-\nming. Advances in neural information processing systems, 32, 2019.\n[54] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\narXiv:2011.14522, 2020.\n[55] Yuri Bakhtin. Noisy heteroclinic networks. Probability theory and related fields, 150:1–42,\n2011.\n[56] Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu\nnetwork features. arXiv preprint arXiv:1803.08367, 2018.\n[57] Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners:\nThe silent alignment effect. In International Conference on Learning Representations, 2021.\n[58] Akshay Kumar and Jarvis Haupt. Directional convergence near small initializations and\nsaddles in two-homogeneous neural networks. arXiv preprint arXiv:2402.09226, 2024.\n[59] Akshay Kumar and Jarvis Haupt. Early directional convergence in deep homogeneous neural\nnetworks for small initializations. arXiv preprint arXiv:2403.08121, 2024.\n[60] Raphaël Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers\nneural networks. Foundations of Computational Mathematics, pages 1–84, 2024.\n[61] Daniel Kunin, Allan Raventós, Clémentine Dominé, Feng Chen, David Klindt, Andrew Saxe,\nand Surya Ganguli. Get rich quick: exact solutions reveal how unbalanced initializations\npromote rapid feature learning. arXiv preprint arXiv:2406.06158, 2024.\n[62] Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How\nincremental learning drives generalization. arXiv preprint arXiv:1909.12051, 2019.\n[63] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay\nGolan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized\nmodels. In Conference on Learning Theory, pages 3635–3673. PMLR, 2020.\n14\n"
    },
    {
      "page_number": 15,
      "text": "[64] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal\nlinear networks: a provable benefit of stochasticity. Advances in Neural Information Processing\nSystems, 34:29218–29230, 2021.\n[65] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. (s) gd over diagonal\nlinear networks: Implicit regularisation, large stepsizes and edge of stability. arXiv preprint\narXiv:2302.08982, 2023.\n[66] Hristo Papazov, Scott Pesme, and Nicolas Flammarion. Leveraging continuous time to\nunderstand momentum when training diagonal linear networks. In International Conference\non Artificial Intelligence and Statistics, pages 3556–3564. PMLR, 2024.\n[67] Ryan J Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 2013.\n[68] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning\nfrom examples without local minima. Neural networks, 2(1):53–58, 1989.\n[69] Kenji Fukumizu. Effect of batch learning in multilayer neural networks. Gen, 1(04):1E–03,\n1998.\n[70] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear\ndynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.\n[71] Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and\ntransfer learning in deep linear networks. arXiv preprint arXiv:1809.10374, 2018.\n[72] Clémentine CJ Dominé, Nicolas Anguita, Alexandra M Proca, Lukas Braun, Daniel Kunin,\nPedro AM Mediano, and Andrew M Saxe. From lazy to rich: Exact learning dynamics in deep\nlinear networks. arXiv preprint arXiv:2409.14623, 2024.\n[73] Javan Tahir, Surya Ganguli, and Grant M Rotskoff. Features are fate: a theory of transfer\nlearning in high-dimensional regression. arXiv preprint arXiv:2410.08194, 2024.\n[74] Dominik Stöger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral\nlearning: Optimization and generalization guarantees for overparameterized low-rank matrix\nreconstruction. Advances in Neural Information Processing Systems, 34:23831–23843, 2021.\n[75] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix\nfactorization. Advances in Neural Information Processing Systems, 32, 2019.\n[76] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homoge-\nneous models: Layers are automatically balanced. Advances in Neural Information Processing\nSystems, 31, 2018.\n[77] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.\nNeural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv\npreprint arXiv:2012.04728, 2020.\n[78] Alan Julian Izenman. Reduced-rank regression for the multivariate linear model. Journal of\nmultivariate analysis, 5(2):248–264, 1975.\n[79] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information processing systems, 33:1877–\n1901, 2020.\n[80] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers\nlearn in-context? a case study of simple function classes. Advances in Neural Information\nProcessing Systems, 35:30583–30598, 2022.\n[81] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Trans-\nformers as algorithms: Generalization and stability in in-context learning. In International\nconference on machine learning, pages 19565–19594. PMLR, 2023.\n15\n"
    },
    {
      "page_number": 16,
      "text": "[82] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity\nand the emergence of non-bayesian in-context learning for regression. Advances in neural\ninformation processing systems, 36:14228–14246, 2023.\n[83] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What\nlearning algorithm is in-context learning? investigations with linear models. arXiv preprint\narXiv:2211.15661, 2022.\n[84] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L\nBartlett. How many pretraining tasks are needed for in-context learning of linear regression?\narXiv preprint arXiv:2310.08391, 2023.\n[85] Kwangjun Ahn, Sébastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang.\nLearning threshold neurons via the\" edge of stability\". arXiv preprint arXiv:2212.07469, 2022.\n[86] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander\nMordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by\ngradient descent. In International Conference on Machine Learning, pages 35151–35174.\nPMLR, 2023.\n[87] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to\nimplement preconditioned gradient descent for in-context learning. Advances in Neural\nInformation Processing Systems, 36:45614–45650, 2023.\n[88] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models\nin-context. Journal of Machine Learning Research, 25(49):1–55, 2024.\n[89] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.\nGrokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint\narXiv:2201.02177, 2022.\n[90] Andrey Gromov. Grokking modular arithmetic. arXiv preprint arXiv:2301.02679, 2023.\n[91] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza:\nTwo stories in mechanistic explanation of neural networks. Advances in Neural Information\nProcessing Systems, 36, 2024.\n[92] Tianyi Zhou, Deqing Fu, Vatsal Sharan, and Robin Jia. Pre-trained large language models use\nfourier features to compute addition. arXiv preprint arXiv:2406.03445, 2024.\n[93] Subhash Kantamneni and Max Tegmark. Language models use trigonometry to do addition.\narXiv preprint arXiv:2502.00873, 2025.\n[94] Neil Mallinar, Daniel Beaglehole, Libin Zhu, Adityanarayanan Radhakrishnan, Parthe Pandit,\nand Mikhail Belkin. Emergence in non-neural models: grokking modular arithmetic via\naverage gradient outer product. arXiv preprint arXiv:2407.20199, 2024.\n[95] Giovanni Luca Marchetti, Christopher J Hillar, Danica Kragic, and Sophia Sanborn. Harmonics\nof learning: Universal fourier features emerge in invariant networks. In The Thirty Seventh\nAnnual Conference on Learning Theory, pages 3775–3797. PMLR, 2024.\n[96] Depen Morwani, Benjamin L Edelman, Costin-Andrei Oncescu, Rosie Zhao, and Sham M\nKakade. Feature emergence via margin maximization: case studies in algebraic tasks. In The\nTwelfth International Conference on Learning Representations, 2023.\n[97] Yuandong Tian. Composing global optimizers to reasoning tasks via algebraic objects in\nneural nets. arXiv preprint arXiv:2410.01779, 2024.\n[98] Yuki Yoshida and Masato Okada. Data-dependence of plateau phenomenon in learning with\nneural network—statistical mechanical analysis. Advances in Neural Information Processing\nSystems, 32, 2019.\n[99] Eric Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural\nscaling. Advances in Neural Information Processing Systems, 36:28699–28722, 2023.\n16\n"
    },
    {
      "page_number": 17,
      "text": "[100] Yoonsoo Nam, Nayara Fonseca, Seok Hyeong Lee, Chris Mingard, and Ard A Louis. An\nexactly solvable model for emergence and scaling laws in the multitask sparse parity problem.\narXiv preprint arXiv:2404.17563, 2024.\n[101] Yunwei Ren, Eshaan Nichani, Denny Wu, and Jason D Lee. Emergence and scaling laws in\nsgd learning of shallow neural networks. arXiv preprint arXiv:2504.19983, 2025.\n[102] Gérard Ben Arous, Murat A Erdogdu, N Mert Vural, and Denny Wu. Learning quadratic\nneural networks in high dimensions: Sgd dynamics and scaling laws.\narXiv preprint\narXiv:2508.03688, 2025.\n[103] Andrew Saxe, Shagun Sodhani, and Sam Jay Lewallen. The neural race reduction: Dynamics\nof abstraction in gated networks. In International Conference on Machine Learning, pages\n19287–19309. PMLR, 2022.\n[104] Devon Jarvis, Richard Klein, Benjamin Rosman, and Andrew M Saxe. Make haste slowly:\nA theory of emergent structured mixed selectivity in feature learning relu networks. arXiv\npreprint arXiv:2503.06181, 2025.\n[105] Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian\nChen, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael\nSklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas\nHenighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam\nZimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. Circuit tracing:\nRevealing computational graphs in language models. Transformer Circuits Thread, 2025. URL\nhttps://transformer-circuits.pub/2025/attribution-graphs/methods.html.\n[106] Akshay Kumar and Jarvis Haupt. Towards understanding gradient flow dynamics of homoge-\nneous neural networks beyond the origin. arXiv preprint arXiv:2502.15952, 2025.\n[107] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick\nRyder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466,\n2022.\n[108] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evo-\nlution in wide neural networks. Advances in Neural Information Processing Systems, 35:\n32240–32256, 2022.\n[109] Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, Lenka Zdeborová, et al.\nThe committee machine: Computational to statistical gaps in learning a two-layers neural\nnetwork. Advances in Neural Information Processing Systems, 31, 2018.\n[110] Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal\nerrors and phase transitions in high-dimensional generalized linear models. Proceedings of the\nNational Academy of Sciences, 116(12):5451–5460, 2019.\n[111] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn represen-\ntations with gradient descent. In Conference on Learning Theory, pages 5413–5452. PMLR,\n2022.\n[112] Emanuele Troiani, Yatin Dandi, Leonardo Defilippis, Lenka Zdeborová, Bruno Loureiro, and\nFlorent Krzakala. Fundamental computational limits of weak learnability in high-dimensional\nmulti-index models. arXiv preprint arXiv:2405.15480, 2024.\n[113] Leonardo Defilippis, Yatin Dandi, Pierre Mergny, Florent Krzakala, and Bruno Loureiro.\nOptimal spectral transitions in high-dimensional multi-index models.\narXiv preprint\narXiv:2502.02545, 2025.\n[114] Yatin Dandi, Luca Pesce, Lenka Zdeborová, and Florent Krzakala. The computational advan-\ntage of depth: Learning high-dimensional hierarchical functions with gradient descent. arXiv\npreprint arXiv:2502.13961, 2025.\n17\n"
    },
    {
      "page_number": 18,
      "text": "[115] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari.\nThe large learning rate phase of deep learning: the catapult mechanism. arXiv preprint\narXiv:2003.02218, 2020.\n[116] Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin. Catapults in\nsgd: spikes in the training loss and their impact on generalization through feature learning.\narXiv preprint arXiv:2306.04815, 2023.\n[117] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.\nHigh-dimensional asymptotics of feature learning: How one gradient step improves the\nrepresentation. Advances in Neural Information Processing Systems, 35:37932–37946, 2022.\n[118] Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue M Lu, Lenka Zdeborová, and\nBruno Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step.\narXiv preprint arXiv:2402.04980, 2024.\n[119] Yatin Dandi, Luca Pesce, Hugo Cui, Florent Krzakala, Yue M Lu, and Bruno Loureiro.\nA random matrix theory perspective on the spectrum of learned features and asymptotic\ngeneralization capabilities. arXiv preprint arXiv:2410.18938, 2024.\n[120] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How\ntwo-layer neural networks learn, one (giant) step at a time. Journal of Machine Learning\nResearch, 25(349):1–65, 2024.\n[121] Dmitry Chistikov, Matthias Englert, and Ranko Lazic. Learning a neuron by a shallow relu\nnetwork: Dynamics and implicit bias for correlated inputs. Advances in Neural Information\nProcessing Systems, 36:23748–23760, 2023.\n[122] Ioannis Bantzis, James B Simon, and Arthur Jacot. Saddle-to-saddle dynamics in deep relu\nnetworks: Low-rank bias in the first saddle escape. arXiv preprint arXiv:2505.21722, 2025.\n[123] Uwe Helmke and John B Moore. Optimization and dynamical systems. Springer Science &\nBusiness Media, 2012.\n[124] Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii.\nSIAM Journal on Numerical Analysis, 7(1):1–46, 1970.\n18\n"
    },
    {
      "page_number": 19,
      "text": "A\nAdditional Related Work\nA.1\nWhat is a feature?\nHere, we briefly review definitions of a feature used in the mechanistic interpretability and deep\nlearning theory literature:\n• In mechanistic interpretability, terms such as feature, circuit, and motif are used to describe\nnetwork components—such as directions in activation space, subnetworks of weights, or\nrecurring activation motifs—that correspond to human-interpretable concepts or functions\nwithin the model’s computation [10]. While this literature has led to many insights into\ninterpretability, these definitions generally lack precise mathematical formalization.\n• In deep learning theory, features and feature learning are defined in terms of the neural\ntangent kernel (NTK). For a network f(x; θ), the NTK is given by the kernel K(x, x′; θ) =\n⟨∇θf(x; θ), ∇θf(x′; θ)⟩, formed by the Jacobian feature map ∇θf(x; θ). Feature learn-\ning—also referred to as rich learning—occurs when this kernel evolves during training.\nWhile this definition is mathematically precise, it offers limited interpretability.\nIn this work, we implicitly adopt the notion of features from the deep learning theory perspective, but\nconsider learning settings often studied in mechanistic interpretability.\nA.2\nAnalysis of feature learning in two-layer networks\nHere, we discuss related theoretical approaches to studying feature learning in two-layer networks.\nThese approaches operate in distinct regimes—mean-field/infinite-width, teacher–student at high-\ndimensional (thermodynamic) limits, and single-step analyses—whereas AGF focuses on the vanish-\ning initialization limit for two-layer networks. Our aim is to position AGF as complementary to, not\na replacement for, these approaches.\nMean-field analysis. An early line of work in the study of feature learning analyzes the population\ndynamics of two-layer neural networks under the mean-field parameterization, yielding analytic\ncharacterizations of training dynamics in the infinite-width limit [40–43]. These ideas have been\nextended to deep networks via the tensor program framework, culminating in the maximal update\nparametrization (µP) [54, 107]. Related analyses have also been obtained through self-consistent\ndynamical mean-field theory [108]. Although feature learning occurs in this regime, these analyses\nprimarily address how to preserve feature learning when changing network depth and width, rather\nthan elucidating what specific features are learned or how they emerge.\nTeacher-student setup. The two-layer teacher–student framework provides a precise setting for\nstudying feature learning by analyzing how a student network trained on supervised examples\ngenerated from a fixed teacher recovers the structure of the teacher. Foundational work by Saad\nand Solla [44, 45, 46] provided closed-form dynamics for online gradient descent in soft committee\nmachines trained on i.i.d. Gaussian inputs in the high-dimensional limit, deriving deterministic\ndifferential equations governing a set of order parameters. Goldt et al. [47], Veiga et al. [48] extended\nthis dynamical theory to study the evolution of generalization error of one-pass SGD under an\narbitrary learning rate, and a range of hidden layer widths. Further developments have analyzed\nconditions for the absence of spurious minima [49], extended the framework to capture more realistic\ndata settings [50], and provided a unified perspective connecting to the infinite hidden width limit of\nthe mean-field regime [51]. Much of this analysis use techniques from statistical physics—including\nthe replica method and approximate message passing—to characterize the generalization error and\nuncover sharp phase transitions and statistical-to-computational gaps in teacher–student models,\ndiscussed further in Aubin et al. [109], Barbier et al. [110].\nMulti-index models. Similar to the teacher–student setting, multi-index models provide a structured\nframework in which a neural network is trained on data whose labels depend on a nonlinear function of\nlow-dimensional projections of high-dimensional inputs. Feature learning in this setting is governed\nby the network’s ability to align with the relevant low-dimensional subspace. This setup often\ngives rise to characteristic staircase-like training dynamics [111, 36, 37, 60, 33–35]. Prior analyses\nhave primarily focused on characterizing generalization and sample complexity, and recent work has\nestablished precise theoretical and computational thresholds for weak learnability in high-dimensional\nmulti-index problems [112, 113]. Other studies have demonstrated that reusing batches in gradient\ndescent can help networks overcome information bottlenecks, accelerating the learning of relevant\n19\n"
    },
    {
      "page_number": 20,
      "text": "projections [38, 39]. Additionally, incorporating depth into these models has been shown to provide\ncomputational advantages for learning hierarchical multi-index functions [114].\nSingle gradient step. A recent line of work has focused on understanding feature learning in neural\nnetworks after just a single gradient step. Empirically, it has been observed that with large initial\nlearning rates, models can undergo “catapult” dynamics, where sudden spikes in the training loss\naccelerate feature learning and improve generalization [115, 116]. Ba et al. [117] initiated the precise\nanalysis of feature learning after a single gradient step by showing, in a high-dimensional single-\nindex setting, how the first update to the first-layer weights can dramatically improve prediction\nperformance beyond random features, depending on the learning rate scaling. This single-step\nanalysis has been extended to characterize the asymptotic behavior of the generalization error [118],\nestablish an equivalence between the updated features and an isotropic spiked random feature model\n[119], and investigate the effects of multiple gradient steps [120]. Collectively, these works highlight\nthat even a single gradient step can induce meaningful feature learning in two-layer networks.\nDistinct phases of alignment and growth in ReLU networks. Maennel et al. [56] first observed a\nstriking phenomenon in two-layer ReLU networks trained from small initializations: the first-layer\nweights concentrate along fixed directions determined by the training data, regardless of network\nwidth. Subsequent studies of piecewise linear networks have refined this effect by imposing structural\nconstraints on the training data, such as orthogonally separable [24, 26, 28], linearly separable and\nsymmetric [25], pair-wise orthonormal [27], correlated [121], small angle [30], binary and sparse\n[29]. Across these analyses, a consistent observation is that the learning dynamics involve distinct\nlearning phases separated by timescales: a fast alignment phase driven by directional movement and\na slow fitting phase driven by radial movement [60, 61]. Concurrently, Bantzis et al. [122] use a\nclosely related directional–radial decomposition to identify the optimal escape directions from the\nsaddle at the origin in deep ReLU networks.\n20\n"
    },
    {
      "page_number": 21,
      "text": "B\nDerivations in Alternating Gradient Flows Framework\nIn this section, we proivde details and motivations for the general framework of AGF. All code and ex-\nperimental details are available at github.com/danielkunin/alternating-gradient-flows.\nB.1\nDeriving dormant dynamics near a saddle point in terms of the utility function\nAt any point in parameter space the gradient of the MSE loss with respect to the parameters of a\nneuron can be expressed as\n−∇θiL(Θ) = ∇θiEx [⟨fi(x; θi), r(x; Θ)⟩] ,\n(10)\nwhere r(x; Θ) = y(x) −P\ni fi(x; θi) is a residual that depends on all neurons. Given a partition of\nneurons into two sets, an active A and dormant D set, then the gradient for a dormant neuron i ∈D\ncan be decomposed into two terms:\n−∇θiL(Θ) = ∇θiEx\n\"*\nfi(x; θi), y(x) −\nX\nj∈A\nfj(x; θj)\n+#\n−∇θiEx\n\"*\nfi(x; θi),\nX\nj∈D\nfj(x; θj)\n+#\n. (11)\nThe first term is the gradient of the utility ∇θUi as defined in Equation (1). The second term only\ndepends on the dormant neurons and is thus negligible when all the dormant neurons are small in\nnorm. Taken together, in the vicinity of a saddle point of the loss defined by an active and dormant\nset, the dormant neurons are driven approximately by gradient flow maximizing their utility:\nd\ndtθi ≈η∇θUi.\n(12)\nThis observation motivates the utility and our AGF ansatz for gradient flow. To formally prove that\nAGF converges to gradient flow in the limit of vanishing initialization one might consider using tools\nlike Hartman–Grobman theorem to make this step rigorous.\nDirectional and radial expansion of the dynamics.\nUsing this approximation, we can further\ndecompose the dynamics of a dormant neuron near a saddle point into a directional and radial\ncomponent:\nd\ndt\nθi\n∥θi∥= η P⊥\nθi\n∥θi∥∇θUi,\nd\ndt∥θi∥= η\n1\n∥θi∥⟨θi, ∇θUi⟩,\n(13)\nwhere P⊥\nθi =\n\u0010\nIm −θiθ⊺\ni\n∥θi∥2\n\u0011\nis a projection matrix. When σ(·) is a homogeneous function of degree\nk, then the utility is a homogeneous function of degree κ = k + 1. Using Euler’s homogeneous\nfunction theorem, Equation (13) then simplifies to Equation (2). When σ(·) is not homogeneous, we\ncan Taylor expand the utility around the origin, such that the utility coincides approximately with a\nhomogeneous function of degree κ, where κ is the leading order of the Taylor expansion.\nDynamics of normalized utility.\nAn interesting observation, that we will use in Appendix C, is\nthat the normalized utility function follows a Riccati-like differential equation\nd\ndt\n¯Ui = ηκ2∥θi∥κ−2\n\u0012∥∇θ ¯Ui∥2\nκ2\n−¯U2\ni\n\u0013\n.\n(14)\nIn general, this ODE is coupled with the dynamics for both the direction and norm, except when\nκ = 2 for which the dependency on the norm disappears.\nB.2\nDeriving the jump time\nTo compute τi∗, we use Equation (2) to obtain the time evolution of the norms of the dormant neurons.\nFor i ∈D, we obtain:\n∥θi(t)∥=\n(\n∥θi(0)∥exp (Si(t))\nif κ = 2,\n\u0000∥θi(0)∥2−κ + (2 −κ)Si(t)\n\u0001\n1\n2−κ\nif κ > 2,\n(15)\nwhere we have defined the accumulated utility as the path integral Si(t) =\nR t\n0 κ ¯Ui(s)ds of the\nnormalized utility. We find τi as the earliest time at which neuron i satisfies ∥θi(τi)∥> 1:\nτi = inf {t > 0 | Si(t) > ci/η} ,\nwhere ci =\n(\n−log ∥θi(0)∥\nif κ = 2,\n−∥θi(0)∥2−κ−1\n2−κ\nif κ > 2.\n(16)\nNote that the expression for ci is continuous at κ = 2.\n21\n"
    },
    {
      "page_number": 22,
      "text": "Lower bound on jump time.\nWhen applying this framework to analyze dynamics, computing the\nexact jump time can be challenging due to the complexity of integrating ¯U. In such cases, we resort\non the following lower bound as a useful analytical approximation. Let U∗\ni be the maximal value of\nthe utility, or at least an upper bound on it. Since Si(t) ≤κU∗\ni t, we deduce:\nτi ≥\n(\n−log ∥θi(0)∥\n2U∗\ni\nif κ = 2,\n−∥θi(0)∥2−κ\nκ(2−κ)U∗\ni\nif κ > 2.\n(17)\nB.3\nActive neurons can become dormant again\nIn the cost minimization phase of AGF, active neurons can become dormant. Here, we motivate and\ndiscuss this phenomenon. Intuitively, this happens when the GF trajectory brings an active neuron\nclose to the origin. Due to the preserved quantities of GF for homogeneous activation functions,\nthe trajectory of active neurons is constrained. Thus, this phenomenon can occur only in specific\nscenarios, as quantified by the following result.\nLemma B.1. Suppose that Θ evolves via the GF of L, and let c = (κ −1)∥ai(0)∥2 −∥wi(0)∥2. The\nnorm ∥θi(t)∥2 satisfies the lower bound,\n∥θi(t)∥2 ≥max\n\u0012\n−c,\nc\nκ −1\n\u0013\n,\n(18)\nwhich holds with equality when either ∥wi(t)∥2 = 0 for c ≥0 or ∥ai(t)∥2 = 0 for c < 0.\nProof. For homogeneous activation functions of degree κ, it is well known that the quantity c =\n(κ −1)∥ai(t)∥2 −∥wi(t)∥2 is preserved along trajectories of gradient flow. Since ∥θi(t)∥2 =\n∥ai(t)∥2 + ∥wi(t)∥2, we have:\n∥θi(t)∥2 = −c + κ∥ai(t)∥2 ≥−c,\n∥θi(t)∥2 =\nc\nκ −1 +\nκ\nκ −1∥wi(t)∥2 ≥\nc\nκ −1.\n(19)\nThe claim follows by combining the above inequalities.\nSince at vanishing initialization (κ −1)∥ai∥2 ∼∥wi∥2, we have c ∼0. Therefore, an active neuron\ncan approach the origin during the cost minimization phase. If this happens, the neuron becomes\ndormant. When a neuron becomes dormant again it does so with an accumulated utility Si(t) = ci\nη .\nSee Figure 8 for an example of a diagonal linear network where this behavior is observable.\nB.4\nInstantaneous alignment in the vanishing initialization limit\nIn this section, we revisit the separation of the dynamics dictated by revisit Equation (2). In\nparticular, we wish to discuss the conditions under which the directional dynamics dominates the\nradial one, resulting, at the vanishing initialization limit, in instantaneous alignment during the utility\nmaximization phase. To this end, we establish the following scaling symmetry with respect to the\ninitialization scale factor α.\nTheorem B.2. Suppose θi(t) = fi(t) solves the initial value problem defined by Equation (2) with\ninitial condition θi(0) = θ0,i. Then for all α > 0, the scaled solution θi(t) = αfi(ακ−2t) solves the\ninitial value problem with initial condition θi(0) = αθ0,i.\nProof. First, we consider the angular dynamics:\nd\ndt\n¯θi = d\ndt\nfi(ακ−2t)\n∥fi(ακ−2t)∥\n= ακ−2 d\nds\nfi(s)\n∥fi(s)∥\n\f\f\f\f\ns=ακ−2t\n= ακ−2∥fi(ακ−2t)∥κ−2P⊥\nθi∇θUi(¯θi(ακ−2t); r)\n= ∥αfi(ακ−2t)∥κ−2P⊥\nθi∇θUi(¯θi(ακ−2t); r)\n22\n"
    },
    {
      "page_number": 23,
      "text": "Therefore, θi(t) = αfi(ακ−2t) satisfies the first identity in Equation (2) above. Next, consider the\nnorm dynamics:\nd\ndt∥θi∥= d\ndt∥αfi(ακ−2t)∥\n= ακ−1 d\nds∥fi(s)∥\n\f\f\f\f\ns=ακ−2t\n= ακ−1κ∥fi(ακ−2t)∥κ−1Ui(¯θi(ακ−2t); r)\n= κ∥αfi(ακ−2t)∥κ−1Ui(¯θi(ακ−2t); r)\nHence, θi(t) = αfi(tακ−2) also satisfies the second equation. Finally, verifying the initial conditions\ngives, θi(0) = αfi(0) = αθ0,i. Therefore, θi(t) = αfi(tακ−2) satisfies the given initial condition.\nTheorem B.2 establishes a transformation among the angular dynamics of different initialization\nscales. Specifically, for a given point (s,\nfi(s)\n∥fi(s)∥) in the solution trajectory of the initial value problem\nwith initial condition θi(0) = θ0,i, the corresponding point in the initial value problem with scaled\ninitial condition θi(0) = αθ0,i is (sα2−κ,\nfi(s)\n∥fi(s)∥). This correspondence reveals that the angular\nalignment process is effectively slowed down by a factor of ακ−2 with initialization scale α.\nNext, we examine the alignment speed in accelerated time in the limit of α →0. In this regime, the\nasymptotics of Equation (16) is given by,\ncκ(α) ∼\n\u001a−log α\nif κ = 2,\nα2−κ\nif κ > 2,\n(20)\nTherefore, in the accelerated time, the alignment speed is effectively scaled by a scaling factor of:\nγ(α) := cκ(α)\nακ−2 =\n\u001a−log α\nif κ = 2,\n1\nif κ > 2,\n(21)\nAnd,\nlim\nα→0 γ(α) =\n\u001a+∞\nif κ = 2,\n1\nif κ > 2,\n(22)\nThe limiting behavior implies that the alignment is instantaneous in accelerated time for almost all\ninitialization θ0,i if and only if κ = 2.\nB.5\nA neuron-specific adaptive learning rate yields instantaneous alignment\nAs discussed above, instantaneous alignment of dormant neurons with their local utility-maximizing\ndirections occurs only when κ = 2. For higher-order activations (κ > 2), the directional dynamics\nacquire a norm-dependent factor ∥θi∥κ−2 that slows their angular evolution. Consequently, directional\nand radial dynamics no longer decouple, even in the vanishing initialization limit, and dormant\nneurons rotate gradually rather than aligning instantaneously.\nThis dependence can be removed by introducing a neuron-specific adaptive learning rate\nηi = ∥θi∥2−κ η,\n(23)\nwhere η is a global base rate. When κ = 2, this scaling has no effect and all neurons evolve at\nthe same rate. For κ > 2, however, neurons with small norm (∥θi∥< 1) are accelerated, while\nthose with large norm (∥θi∥> 1) are slowed down. This rescaling effectively reparametrizes\ntime so that the directional dynamics become norm-independent while the radial dynamics remain\nnorm-dependent. Substituting this adaptive rate into Equation (2) yields an evolution equivalent\nto that of the κ = 2 case, resulting in the decoupling between directional and radial dynamics in\nthe vanishing-initialization limit for all κ. In practice, this scaling acts analogously to a form of\nneuron-wise adaptive optimization—resembling RMSProp or Adam—but derived directly from the\nanalytical structure of the κ-homogeneous gradient flow.\n23\n"
    },
    {
      "page_number": 24,
      "text": "C\nComplete Proofs for Diagonal Linear Networks\nIn this section, we derive the AGF dynamics for the two-layer diagonal linear network; see Section 3\nfor the problem setup and notation. Throughout this section, we assume the initialization θi(0) =\n(\n√\n2α, 0), following the convention in Pesme and Flammarion [18].\nC.1\nUtility maximization\nLemma C.1. After the kth iteration of AGF, the utility for the ith neuron is\nUi\n\u0010\nθi; r(k)\u0011\n= −uivi∇βiL\n\u0010\nβ(k)\u0011\n,\n(24)\nwhich is maximized on the unit sphere ∥θi∥= 1 when sgn(uivi) = −sgn\n\u0000∇βiL\n\u0000β(k)\u0001\u0001\nand\n|ui| = |vi| resulting in a maximal utility value of ¯U∗\ni = 1\n2\n\f\f∇βiL\n\u0000β(k)\u0001\f\f.\nProof. Substituting the residual r(k)\nj\n= yj −\n\u0000X⊺β(k)\u0001\nj into the definition of the utility function:\nUi(θi; r(k)) = 1\nn\nn\nX\nj=1\nuiviXijrj = −uivi∇βiL\n\u0010\nβ(k)\u0011\n.\n(25)\nObserve that the expression for Ui is linear in uivi. Therefore, under the normalization constraint\n∥θi∥= 1, the utility is maximized when |ui| = |vi| and sgn(uivi) = −sgn\n\u0000∇βiL\n\u0000β(k)\u0001\u0001\n, yielding\nthe maximal value ¯U∗\ni = 1\n2\n\f\f∇βiL\n\u0000β(k)\u0001\f\f.\nLemma C.2. At any time t during AGF, the parameters of the ith neuron satisfy:\nui(t)2 −vi(t)2 = 2α,\n2ui(t)vi(t) = ±\np\n∥θi(t)∥2 −4α4\n(26)\nProof. Both the utility and loss are invariant under the transformation (ui, vi) 7→(gui, g−1vi) for\nany g ̸= 0. This continuous symmetry, together with the fact that each AGF step consists of gradient\nflow in one of these functions, implies that the quantity ui(t)2 −vi(t)2 is conserved throughout\nAGF. Plugging the initialization into this expression gives the first identity ui(t)2 −vi(t)2 = 2α.\nSee Kunin et al. [77] for a general connection between continuous symmetries and conserved\nquantities in gradient flow. For the second identity, we solve for the intersection of the hyperbola\nui(t)2 −vi(t)2 = 2α and the circle ∥θi(t)∥2 = ui(t)2 + vi(t)2, which gives\nui(t) = ±\nr\n∥θi(t)∥2 + 2α2\n2\n,\nvi(t) = ±\nr\n∥θi(t)∥2 −2α2\n2\n.\n(27)\nMultiplying these expressions together gives the identity for the product.\nLemma C.3. After the kth iteration of AGF, the normalized utility for the ith neuron is driven by a\nRiccati ODE d\ndt ¯Ui = 4ηα\n\u0010\u0000 ¯U∗\ni\n\u00012 −¯U2\ni\n\u0011\nwith the unique solution,\n¯Ui(τ (k) + t) = ¯U∗\ni tanh\n\u0010\nδ(k)\ni\n+ 4ηα ¯U∗\ni t\n\u0011\n,\nwhere δ(k)\ni\n= tanh−1\n ¯Ui\n\u0000τ (k)\u0001\n¯U∗\ni\n!\n.\n(28)\nProof. We begin by observing the gradient of the utility function takes the form\n∇θUi\n\u0010\nθi; r(k)\u0011\n= −∇βiL\n\u0010\nβ(k)\u0011 \u0014\nvi\nui\n\u0015\n.\n(29)\nEvaluating this quantity at the normalized parameters ¯θi, and recalling the expression for the maximal\nutility from Lemma C.1, we obtain,\n∥∇θUi(¯θi; r(k))∥2 = 4\n\u0000 ¯U∗\ni\n\u00012 .\n(30)\nSubstituting this into the normalized utility dynamics derived previously (see Equation (14)), we\nobtain the Riccati equation. This is a standard ODE with a known solution, where the constant δ(k)\ni\nis\ndetermined by the initial condition ¯Ui\n\u0000τ (k)\u0001\n.\n24\n"
    },
    {
      "page_number": 25,
      "text": "Theorem C.4. After the kth iteration of AGF, the accumulated utility for the ith neuron is\nSi\n\u0010\nτ (k) + t\n\u0011\n=\n1\n2ηα\nlog cosh\n\u0012\n2ηα\n\u0012\n2U∗\ni t + ζi\n2ηα\ncosh−1 exp\n\u0010\n2ηαSi\n\u0010\nτ (k)\u0011\u0011\u0013\u0013\n,\n(31)\nwhere ηα = −log(\n√\n2α) is the learning rate and ζi = sgn\n\u0000−∇βiL\n\u0000β(k)\u0001\u0001\nρi\n\u0000τ (k)\u0001\n. The quantity\nρi\n\u0000τ (k)\u0001\n= sgn\n\u0000ui\n\u0000τ (k)\u0001\nvi\n\u0000τ (k)\u0001\u0001\nis determined by the recursive formula\nρi\n\u0010\nτ (k) + t\n\u0011\n= sgn\n \nρi\n\u0000τ (k)\u0001\n2ηα\ncosh−1 exp\n\u0010\n2ηαSi\n\u0010\nτ (k)\u0011\u0011\n−∇βiL\n\u0010\nβ(k)\u0011\nt\n!\n,\n(32)\nwhere Si(0) = 0 and ρi(0) = 0.\nProof. Using the expression for the normalized utility derived in Lemma C.3, we can derive an exact\nexpression for the integral of the normalized utility, i.e., the accumulated utility:\nSi\n\u0000τ (k) + t\n\u0001\n= Si\n\u0000τ (k)\u0001\n+\nR t\n0 2 ¯Ui(s)ds = Si\n\u0000τ (k)\u0001\n+\n1\n2ηα log\n\u0012\ncosh\n\u0010\n4ηα ¯U∗\ni t+δ(k)\ni\n\u0011\ncosh\n\u0010\nδ(k)\ni\n\u0011\n\u0013\n.\n(33)\nUsing the hyperbolic identity tanh−1(x) = sgn(x) cosh−1 \u0010\n1\n√\n1−x2\n\u0011\n, we can express the constant\nδ(k)\ni\nintroduced in Lemma C.3:\nδ(k)\ni\n= sgn\n\u0010\n−∇βiL\n\u0010\nβ(k)\u0011\u0011\ntanh−1\n\u00122ui(τ (k))vi(τ (k))\n∥θi(τ (k))∥2\n\u0013\nLemma C.1\n(34)\n= sgn\n\u0010\n−∇βiL\n\u0010\nβ(k)\u0011\u0011\nρi\n\u0010\nτ (k)\u0011\ncosh−1\n\u0012∥θ(τ (k))∥2\n2α2\n\u0013\nLemma C.2\n(35)\n= sgn\n\u0010\n−∇βiL\n\u0010\nβ(k)\u0011\u0011\nρi\n\u0010\nτ (k)\u0011\ncosh−1 \u0010\nexp\n\u0010\n2ηαSi\n\u0010\nτ (k)\u0011\u0011\u0011\n,\n(36)\nwhere in the last equality we used the simplification ∥θ(τ (k))∥2 = 2α2 exp\n\u00002ηαSi\n\u0000τ (k)\u0001\u0001\n. Substi-\ntuting this expression into Equation (33), notice that the denominator inside the logarithm simplifies\nsubstantially, as the Si(τ (k)) term cancels out, yielding Equation (31). Finally, by Lemma C.2, ρi\nchanges sign only when Si = 0, yielding Equation (32).\nCorollary C.5. After the kth iteration of AGF, the next dormant neuron to activate is i∗=\narg mini∈D ∆τi where\n∆τi = cosh−1 exp (2ηα) −ζi cosh−1 exp\n\u00002ηαSi(τ (k))\n\u0001\n2ηα · 2 ¯U∗\ni\n,\n(37)\nζi = sgn\n\u0000−∇βiL\n\u0000β(k)\u0001\u0001\nρi\n\u0000τ (k)\u0001\n, and the next jump time τ (k+1) = τ (k) + ∆τi∗.\nProof. From Theorem C.4, we solve for the time t such that Si(τ (k) + t) = 1 by inverting the\nexpression for accumulated utility. This time is ∆τi.\nC.2\nCost minimization\nActive neurons represents non-zero regression coefficients of β. During the cost minimization step,\nthe active neurons work to collectively minimize the loss. When a neuron activates it does so with a\ncertain sign sgn(uivi) = −sgn(∇βiL(βA)). If during the cost minimization phase a neuron changes\nsign, then it can do so only by returning to dormancy first. This is due to the fact that throughout\nthe gradient flow dynamics the quantity u2\ni −v2\ni = 2α2 is conserved and thus in order to flip the\nsign of the product uivi, the parameters must return to their initialization. As a result, the critical\npoint reached during the cost minimization step is the unique solution to the constrained optimization\nproblem,\nβ∗\nA = arg min\nβ∈Rd\nL(β)\nsubject to\n\u001aβi = 0\nif i /∈A,\nβi · sgn(uivi) ≥0\nif i ∈A,\n(38)\nwhere uniqueness follows from the general position assumption. All coordinates where (β∗\nA)i = 0\nare dormant at the next step of AGF.\n25\n"
    },
    {
      "page_number": 26,
      "text": "τ(1) τ(2)\nτ(3)\nℓ(0)\nℓ(1)\nℓ(2)\nα = 0.50\nα = 0.10\nα = 1e-02\nα = 1e-04\nα = 1e-06\nα →0\n(a) Loss L(t)\nβ1 = u1v1\nβ2 = u2v2\nβ (0)\nβ (1)\nβ (2)\nβ (3)\n2\n2\n4\n4\n6\n6\n8\n8\n10\n10\n12\n12\n14\n16\n18\n(b) Function β(t)\n−π/4\n0\nπ/4\n10−6\n10−4\n10−2\n100\nθ1 = (u1, v1)\n−π/4\n0\nπ/4\n10−6\n10−4\n10−2\n100\nθ2 = (u2, v2)\n(c) Parameters θ(t)\n0\n1\n˜S(t)\nθ1\nθ2\nτ(1) τ(2)\nτ(2) + τ(3)\n2\nτ(3)\n−π/4\n0\nπ/4\nˆθ(t)\nτ(1) τ(2)\nτ(2) + τ(3)\n2\nτ(3)\n(d) Utility ˜S(t) and ˆθ(t)\nFigure 8: AGF = GF as α →0 in diagonal linear networks. We consider a diagonal linear network\nβ = u ⊙v ∈R2, trained by gradient flow from initialization u =\n√\n2α1, v = 0, with varying α,\ninspired by Figure 2 of Pesme and Flammarion [18]. This example is special as it demonstrates how\nan active neuron, in this case β(1), can return to dormancy during the cost minimization phase, as\ndiscussed theoretically in Appendix B. In (a) we plot the loss over accelerated time and in (b) the loss\nlandscape over β. For small α, trajectories evolve from β(0) to β(3), passing near intermediate saddle\npoints β(1) and β(2). Each saddle point is associated with a plateau in the loss. As α →0, gradient\nflow spends all its time at these saddles, jumping instantaneously between them at τ (1), τ (2), τ (3),\nmatching the stepwise loss drops. In (c) we plot trajectories in parameter space and in (d) the same\ndynamics visualized in terms of their accumulated utility and angle. The jump times between critical\npoints correspond to when the accumulated utility satisfies ˜Si(τ) = 1. When the first coordinate\nreturns to dormancy, the accumulated utility first touches zero, causing the angle to flip sign, before\nreactivating with the opposite sign.\nC.3\nAGF in action\nCombining the results of utility maximization (Appendix C.1) and cost minimization (Appendix C.2)\nyields an explicit recursive expression for the sequence generated by AGF. For each neuron we\nonly need to track the accumulated utility Si(t) and the sign ρi(t), which at initialization are both\nzero. We can now consider the relationship between the sequence produced by AGF in the vanishing\ninitialization limit α →0 and the sequence for gradient flow in the same initialization limit introduced\nby Pesme and Flammarion [18].\nAlgorithm 2: Pesme and Flammarion [18]\nInitialize: t ←0, β ←0 ∈Rd, S ←0 ∈Rd;\nwhile ∇L(β) ̸= 0 do\nD ←{j ∈[d] | ∇L(β)j ̸= 0}\nτ ∗←inf {τi > 0 | ∃i ∈D, Si −τi∇L(β)i = ±1}\nt ←t + τ ∗,\nS ←S −τ ∗∇L(β)\nβ = arg min L(β) where β ∈\n\n\n\n\n\nβ ∈Rd\n\f\f\f\f\f\f\f\nβi ≥0\nif Si = +1,\nβi ≤0\nif Si = −1,\nβi = 0\nif Si ∈(−1, 1)\n\n\n\n\n\nreturn Sequence of (β, t);\nWe adapt the original notation to fit our framework, highlighting\nutility maximization (blue) and cost minimization (red) steps.\nIn Pesme and Flammarion [18], they\nprove that in the vanishing initializa-\ntion limit α →0, the trajectory of\ngradient flow, under accelerated time\n˜tα(t) = −log(α) · t, converges to-\nwards a piecewise constant limiting\nprocess corresponding to the sequence\nof saddles produced by Algorithm 2.\nThis algorithm can be split into two\nsteps, which match exactly with the\ntwo steps of AGF in the vanishing ini-\ntialization limit.\nAs in AGF, at each iteration of their\nalgorithm a coefficient that was zero becomes non-zero. The new coefficient comes from the set\n{i ∈[d] : ∇L(β)i ̸= 0}, which is equivalent to the set of dormant neurons with non-zero utility, as\nall active neurons will be in equilibrium from the previous cost minimization step. The index for the\nnew active coefficient is determined by the following expression,\ni∗= arg min{τi > 0 : ∃i ∈D, si −τi∇L(β)i = ±1}.\n(39)\nBecause τi > 0 and si ∈(−1, 1) for all coefficients i ∈D, then this expression only makes sense if\nwe choose the boundary associated with sgn(∇L(β)i). Taking this into account, we get the following\nsimplified expression for the jump times proposed by Pesme and Flammarion [18],\nτi = 1 + sgn(∇L(β)i)si\n|∇L(β)i|\n.\n(40)\nThis expression is equivalent to the expression given in Corollary C.5 in the limit ηα →∞. To see\nthis, we use the asymptotic identity cosh−1(exp(x)) ∼x + log 2 as x →∞. Combined with the\n26\n"
    },
    {
      "page_number": 27,
      "text": "simplification 2 ¯U∗\ni = |∇βiL(β)|, this allows us to simplify the expression for the jump time and\nshow that ρiSi = si where si is the integral from Algorithm 2.\nThe second step of Algorithm 2, is that the new β is determined by the following constrained\nminimization problem,\nβ∗= arg min\nβ∈Rd\nL(β)\nsubject to\n\n\n\nβi ≥0,\nif si = 1\nβi ≤0,\nif si = −1\nβi = 0,\nif si ∈(−1, 1)\n(41)\nUsing the correspondence ρi = sgn(si), and noting that neurons with si ∈(−1, 1) correspond to\nthe dormant set, this constrained optimization problem is equivalent to the cost minimization step of\nAGF, presented in Appendix C.2.\n27\n"
    },
    {
      "page_number": 28,
      "text": "D\nComplete Proofs for Fully Connected Linear Networks\nIn this section, we provide the details behind the results around linear networks in Section 4.\nWe begin by clarifying the notion of ‘neuron’ in this context. As briefly explained in Section 4,\ndue to the symmetry of the parametrization of fully-connected linear networks, the usual notion of\na neuron does not define a canonical decomposition of f into rank-1 maps. Instead, the singular\nvalue decomposition of the linear map AW computed by the network, defines such a canonical\ndecomposition, being the unique orthogonal one. Therefore, we will think of neurons as basis vectors\n(˜ai, ˜wi) ∈Rc+d under orthogonality constraints. These vectors align with the singular vectors and\nare partitioned into dormant and active sets, D and A, based on whether their corresponding singular\nvalue is O(1).\nD.1\nUtility maximization\nDifferently from the usual setting of AGF, the orthogonality constraint on the dormant basis vectors\nimplies that the utility function is not decoupled. Instead, it is maximized over the space of |D|\northonormal basis vectors, i.e., the Stiefel manifold.\nLemma D.1. After the kth iteration of AGF, the utility function of the ith dormant basis vector with\nparameters θi = (˜ai, ˜wi) is:\nUi\n\u0010\nθi; r(k)\u0011\n= −˜a⊺\ni ∇βL\n\u0010\nβ(k)\nA\n\u0011\n˜wi.\n(42)\nThe total utility P\ni∈D Ui\n\u0000θi; r(k)\u0001\nis maximized on the Stiefel manifold when {˜ai, ˜wi}|D|\ni=1 coincides\nwith the set of the top |D| = H−k left and right singular vectors of ∇βL(β(k)\nA ), resulting in a maximal\nutility value for the ith dormant basis vector of ¯U∗\ni = σ(k)\ni\n/2, where σ(k)\ni\nis the corresponding singular\nvalue. Moreover, Ui has no other local maxima.\nProof. After substituting r(k) = y −β(k)\nA x, the computation of the utility is straightforward. Every-\nthing else follows from the standard theory of Reyleigh quotients over Stiefel manifolds [123].\nThis means that the gradient flow of the utility function aligns the dormant basis vectors with the\nsingular vectors of −∇βL(β(k)\nA ).\nD.2\nCost minimization\nAs discussed in Section 4, the cost minimization phase is governed by the well-known Eckart-Young\ntheory of reduced-rank regression [78]. According to the latter, during cost minimization, the active\nbasis vectors converge to\nβ(k)\nA\n= PUkΣyxΣ−1\nxx ,\n(43)\nwhere PUk is the projection onto the top k eigenvectors of ΣyxΣ−1\nxx Σ⊺\nyx. Plugging this solution into\nthe expression for the gradient, we find that the utility at the next iteration will be computed with the\nprojection P⊥\nUkΣyx:\n−∇βL(β(k)\nA ) = Σyx −β(k)\nA Σxx = (Ic −PUk)Σyx.\n(44)\nD.3\nAGF in action\nAlgorithm 3: Greedy Low-Rank Learning Li et al. [20]\nInitialize: r ←0, W0 ←0 ∈Rd×d, U0(∞) ∈Rd×0\nwhile λ1(−∇L(Wr)) > 0 do\nr ←r + 1;\nur ←unit top eigenvector of −∇L(Wr−1);\nUr(0) ←[Ur−1(∞)\n√εur] ∈Rd×r;\nfor t = 0, 1, . . . , T do\nUr(t + 1) ←Ur(t) −η∇L(Ur(t));\nWr ←Ur(∞)U ⊤\nr (∞)\nreturn Sequence of Wr\nPutting together the utility maximiza-\ntion and cost minimization steps, AGF\nprogressively learns the projected\nOLS solution (Equation (43)), coin-\nciding with the GLRL algorithm by\nLi et al. [20], shown in Algorithm 3,\nwith notation from the original work.\nAt each stage, the GLRL algorithm\nselects a top eigenvector (blue: utility\nmaximization) and performs gradient\ndescent in the span of selected direc-\ntions (red: cost minimization).\n28\n"
    },
    {
      "page_number": 29,
      "text": "We now discuss jump times, and motivate Conjecture 4.1. Recall from Section B.4 that in this setting,\nbecause κ = 2, in the limit of vanishing initialization, the alignment in the utility maximization\nphase occurs instantaneously. As a consequence, the individual utility functions remain, essentially,\nconstantly equal to their corresponding maximal value throughout each utility maximization phase.\nFrom the definition of accumulated utility with ηα = −log(α), we immediately deduce the recursive\nrelation:\nSi\n\u0010\nτ (k) + t\n\u0011\n= Si\n\u0010\nτ (k−1)\u0011\n+ σ(k)\ni\nt.\n(45)\nHowever, since −∇βL(β(k)\nA ) = P⊥\nUkΣyx and −∇βL(β(k−1)\nA\n) = P⊥\nUk−1Σyx have different singular\nvalues and vectors, the relation between the values of Equation (45) as i ∈D varies is unclear, and it\nis hard to determine the first dormant basis vector to accumulate a utility value of 1. Yet, suppose\nthat the dormant basis vectors align in such a way that the ordering of the corresponding singular\nvalues is preserved across the utility maximization phases. In this case, the ordering of the cumulated\nutilities in Equation (45) is also preserved for all iterations k and all times t. In particular, it follows\nby induction that the basis vector with index i∗corresponding to the largest eigenvalue σ(k)\ni∗jumps\nafter a time of:\n∆τ (k) =\n1\nσ(k)\ni∗\n\u0010\n1 −Si∗\n\u0010\nτ (k−1)\u0011\u0011\n.\n(46)\nOnce unrolled, the above recursion is equivalent to the statement on jump times in Conjecture 4.1.\nWhen Σxx commutes with Σ⊺\nyxΣyx, the left singular vectors of Σyx simultaneously diagonalize Σxx\nand ΣyxΣ−1\nxx Σ⊺\nyx. In this case, the singular values of P⊥\nUkΣyx correspond to the bottom ones of\nΣyx, with the same associated singular vectors. Therefore, the dormant basis vectors are correctly\naligned already from the first iteration of AGF. By the discussion above, this confirms Conjecture\n4.1 in this specific scenario. Moreover, the recursive expression from Equation (46) reduces to\n∆τ (k) = 1/σk −1/σk−1, where σk is the kth largest singular value of Σyx. Therefore, the jump\ntimes are simply:\nτ (k) = 1\nσk\n.\n(47)\nThese values, together with the loss values in Conjecture 4.1, coincide with the ones derived by Gidel\net al. [19] for GF at vanishing initialization. This means that under the commutativity assumption,\nAGF and GF converge to the same limit, similarly to the setting of diagonal linear networks.\nWhen Σxx and Σ⊺\nyxΣyx do not commute, in order to prove Conjecture 4.1, it is necessary to understand\nthe relation between the SVD of P⊥\nUkΣyx and of Σyx, as k varies. The Poincaré separation theorem\ndescribes this relation for the singular values, stating that they are interlaced, i.e., they alternate\nbetween each other when ordered. This is not sufficient, since the geometry of the singular vectors\nplays a crucial role in the alignment phase. Even though there exist classical results from linear\nalgebra in this direction [124], we believe that Conjecture 4.1 remains open.\n29\n"
    },
    {
      "page_number": 30,
      "text": "E\nComplete Proofs for Attention-only Linear Transformer\nHere, we provide additional details and derivations for the results around transformers (see Section 4\nfor the problem setup and notation). This analysis is based on the setting presented by Zhang et al.\n[32], to which we refer the reader for further details.\nNote that this scenario does not exactly fit the formalism of Section 2, since we are considering a\nneural architecture different from a fully-connected network. Yet, due to our assumption on the rank\nof the attention heads, each head behaves as a bottleneck, resembling a ‘cubical version’ of a neuron.\nTherefore, we will interpret attention heads as ‘neurons’, and apply AGF to this context (with κ = 3).\nWe will show in the following sections that at the end of the kth iteration of AGF, the ith attention\nhead, h = 1, . . . , k, learns the function:\nfh(X; θ∗\nh)D+1,N+1 =\n1\ntrΣxx + (N + 1)λh\nN\nX\nn=1\nynx⊺\nnvhv⊺\nhxN+1,\n(48)\nwhere λh is the hth largest eigenvalue of Σxx, and vh is the corresponding eigenvector. We will\nproceed by induction on k.\nE.1\nUtility maximization\nWe prove the following lemma, establishing, inductively, the utility function for the kth iteration of\nAGF.\nLemma E.1. At the kth iteration of AGF, assume that the (k −1) active attention heads have the\nfunction form as in Equation (48). Then, the utility function of a dormant head with parameters\nθi = (Vi, Qi, Ki) is given by:\nUi\n\u0010\nθi; r(k)\u0011\n= NVi Q⊺\ni\n \nΣ2\nxx −\nk−1\nX\nh=1\nλ2\nhvhv⊺\nh\n!\nKi\n(49)\nMoreover,\nthe utility is maximized over normalized parameters by\n( ¯V ∗\ni , ¯Q∗\ni , ¯K∗\ni )\n=\n(±1/\n√\n3, ±vk/\n√\n3, ±vk/\n√\n3), where the sign is determined such that ¯U∗\ni = Nλ2\nk/(3\n√\n3). Moreover,\nUi has no other local maxima.\nProof. By assumption, we have:\nUi\n\u0010\nθi; r(k)\u0011\n= Ex1,...,xN+1,β\n\" \nyN+1 −\nk−1\nX\nh=1\nfh(X; θ∗\nh)D+1,N+1\n!\nfi(X, θi)D+1,N+1\n#\n.\n(50)\nThe first term, which coincides with the initial utility, can be computed as:\nUi (θi; y) = Ex1,...,xN+1,β [yN+1fi(X, θi)D+1,N+1]\n= Ex1,...,xN+1,β\n\"\nβ⊺xN+1Vi\nN\nX\nn=1\nynx⊺\nnKiQ⊺\ni xN+1\n#\n= Vi Q⊺\ni ExN+1[xN+1x⊺\nN+1]Eβ[ββ⊺]\nN\nX\nn=1\nExn[xnx⊺\nn]Ki\n= NVi Q⊺\ni Σ2\nxxKi.\n(51)\nDenote Ah = (trΣxx + (N + 1)λk)−1. Then, the summand can be computed as:\nEx1,...,xN+1,β [f ∗\nh(X, θh)D+1,N+1fi(X, θi)D+1,N+1]\n= AhViEx1,...,xN+1,βtr\n \nββ⊺\nN\nX\nn=1\nxnx⊺\nnKiQ⊺\ni xN+1x⊺\nN+1vhv⊺\nh\nN\nX\nn=1\nxnx⊺\nn\n!\n= AhVitr\n\nKiQ⊺\ni Σxxvhv⊺\nhEx1,...,xN\n N\nX\nn=1\nxnx⊺\nn\n!2\n\n= AhVitr\n\u0000KiQ⊺\ni λhvhv⊺\nh\n\u0000NΣxxtrΣxx + N(N + 1)Σ2\nxx\n\u0001\u0001\n= NViQ⊺\ni λ2\nhvhv⊺\nhKi,\n(52)\n30\n"
    },
    {
      "page_number": 31,
      "text": "where in the second equality, we have used the fact that\nEx1,...,xN\n\n\n N\nX\nn=1\nxnx⊺\nn\n!2\n= Ex1,...,xN\n\n\nN\nX\nn=1\n(xnx⊺\nn)2 + 2\nX\n1≤i<j≤N\nxix⊺\ni xjx⊺\nj\n\n\n= NΣxxtrΣxx + N(N + 1)Σ2\nxx.\n(53)\nThis provides the desired expression for the utility, which corresponds to a Rayleigh quotient. Since\nthe largest eigenvalue of (Σ2\nxx −Pk−1\nh=1 λ2\nhvhv⊺\nh) is λ2\nk, the rest of the statement follows from the\nstandard theory of Rayleigh quotients.\nE.2\nCost minimization\nAfter utility maximization, a new head aligns to the corresponding eigenvector and becomes active.\nThis implies that at the beginning of the cost minimization phase of the kth iteration of AGF, the k\nactive attention heads have parameters θh = (Vh, Qh, Kh), such that Qh and Kh coincide up to a\nmultiplicative scalar to vh for all 1 ≤h ≤k. Moreover, Equation (48) holds for 1 ≤h < k.\nWe wish to show that during the cost minimization phase, the newly-activated head with parameters\nθk learns a function in the same form. First, we prove that the loss function is decoupled for each\nactive attention head.\nLemma E.2. Assume that the k active attention heads have the same function form, up to multiplica-\ntive scalar, as in Equation (48), Then, the loss over active heads decoupled into sum of individual\nlosses, that is\nL (ΘA) =\nk\nX\ni=1\nL (θi) .\n(54)\nMoreover, for all h = 1, . . . , k, the directional derivatives\n∂L\n∂Qh L(ΘA) and\n∂L\n∂Kh L(ΘA) are propor-\ntional to vh throughout the cost minimization phase.\nProof. The first statement follows from a straightforward calculation using the orthogonality of\neigenvectors. As a result,\n∂L\n∂Qh\n(ΘA) = ∂L\n∂Qh\n(θh) = Ex1,...,xN,β\n \nVh\nN\nX\nn=1\nynx⊺\nnKh\n!2\nΣxxQh,\n(55)\nFrom this expression, we see that if Qh starts as an eigenvector of Σxx, then subsequent gradient\nupdates will be in the same direction. As a result, the direction of Qh remains unchanged in cost\nminimization. A similar argument holds for Kh as well.\nSince the active heads follow the GF of L, the decoupling of the losses implies that during the\ncost minimization phase, each head actually follows the gradient of the loss of its own parameters,\ndisregarding the other heads. In particular, θh remains at equilibrium for h = 1, . . . , k −1, since\nits loss has been optimized during the previous iterations of AGF. By Lemmas E.1 and E.2, the\nnewly-activated head remains aligned to its eigenvector vk, learning a function of the form\nfk (X; θk)D+1,N+1 = Ak\nN\nX\nn=1\nynx⊺\nnvk+1v⊺\nk+1xN+1,\n(56)\nwhere Ak ∈R is a parameter that is optimized in cost minimization. L(θk) is convex with respect to\nAk, and the corresponding minimization problem is easily seen to be solved by Ak = 1/(trΣxx +\n(N + 1)λk). This concludes our inductive argument.\n31\n"
    },
    {
      "page_number": 32,
      "text": "E.3\nAGF in action\nAs we have seen above, AGF describes a recursive procedure, where each head sequentially learns a\nprincipal component. We can easily compute the loss value at the end of each iteration:\nℓ(k) ≡Ex1,...,xN+1,β\n1\n2\n \nyN+1 −\nk\nX\nh=1\nAh\nN\nX\nn=1\nynx⊺\nnvhv⊺\nhxN+1\n!2\n= 1\n2\n D\nX\ni=1\nλi −\nk\nX\nh=1\nNλ2\nh\ntrΣxx + (N + 1)λh\n!\n.\n(57)\nFurthermore, as an immediate consequence of Lemma E.1, we can lower bound the jump times. The\naccumulated utility at the kth iteration of AGF is upper-bounded by Si(t) ≤3t ¯U∗\ni = tNλ2\nk/\n√\n3. The\njump time is given by the first head that reaches Si(τ (k) −τ (l)) = 1/∥θi(τ (l))∥, which yields\nτ (k) −τ (l) ≥\n√\n3\nNλ2\nkµl\n,\n(58)\nwhere µk = maxi ∥θi(τ (l))∥and τ (0) = 0.\n32\n"
    },
    {
      "page_number": 33,
      "text": "F\nComplete Proofs for Generalized Modular Addition\nIn this section, we provide the proofs for the results summarized in Section 5. Instead of reasoning\ninductively as in Section E, for simplicity of explanation we will derive in detail the steps of the\nfirst iteration of AGF. As we will explain in Section F.3, all the derivations can be straightforwardly\nextended to the successive iterations, completing the inductive argument.\nF.1\nUtility maximization\nIn order to compute the utility for a single neuron, we will heavily rely on the Discrete Fourier\nTransform (DFT) and its properties. To this end, recall that for u ∈Rp, its DFT ˆu ∈Rp is defined as:\nˆu[k] =\np−1\nX\na=0\nu[a]e−2πika/p,\n(59)\nwhere i = √−1 is the imaginary unit. We start by proving a technical result.\nLemma F.1. Let x, u, v, w ∈Rp. Then:\np−1\nX\na,b=0\n⟨u, a · x⟩⟨v, b · x⟩⟨w, (a + b) · x⟩= 1\np\np−1\nX\nk=0\n|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k].\n(60)\nProof. Notice that the inner product ⟨u, a · x⟩can be expressed as\n⟨u, a · x⟩=\np\nX\nk=0\nx[k]u[k + a] = (x ⋆u)[a],\n(61)\nwhere x ⋆u is the cross-correlation between x and u. Thus, the left-hand side of Equation (60) can\nbe rewritten as:\np−1\nX\na,b=0\n(x ⋆u)[a] (x ⋆v)[b] (x ⋆w)[a + b] = ⟨x ⋆u, x ⋆v ⋆x ⋆w⟩\nBy using Plancharel’s theorem ⟨x, y⟩=\n1\np⟨ˆx, ˆy⟩and the cross-correlation property [\nx ⋆y[k] =\nˆx[k]ˆy[k], the above expression equals\n1\np\np−1\nX\nk=0\nˆx[k]ˆu[k]ˆv[k] ˆw[k]ˆx[k]2,\n(62)\nwhich corresponds to the desired result via the simplification ˆx[k]ˆx[k] = |ˆx[k]|2.\nLemma F.2. Under the modular addition task with embedding vector x and mean centered labels,\nthe initial utility function for a single neuron parameterized by θ = (u, v, w) can be expressed as\nU(θ; y) = 2\np3\np−1\nX\nk=1\n|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k].\n(63)\nProof. By definition, the utility is:\nU(θ; y) = 1\np2\np−1\nX\na,b=0\n\u001c\n(⟨u, a · x⟩+ ⟨v, b · x⟩)2 w, (a + b) · x −⟨x, 1⟩\np\n1\n\u001d\n= 1\np2\np−1\nX\na,b=0\n\u0000⟨u, a · x⟩2 + ⟨v, b · x⟩2 + 2⟨u, a · x⟩⟨v, b · x⟩\n\u0001 \u0012\n⟨w, (a + b) · x⟩−⟨w, 1⟩⟨x, 1⟩\np\n\u0013\n.\n(64)\n33\n"
    },
    {
      "page_number": 34,
      "text": "Due to the cyclic structure of ⟨w, (a+b)·x⟩, the contributions from the terms ⟨u, a·x⟩2 and ⟨v, b·x⟩2\nterms vanish. This reduces the utility to\n2\np2\np−1\nX\na,b=0\n⟨u, a · x⟩⟨v, b · x⟩⟨w, (a + b) · x⟩−2\np3\n p−1\nX\na=0\n⟨u, a · x⟩\n!  p−1\nX\nb=0\n⟨v, b · x⟩\n!  p−1\nX\nc=0\n⟨w, c · x⟩\n!\n.\n(65)\nThe first summand in the above expression is provided by Lemma F.1. Moreover, since the mean of a\nvector corresponds to the zero-frequency component k = 0 of its DFT, the second summand reduces\nto:\n2\np3\n p−1\nX\na=0\n⟨u, a · x⟩\n!  p−1\nX\nb=0\n⟨v, b · x⟩\n!  p−1\nX\nc=0\n⟨w, c · x⟩\n!\n= 2\np3 [\nx ⋆u[0] [\nx ⋆v[0] [\nx ⋆w[0]\n= 2\np3 |ˆx[0]|3 ˆu[0]ˆv[0] ˆw[0].\n(66)\nSince the latter coincides with the term k = 0 in Equation (61), we obtain the desired expression.\nWe now solve the constrained optimization problem involved in the utility maximization step of our\nframework.\nTheorem F.3. Let ξ be a frequency that maximizes |ˆx[k]|, k = 1, . . . , p −1, and denote by sx the\nphase of ˆx[ξ]. Then the unit vectors θ∗= (u∗, v∗, w∗) that maximize the initial utility function U(θ; y)\ntake the form\nu∗[a] =\nr 2\n3p cos\n\u0012\n2π ξ\npa + su\n\u0013\nv∗[b] =\nr 2\n3p cos\n\u0012\n2π ξ\npb + sv\n\u0013\nw∗[c] =\nr 2\n3p cos\n\u0012\n2π ξ\npc + sw\n\u0013\n,\n(67)\nwhere a, b, c ∈{0, . . . , p −1} are indices and su, sv, sw ∈R are phase shifts satisfying su + sv ≡\nsw + sx (mod 2π). They achieve a maximal value of ¯U∗=\np\n2/(27p3)|ˆx[ξ]|3. Moreover, the utility\nfunction has no local maxima other than the ones described above.\nProof. By Plancharel’s theorem, the constraint ∥θ∥2 = ∥u∥2 + ∥v∥2 + ∥w∥2 = 1 is equivalent to a\nconstraint on the squared norm of its DFT, up to a scaling. Together with Lemma F.2, this allows us\nto reformulate the original optimization problem in the frequency domain as:\nmaximize\n2\np3\np−1\nX\nk=1\n|ˆx[k]|2 ˆu[k]ˆv[k] ˆw[k]ˆx[k]\nsubject to\n∥ˆu∥2 + ∥ˆv∥2 + ∥ˆw∥2 = p.\n(68)\nTo solve this optimization problem, consider the magnitudes µx, µu, µv, µw ∈Rp\n+ and phases\nSx, Su, Sv, Sw ∈[0, 2π)p of the DFT coefficient of x, u, v, w, respectively. This means that ˆx[k] =\nµx[k] exp(iSx[k]) for every k, and similarly for u, v, w.\nSince u, v, and w are real-valued, their DFTs satisfy conjugate symmetry (i.e., ˆu[−k] = ˆu[k]). Since\np is odd, the periodicity of the DFT (i.e., ˆu[k] = ˆu[k + p]) allows us to simplify the objective by only\nconsidering the first half of the frequencies. Substituting the magnitude and phase expressions we\nobtain the equivalent optimization,\nmaximize\n4\np3\np−1\n2\nX\nk=1\nµx[k]3µu[k]µv[k]µw[k] cos(Su[k] + Sv[k] −Sw[k] −Sx[k])\nsubject to\np−1\n2\nX\nk=0\nµu[k]2 +\np−1\n2\nX\nk=0\nµv[a]2 +\np−1\n2\nX\nk=0\nµw[a]2 = p\n2.\n(69)\n34\n"
    },
    {
      "page_number": 35,
      "text": "The phase terms can be chosen independent of the constraints to maximize the cosine term. Since the\nonly local maximum of the cosine is 0 (mod 2π), the only local optimum of that term is achieved\nby setting Su[k] + Sv[k] −Sw[k] −Sx[k] ≡0 (mod 2π). But then the optimization problem\nreduces to maximizing P\nk∈[(p−1)/2] µx[k]3µu[k]µv[k]µw[k], subject to the constraints. The only\nlocal maximum of this problem is easily seen to be achieved by concentrating all the magnitude at\nthe dominant frequency ξ of x (and its conjugate), i.e.:\nµu[k] = µv[k] = µw[k] =\n\u001ap p\n6\nif k = ±ξ\n(mod p)\n0\notherwise.\n(70)\nThis gives a maximal objective value of ¯U∗= (4/p3)µx[ξ]3(p/6)3/2 =\np\n2/(27p3)|ˆx[ξ]|3. By\napplying the inverse DFT with these choices for magnitudes and phases, while accounting for\nconjugate symmetry, yields the desired result. Note that in the statement, sx, su, sv, sw denote\nSx[ξ], Su[ξ], Sv[ξ], Sw[ξ], respectively.\nWe highlight that Theorem F.3 and its proof is very similar to Theorem 7 and its proof in Morwani\net al. [96]. They derived this optimization problem by looking for a maximum margin solution,\nsuggesting there may be an interesting connection between utility maximization and maximum\nmargin biases of gradient descent.\nF.2\nCost minimization\nWe now discuss cost minimization. As demonstrated in the previous section, during the utility\nmaximization step, the parameters of each neuron align with the harmonic of the dominant frequency\nξ of x, albeit with phase shifts si\nu, si\nv, si\nw. The goal of this section is two-fold. First, in Appendix F.2.1\nwe discuss our assumption that during the cost minimization phase, the neurons remain aligned with\nthe same harmonic, possibly varying the amplitudes and phase shifts of their parameters. Second,\nin Appendix F.2.2 we solve the cost minimization problem over such aligned neurons. Therefore,\nthroughout the section, we consider N neurons parametrized by Θ = (ui, vi, wi)N\ni=1, where:\nui[a] = Ai\nu\nr 2\n3p cos\n\u0012\n2π ξ\npa + si\nu\n\u0013\n,\nvi[b] = Ai\nv\nr 2\n3p cos\n\u0012\n2π ξ\npb + si\nv\n\u0013\n,\nwi[c] = Ai\nw\nr 2\n3p cos\n\u0012\n2π ξ\npc + si\nw\n\u0013\n,\n(71)\nfor some amplitudes Ai\nu, Ai\nv, Ai\nw ∈R≥0 and some phase shifts si\nu, si\nv, si\nw ∈R.\nTo begin with, note that loss splits as:\nL(Θ) =\n1\n2p2\np−1\nX\na,b=0\n\r\r\r\r\r\nN\nX\ni=1\nfi(a · x, b · x; θi) −(a + b) · x + ⟨x, 1⟩\np\n1\n\r\r\r\r\r\n2\n= C(Θ) −U(Θ; y) + 1\n2\n\u0012\n∥x∥2 −⟨x, 1⟩2\np\n\u0013\n,\n(72)\nwhere\nC(Θ) =\n1\n2p2\nN\nX\ni,j=1\n⟨wi, wj⟩\np−1\nX\na,b=0\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u00012 \u0000⟨uj, a · x⟩+ ⟨vj, b · x⟩\n\u00012\n(73)\nand U(Θ; y) = PN\ni=1 U(θi; y) is the cumulated utility function of the N neurons. From Lemma F.2,\nwe know that:\nU(θi; y) =\nr\n2\n27p3 |ˆx[ξ]|3Ai\nuAi\nvAi\nw cos(si\nu + si\nv −si\nw −sx).\n(74)\n35\n"
    },
    {
      "page_number": 36,
      "text": "Throughout this section, we repeatedly use the following identity: for any p ∈N, k ∈Z, and s ∈R,\np−1\nX\na=0\ncos\n\u0010\ns −2πka\np\n\u0011\n=\n(\np cos(s),\nif k = 0\n(mod p),\n0,\notherwise.\n(75)\nThis follows by writing cos(θ) = Re(eiθ) and noting that the geometric sum Pp−1\na=0 e−2πika/p\nvanishes unless k = 0 (mod p).\nF.2.1\nPreservation of harmonic alignment\nTo analyze cost minimization, we restrict attention to a regime in which the newly activated neurons\ndo not change their aligned harmonic during this phase.\nAssumption F.4. During cost minimization, the N newly activated neurons remain aligned to the\nharmonic ξ.\nThis restriction allows us to solve cost minimization within the subspace spanned by these N neurons,\nin close analogy with prior sections. However, first we characterize the conditions under which this\nassumption is valid and clarify in what sense it provides a faithful description of the dynamics.\nTheorem F.5. Let h ∈Rp be a vector in the form:\nh[a] = A cos\n\u0012\n2π ξ′\np a + s\n\u0013\n,\n(76)\nfor some A, s ∈R and ξ′ ̸= 0, ±ξ. If for all a, b ∈[p]\nN\nX\ni=1\nwi⟨ui, a · x⟩2 =\nN\nX\ni=1\nwi⟨vi, b · x⟩2 = 0,\n(77)\nthen for all i = 1, . . . , N:\n\u001c\nh, ∂L\n∂ui (Θ)\n\u001d\n=\n\u001c\nh, ∂L\n∂vi (Θ)\n\u001d\n=\n\u001c\nh, ∂L\n∂wi (Θ)\n\u001d\n= 0.\n(78)\nProof. A direct calculation leads to the following expressions for the derivatives:\n\u001c\nh, ∂L\n∂ui (Θ)\n\u001d\n= 4\np−1\nX\na,b=0\n⟨h, a · x⟩\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u0001 \nwi, f(a · x, b · x; Θ) −(a + b) · x\n\u000b\n,\n\u001c\nh, ∂L\n∂vi (Θ)\n\u001d\n= 4\np−1\nX\na,b=0\n⟨h, b · x⟩\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u0001 \nwi, f(a · x, b · x; Θ) −(a + b) · x\n\u000b\n,\n\u001c\nh, ∂L\n∂wi (Θ)\n\u001d\n= 2\np−1\nX\na,b=0\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u00012 ⟨h, f(a · x, b · x; Θ) −(a + b) · x⟩.\n(79)\nVia a tedious goniometric computation, the three expressions above can be expanded into a sum\nof cosine terms. Since ξ ̸= ξ′, each of these terms depends on a or b, and therefore averages out.\nWe do not report the details of the computation here, but refer to the proof of Lemma F.6 for more\ndetails.\nThe condition Equation (77) rules out square terms that could otherwise generate gradient components\nat resonant frequencies ξ′ ̸= ±ξ. If the squared activations ⟨ui, a · x⟩2 or ⟨vi, b · x⟩2 have a nonzero\naggregate contribution, their trigonometric expansion produces cosine terms at doubled or mixed\nfrequencies that may survive the averaging over a or b via the cosine-sum identity Equation (75). In\nthis case, the gradient can acquire components outside the span of the aligned harmonic, allowing\nneurons to escape alignment.\nHowever, pairs of aligned neurons can easily satisfy Equation (77) by coordinating their amplitudes\nand phases. For example, using the identity z1z2 = 1\n4(z1 + z2)2 −1\n4(z1 −z2)2, neurons can arrange\nfor square terms to cancel in aggregate while preserving their linear contributions. Such cancellations\ncorrespond to directional adjustments that can occur rapidly during cost minimization.\n36\n"
    },
    {
      "page_number": 37,
      "text": "F.2.2\nCost minimization over aligned neurons\nNext, in order to solve the cost minimization problem over aligned neurons, we explicitly compute\nthe term C(Θ) in the loss function.\nLemma F.6. We have:\nC(Θ) = |ˆx[ξ]|4\n54p2\nN\nX\ni,j=1\nAi\nwAj\nw cos(si\nw −sj\nw)\n\u0010 \u0000(Ai\nu)2 + (Ai\nv)2\u0001 \u0000(Aj\nu)2 + (Aj\nv)2\u0001\n+(Ai\nuAj\nu)2\n2\ncos(2(si\nu −sj\nu)) + (Ai\nvAj\nv)2\n2\ncos(2(si\nv −sj\nv))\n+2Ai\nuAi\nvAj\nuAj\nv\n\u0000cos(si\nu + si\nv −sj\nu −sj\nv) + cos(si\nu −si\nv −sj\nu + sj\nv)\n\u0001 \u0011\n.\n(80)\nProof. This follows from a rather tedious computation, which we summarize here. First, note that\n⟨wi, wj⟩= 2Ai\nwAj\nw\n3p\np−1\nX\nc=0\ncos\n\u0012\nsi\nw + 2π ξ\npc\n\u0013\ncos\n\u0012\nsj\nw + 2π ξ\npc\n\u0013\n= Ai\nwAj\nw\n3\ncos(si\nw −sj\nw).\n(81)\nNext, the shift-equivariance property of the Fourier transform and Plancharel’s theorem imply that\nfor all i and a we have:\n⟨ui, a · x⟩= 1\np⟨bui, d\na · x⟩=\nr 2\n3p|ˆx[ξ]|Ai\nu cos\n\u0012\nsx −si\nu −2πξ a\np\n\u0013\n,\n(82)\nand similarly for vi.\nWe can plug the above expression into the quadratic term\n\u0000⟨ui, a · x⟩+ ⟨vi, b · x⟩\n\u00012 from Equation (73) and expand it via the goniometric identity:\n(A cos(α)+B cos(β))2 = 1\n2(A2+B2+A2 cos(2α)+B2 cos(2β))+AB(cos(α+β)+cos(α−β)).\n(83)\nWe similarly expand the term\n\u0000⟨uj, a · x⟩+ ⟨vj, b · x⟩\n\u00012. By leveraging on the goniometric identity\n2 cos(α) cos(β) = cos(α −β) + cos(α + β), the product of these quadratic terms for the ith and\njth neurons expands into 16 unique terms. Since p is odd, 2ξ, 4ξ ̸= 0 (mod p), and thus, by\nEquation (75), only the terms independent from both a and b do not average out. This leaves four\nterms, providing the desired result.\nWe now provide a lower bound for the (meaningful terms of the) loss function.\nTheorem F.7. We have the following lower bound:\nC(Θ) −U(Θ; y) ≥−|ˆx[ξ]|2\np\n.\n(84)\nMoreover, equality holds if, and only if, we have that PN\ni=1 Ci cos(αi) = PN\ni=1 Ci sin(αi) = 0 for\nany choice of (Ci, αi) among\n(Ai\nw((Ai\nu)2 + (Ai\nv)2), si\nw),\n(Ai\nw(Ai\nu)2, si\nw ± 2si\nu),\n(Ai\nw(Ai\nv)2, si\nw ± 2si\nv),\n(Ai\nwAi\nuAi\nv, si\nw ± (si\nu −si\nv)),\n(Ai\nwAi\nuAi\nv, si\nw + si\nu + si\nv),\n(85)\nand, moreover, PN\ni=1 Ai\nwAi\nuAi\nv sin(si\nw + sx −si\nu −si\nv) = 0 and PN\ni=1 Ai\nwAi\nuAi\nv cos(si\nw + sx −\nsi\nu −si\nv) = √54p/|ˆx[ξ]|.\nProof. Given amplitudes C1, . . . , CN and angles α1, . . . , αN, consider the goniometric identity\nN\nX\ni,j=1\nCiCj cos(αi −αj) =\n N\nX\ni=1\nCi cos(αi)\n!2\n+\n N\nX\ni=1\nCi sin(αi)\n!2\n.\n(86)\n37\n"
    },
    {
      "page_number": 38,
      "text": "By expanding each product of two cosines in Equation (80) into a sum of two cosines, we can apply\nthe above identity to each summand by choosing αi ∈{si\nw, si\nw ±2si\nu, si\nw ±2si\nv, si\nw ±(si\nu −si\nv), si\nw +\nsi\nu + si\nv, si\nw + sx −si\nu −si\nv}. Since both the summands in the right-hand side of Equation (86) are\npositive, we conclude that:\nC(Θ) ≥|ˆx[ξ]|4\n54p2\n N\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv)\n!2\n,\n(87)\nwith equality holding only if Equation (85) is satisfied. Therefore, using Equation (74), we deduce:\nC(Θ) −U(Θ; y) ≥|ˆx[ξ]|4\n54p2\n N\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv)\n!2\n−\nr\n2\n27p3 |ˆx[ξ]|3\nN\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv)\n=\n \n|ˆx[ξ]|2\n√\n54p\nN\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv) −|ˆx[ξ]|\n√p\n!2\n−|ˆx[ξ]|2\np\n≥−|ˆx[ξ]|2\np\n,\n(88)\nwith equality only when (|ˆx[ξ]|2/(\n√\n54p)) PN\ni=1 Ai\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv) = |ˆx[ξ]|/√p,\nwhich concludes the proof.\nFor N ≥6, the lower bound from Equation (84) is tight. For even N, minimizers can be constructed\nby setting si\nw + sx = si\nu + si\nv =\n2πi\nN , si\nu −si\nv ∈{0, π} alternating (depending on N), and\nAi\nwAi\nuAi\nv = √54p/(N|ˆx[ξ]|). For N odd, a similar construction holds.\nBy combining Theorem F.7 and Equation (72), we deduce the following tight lower bound on the\nloss function:\nL(Θ) ≥1\n2\n\u0012\n∥x∥2 −⟨x, 1⟩2\np\n−2|ˆx[ξ]|2\np\n\u0013\n.\n(89)\nF.3\nUtility update\nNext, we consider the utility for a new neuron after some group of N ≥6 neurons has undergone\nthe previous steps AGF. Suppose that Θ∗= (ui\n∗, vi\n∗, wi\n∗)N\ni=1 are parameters minimizing the loss as in\nSection F.2. We start by describing the function computed by the network with parameters Θ∗.\nLemma F.8. For all 0 ≤a, b < p, the network satisfies:\nf(a · x, b · x; Θ∗) = 2|ˆx[ξ]|\np\n(a + b) · χξ,\n(90)\nwhere χξ is defined as χξ[c] = cos\n\u0010\n2π ξ\npc + sx\n\u0011\n.\nProof. The N neurons compute the function\nf(a · x, b · x; Θ∗)[c] =\nN\nX\ni=1\n\u0000⟨ui\n∗, a · x⟩+ ⟨vi\n∗, b · x⟩\n\u00012 wi\n∗[c].\n(91)\nThe equation above can be expanded via a computation analogous to the one in the proof of Lemma\nF.6. The conditions on minimizers from Theorem F.7 imply that the only non-vanishing term is of the\n38\n"
    },
    {
      "page_number": 39,
      "text": "form:\nr\n2\n27p3 |ˆx[ξ]|2\nN\nX\ni=1\nAi\nwAi\nuAi\nv cos\n\u0012\n2sx + si\nw −si\nu −si\nv + 2π ξ\np(c −a −b)\n\u0013\n=\nr\n2\n27p3 cos\n\u0012\nsx + 2π ξ\np(c −a −b)\n\u0013 N\nX\ni=1\nAi\nwAi\nuAi\nv cos(si\nw + sx −si\nu −si\nv)\n|\n{z\n}\n√54p/|ˆx[ξ]|\n=2|ˆx[ξ]|\np\ncos\n\u0012\nsx + 2π ξ\np(c −a −b)\n\u0013\n,\n(92)\nwhere in the first identity we used the fact that PN\ni=1 Ai\nwAi\nuAi\nv sin(si\nw + sx −si\nu −si\nv) = 0. This\nconcludes the proof.\nFinally, we compute the utility function with the updated residual r(a · x, b · x) = (a + b) · x −\n(1/p)⟨x, 1⟩1 −f(a · x, b · x; Θ∗), which is maximized by new neurons in the second iteration of\nAGF.\nTheorem F.9. The utility function with the updated residual for a neuron with parameters θ =\n(u, v, w) is:\nU(θ; r) = 2\np3\nX\nk∈[p]\\{0,±ξ}\n|ˆx[k]|2ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ].\n(93)\nProof. The new utility function is\nU(θ; r) = U(θ; y) −1\np2\np−1\nX\na,b=0\nD\n(⟨u, a · x⟩+ ⟨v, b · x⟩)2 w, f(a · x, b · x; Θ∗)\nE\n|\n{z\n}\n∆(θ)\n(94)\nWe wish to express ∆in the frequency domain. By plugging in Equation (90), and due to the cyclic\nstructure of χξ, the squared terms average out:\n∆(θ) =\np−1\nX\na,b=0\n\u0000⟨u, a · x⟩2 + 2⟨u, a · x⟩⟨v, b · x⟩+ ⟨v, b · x⟩2\u0001\n⟨w, f(a · x, b · x; Θ∗)⟩\n(95)\n= 2\np−1\nX\na,b=0\n⟨u, a · x⟩⟨v, b · x⟩⟨w, f(a · x, b · x; Θ∗)⟩.\n(96)\nNow, by reasoning analogously to the proof of Lemma F.1, we obtain:\n∆(θ) = 4|ˆx[ξ]|\np\np−1\nX\na,b=0\n(x ⋆u)[a] (x ⋆y)[b] (χξ ⋆w)[a + b]\n= 4|ˆx[ξ]|\np2\np−1\nX\nk=0\n\\\n(x ⋆u)[k] \\\n(x ⋆v)[k]\n\\\n(χξ ⋆w)[k]\n= 2|ˆx[ξ]|\np\n\u0010\nˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ] + ˆu[ξ]ˆv[ξ] ˆw[ξ]ˆx[ξ]\n\u0011\n,\n(97)\nwhere in the last equality we used the fact that c\nχξ[k] = p\n2e±isx if k = ±ξ and 0 otherwise. By\nsubtracting the above expression to the expression for the initial utility from Lemma F.2, we obtain\nthe desired result.\nIn summary, after an iteration of AGF, the utility for a new neuron has the same form as the\ninitial one, but with the summand corresponding to the dominant (conjugate) frequencies ±ξ of x\nremoved. Consequently, by the same reasoning as in Section F.1, we conclude that during the utility\n39\n"
    },
    {
      "page_number": 40,
      "text": "maximization phase of the second iteration of AGF, some new group of dormant neurons aligns with\nthe harmonic whose frequency has the second largest magnitude in ˆx.\nA subtlety arises during the cost minimization phase of the second iteration, as the neurons that\naligned during the first phase are still involved in the optimization process. However, note that in\nthe loss component C(Θ) (Equation (73)), the terms of the form ⟨wi, wj⟩vanish when i and j are\nindices of neurons aligned with harmonics of different frequencies. Therefore, during the second cost\nminimization phase, the loss L(Θ) splits into the sum of two losses, corresponding to the neurons\naligned during the first and second iteration of AGF, respectively. The neurons from the first phase are\nalready at a critical point of their respective loss term, thus the second group of neurons is optimized\nindependently, via the same arguments as in Section F.1. This scenario is analogous to the one\ndiscussed in Section E.2 for linear transformers (cf. Lemma E.2). In conclusion, after the second\niteration of AGF, the new utility will again have the same form as the initial one, but with the two\n(conjugate pairs of) frequencies removed. This argument iterates recursively, until either all the\nfrequencies are exhausted, or all the H neurons have become active. The quantities in Equation (9)\ncan be derived for the successive iterations of AGF analogously to the previous sections. Lastly, the\nestimate on jump times is obtained by the same argument as in Section E.3.\n40\n"
    },
    {
      "page_number": 41,
      "text": "NeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: The abstract describes the contributions on the high level, while the introduction\ncontains a precise statement of contributions, with references to the sections where such\ncontributions are discussed.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The main limitation of our work is that our theory is developed for two-layer\nneural networks. We highlight this limitation in the title, abstract, and introduction. When\nintroducing our framework in Section 2 we clearly explain the assumptions used to develop\nour algorithm. At the end of the paper in Section 6 we discuss additional limitations of our\nwork and how future work could address them.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n41\n"
    },
    {
      "page_number": 42,
      "text": "• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: All theorems and formal statements are stated either in the main body, or in\nthe appendix, with the full set of assumptions and clearly defined notation. All the formal\nproofs are provided in the appendix with intuition discussed in the main.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide a detailed description of our algorithm AGF in Section 2.2 and\nprovide pseudocode in Figure 2. In each setting where we applied AGF we fully describe the\nexperimental setup, including necessary assumptions on initialization and hyperparameters,\nsuch that a reader could faithfully reconstruct our experiments. Every figure clearly discusses\nwhat quantities from the experiments are being visualized.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\n42\n"
    },
    {
      "page_number": 43,
      "text": "of a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: In our supplementary material we enclose a set of notebooks that can be run to\ngenerate all figures and experiments discussed in the paper. In these notebooks we have also\nimplemented a version of our main algorithm AGF such that others could adapt our code to\nrun in settings not discussed in this work.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\n43\n"
    },
    {
      "page_number": 44,
      "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: The primary purpose of our experiment is to describe the feature learning\ndynamics of gradient flow. Thus, our experiments do not discuss train or test splits, rather\nthey show faithful reconstruction of gradient flow dynamics from our proposed theory of\nAGF. As mentioned above, we provide all the necessary hyperparameters (learning rate and\ninitialization scheme) to reconstruct the figures introduced in this work.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [NA]\nJustification: The experiments we introduce are for gradient flow with complete access to\nthe training data. Thus, in the figures we present there should be no variability between runs\nwith the same initialization as each run is essentially a solution to an ODE up to numerical\nprecision. Our provided code sets a random seed for each experiment such that a reader can\nexactly reproduce each figure introduced in our work.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\n44\n"
    },
    {
      "page_number": 45,
      "text": "Question: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: All experiments in this work involve shallow networks with small constructed\ndatasets that can be run locally. Our provided code describe the necessary packages needed\nto run our code, but specialized computer resources are not needed.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We have reviewed the Code of Ethics, and believe that the research in the\npaper conforms to it in every aspect (e.g., complete anonymity).\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: This paper discusses general theoretical properties of neural networks. As\nsuch, we do not see any potential societal impact of the research presented here.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\n45\n"
    },
    {
      "page_number": 46,
      "text": "that a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper does not involve any model or dataset with a high risk for misuse.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: The paper does not use any existing asset.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n46\n"
    },
    {
      "page_number": 47,
      "text": "• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: The paper does not introduce any new asset.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n47\n"
    },
    {
      "page_number": 48,
      "text": "• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: No LLM was used in the development of the core results of the paper. LLMs\nwere only used for writing and editing purposes.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n48\n"
    }
  ]
}