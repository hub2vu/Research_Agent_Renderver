{
  "filename": "n5PrId7pk5_Linear combinations of latents in generative models_ subspaces and beyond.pdf",
  "total_pages": 30,
  "full_text": "Published as a conference paper at ICLR 2025\nLINEAR COMBINATIONS OF LATENTS IN GENERATIVE\nMODELS: SUBSPACES AND BEYOND\nErik Bodin1\nAlexandru Stere4\nDragos D. Margineantu5\nCarl Henrik Ek1,3\nHenry Moss1,2\n1University of Cambridge\n2Lancaster University\n3Karolinska Institutet\n4Boeing Commercial Airplanes\n5Boeing AI\nABSTRACT\nSampling from generative models has become a crucial tool for applications like\ndata synthesis and augmentation. Diffusion, Flow Matching and Continuous Nor-\nmalising Flows have shown effectiveness across various modalities, and rely on\nlatent variables for generation. For experimental design or creative applications that\nrequire more control over the generation process, it has become common to manip-\nulate the latent variable directly. However, existing approaches for performing such\nmanipulations (e.g. interpolation or forming low-dimensional representations) only\nwork well in special cases or are network or data-modality specific. We propose\nLatent Optimal Linear combinations (LOL) as a general-purpose method to form\nlinear combinations of latent variables that adhere to the assumptions of the gener-\native model. As LOL is easy to implement and naturally addresses the broader task\nof forming any linear combinations, e.g. the construction of subspaces of the latent\nspace, LOL dramatically simplifies the creation of expressive low-dimensional\nrepresentations of high-dimensional objects.\n1\nINTRODUCTION\nGenerative models are a cornerstone of machine learning, with diverse applications including image\nsynthesis, data augmentation, and creative content generation. Diffusion models (Ho et al., 2020;\nSong et al., 2020a;b) have emerged as a particularly effective approach to generative modeling for\nvarious modalities, such as for images (Ho et al., 2020), audio (Kong et al., 2020), video (Ho et al.,\n2022), and 3D models (Luo & Hu, 2021). A yet more recent approach to generative modelling is\nFlow Matching (Lipman et al., 2022), built upon Continuous Normalising Flows (Chen et al., 2018),\nthat generalises diffusion to allow for different probability paths between data and latent distribution,\ne.g. defined through optimal transport (Gulrajani et al., 2017; Villani et al., 2009).\nAs well as generation, these models allow inversion where, by running the generative procedure\nin the opposite direction, data objects can be transformed deterministically into a corresponding\nrealisation in the latent space. Such invertible connections between latent and data space provide a\nconvenient mechanism for controlling generated objects by manipulating their latent vectors. The\nmost common manipulation is to attempt semantically meaningful interpolation of two generated\nobjects (Song et al., 2020a;b; Luo & Hu, 2021) by interpolating their corresponding latent vectors.\nHowever, the optimal choice of interpolant remains an open question, with simple approaches like\nlinear interpolation leading to intermediates for which the model fails to generate plausible objects.\nWhite (2016) argues that poor quality generation under linear interpolation is due to a mismatch\nbetween the norms of the intermediate vectors and those of the Gaussian vectors that the model\nhas been trained to expect. Indeed, it is well-known that the squared norm of a D-dimensional unit\nGaussian follows the chi-squared distribution. Consequently, likely samples are concentrated in a tight\nannulus around a radius\n√\nD — a set which is not closed under linear interpolation. Because of this it\nis common to rely instead on spherical interpolation (Shoemake, 1985) (SLERP) that maintains similar\n1\nPublished as a conference paper at ICLR 2025\nnorms as the endpoints. Motivated by poor performance of interpolation between the latent vectors\nprovided by inverting natural images, alternatives to SLERP have recently been proposed (Samuel\net al., 2023; Zheng et al., 2024). However, these procedures are costly, have parameters to tune, and\n— like SLERP — are challenging to generalise to other types of manipulations beyond interpolation,\nsuch as constructing the subspaces needed for latent space optimisation methods (G´omez-Bombarelli\net al., 2018), or adapting to more diverse latent distributions beyond Gaussian.\nIn this work, we demonstrate that successful generation from latent vectors is strongly dependent on\nwhether their broader statistical characteristics match those of samples — a stronger condition than\njust having e.g. likely norms. We show that effective manipulation of latent spaces can be achieved\nby following the simple guiding principle of adhering to the modelling assumptions of the generation\nprocess. Our primary contributions are as follows:\n• We show that even if a latent leads to a valid object upon generation, it can fail to provide\neffective interpolation if it lacks the typical characteristics of samples from the latent\ndistribution — as diagnosed by statistical distribution tests.\n• We introduce Latent Optimal Linear combinations (LOL) — an easy-to-implement method\nfor ensuring that interpolation intermediates continue to match the latent distribution.\n• We show that LOL — a closed-form transform that makes no assumptions about the\nnetwork structure or data modality — constitutes a Monge optimal transport map for linear\ncombinations to a general class of latent distributions.\n• We demonstrate that LOL can be applied to define general linear combinations beyond\ninterpolations, including centroids (Figure 2) and, in particular, to define meaningful low-\ndimensional latent representations (Figure 1) — a goal achieved previously with significant\nexpense and only for specific architectures.\nImplementation and examples: https://github.com/bodin-e/linear-combinations-of-latents\nx1\nx2\nx3\nx4\nx5\nFigure 1: Low-dimensional latent subspaces. A 5-dimensional subspace from the flow matching\nmodel Stable Diffusion 3 (Esser et al., 2024) extracted using LOL (left) from the latents corresponding\nto images x1, . . . , x5. The left plot show generations from uniform grid points across an axis-aligned\nslice of the subspace coordinate system, centered around the coordinate for x1. Each coordinate in\nthe subspace correspond to a linear combination of latents, which define basis vectors. The right plot\nshows the corresponding subspace without the proposed LOL transformation. See Figure 6, Figure 7\nand Section G in the appendix for additional examples.\n2\nBACKGROUND\n2.1\nGENERATIVE MODELLING WITH LATENT VARIABLES\nThe methodology in this paper applies to any generative model that transforms samples from a known\ndistribution in order to model another distribution — typically, a complicated distribution of high-\n2\nPublished as a conference paper at ICLR 2025\nx1\nx2\nx3\nEuclidean\nS. Euclidean\nM. n. Euclidean\nNAO\nLOL\nFigure 2: Centroid determination. Generation using Stable Diffusion 2.1 (Rombach et al., 2022)\nfrom the centroid of the latents corresponding to images x1, x2, x3 using different methods. Note\nthat our proposed method removes several artifacts, such as unrealistic headlights and chassi texture.\ndimensional objects. For clarity of exposition, we focus on popular Diffusion and Flow Matching\nmodels using Gaussian latents, and then extend to general latent distributions in the appendix.\nDiffusion models learn a process that reverses the effect of gradually adding noise to training data (Ho\net al., 2020). The noise level is governed by a schedule designed so that the noised samples at the final\ntime index T follow the latent distribution x(T ) ∼p(x). To sample from the diffusion model, one\nstarts by drawing a sample from the latent distribution before iteratively evaluating the reverse process\nwith learnt parameters θ, denoising step-by-step, generating a sequence {x(T ), x(T −1), . . . , x(0)}\nx(t−1) ∼pθ(x(t−1) | x(t)),\nfinally producing a generated object x(0). Diffusion has also been extended to the continuous time\nsetting by Song et al. (2020b), where the diffusion is expressed as a stochastic differential equation.\nNote that, by using the Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020a) or the\nprobability flow formulation in Song et al. (2020b), the generation process can be made deterministic,\ni.e. the latent representation x(T ) ∼p(x) completely specifies the generated object x(0).\nFlow Matching Lipman et al. (2022) is an efficient approach to train Continuous Normalising Flows\n(CNF) Chen et al. (2018) — an alternative class of generative models that builds complicated distribu-\ntions from simple latent distributions using differential equations. CNFs model the evolution of data\npoints over continuous time using an ordinary differential equation (ODE) parameterized by a neural\nnetwork, providing a deterministic relationship between latent and object space. Mathematically,\nthe transformation of a latent sample x(T ) ∼p(x) at time t = T to x(t) at time t is governed\nby f(x(t), t; θ), where f(·; θ) is a neural network with parameters θ. Flow matching allows the\ntransformation dynamics of the CNF to be learnt without needing to simulate the entire forward\nprocess during training, which improves stability and scalability.\nNote that both diffusion and flow matching models can generate in a deterministic manner, where\nthe realisation of the latent variable completely specifies the generated data object, e.g. the image.\nMoreover, by running their deterministic generation formulation in reverse, one can obtain the\ncorresponding latent representation associated with a known object, referred to as “inversion”. In\nthis paper we focus on the effect of properties of the latent vectors, and their manipulation, on the\ngeneration process. For simplicity of notation, henceforth we drop the time index and refer to the\nlatent variable x(T ) as x, and use subscript indexing to denote realisations of the latent variable.\n2.2\nINTERPOLATION OF LATENTS\nLinear Interpolation. The most common latent space manipulation is interpolation, where we are\ngiven two latent vectors x1 and x2, referred to as seed latents (or seeds), and obtain intermediate\nlatent vectors. The simplest approach is to interpolate linearly between the seeds to get intermediates\nyw\nlin = wx1 + (1 −w)x2\nfor\nw ∈[0, 1].\nSpherical Interpolation. However, as discussed in (White, 2016) in the context of Variational\nAutoencoders (Kingma, 2013) and Generative Adversarial Networks (Goodfellow et al., 2014), linear\ninterpolation yields intermediates with unlikely norms for Gaussian samples and results in highly\nimplausible generated objects (e.g. all-green images). Consequently, it is common to instead use\n3\nPublished as a conference paper at ICLR 2025\nspherical interpolation (SLERP) (Shoemake, 1985), which instead builds interpolants via\nyw\nSLERP = sin wθ\nsin θ x1 + sin((1 −w)θ)\nsin θ\nx2\nfor\nw ∈[0, 1]\ncos θ =\n⟨x1, x2⟩\n||x1||||x2||,\nto maintain similar norms for the intermediates as the endpoints. SLERP has become popular and is\nthe standard method for interpolation also in the more recent models (Song et al., 2020a;b).\nNorm-aware Optimisation. Motivated by poor performance of SLERP when interpolating between\nthe latent vectors corresponding to inverted natural images (rather than using those sampled directly\nfrom the model’s latent distribution), Zheng et al. (2024) propose to add additional Gaussian noise\nto interpolants via the inversion procedure to control a trade-off between generation quality and\nadherence to the seed images (i.e. the images corresponding to the latents at the interpolation end-\npoints). Alternatively, Samuel et al. (2023) advocate for Norm-Aware Optimisation (NAO) which\nuses back-propagation to identify interpolation paths γ : [0, 1] →Rd that solve\ninf\nγ −\nZ\nlog P(γ(s))ds\ns.t.\nγ(0) = x1, γ(1) = x2,\nwhere log P : Rd →R is the log likelihood of the squared norm under its sampling distribution\nχ2(D) for unit Gaussian samples. NAO can also calculate centroids of K latents by simultaneous\noptimisation of paths between the centroid and each respective latent.\nAll the proposed interpolation methods above aim to make the intermediates adhere to statistics of\nGaussian samples, however, require significant computation, have hyperparameters to tune, or lack\ntheoretical guarantees. Moreover — in common with spherical interpolation — they are difficult to\ngeneralise beyond Gaussian latent variables or to more general expressions of the latents beyond\ninterpolation, such as building subspaces based on their span — as required to build expressive\nlow-dimensional representations.\nIn this work we will propose a simple technique that guarantees that interpolations follow the latent\ndistribution if the original (seed) latents do, applies to a general class of latent distributions, and\nenables us go beyond interpolation. Moreover, we will address how to assess the distributional\nassumption of the seed latents, to simplify the diagnosis of errors at the source.\nSD2.1 (diffusion)\nSD3 (flow matching)\nx(i) ∼N (µ, Σ)\n1\nargmax p(|| · ||)\n2\nargmax p(|| · ||)\n3\nN (x∗; µ, Σ)\nx(i) ∼N (µ, Σ)\n1\nargmax p(|| · ||)\n2\nargmax p(|| · ||)\n3\nN (x∗; µ, Σ)\nFigure 3: Likelihood and norm insufficient. Column (1) of each panel shows an image generated\nusing a random sample from the associated Gaussian latent distribution for the diffusion model\nof Rombach et al. (2022) (left side) and the flow matching model of Esser et al. (2024) (right side).\nColumns (2) and (3) both show images generated from latents with the most likely norm according\nto their respective latent distribution. Columns (2) use the same Gaussian samples as in columns\n(1) but rescaled to have this norm, also yielding realistic images. Meanwhile, columns (3) show\nthe failed generation from constant vectors sI scaled to have the most likely norm according to the\nlatent distribution but lacking other characteristics (e.g. not having all-equal values) that the network\nwas trained to expect, even though its likelihood N(sI; µ, Σ) is typical of real samples. Moreover,\nthe distribution mode µ, which also lacks needed characteristics , has vastly higher log likelihood\nthan any realistic sample; -33875 and -135503 for the two models, respectively. See Table 2 for an\nexample of failed generation using the mode µ.\n3\nASSESSING VALIDITY OF LATENTS VIA DISTRIBUTION TESTING\nBefore introducing our proposed method, we first explore evidence for our central hypothesis that\ntackles a misconception underpinning current interpolation methods for Gaussian latents:\nHaving a latent vector with a norm that is likely for a sample from the latent distribution N(µ, Σ)\nis not a sufficient condition for plausible sample generation. Rather, plausible generation requires\n4\nPublished as a conference paper at ICLR 2025\na latent vector with characteristics that match those of a sample from N(µ, Σ) more generally (as\nevaluated by normality tests), with a likely norm being only one such characteristic.\nThe characteristics of the random variable x may be described by a collection of statistics, e.g. its\nmean, variance or norm. Above, we hypothesise that if a specified latent x∗has an extremely unlikely\nvalue for a characteristic for which the network has come to rely, then implausible generation will\nfollow. A norm is unlikely if it has a low likelihood under its sampling distribution; for unit Gaussians\nit is χ(D), see Section 1. While the norm has been shown to often be an important statistic (Samuel\net al., 2023), our Figure 3 illustrates on a popular diffusion model (Rombach et al., 2022) and a flow\nmatching model (Esser et al., 2024) that a latent with a likely norm — even the most likely — can\nstill induce failure in the generative process if other evidently critical characteristics are unmet.\nInversion budget\nOriginal\n1 step\n2 steps\n4 steps\n8 steps\n16 steps\n32 steps\n64 steps\n100 steps\n200 steps\n500 steps\n999 steps\nImage\nRec. error:\np-value\n0.340\n0\n0.390\n0\n0.184\n1.2e−58\n0.230\n2.5e−15\n0.0539\n2.3e−5\n0.0295\n7.4e−3\n0.0225\n1.9e−2\n0.0185\n8.4e−2\n0.0181\n1.2e−1\n0.0199\n1.4e−1\n0.0178\n1.6e−1\nFigure 4: Normality testing of latent vectors obtained from inversion. We show the LPIPS (Zhang\net al., 2018) reconstruction errors (second row, in red) of 200 inverted randomly selected images\nacross 50 random classes from ImageNet1k (Deng et al., 2009), the p-values of their inversions (third\nrow), and rejection rates (bottom row) of the Kolmogorov-Smirnov normality test applied to the\ncorresponding latent obtained from inversion under various step budgets. We use the diffusion model\nof Rombach et al. (2022), always using its maximum number of steps (999) for generation, and\ndenote the 10th, 50th and 90th percentiles with black lines. The first row shows image reconstructions\nusing its inversion at each budget, highlighted in red when the latent was rejected (p < 1e−3), with\nthe interpretation that the characteristics of the latent were unlikely for a real sample according to\nthe KS test. We note the strong correlation between inversion budgets providing low reconstruction\nerrors and those for which the p-values of the latents are realistic — taking values likely to occur\nby chance for real samples. However, as we will see in Figure 5, there are still many latents with\nlow reconstruction error yet extremely low p-value, and this often severely affects the quality of its\ninterpolants.\nIn general, neural networks have many degrees of freedom, and so it is difficult to determine\nand list the input characteristics that a specific model relies on. Therefore, rather than trying to\ndetermine all necessary statistics for each model, we propose instead to rely on standard methods\nfor distribution testing, a classic area of statistics (Razali et al., 2011; Kolmogorov, 1933; Shapiro\n& Wilk, 1965) — here exploring the (null) hypothesis that a latent vector x∗∈RD is drawn from\nN(µ, Σ). Popular normality tests consider broad statistics associated with normality. For example,\nthe Kolmogorov–Smirnov (Kolmogorov, 1933) test considers the distribution of the largest absolute\ndifference between the empirical and the theoretical cumulative density function across all values.\n5\nPublished as a conference paper at ICLR 2025\nSeeds\nInterpolations\nVisual quality ≈1\nVisual quality ≈2\nVisual quality ≈3\nVisual quality ≈4\nVisual quality ≈5\nFigure 5: Lack of normality is linked to failure of interpolants The left and middle panels shows\nLPIPS (Zhang et al., 2018) reconstruction error (after 999 generation steps) and the Kolmogorov-\nSmirnov p-value for all inversions presented in Figure 4, split into two plots due to the vast dynamic\nrange of p-values. Although latents with high (realistic) p-values tend to have low reconstruction\nerrors, there are many latents with low reconstruction errors that also have low p-values. The right\npanel shows Q-Align visual quality scores (Wu et al., 2023) for spherical interpolants between pairs\nof inversions selected from matching ImageNet1k (Deng et al., 2009) image classes, demonstrating\nthat choosing seed latents with both low reconstruction error (< 0.05) and high p-values (> 1e−3)\nallows us to avoid low-quality interpolants that would arise when choosing seeds by reconstruction\nerror alone. For reference, we include examples of interpolants at each visual quality level.\n3.1\nTHE SAMPLE CHARACTERISTICS OF SEED LATENTS AFFECT THEIR INTERPOLATIONS\nAn ability to identify latents that lack the necessary characteristics for good quality generation\nis critical when manipulating latents that are not acquired by explicitly sampling from the latent\ndistribution. Indeed, as we show below, using such deficient latents as seeds for interpolation can\nseverely affect the quality of interpolants, for example when interpolating between inverted natural\nimages (the most common way to obtain a seed latent, see Section 2). As inversion is subject to\na finite computation budget (i.e. finite inversion steps), numerical imprecision, and because the\nparticular data instance may not be drawn from the distribution of the model’s training data, the\nresulting latent vector x∗may not correspond to a sample from the latent distribution. We will now\nshow that distribution testing is helpful for identifying such deficient latents, providing an opportunity\nto apply corrections or review the examples or inversion setup, before performing interpolation.\nFigure 4 demonstrates that inversions having realistic p-values are strongly correlated with them\nreproducing their original image with good quality. The p-value indicates the probability of observing\nthe latent vector by chance given that it is drawn from the latent distribution N(µ, Σ). If this value\nis extremely low that is a strong indicator that the latent lacks characteristics expected of samples\nfrom N(µ, Σ). In the figure we see that at low inversion budgets (with high reconstruction errors)\nmost p-values are 1e−50 or lower, and then reach values you expect to see by chance for real samples\n(around 1e−3) at budgets where the reconstruction errors tend to be small. However, we will show\nthat the p-value provides additional information about the quality of the latents than provided by\nreconstruction error alone.\nWe now examine how the p-values of two inversions relate to our ability to interpolate them with good\nquality. Figure 5 shows that although latents with realistic p-values tend to have low reconstruction\nerrors, there are many latents with low reconstruction errors that have low p-values. Therefore, just\nbecause the process of going from object to latent and back reproduces the object, this does not\nnecessarily mean that the resulting latent is well characterised as a sample from N(µ, Σ). The lack\nof such characteristics can be inherited when manipulating the latent; in Appendix 8 we show that\ninterpolants typically inherit low p-values from their seed latents. Figure 5 also demonstrates that if\n6\nPublished as a conference paper at ICLR 2025\nwe beyond requiring inversions to reconstruct their original objects also require their p-values to be\nrealistic — at levels expected of real samples — we are able to avoid many low quality interpolants\ndownstream, by not using the rejected latents as seeds. In other words, the normality test helps us\ndiscover when the latents acquired from inversion lack characteristics of real samples, as this lack\nprevents them from being reliably used as seeds. Distribution testing the latents can be helpful for\ndebugging the inversion setup — which typically involves solving an ODE — and assessing the\nsettings used. In Appendix E we investigate the effectiveness of a range of other classic normality\ntests and tests based on the likelihood N(x∗; µ, Σ) as well as the likelihood of the norm statistic.\nWe assess these methods across a suite of test cases including vectors with unlikely characteristics,\nand for which we know, through failed generation, violate the assumptions of the generative model.\n4\nLINEAR COMBINATIONS OF LATENTS\nNow, equipped with the knowledge that matching broad characteristics of a sample from the latent\ndistribution is critical, we propose a simple scheme for forming linear combinations of seed latents\n— which we now will assume are sampled from the latent distribution — to maintain their sample\ncharacteristics. We will focus on Gaussian latents in this section, and present an extension to general\nlatent distributions in Appendix A. In this section we change the notation slightly, where a seed\nlatent xk ∈RD rather than being a realisation of a random variable — a vector of known values — it\nis a random variable following the latent distribution (here, N(µ, Σ)). We assume access to K such\nseed latent variables {xk}K\nk=1 and attempt to form new variables following the same distribution.\nLet y be a linear combination of the K Gaussian latent variables xk ∼N(µ, Σ)\ny :=\nK\nX\nk=1\nwkxk = wT X,\n(1)\nwhere wk ∈R, w = [w1, w2, . . . , wK] and X = [x1, x2, . . . , xK]. Then we have that y is also a\nGaussian random variable, with mean and covariance\ny ∼N(αµ, βΣ)\nα =\nK\nX\nk=1\nwk\nβ =\nK\nX\nk=1\nw2\nk.\n(2)\nIn other words, y is only distributed as N(µ, Σ) in the specific case where (a) αµ = µ and (b)\nβΣ = Σ — an observation which we now use to explain the empirically observed behaviour of\nexisting interpolation methods. Firstly, for linear interpolation, where w = [v, 1 −v], v ∈[0, 1], (b)\nholds only for the endpoints v = {0, 1}, and so leads to implausible generations for interpolants (as\nwe demonstrate empirically in Figure 11). In contrast, in the popular case of high-dimensional unit\nGaussian latent vectors, spherical interpolants have β ≈1, ∀v ∈[0, 1], as proven in Appendix C,\nand (a) is met as α0 = 0, which is consistent with plausible interpolations (see Figure 11).\nIn this work, we instead propose transforming linear combinations such that α = β = 1, for any\nw ∈RK, thus exactly meeting the criteria for any linear combination and any choice of µ and Σ.\nWe define a transformed random variable z to use as the latent instead of the linear combination y\nz := Tw,µ(y) = Tw,µ(\nK\nX\nk=1\nwkxk),\n(3)\nwhere Tw,µ(y) := (1 −\nα\n√β )µ +\ny\n√β , for which it holds that z ∼N(µ, Σ) given latent variables\nxk ∼N(µ, Σ). Here, Tw,µ : RD →RD is the map that transforms samples from the distribution of\na linear combination of N(µ, Σ)-variables with weights w into N(µ, Σ). In Appendix B we show\nthat this transport map is Monge optimal, and extend it to general distributions in Appendix A. The\nweights w, which via α and β together with the set of K seed latents specify the transformed linear\ncombination z, depend on the operation, represented as particular linear combinations. Below are a\nfew examples of popular operations (linear combinations) to form y; these are used as above to, for\nthe corresponding weights w, obtain z following the distribution expected by the generative model.\n• Interpolation: y = wT X, where X = [x1, x2], w = [w1, 1 −w1] and w1 ∈[0, 1].\n7\nPublished as a conference paper at ICLR 2025\n• Centroid Determination: y = wT X, where X = [x1, . . . , xK], w = [ 1\nK ]K.\n• Subspaces: Suppose we wish to build a navigable subspace spanned by linear combinations\nof K latent variables. By performing the QR decomposition of X := [x1, x2, . . . , xK] ∈\nRD×K to produce a semi-orthonormal matrix U ∈RD×K (as the Q-matrix), we can\nthen define a subspace projection of any new x into the desired subspace via s(x) :=\nUU T x = Uh ∈RD. The weights w for a given point in the subspace s(x) are given\nby w = X†s(x) = X†Uh ∈RK where X† is the Moore–Penrose inverse of X. See\nthe derivation of the weights and proof in Appendix D. One can directly pick coordinates\nh ∈RK, compute the subspace projection y = Uh, and then subsequently use the transport\nmap (defined by Equation 3) to the latent space to obtain z. In Figure 1 we use grids in RK\nto set h, used as above together with a basis (defined by U) from a set of latents.\n5\nEXPERIMENTS\nWe now assess our proposed transformation scheme LOL experimentally. To verify that LOL matches\nor exceeds current methods for Gaussian latents for currently available operations — interpolation\nand centroid determination — we perform qualitative and quantitative comparisons to their respective\nbaselines. We then demonstrate new capabilities with several examples of low-dimensional subspaces\non popular diffusion models and a popular flow matching model.\n5.1\nINTERPOLATION AND CENTROID DETERMINATION\nFor the application of interpolation, we compare our proposed LOL to linear interpolation (LERP),\nspherical linear interpolation (SLERP), and Norm-Aware Optimization (NAO) Samuel et al. (2023).\nIn contrast to the all the other considered\ninterpolation methods (including LOL)\nwhich only involve closed-form expres-\nsions, NAO requires numerical optimisa-\ntion.\nWe closely follow the evaluation\nprotocol in Samuel et al. (2023), basing\nthe experiments on Stable Diffusion (SD)\n2.1 (Rombach et al., 2022) and inversions\nof random images from 50 random classes\nfrom ImageNet1k (Deng et al., 2009), and\nassess visual quality and preservation of se-\nmantics using Fr´echet Inception Distance\n(FID) (Heusel et al., 2017) and class pre-\ndiction accuracy, respectively. For the in-\nterpolation we (randomly without replace-\nment) pair the 50 images per class into 25\npairs, forming 1250 image pairs in total.\nInterpolation\nMethod\nAccuracy\nFID ↓\nTime\nLERP\n3.92%\n199\n6e−3s\nSLERP\n64.6%\n42.6\n9e−3s\nNAO\n62.1%\n46.0\n30s\nLOL (ours)\n67.4%\n38.9\n6e−3s\nCentroid determination\nMethod\nAccuracy\nFID ↓\nTime\nEuclidean\n0.286%\n310\n4e−4s\nStandardised Euclidean\n44.6%\n88.8\n1e−3s\nMode norm Euclidean\n44.6%\n88.4\n1e−3s\nNAO\n44.0%\n93.0\n90s\nLOL (ours)\n46.3%\n87.7\n6e−4s\nTable 1: Quantitative comparisons of baselines.\nFor the centroid determination we compare to NAO, the Euclidean centroid ¯x =\n1\nK\nPK\nk=1 xk and\ntwo transformations thereof; “standardised Euclidean”, where ¯x is subsequently standardised to have\nmean zero and unit variance (as SD 2.1 assumes), and “mode norm Euclidean”, where ¯x is rescaled\nfor its norm to equal the maximum likelihood norm\n√\nD. For each class, we form 10 3-groups, 10\n5-groups, 4 10-groups and 1 25-group, sampled without replacement per group, for a total of 1250\ncentroids per method. For more details on the experiment setup and settings, see Appendix F.\nIn Table 1 we show that our method outperforms or maintains the performance of the baselines\nin terms of FID distance and accuracy, as calculated using a pre-trained classifier following the\nevaluation methodology of Samuel et al. (2023). For an illustration of centroids and interpolations,\nsee Figure 2 and Figure 16, respectively. The evaluation time of an interpolation path and centroid,\nshown with one digit of precision, illustrate that the closed-form expressions are significantly faster\nthan NAO. Surprisingly, NAO did not perform as well as spherical interpolation and several other\nbaselines, despite using their implementation, which was outperforming these methods in Samuel\net al. (2023). We note one discrepancy is that we report FID distances using (2048) high-level features,\nwhile in their work they are using (64) low-level features, which in Seitzer (2020) is recommended\n8\nPublished as a conference paper at ICLR 2025\nagainst as it does not necessarily correlate with visual quality. In the appendix we include FID\ndistances using all settings of features. We note that, in our setup, the baselines perform substantially\nbetter than reported in Samuel et al. (2023) — including NAO in terms of FID distances using the\n64 low-level feature setting (1.30 in our setup vs 6.78 in their setup), and class accuracy during\ninterpolation (62% vs 52%). See Section H in the appendix for more details and ablations.\nx1\nx2\nx3\nx4\nx5\nFigure 6: Low-dimensional subspaces. The latents x1, . . . , x5 (corresponding to images) are\nconverted into basis vectors and used to define a 5-dimensional subspace. The grids show generations\nfrom the flow matching model Stable Diffusion 3 (Esser et al., 2024) over uniform grid points in the\nsubspace coordinate system, where the left and right grids are for the dimensions {1, 2} and {3, 4},\nrespectively, centered around the coordinate for x1. Each coordinate in the subspace correspond to a\nlinear combination of the basis vectors, which through LOL all yield high-quality generations.\n5.2\nLOW-DIMENSIONAL SUBSPACES\nx1\nx2\nx3\nFigure 7: Model-agnostic subspace definitions. The latents x1, x2 and x3 — which via the\ngenerative model correspond to 3D designs — are converted into basis vectors and used to define a\nthree-dimensional subspace. The left plot shows the generated designs corresponding to a ten-by-ten\ngrid in a two-dimensional slice of the subspace, shown over a region. The right plot shows the\nwavedrag, as evaluated by simulation in OpenVSP (McDonald & Gloudemans, 2022) for the designs\nover a 100-by-100 grid in the same region, with the respective design coordinates (from the left plot)\nshown as black dots. We trained the SLIDE (Lyu et al., 2023) diffusion model on ShapeNet (Chang\net al., 2015). LOL allows subspaces to be defined without any model-specific treatment.\nIn Figure 1 and Figure 6 we illustrate slices of a 5-dimensional subspace of the latent space of a flow\nmatching model, indexing high-dimensional images of sports cars and rocking chairs, respectively.\n9\nPublished as a conference paper at ICLR 2025\nThe subspaces here are defined using five images (one per desired dimension), formed using our LOL\nmethod described in Section 4 to transform the linear combinations of the corresponding projections.\nOur approach is dimensionality and model-agnostic, which we illustrate in Figure 7, where the\nsame procedure is used on a completely different model without adaptations — where we define\na three-dimensional subspace based on three designs in a point cloud diffusion model with mesh\nreconstruction. We evaluate designs in this subspace using a simple computational fluid dynamics\nsimulation to illustrate that our approach allows objective functions to be defined over the space, in\nturn allowing off-the-shelf optimisation methods to be applied to search for designs. In Figure 9\nin the appendix we show corresponding grids to the sports cars in Figure 1 without the proposed\nLOL transformation, which either leads to implausible images or the same image for each coordinate,\ndepending on the model. In the appendix (Section G) we also include more slices and examples;\nincluding subspaces using the diffusion model Stable Diffusion 2.1 (Rombach et al., 2022).\n6\nRELATED WORK\nGenerative models with non-Gaussian priors. In Fadel et al. (2021) and Davidson et al. (2018),\nin the context of normalizing flows and VAEs, respectively, Dirichlet and von Mises-Fisher prior\ndistributions are explored with the goal of improving interpolation performance. However, these\nmethods require training the model with the new latent distribution, which is impractical and untested\nfor the large pretrained models we consider.\nConditional Diffusion. An additional way to control the generation process of a diffusion model is to\nguide the generative process with additional information, such as text and labels, to produce outputs\nthat meet specific requirements, such as where samples are guided towards a desired class (Dhariwal\n& Nichol, 2021; Ho & Salimans, 2022), or to align outputs with text descriptions (Radford et al.,\n2021). Conditioning is complementary to latent space manipulation. For example, when making\nFigure 1 we used conditioning (a prompt) to constrain the generation to sports cars, however, the\nvariation of images fulfilling this constraint is encoded in the latent space.\nLow-dimensional representations.\nWe have shown that LOL can provide expressive low-\ndimensional representations of latent spaces of generative models. To the best of our knowledge, the\nmost closely related line of work was initiated in Kwon et al. (2022), where it was shown that semantic\nedit directions can recovered from activations of a UNet (Ronneberger et al., 2015) denoiser network\nduring generation. Using a pretrained CLIP (Contrastive Language-Image Pre-Training) (Radford\net al., 2021) model to define directions within the inner-most feature map of the UNet architecture,\nnamed h-space, they show that some semantic edits, such as adding glasses to an image of a per-\nson, correspond to linear edits in the h-space. More recently, Haas et al. (2024) demonstrate that\nh-space edit directions can also be found through Principle Component Analysis, and Park et al.\n(2023) propose a pullback metric that transfers edits in the h-space into the original latent space.\nHowever, while these three approaches demonstrate impressive editing capabilities on images, they\nall require a modification to the generative process and are limited to diffusion models built with\nUNet architectures. In our work low-dimensional representations are instead formed in closed-form\nas linear subspaces based on a (free) choice of latent basis vectors. These basis vectors could be\nobtained through various methodologies, including via the pull-back metric in Park et al. (2023),\nwhich may be interesting to explore in future work.\n7\nCONCLUSION\nIn this paper we propose LOL, a general and simple scheme to define linear combinations of latents\nthat maintain a prespecified latent distribution and demonstrate its effectiveness for interpolation\nand defining subspaces within the latent spaces of generative models. Of course, these linear\ncombinations only follow the desired distribution if the original (seed) latents do. Therefore, we\npropose the adoption of distribution tests to assess the validity of latents obtained from e.g. inversions.\nStill, the existing methods for distribution testing may not align perfectly with a given network’s\nexpectations — e.g. many tests may be stricter than necessary for the network — and it may be\nan interesting direction of future work to develop tailored methods, along with more extensive\ncomparisons of existing tests in the context of generative models.\n10\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGMENTS\nWe thank Samuel Willis and Daattavya Aggarwal (University of Cambridge) for their valuable\ncontributions. Samuel contributed important ideas related to computational fluid dynamics simulation\nand the SLIDE model, and constructed the setup and plots used in Figure 7. Daattavya assisted in\nverifying proofs and offered comments that greatly improved the manuscript.\nREFERENCES\nRobert B Ash. Information theory. Courier Corporation, 2012.\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\nSilvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\nmodel repository. arXiv preprint arXiv:1512.03012, 2015.\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\ndifferential equations. Advances in neural information processing systems, 31, 2018.\nHarald Cram´er. On the composition of elementary errors: First paper: Mathematical deductions.\nScandinavian Actuarial Journal, 1928(1):13–74, 1928.\nTim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspherical\nvariational auto-encoders. In 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI\n2018, pp. 856–865. Association For Uncertainty in Artificial Intelligence (AUAI), 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780–8794, 2021.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In Forty-first International Conference on Machine Learning,\n2024.\nSamuel G Fadel, Sebastian Mair, Ricardo da S. Torres, and Ulf Brefeld. Principled interpolation in\nnormalizing flows. In Machine Learning and Knowledge Discovery in Databases. Research Track:\nEuropean Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings,\nPart II 21, pp. 116–131. Springer, 2021.\nRafael G´omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos´e Miguel Hern´andez-Lobato,\nBenjam´ın S´anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,\nRyan P Adams, and Al´an Aspuru-Guzik. Automatic chemical design using a data-driven continuous\nrepresentation of molecules. ACS central science, 4(2):268–276, 2018.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.\nImproved training of wasserstein gans. Advances in neural information processing systems, 30,\n2017.\nRen´e Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Stella Graßhof, Sami S Brandt, and\nTomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion\nmodels. In 2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition\n(FG), pp. 1–9. IEEE, 2024.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural\ninformation processing systems, 30, 2017.\n11\nPublished as a conference paper at ICLR 2025\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840–6851, 2020.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633–8646,\n2022.\nDiederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nAn Kolmogorov. Sulla determinazione empirica di una legge didistribuzione. Giorn Dell’inst Ital\nDegli Att, 4:89–91, 1933.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\ndiffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020.\nMingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent\nspace. arXiv preprint arXiv:2210.10960, 2022.\nRichard J Larsen and Morris L Marx. An introduction to mathematical statistics. Prentice Hall\nHoboken, NJ, 2005.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching\nfor generative modeling. arXiv preprint arXiv:2210.02747, 2022.\nShitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2837–2845, 2021.\nZhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua Lin, and Bo Dai. Controllable mesh genera-\ntion through sparse latent point diffusion models. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 271–280, 2023.\nRobert A McDonald and James R Gloudemans. Open vehicle sketch pad: An open source parametric\ngeometry and analysis tool for conceptual aircraft design. In AIAA SciTech 2022 Forum, pp. 0004,\n2022.\nEric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-\ndistribution inputs to deep generative models using typicality. arXiv preprint arXiv:1906.02994,\n2019.\nYong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the\nlatent space of diffusion models through the lens of riemannian geometry. Advances in Neural\nInformation Processing Systems, 36:24129–24142, 2023.\nKarl Pearson. X. on the criterion that a given system of deviations from the probable in the case of\na correlated system of variables is such that it can be reasonably supposed to have arisen from\nrandom sampling. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of\nScience, 50(302):157–175, 1900.\nGabriel Peyr´e and Marco Cuturi. Computational optimal transport, 2020. URL https://arxiv.\norg/abs/1803.00567.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nNornadiah Mohd Razali, Yap Bee Wah, et al. Power comparisons of shapiro-wilk, kolmogorov-\nsmirnov, lilliefors and anderson-darling tests. Journal of statistical modeling and analytics, 2(1):\n21–33, 2011.\n12\nPublished as a conference paper at ICLR 2025\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684–10695, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical image computing and computer-assisted intervention–MICCAI\n2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III\n18, pp. 234–241. Springer, 2015.\nDvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, and Gal Chechik. Norm-guided latent space\nexploration for text-to-image generation. Advances in Neural Information Processing Systems, 36,\n2023.\nMaximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/\npytorch-fid, August 2020. Version 0.3.0.\nSamuel Sanford Shapiro and Martin B Wilk. An analysis of variance test for normality (complete\nsamples). Biometrika, 52(3-4):591–611, 1965.\nKen Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual\nconference on Computer graphics and interactive techniques, pp. 245–254, 1985.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020a.\nYang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020b.\nMichel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces.\nPublications Math´ematiques de l’Institut des Hautes Etudes Scientifiques, 81:73–205, 1995.\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao\nLi. Maxvit: Multi-axis vision transformer. In European conference on computer vision, pp.\n459–479. Springer, 2022.\nC´edric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.\nTom White. Sampling generative networks. arXiv preprint arXiv:1609.04468, 2016.\nHaoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao,\nAnnan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via\ndiscrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 586–595, 2018.\nPengFei Zheng, Yonggang Zhang, Zhen Fang, Tongliang Liu, Defu Lian, and Bo Han. Noisediffusion:\nCorrecting noise for image interpolation with diffusion models beyond spherical linear interpolation.\narXiv preprint arXiv:2403.08840, 2024.\nA\nOPTIMAL TRANSPORT MAP FOR GENERAL LATENT LINEAR COMBINATIONS\nWITH INDEPENDENT ELEMENTS\nIn Section 4 we introduced LOL for Gaussian latent variables, the most commonly used latent\ndistribution in modern generative models. We now extend LOL to general latent distributions with\nindependent real-valued elements, provided the cumulative distribution function (CDF) and its inverse\nis known for each respective element.\nLet p(x) be a distribution with independent components across elements, such that p(x) =\nQD\nd=1 pd(x(d)), where pd is the distribution of element x(d) ∈R with CDF Φd and inverse CDF Φ−1\nd .\n13\nPublished as a conference paper at ICLR 2025\nGiven linear combination weights {wk}K\nk=1, wk ∈R, and seeds {xk}K\nk=1, xk ∼p, we obtain z ∼p\nvia the transport map T{w} := {T (d)\n{w}}D\nd=1 defined per latent element z(d) as:\nz(d) := T (d)\n{w}({x(d)\nk }) = Φ−1\nd (Φϵ(ϵ(d)))\nϵ(d) = Tw,0 (PK\nk=1 wkϵ(d)\nk )\nϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )), (4)\nwhere Φϵ is the CDF of the standard Gaussian, and Tw,0 : R →R is the transport map for standard\nGaussians as defined in Equation 3. We will now show that the proposed map leads to the target\ndistribution p and then show that this map is Monge optimal, leveraging results from Section B.\nLemma 1. Let z be a transformed variable defined through the transport map T{w} applied to a\nlinear combination of independent latent variables xk ∼p(x), such that\nz(d) = T (d)\n{w}({x(d)\nk }) = Φ−1\nd (Φϵ(Tw,0(\nK\nX\nk=1\nwkϵ(d)\nk )))\nwhere\nϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )).\nThen, z ∼p(x), meaning that the transformed variable follows the target latent distribution.\nProof. We prove that z ∼p(x) by verifying that each element z(d) follows the corresponding\nmarginal distribution pd.\nSince x(d)\nk\n∼pd, its cumulative distribution function (CDF) is given by Φd(x(d)\nk ). Defining the\nstandard normal equivalent ϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )), we know that ϵ(d)\nk\n∼N(0, 1) because the\ntransformation via Φ−1\nϵ\npreserves uniformity.\nNow, consider the weighted sum:\nϵ(d) =\nK\nX\nk=1\nwkϵ(d)\nk .\nSince the ϵ(d)\nk\nare independent standard normal variables, the resulting sum follows:\nϵ(d) ∼N(α · 0, β · 1) = N(0, β),\nwhere α = PK\nk=1 wk and β = PK\nk=1 w2\nk.\nApplying the optimal Gaussian transport map Tw,0 (as proven in Section B) to ϵ(d), we obtain:\nTw,0(ϵ(d)) = ϵ(d)\n√β .\nSince ϵ(d) ∼N(0, β), it follows that:\nϵ(d)\n√β ∼N(0, 1).\nThus, applying the standard normal CDF, we get:\nΦϵ(Tw,0(ϵ(d))) ∼U(0, 1),\nwhich is a uniform distribution.\nFinally, applying the inverse CDF Φ−1\nd\nof the target distribution pd results in:\nz(d) = Φ−1\nd (Φϵ(Tw,0(ϵ(d)))) ∼pd.\nSince this holds for each d, we conclude that z ∼p(x), proving that the transport map correctly\ntransforms the linear combination into the desired latent distribution.\n14\nPublished as a conference paper at ICLR 2025\nLemma 2. The transport map T{w} defined element-wise as\nz(d) = T (d)\n{w}({x(d)\nk }) = Φ−1\nd (Φϵ(Tw,0(\nK\nX\nk=1\nwkϵ(d)\nk )))\nwhere\nϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )),\nis the Monge optimal transport map, minimizing the quadratic Wasserstein distance W2 between the\nsource distribution of the weighted sum of independent elements and the target latent distribution p.\nProof. We want to show that T{w} minimizes the quadratic Wasserstein distance W2 between the\nsource distribution (the weighted sum of independent seeds) and the target distribution.\nStep 1: General Theorem for One-Dimensional Optimal Transport\nFor two probability mea-\nsures P and Q on R with cumulative distribution functions (CDFs) FP and FQ, respectively, the\noptimal transport map in the W2 sense (which minimizes the expected squared Euclidean distance) is\ngiven by the monotone increasing function:\nT ∗= F −1\nQ\n◦FP .\nThis result follows from classical optimal transport theory for univariate distributions (see Remark\n2.30 in Peyr´e & Cuturi (2020)).\nStep 2: Applying to Our Case\nFor each element d, the source distribution is given by the weighted\nsum:\nϵ(d) =\nK\nX\nk=1\nwkϵ(d)\nk .\nEach ϵ(d)\nk\nis obtained by transforming the original variable x(d)\nk\nusing:\nϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )).\nSince the ϵ(d)\nk\nare independent standard normal variables, their weighted sum follows:\nϵ(d) ∼N(0, β),\nwhere\nβ =\nK\nX\nk=1\nw2\nk.\nTo map this distribution to the standard normal N(0, 1), the optimal transport map for Gaussians\n(derived in Section B) is:\nTw,0(ϵ(d)) = ϵ(d)\n√β .\nApplying the standard normal CDF Φϵ to this transformation ensures that:\nΦϵ(Tw,0(ϵ(d))) ∼U(0, 1).\nBy the general optimal transport theorem stated in Step 1, the optimal transport map to the target\ndistribution pd is then:\nz(d) = Φ−1\nd (Φϵ(Tw,0(ϵ(d)))).\n15\nPublished as a conference paper at ICLR 2025\nSince each element d is mapped independently using the element-wise Monge optimal transport map,\nthe full transformation T{w} is optimal in the Monge sense, minimizing the quadratic Wasserstein\ndistance W2.\nThus, T{w} is the Monge optimal transport map from the distribution of the weighted sum of\nindependent elements to the target latent distribution p, completing the proof.\nB\nOPTIMAL TRANSPORT MAP FOR GAUSSIAN LATENT LINEAR COMBINATIONS\nIn Section 4 we introduced LOL, which we use to transform random variables arising from linear com-\nbinations — for example interpolations or subspace projections — to the latent distribution N(µ, Σ)\nvia a transport map Tw,µ defined per set of linear combination weights w = [w1, . . . , wK}, wk ∈R.\nWe will now derive the distribution of a (uncorrected) linear combination y — which we show does\nnot match the latent distribution — followed by the distribution of the transformed variable z, which\nwe show does. We will then show that the proposed transport map Tw,µ is Monge optimal.\nLemma 3. Let y be a linear combination of K i.i.d. random variables xk ∼N(µ, Σ), where y is\ndefined as\ny :=\nK\nX\nk=1\nwkxk = wT X,\nwith wk ∈R, y ∈RD, xk ∈RD, w = [w1, w2, . . . , wK], K ∈N>0 and X = [x1, x2, . . . , xK].\nThen y is a Gaussian random variable with the distribution\ny ∼N(αµ, βΣ),\nwhere\nα =\nK\nX\nk=1\nwk\nand\nβ =\nK\nX\nk=1\nw2\nk.\nProof. Given xk ∼N(µ, Σ), we know that each xk has mean µ and covariance matrix Σ.\nDefine y = wT X = PK\nk=1 wkxk.\nFirst, we calculate the mean of y:\nE[y] = E\n\" K\nX\nk=1\nwkxk\n#\n=\nK\nX\nk=1\nwkE[xk] =\nK\nX\nk=1\nwkµ =\n K\nX\nk=1\nwk\n!\nµ = αµ.\nNext, we calculate the covariance of y:\nCov(y) = Cov\n K\nX\nk=1\nwkxk\n!\n=\nK\nX\nk=1\nw2\nkCov(xk),\nsince the xk are i.i.d. and thus Cov(x(i), x(j)) = 0 for i ̸= j.\nGiven that xk ∼N(µ, Σ), we have Cov(xk) = Σ. Therefore,\nCov(y) =\nK\nX\nk=1\nw2\nkΣ =\n K\nX\nk=1\nw2\nk\n!\nΣ = βΣ.\nHence, y ∼N(αµ, βΣ) with α = PK\nk=1 wk and β = PK\nk=1 w2\nk.\n16\nPublished as a conference paper at ICLR 2025\nLemma 4. Let z be defined as\nz = Tw,µ(y)\nTw,µ(y) = (1 −α\n√β )µ + y\n√β\nα =\nK\nX\nk=1\nwk\nβ =\nK\nX\nk=1\nw2\nk\nwith y ∼N(αµ, βΣ). Then z ∼N(µ, Σ).\nProof. Given y ∼N(αµ, βΣ), we need to show that z ∼N(µ, Σ).\nFirst, we calculate the mean of z:\nE[z] = E [a + By] = a + BE[y] =\n\u0012\n1 −α\n√β\n\u0013\nµ +\n1\n√β αµ = µ,\nwhere\na =\n\u0012\n1 −α\n√β\n\u0013\nµ\nand\nB =\n1\n√β I.\nNext, we calculate the covariance of z:\nCov(z) = Cov (a + By) = BCov(y)BT =\n1\n√β βΣ 1\n√β = Σ.\nSince z has mean µ and covariance Σ, we conclude that\nz ∼N(µ, Σ).\nLemma 5. The transport map Tw,µ defined by\nTw,µ(y) = (1 −α\n√β )µ + y\n√β\nis the Monge optimal transport map, minimizing the quadratic Wasserstein distance W2 between\nN(αµ, βΣ) and N(µ, Σ).\nProof. In optimal transport theory, the Monge problem seeks a transport map T : RD →RD\nthat pushes forward the source distribution N(µy, Σy) to the target distribution N(µz, Σz) while\nminimizing the expected squared Euclidean distance:\nW 2\n2 (N(µy, Σy), N(µz, Σz)) = inf\nT ∈M E[∥T (y) −y∥2].\nThe optimal transport map for Gaussian distributions is given by the affine transformation:\nT (y) = µz + A(y −µy),\nwhere A is defined as\nA := Σ−1/2\ny\n(Σ1/2\ny\nΣzΣ1/2\ny\n)1/2Σ−1/2\ny\n.\nSee Remark 2.31 in Peyr´e & Cuturi (2020).\nSetting the parameters\nµy = αµ,\nΣy = βΣ,\n(5)\nµz = µ,\nΣz = Σ,\n(6)\n17\nPublished as a conference paper at ICLR 2025\nwe compute A:\nA = (βΣ)−1/2((βΣ)1/2Σ(βΣ)1/2)1/2(βΣ)−1/2 =\n1\n√β I.\n(7)\nThus, the optimal transport map simplifies to\nz := T (y) = µ +\n1\n√β (y −αµ) = (1 −α\n√β )µ + y\n√β .\n(8)\nSince this matches exactly with Tw,µ, we conclude that Tw,µ is the Monge optimal transport map\nminimising the quadratic Wasserstein distance.\nC\nSPHERICAL INTERPOLANTS OF HIGH-DIMENSIONAL UNIT GAUSSIAN\nRANDOM VARIABLES ARE APPROXIMATELY UNIT GAUSSIAN\nSpherical linear interpolation (SLERP) (Shoemake, 1985) is defined as\ny = w1x1 + w2x2\n(9)\nwhere\nwi := sin viθ\nsin θ ,\n(10)\nvi ∈[0, 1] and v2 = 1 −v1 and cos θ =\n⟨x1,x2⟩\n||x1||||x2||, where cos θ is typically referred to as the cosine\nsimilarity of x1 and x2.\nAs such, using Equation 2, we obtain\nα =\nK\nX\nk=1\nwk = sin v1θ\nsin θ\n+ sin v2θ\nsin θ\nand\nβ = sin2 v1θ\nsin2 θ\n+ sin2 v2θ\nsin2 θ\n(11)\nAs discussed in Section 4, for a linear combination y to be a random variable following distribution\nN(µ, Σ), given that x1 and x2 do, it must be true that αµ = µ and βΣ = Σ.\nA common case is using unit Gaussian latents (as in e.g. the models Esser et al. (2024) and Rombach\net al. (2022) used in this paper), i.e. where µ = 0, Σ = I. In this case it trivially follows that\nαµ = µ since α0 = 0. We will now show that β ≈1 in this special (i.e. unit Gaussian) case.\nLemma 6. Let\nβ = sin2 vθ\nsin2 θ + sin2(1 −v)θ\nsin2 θ\n,\nwhere cos θ =\n⟨x1,x2⟩\n∥x1∥∥x2∥and x1, x2 ∼N(0, I). Then β ≈1 for large D, ∀v ∈[0, 1].\nProof. Since x1, x2 ∼N(0, I), each component x1j and x2j for j = 1, . . . , D are independent\nstandard normal random variables. The inner product ⟨x1, x2⟩is given by:\n⟨x1, x2⟩=\nD\nX\nj=1\nx1jx2j.\nThe product x1jx2j follows a distribution known as the standard normal product distribution. For\nlarge D, the sum of these products is approximately normal due to the Central Limit Theorem (CLT),\nwith:\n⟨x1, x2⟩∼N(0, D).\n18\nPublished as a conference paper at ICLR 2025\nNext, consider the norms ∥x1∥and ∥x2∥. Each ∥xi∥2 = PD\nj=1 x2\nij is a chi-squared random variable\nwith D degrees of freedom. For large D, by the central limit theorem, ∥xi∥2 ∼N(D, 2D), and\ntherefore ∥xi∥is approximately\n√\nD.\nThus, for large D,\ncos(θ) = ⟨x1, x2⟩\n∥x1∥∥x2∥≈N(0, D)\n√\nD ·\n√\nD\n= N(0, D)\nD\n= N\n\u0012\n0, 1\nD\n\u0013\n.\nThus, θ ≈π/2, which implies sin(θ) ≈1. Therefore:\nβ = sin2(vθ)\nsin2 θ\n+ sin2((1 −v)θ)\nsin2 θ\n≈sin2(vθ) + sin2((1 −v)θ).\nUsing the identity sin2(a) + sin2(b) = 1 −cos2(a −b),\nβ ≈1 −cos2(vθ −(1 −v)θ) = 1 −cos2(θ)\nGiven v ∈[0, 1] and θ ≈π/2 for large D, the argument of cos remains small, leading to cos(·) ≈0.\nHence,\nβ ≈1.\nTherefore, for large D, it follows that β ≈1.\nEmpirical verification\nTo confirm the effectiveness of the approximations used above for values\nof D which are typical for popular generative models that use unit Gaussian latents, we estimate the\nconfidence interval of β using 10k samples of x1 and x2, respectively, where xi ∼N(0, I). For\nStable Diffusion 3 (Esser et al., 2024), a flow matching model with D = 147456, the estimated 99%\nconfidence interval of β is [0.9934, 1.0067] for v = 0.5 where the error is largest. For Stable Diffusion\n2.1 (Rombach et al., 2022), a diffusion model with D = 36864, the corresponding confidence interval\nof β is [0.9868, 1.014].\nD\nLINEAR COMBINATION WEIGHTS OF SUBSPACE PROJECTIONS\nIn Section 4 we introduced LOL, which we use to transform latent variables arising from linear\ncombinations such that they maintain the the latent distribution. This transformation depends on the\nweights w which specify the linear combination. We will now derive the linear combination weights\nfor subspace projections.\nLemma 7. Let U ∈RD×K be a semi-orthonormal matrix. For a given point x ∈RD, the subspace\nprojection is s(x) = UU T x. The weights w ∈RK such that s(x) is a linear combination of\nx1, x2, . . . , xK (columns of A = [x1, x2, . . . , xK] ∈RD×K) can be expressed as w = A†s(x),\nwhere A† is the Moore-Penrose inverse of A.\nProof. The subspace projection s(x) of x ∈RD is defined as:\ns(x) = UU T x.\nWe aim to express s(x) as a linear combination of the columns of A = [x1, x2, . . . , xK] ∈RD×K.\nThat is, we seek w ∈RK such that:\ns(x) = Aw.\nBy definition, the Moore-Penrose inverse A† of A satisfies the following properties:\n1. AA†A = A\n2. A†AA† = A†\n19\nPublished as a conference paper at ICLR 2025\n3. (AA†)T = AA†\n4. (A†A)T = A†A\nSince s(x) is in the subspace spanned by the columns of A, there exists a w such that:\ns(x) = Aw.\nConsider a w′ ∈RK constructed using the Moore-Penrose inverse A†:\nw′ = A†s(x).\nWe now verify that this w′ satisfies the required equation. Substituting back\nAw′ = A(A†s(x))\nand using the property of the Moore-Penrose inverse AA†A = A, we get:\nAA†s(x) = s(x).\nThus:\nAw′ = s(x),\nwhich shows that w′ = A†s(x) is indeed the correct expression for the weights.\nFrom uniqueness of w for a given set of columns x1, x2, . . . , xK (see Lemma 8), we have proven\nthat the weights w for a given point in the subspace s(x) are given by:\nw = A†s(x).\nLemma 8. The weights w ∈RK such that s(x) is a linear combination of x1, x2, . . . , xK (columns\nof A = [x1, x2, . . . , xK] ∈RD×K) are unique.\nProof. Suppose there exist two different weight vectors w1 and w2 such that both satisfy the equation:\ns(x) = Aw1 = Aw2.\nThen, subtracting these two equations gives:\nAw1 −Aw2 = 0.\nThis simplifies to:\nA(w1 −w2) = 0.\nLet v = w1 −w2. Then:\nAv = 0.\nSince v ∈RK, this equation implies that v lies in the null space of A. However, the assumption that\nA has full column rank (since A is used to represent a linear combination for s(x)) implies that A\nhas no non-zero vector in its null space, i.e., Av = 0 only when v = 0.\nTherefore:\nv = 0 =⇒w1 = w2.\nThis shows that the weights w are unique, and there cannot be two distinct sets of weights w1 and\nw2 that satisfy the equation s(x) = Aw.\nHence, we conclude that the weights w such that s(x) is a linear combination of x1, x2, . . . , xK are\nunique.\n20\nPublished as a conference paper at ICLR 2025\nE\nNORMALITY TEST COMPARISON\nWe will now investigate the effectiveness of a range of classic normality tests and tests based on the\nlikelihood N(x∗; µ, Σ) as well as the likelihood of the norm statistic. We assess these methods across\na suite of test cases including for vectors with unlikely characteristics and which we know, through\nfailed generation, violate the assumptions of the model. In order to improve statistical power and\ndetect even small deviations from normality of a single instance x∗∈RD, we transform the single\nhigh-dimensional latent into a large collection of (D) i.i.d. unit Gaussian samples {ϵi}D\ni=1, and test\nthe hypothesis ϵ1, . . . , ϵd ∼N(0, 1) where ϵ = Σ−1(x −µ), ϵ = [ϵ1, . . . , ϵd]. Note that rejecting\nthe hypothesis ϵ1, . . . , ϵd ∼N(0, 1) corresponds to rejecting the hypothesis that x∗∼N(µ, Σ).\nWe compare popular alternatives for normality testing that assume a known mean and variance and\nthat are applicable to very large sample sizes, including Kolmogorov-Smirnov (Kolmogorov, 1933),\nShapiro-Wilk (Shapiro & Wilk, 1965), Chi-square (Pearson, 1900), and Cram´er–von Mises (Cram´er,\n1928). We assess these methods on a suite of test cases of vectors which are extremely unlikely\nunder the model’s latent distribution and which we know through failed generation violates the\ncharacteristic assumptions of the diffusion model, as well as assess the ”positive case” where the\nsample does follow the latent distribution, x∗∼N(µ, Σ). We report the [0.1%, 99.9%]-confidence\ninterval of the p-value produced by each respective method on 1e4 random samples in the stochastic\ntest cases. We also include the likelihood N(x∗; µ, Σ) and the likelihood of the norm statistic.\nThe results are reported in Table 2. We find that Kolmogorov-Smirnov and Cram´er–von Mises both\nsucceed in reporting a lower 99.9th percentile p-value for each tested failure case than the 0.1th\npercentile assigned to real samples. Kolmogorov-Smirnov did so with the largest gap, reporting\nextremely low p-values for all failure cases while reporting calibrated p-values for the positive case.\nShapiro-Wilk, which in the literature often is regarded as one of the most powerful tests (Razali\net al., 2011), did not work well in our context in contrast to the other normality testing methods, as\nit produced high p-values also for several of the failure cases. An explanation may be that in our\ncontext we have sample sizes of tens of thousands (same as the dimensionality of the latent D), while\ncomparisons in the literature typically focuses on smaller sample sizes, such as up to hundreds or a\nfew thousand (Razali et al., 2011). The Chi-square test is a test for discrete distributions. However, it\nis commonly used on binned data (Larsen & Marx, 2005) to approximate a continuous distribution as\ndiscrete. We do this, using histograms of 30 bins. This test successfully produced very low p-values\nfor each failure case, but also did so for many valid samples, yielding a 0.1% of 0. The likelihood\nN(x∗; µ, Σ) assigns high likelihoods to vectors x∗near the mode irrespective of its characteristics.\nWe note that it fails to distinguish to failure cases from real samples, and in some cases assigns\nmuch higher likelihood to the failure cases than real data. The insufficiency of the likelihood to\ndescribe the feasibility of samples is a phenomenon present in cases of high-dimensionality and\nlow joint dependence between these dimensions; this is due to the quickly shrinking concentration\nof points near the centre (or close to µ, where the likelihood is highest) as the dimensionality is\nincreased (Talagrand, 1995; Ash, 2012; Nalisnick et al., 2019). The norm statistic we note successfully\nassigns low likelihoods to some failure cases, where the norm is atypical, but fails to do so in most of\nthe tested cases.\nIn summary, we find that Kolmogorov-Smirnov and Cram´er–von Mises both correctly rejects the\nfailure cases (by assigning low p-values) while reporting calibrated p-values for real samples, with\nKolmogorov-Smirnov doing so with the lowest p-values for all failure cases.\nF\nINTERPOLATION AND CENTROID DETERMINATION SETUP DETAILS\nBaselines\nFor the application of interpolation, we compare to linear interpolation (LERP), spherical\nlinear interpolation (SLERP), and Norm-Aware Optimization (NAO) (Samuel et al., 2023), a recent\napproach which considers the norm of the noise vectors. In contrast to the other approaches which\nonly involve closed-form expressions, NAO involves a numerical optimization scheme based on a\ndiscretion of a line integral. For the application of centroid determination we compare to NAO, the\nEuclidean centroid ¯x = 1\nK\nPK\nk=1 xk, and two transformations of the Euclidean centroid; ”standard-\nised Euclidean”, where ¯x is subsequently standardised to have mean zero and unit variance, and\n”mode norm Euclidean”, where ¯x is rescaled to have the norm equal to the (square root of the) mode\n21\nPublished as a conference paper at ICLR 2025\nTest case:\nx(i) ∼N(0, I)\n0\ns1\nn(1)\nn(ramp)\n0.1x(i)\nx(i) + 0.1\nn(duplicates)\nn(5% rescaled) n(1% rescaled)\nMethod/value\nlog N(x(i); 0, I)\n[−5e4, −5e4]\n−3e4\n−5e4\n−3e4\n−5e4\n[−3e4, −3e4]\n[−5e4, −5e4]\n[−5e4, −5e4]\n[−5e4, −5e4]\n[−5e4, −5e4]\nlog χ2(||x(i)||2; D)\n[−1e1, −7]\n−∞\n−7\n−∞\n−7\n[−7e4, −7e4]\n[−2e1, −7]\n[−7, −7]\n[−7, −7]\n[−7, −7]\nShapiro-Wilk\n[2e−4, 1]\n1\n1\n1\n5e−72\n[8e−3, 1]\n[8e−3, 1]\n[1e−50, 8e−19]\n[3e−105, 1e−99]\n[2e−85, 2e−69]\nChi-square\n[0, 1]\n0\n0\n0\n0\n[0, 0]\n[6e−149, 9e−45]\n[0, 1e−216]\n[0, 0]\n[0, 0]\nCramer-von-Mises\n[2e−4, 1]\n5e−7\n0.0\n5e−7\n1e−8\n[4e−7, 4e−7]\n[2e−9, 2e−8]\n[4e−12, 7e−4]\n[2e−8, 6e−8]\n[2e−11, 5e−9]\nKolmogorov-Smirnov\n[5e−4, 1]\n0\n0\n0\n2e−105\n[0, 0]\n[9e−77, 9e−37]\n[1e−129, 7e−10]\n[8e−252, 2e−177]\n[1e−31, 2e−12]\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTable 2: Normality testing of latents Reported is the [0.1%, 99.9%]-confidence interval of the p-\nvalue produced by each respective normality testing method on 1e4 random samples in the stochastic\ntest cases. We also report the log likelihood of the vector x(i) under the latent distribution and the\nlikelihood of the norm statistic. The diffusion model in Rombach et al. (2022) is used in this example.\nGreen colour for the respective failure test case indicate the method assigning a lower p-value or\nlikelihood to the failure case to at least 99.9% of the failure samples than the lowest 0.1% of real\nsamples, to illustrate that it was successful in distinguishing the failure samples. Chi-square assigns\nlow p-values to many real samples as well, which we indicate in red. The images show the generated\nimage from a random failure sample of the test case. In (failure) Test 1 in column 1 the latent is 0,\nthe mode of the latent distribution for this model N(0, I). In Test 2 it is the constant vector with\nthe maximum likelihood norm of the norm sampling distribution. In Test 3 the constant vector is\ninstead normalised to have its first two moments matching the latent’s marginal distribution. n(·)\nindicate that the vector(s) of the test case is normalised to have zero mean and unit variance. In Test 4\na linspace vector [−1, . . . , 1]D have been similarly normalised. In Test 5 and 6 the normal samples\n(from the left most column) have been transformed with scaling and a bias, respectively. In Test 7\nrandom selections of 1% of the dimensions [1, · · · , D] of the normal samples have been repeated\n(100 times) and subsequently normalised by n(·). In Test 8 and 9 a proportion of the dimensions of\nthe normal samples have been multiplied by 5 (with indices selected randomly) and the whole vectors\nare subsequently normalised by n(·), where the proportion is 5% and 1%, respectively.\nof χ2(D), the chi-squared distribution with D degrees of freedom, which is the maximum likelihood\nnorm given that x has been generated from a unit Gaussian with D dimensions.\nEvaluation sets\nWe closely follow the evaluation protocol in Samuel et al. (2023), where we base\nthe experiments on Stable Diffusion 2 (Rombach et al., 2022) and inversions of random images from\nImageNet1k (Deng et al., 2009). We (uniformly) randomly select 50 classes, each from which we\nrandomly select 50 unique images, and find their corresponding latents through DDIM inversion (Song\net al., 2020a) using the class name as prompt. We note that the DDIM inversion can be sensitive to\nthe number of steps. Therefore, we made sure to use a sufficient number of steps for the inversion\n(we used 400 steps), which we then matched for the generation; see Figure 15 for an illustration\nof the importance of using a sufficient number of steps. We used the guidance scale 1.0 (i.e. no\nguidance) for the inversion, which was then matched during generation. Note that using no guidance\nis important both for accurate inversion as well as to not introduce a factor (the dynamics of prompt\nguidance) which would be specific to the Stable Diffusion 2.1 model.\nFor the interpolation setup we randomly (without replacement) pair the 50 images per class into 25\npairs, forming 1250 image pairs in total. In between the ends of each respective pair, each method then\nis to produce three interpolation points (and images). For the NAO method, which needs additional\ninterpolation points to approximate the line integral, we used 11 interpolation points and selected\nthree points from these at uniform (index) distance, similar to in Samuel et al. (2023).\nFor the centroid determination setup we for each class form 10 3-groups, 10 5-groups, 4 10-groups\nand 1 25-group, sampled without replacement per group1; i.e. each method is to form 25 centroids\nper class total, for an overall total of 1250 centroids per method. Similarly to the interpolation setup,\nfor NAO we used 11 interpolations points per path, which for their centroid determination method\nentails K paths per centroid.\n1But with replacement across groups, i.e. the groups are sampled independently from the same collection of\nimages.\n22\nPublished as a conference paper at ICLR 2025\nFigure 8: Low p-values are inherited by interpolants The x-axis show the lower Kolmogorov-\nSmirnov p-value of the two seed latents for each corresponding spherical interpolation in Figure 5,\nand the y-axis show the p-value of the resulting interpolant. Due to the vast dynamic range of the\np-values, the scatter plot is shown in full on the left, and zoomed in on the upper-right quadrant on\nthe right. We note that when any of the two seed latents have a small p-value this largely tend to be\ninherited by the interpolant. The Pearson correlation coefficient is 0.79 for the indicator of acceptance\n(defined as 1 if the p-value is greater than 1e−3 and 0 otherwise) and 0.66 for the log10 p-values.\nEvaluation\nWe, as in Samuel et al. (2023), assess the methods quantitatively based on visual quality\nand preservation of semantics using FID distances and class prediction accuracy, respectively.\nThe FID distances are computed using the pytorch-fid library (Seitzer, 2020), using all evaluation im-\nages produced per method for the interpolation and centroid determination respectively, to maximize\nthe FID distance estimation accuracy.\nFor the classification accuracy, we used a pre-trained classifier, the MaxViT image classification\nmodel (Tu et al., 2022) as in Samuel et al. (2023), which achieves a top-1 of 88.53% and 98.64%\ntop-5 accuracy on the test-set of ImageNet.\nSee results in Section H.\nG\nADDITIONAL QUALITATIVE RESULTS\nSee Figure 9 for a demonstration of subspaces of latents without the LOL transformation introduced\nin Equation 3, using a diffusion model (Rombach et al., 2022) and a flow matching model (Esser\net al., 2024), respectively. The setup is identical (with the same original latents) as in Figure 12 and\nFigure 1, respectively, except without applying the proposed (LOL) transformation.\nSee Figure 10 for additional slices of the sports car subspace shown in Figure 1.\nSee Figure 12 and Figure 13 for Stable Diffusion 2.1 (SD2.1) versions of the Stable Diffusion 3 (SD3)\nexamples in Figure 1 and Figure 6, with an otherwise identical setup including the prompts. Just like\nin the SD3 examples LOL defines working subspaces. However, as expected since SD2.1 is an older\nmodel than SD3, the visual quality of the generations is better using SD3.\nSee Figure 11 for an interpolation example.\nIn the examples above the SD2.1 and SD3 models are provided with a text prompt during the\ngeneration. See Figure 14 for an example where the original latents ({xi}) were obtained using\nDDIM inversion (from images) without a prompt (and guidance scale 1.0, i.e. no guidance), allowing\ngeneration without a prompt. This allows us to also interpolate without conditioning on a prompt.\nWe note that, as expected without a prompt, the intermediates are not necessarily related to the end\npoints (the original images) but still realistic images are obtained as expected (except for using linear\ninterpolation, see discussion in Section 4) and the interpolations yield smooth gradual changes.\n23\nPublished as a conference paper at ICLR 2025\nx1\nx2\nx3\nx4\nx5\nx1\nx2\nx3\nx4\nx5\nFigure 9: Without LOL transformation. The setup here is exactly the same as in Figure 12 and\nFigure 1, respectively, except without the proposed (LOL) transformation (see Equation 3). The\nprompt used is ”A high-quality photo of a parked, colored sports car taken with a DLSR camera with\na 45.7MP sensor. The entire sports car is visible in the centre of the image. The background is simple\nand uncluttered to keep the focus on the sports car, with natural lighting enhancing its features.”.\nWe note that the diffusion model does not produce images of a car without the transformation, and\nneither model produce anything else than visually the same image for all coordinates.\nx1\nx2\nx3\nx4\nx5\nFigure 10: Additional slices of a sports car subspace. The latents x1, . . . , x5 (corresponding to\nimages) are converted into basis vectors and used to define a 5-dimensional subspace. The grids\nshow generations from uniform grid points in the subspace coordinate system, where the left and\nright grids are for the dimensions {1, 2} and {3, 4}, respectively, centered around the coordinate for\nx1. Each coordinate in the subspace correspond to a linear combination of the basis vectors. The\nflow matching model Stable Diffusion 3 (Esser et al., 2024) is used in this example. See Figure 1 for\nanother slice of the latent subspace.\nH\nADDITIONAL QUANTITATIVE RESULTS AND ANALYSIS\nIn Table 1 we show that our method outperforms or maintains the performance of the baselines\nin terms of FID distances and accuracy, as calculated using a pre-trained classifier following the\nevaluation methodology of Samuel et al. (2023). For an illustration of centroids and interpolations,\nsee Figure 2 and Figure 16, respectively. The evaluation time of an interpolation path and centroid,\nshown with one digit of precision, illustrate that the closed-form expressions are significantly faster\nthan NAO. Surprisingly, NAO did not perform as well as spherical interpolation and several other\nbaselines, despite using their implementation, which was outperforming these methods in Samuel\n24\nPublished as a conference paper at ICLR 2025\nx1\ninterpolations\nx2\nLERP\nSLERP\nNAO\nLOL\nFigure 11: Interpolation. Shown are generations from equidistant interpolations of latents x1 and\nx2 (in v ∈[0, 1]) using the respective method. The diffusion model Stable Diffusion 2.1 (Rombach\net al., 2022) is used in this example.\nx1\nx2\nx3\nx4\nx5\nFigure 12: Low-dimensional subspaces. The latents x1, . . . , x5 (corresponding to images) are\nconverted into basis vectors and used to define a 5-dimensional subspace. The grids show generations\nfrom uniform grid points in the subspace coordinate system, where the left and right grids are for the\ndimensions {1, 3} and {2, 4}, respectively, centered around the coordinate for x1. Each coordinate\nin the subspace correspond to a linear combination of the basis vectors. The diffusion model Stable\nDiffusion 2.1 (Rombach et al., 2022) is used in this example.\net al. (2023). We note one discrepancy is that we report FID distances using (2048) high-level features,\nwhile in their work they are using (64) low-level features, which in Seitzer (2020) is recommended\nagainst as it does not necessarily correlate with visual quality. In Table 3 we report results with all\nfeature settings. We note that, in our setup, the baselines perform substantially better than reported\nin Samuel et al. (2023) — including NAO in terms of FID distances using the 64 low-level feature\nsetting (1.30 in our setup vs 6.78 in their setup), and class accuracy during interpolation (62% vs\n52%), which we study below.\nThe number of DDIM inversion steps used for the ImageNet images is not reported in the NAO\nsetup (Samuel et al., 2023), but if we use 1 step instead of 400 (400 is what we use in all our\nexperiments) the class preserving accuracy of NAO for centroid determination, shown in Table 4,\nresemble theirs more, yielding 58%. As we illustrate in Figure 5 and Figure 15, a sufficient number of\ninversion steps is critical to acquire latents which not only reconstructs the original image but which\ncan be successfully interpolated. We hypothesise that a much lower setting of DDIM inversions\n25\nPublished as a conference paper at ICLR 2025\nx1\nx2\nx3\nx4\nx5\nFigure 13: Low-dimensional subspaces. The latents x1, . . . , x5 (corresponding to images) are\nconverted into basis vectors and used to define a 5-dimensional subspace. The grids show generations\nfrom uniform grid points in the subspace coordinate system, where the left and right grids are for the\ndimensions {1, 3} and {2, 4}, respectively, centered around the coordinate for x1. Each coordinate\nin the subspace correspond to a linear combination of the basis vectors. The diffusion model Stable\nDiffusion 2.1 (Rombach et al., 2022) is used in this example.\nsteps were used in their setup than in ours, which would be consistent with their norm-correcting\noptimization method having a large effect, making otherwise blurry/superimposed images intelligible\nfor the classification model.\nInterpolation\nMethod\nAccuracy\nFID 64\nFID 192\nFID 768\nFID 2048\nTime\nLERP\n3.92%\n63.4\n278\n2.18\n199\n6e−3s\nSLERP\n64.6%\n2.28\n4.95\n0.173\n42.6\n9e−3s\nNAO\n62.1%\n1.30\n4.11\n0.195\n46.0\n30s\nLOL (ours)\n67.4%\n1.72\n3.62\n0.156\n38.9\n6e−3s\nCentroid determination\nMethod\nAccuracy\nFID 64\nFID 192\nFID 768\nFID 2048\nTime\nEuclidean\n0.286%\n67.7\n317\n3.68\n310\n4e−4s\nStandardized Euclidean\n44.6%\n5.92\n21.0\n0.423\n88.8\n1e−3s\nMode norm Euclidean\n44.6%\n6.91\n21.6\n0.455\n88.4\n1e−3s\nNAO\n44.0%\n4.16\n15.6\n0.466\n93.0\n90s\nLOL (ours)\n46.3%\n9.38\n25.5\n0.455\n87.7\n6e−4s\nTable 3: Full table with class accuracy and FID distances using each setting in Seitzer (2020). Note\nthat FID 64, 192, and 768 is not recommended by Seitzer (2020), as it does not necessarily correlate\nwith visual quality. FID 2048 is based on the final features of InceptionV3 (as in Heusel et al. (2017)),\nwhile FID 64, 192 and 768 are based on earlier layers; FID 64 being the first layer.\nI\nADDITIONAL IMAGENET INTERPOLATIONS AND CENTROIDS\n26\nPublished as a conference paper at ICLR 2025\nx1\ninterpolations\nx2\nLERP\nSLERP\nNAO\nLOL\nFigure 14: Interpolation during unconditional generation. Shown are generations from equidistant\ninterpolations of latents x1 and x2 using the respective method. The latents x1 and x2 were obtained\nfrom DDIM inversion (Song et al., 2020a) with an empty prompt, and all generations in this example\nare then carried out with an empty prompt. The larger images show interpolation index 8 for SLERP,\nNAO and LOL, respectively. The diffusion model Stable Diffusion 2.1 (Rombach et al., 2022) is used\nin this example.\nCentroid determination of invalid latents\nMethod\nAccuracy\nFID 64\nFID 192\nFID 768\nFID 2048\nTime\nEuclidean\n42.3%\n25.7\n59.3\n1.23\n170\n4e−4s\nStandardized Euclidean\n46.6%\n3.35\n16.6\n1.23\n175\n1e−3s\nMode norm Euclidean\n50.6%\n2.08\n8.74\n1.19\n173\n1e−3s\nNAO\n57.7%\n3.77\n13.3\n1.05\n150\n90s\nLOL (ours)\n48.6%\n2.96\n12.5\n1.22\n171\n6e−4s\nTable 4: Centroid determination results if we would use a single step for both the DDIM inversion\nand generation (not recommended). This results in latents which do not follow the correct distribution\nN(µ, Σ) and which, despite reconstructing the original image exactly if generating also using a\nsingle step, cannot be used successfully for interpolation, see Figure 5. As illustrated in the figure,\ninterpolations of latents obtained through a single step of DDIM inversion merely results in the\ntwo images being superimposed. Note that the class accuracy is much higher for all methods on\nsuch superimposed images, compared to using DDIM inversion settings which allow for realistic\ninterpolations (see Table 3). Moreover, the FID distances set to use fewer features are lower for these\nsuperimposed images, while with recommended setting for FID distances (with 2048 features) they\nare higher (i.e. worse), as one would expect by visual inspection. The NAO method, initialized by the\nlatent yielding the superimposed image, then optimises the latent yielding a norm close to norms of\ntypical latents, which as indicated in the table leads to better ”class preservation accuracy”. However,\nthe images produced with this setting are clearly unusable as they do not look like real images.\n27\nPublished as a conference paper at ICLR 2025\n1 step\n2 steps\n16 steps\n100 steps\n500 steps\n999 steps\nImage 1\nLERP\nSLERP\nNAO\nLOL\nImage 2\nFigure 15: Accurate DDIM inversions are critical for interpolation. Shown is the interpolation\n(center) of Image 1 and Image 2 using each respective method, after a varying number of budgets\nof DDIM inversion steps (Song et al., 2020a). For each budget setting, the inversion was run\nfrom the beginning. We note that although a single step of DDIM inversion yields latents which\nperfectly reconstructs the original image such latents do not lead to realistic images, but merely visual\n”superpositions” of the images being interpolated.\n28\nPublished as a conference paper at ICLR 2025\nx1\ninterpolations\nx2\nLERP\nSLERP\nNAO\nLOL\nFigure 16: Interpolation. Shown are generations from equidistant interpolations of latents x1 and\nx2 using the respective method. The latents x1 and x2 were obtained from DDIM inversion (Song\net al., 2020a) of two random image examples from one of the randomly selected ImageNet classes\n(”Chesapeake Bay retriever”) described in Section 5. The larger images show interpolation index 6\nfor SLERP, NAO and LOL, respectively. The diffusion model Stable Diffusion 2.1 (Rombach et al.,\n2022) is used in this example.\nx1\nx2\nx3\nx4\nx5\nEuclidean\nS. Euclidean\nM.n. Euclidean\nNAO\nLOL\nFigure 17: ImageNet centroid determination. The centroid of the latents x1, x2, x3 as determined\nusing the different methods, with the result shown in the respective (right-most) plot. The diffusion\nmodel Stable Diffusion 2.1 (Rombach et al., 2022) is used in this example.\n29\nPublished as a conference paper at ICLR 2025\nx1\nx3\nx5\nx7\nx10\nEuclidean\nS. Euclidean\nM.n. Euclidean\nNAO\nLOL\nx1\nx5\nx10\nx15\nx25\nEuclidean\nS. Euclidean\nM.n. Euclidean\nNAO\nLOL\nFigure 18: Centroid determination of many latents. The centroid of the latents as determined\nusing the different methods, with the result shown in the respective (right-most) plot. Ten and 25\nlatents are used in the first two and second two examples (rows), respectively, and the plots show a\nsubset of these in the left-most plots. We noted during centroid determination of many latents (such\nas 10 and 25) that the centroids were often unsatisfactory using all methods; blurry, distorted etc. For\napplications needing to find meaningful centroids of a large number of latents, future work is needed.\nThe diffusion model Stable Diffusion 2.1 (Rombach et al., 2022) is used in this example.\n30\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Published as a conference paper at ICLR 2025\nLINEAR COMBINATIONS OF LATENTS IN GENERATIVE\nMODELS: SUBSPACES AND BEYOND\nErik Bodin1\nAlexandru Stere4\nDragos D. Margineantu5\nCarl Henrik Ek1,3\nHenry Moss1,2\n1University of Cambridge\n2Lancaster University\n3Karolinska Institutet\n4Boeing Commercial Airplanes\n5Boeing AI\nABSTRACT\nSampling from generative models has become a crucial tool for applications like\ndata synthesis and augmentation. Diffusion, Flow Matching and Continuous Nor-\nmalising Flows have shown effectiveness across various modalities, and rely on\nlatent variables for generation. For experimental design or creative applications that\nrequire more control over the generation process, it has become common to manip-\nulate the latent variable directly. However, existing approaches for performing such\nmanipulations (e.g. interpolation or forming low-dimensional representations) only\nwork well in special cases or are network or data-modality specific. We propose\nLatent Optimal Linear combinations (LOL) as a general-purpose method to form\nlinear combinations of latent variables that adhere to the assumptions of the gener-\native model. As LOL is easy to implement and naturally addresses the broader task\nof forming any linear combinations, e.g. the construction of subspaces of the latent\nspace, LOL dramatically simplifies the creation of expressive low-dimensional\nrepresentations of high-dimensional objects.\n1\nINTRODUCTION\nGenerative models are a cornerstone of machine learning, with diverse applications including image\nsynthesis, data augmentation, and creative content generation. Diffusion models (Ho et al., 2020;\nSong et al., 2020a;b) have emerged as a particularly effective approach to generative modeling for\nvarious modalities, such as for images (Ho et al., 2020), audio (Kong et al., 2020), video (Ho et al.,\n2022), and 3D models (Luo & Hu, 2021). A yet more recent approach to generative modelling is\nFlow Matching (Lipman et al., 2022), built upon Continuous Normalising Flows (Chen et al., 2018),\nthat generalises diffusion to allow for different probability paths between data and latent distribution,\ne.g. defined through optimal transport (Gulrajani et al., 2017; Villani et al., 2009).\nAs well as generation, these models allow inversion where, by running the generative procedure\nin the opposite direction, data objects can be transformed deterministically into a corresponding\nrealisation in the latent space. Such invertible connections between latent and data space provide a\nconvenient mechanism for controlling generated objects by manipulating their latent vectors. The\nmost common manipulation is to attempt semantically meaningful interpolation of two generated\nobjects (Song et al., 2020a;b; Luo & Hu, 2021) by interpolating their corresponding latent vectors.\nHowever, the optimal choice of interpolant remains an open question, with simple approaches like\nlinear interpolation leading to intermediates for which the model fails to generate plausible objects.\nWhite (2016) argues that poor quality generation under linear interpolation is due to a mismatch\nbetween the norms of the intermediate vectors and those of the Gaussian vectors that the model\nhas been trained to expect. Indeed, it is well-known that the squared norm of a D-dimensional unit\nGaussian follows the chi-squared distribution. Consequently, likely samples are concentrated in a tight\nannulus around a radius\n√\nD — a set which is not closed under linear interpolation. Because of this it\nis common to rely instead on spherical interpolation (Shoemake, 1985) (SLERP) that maintains similar\n1\n"
    },
    {
      "page_number": 2,
      "text": "Published as a conference paper at ICLR 2025\nnorms as the endpoints. Motivated by poor performance of interpolation between the latent vectors\nprovided by inverting natural images, alternatives to SLERP have recently been proposed (Samuel\net al., 2023; Zheng et al., 2024). However, these procedures are costly, have parameters to tune, and\n— like SLERP — are challenging to generalise to other types of manipulations beyond interpolation,\nsuch as constructing the subspaces needed for latent space optimisation methods (G´omez-Bombarelli\net al., 2018), or adapting to more diverse latent distributions beyond Gaussian.\nIn this work, we demonstrate that successful generation from latent vectors is strongly dependent on\nwhether their broader statistical characteristics match those of samples — a stronger condition than\njust having e.g. likely norms. We show that effective manipulation of latent spaces can be achieved\nby following the simple guiding principle of adhering to the modelling assumptions of the generation\nprocess. Our primary contributions are as follows:\n• We show that even if a latent leads to a valid object upon generation, it can fail to provide\neffective interpolation if it lacks the typical characteristics of samples from the latent\ndistribution — as diagnosed by statistical distribution tests.\n• We introduce Latent Optimal Linear combinations (LOL) — an easy-to-implement method\nfor ensuring that interpolation intermediates continue to match the latent distribution.\n• We show that LOL — a closed-form transform that makes no assumptions about the\nnetwork structure or data modality — constitutes a Monge optimal transport map for linear\ncombinations to a general class of latent distributions.\n• We demonstrate that LOL can be applied to define general linear combinations beyond\ninterpolations, including centroids (Figure 2) and, in particular, to define meaningful low-\ndimensional latent representations (Figure 1) — a goal achieved previously with significant\nexpense and only for specific architectures.\nImplementation and examples: https://github.com/bodin-e/linear-combinations-of-latents\nx1\nx2\nx3\nx4\nx5\nFigure 1: Low-dimensional latent subspaces. A 5-dimensional subspace from the flow matching\nmodel Stable Diffusion 3 (Esser et al., 2024) extracted using LOL (left) from the latents corresponding\nto images x1, . . . , x5. The left plot show generations from uniform grid points across an axis-aligned\nslice of the subspace coordinate system, centered around the coordinate for x1. Each coordinate in\nthe subspace correspond to a linear combination of latents, which define basis vectors. The right plot\nshows the corresponding subspace without the proposed LOL transformation. See Figure 6, Figure 7\nand Section G in the appendix for additional examples.\n2\nBACKGROUND\n2.1\nGENERATIVE MODELLING WITH LATENT VARIABLES\nThe methodology in this paper applies to any generative model that transforms samples from a known\ndistribution in order to model another distribution — typically, a complicated distribution of high-\n2\n"
    },
    {
      "page_number": 3,
      "text": "Published as a conference paper at ICLR 2025\nx1\nx2\nx3\nEuclidean\nS. Euclidean\nM. n. Euclidean\nNAO\nLOL\nFigure 2: Centroid determination. Generation using Stable Diffusion 2.1 (Rombach et al., 2022)\nfrom the centroid of the latents corresponding to images x1, x2, x3 using different methods. Note\nthat our proposed method removes several artifacts, such as unrealistic headlights and chassi texture.\ndimensional objects. For clarity of exposition, we focus on popular Diffusion and Flow Matching\nmodels using Gaussian latents, and then extend to general latent distributions in the appendix.\nDiffusion models learn a process that reverses the effect of gradually adding noise to training data (Ho\net al., 2020). The noise level is governed by a schedule designed so that the noised samples at the final\ntime index T follow the latent distribution x(T ) ∼p(x). To sample from the diffusion model, one\nstarts by drawing a sample from the latent distribution before iteratively evaluating the reverse process\nwith learnt parameters θ, denoising step-by-step, generating a sequence {x(T ), x(T −1), . . . , x(0)}\nx(t−1) ∼pθ(x(t−1) | x(t)),\nfinally producing a generated object x(0). Diffusion has also been extended to the continuous time\nsetting by Song et al. (2020b), where the diffusion is expressed as a stochastic differential equation.\nNote that, by using the Denoising Diffusion Implicit Model (DDIM) (Song et al., 2020a) or the\nprobability flow formulation in Song et al. (2020b), the generation process can be made deterministic,\ni.e. the latent representation x(T ) ∼p(x) completely specifies the generated object x(0).\nFlow Matching Lipman et al. (2022) is an efficient approach to train Continuous Normalising Flows\n(CNF) Chen et al. (2018) — an alternative class of generative models that builds complicated distribu-\ntions from simple latent distributions using differential equations. CNFs model the evolution of data\npoints over continuous time using an ordinary differential equation (ODE) parameterized by a neural\nnetwork, providing a deterministic relationship between latent and object space. Mathematically,\nthe transformation of a latent sample x(T ) ∼p(x) at time t = T to x(t) at time t is governed\nby f(x(t), t; θ), where f(·; θ) is a neural network with parameters θ. Flow matching allows the\ntransformation dynamics of the CNF to be learnt without needing to simulate the entire forward\nprocess during training, which improves stability and scalability.\nNote that both diffusion and flow matching models can generate in a deterministic manner, where\nthe realisation of the latent variable completely specifies the generated data object, e.g. the image.\nMoreover, by running their deterministic generation formulation in reverse, one can obtain the\ncorresponding latent representation associated with a known object, referred to as “inversion”. In\nthis paper we focus on the effect of properties of the latent vectors, and their manipulation, on the\ngeneration process. For simplicity of notation, henceforth we drop the time index and refer to the\nlatent variable x(T ) as x, and use subscript indexing to denote realisations of the latent variable.\n2.2\nINTERPOLATION OF LATENTS\nLinear Interpolation. The most common latent space manipulation is interpolation, where we are\ngiven two latent vectors x1 and x2, referred to as seed latents (or seeds), and obtain intermediate\nlatent vectors. The simplest approach is to interpolate linearly between the seeds to get intermediates\nyw\nlin = wx1 + (1 −w)x2\nfor\nw ∈[0, 1].\nSpherical Interpolation. However, as discussed in (White, 2016) in the context of Variational\nAutoencoders (Kingma, 2013) and Generative Adversarial Networks (Goodfellow et al., 2014), linear\ninterpolation yields intermediates with unlikely norms for Gaussian samples and results in highly\nimplausible generated objects (e.g. all-green images). Consequently, it is common to instead use\n3\n"
    },
    {
      "page_number": 4,
      "text": "Published as a conference paper at ICLR 2025\nspherical interpolation (SLERP) (Shoemake, 1985), which instead builds interpolants via\nyw\nSLERP = sin wθ\nsin θ x1 + sin((1 −w)θ)\nsin θ\nx2\nfor\nw ∈[0, 1]\ncos θ =\n⟨x1, x2⟩\n||x1||||x2||,\nto maintain similar norms for the intermediates as the endpoints. SLERP has become popular and is\nthe standard method for interpolation also in the more recent models (Song et al., 2020a;b).\nNorm-aware Optimisation. Motivated by poor performance of SLERP when interpolating between\nthe latent vectors corresponding to inverted natural images (rather than using those sampled directly\nfrom the model’s latent distribution), Zheng et al. (2024) propose to add additional Gaussian noise\nto interpolants via the inversion procedure to control a trade-off between generation quality and\nadherence to the seed images (i.e. the images corresponding to the latents at the interpolation end-\npoints). Alternatively, Samuel et al. (2023) advocate for Norm-Aware Optimisation (NAO) which\nuses back-propagation to identify interpolation paths γ : [0, 1] →Rd that solve\ninf\nγ −\nZ\nlog P(γ(s))ds\ns.t.\nγ(0) = x1, γ(1) = x2,\nwhere log P : Rd →R is the log likelihood of the squared norm under its sampling distribution\nχ2(D) for unit Gaussian samples. NAO can also calculate centroids of K latents by simultaneous\noptimisation of paths between the centroid and each respective latent.\nAll the proposed interpolation methods above aim to make the intermediates adhere to statistics of\nGaussian samples, however, require significant computation, have hyperparameters to tune, or lack\ntheoretical guarantees. Moreover — in common with spherical interpolation — they are difficult to\ngeneralise beyond Gaussian latent variables or to more general expressions of the latents beyond\ninterpolation, such as building subspaces based on their span — as required to build expressive\nlow-dimensional representations.\nIn this work we will propose a simple technique that guarantees that interpolations follow the latent\ndistribution if the original (seed) latents do, applies to a general class of latent distributions, and\nenables us go beyond interpolation. Moreover, we will address how to assess the distributional\nassumption of the seed latents, to simplify the diagnosis of errors at the source.\nSD2.1 (diffusion)\nSD3 (flow matching)\nx(i) ∼N (µ, Σ)\n1\nargmax p(|| · ||)\n2\nargmax p(|| · ||)\n3\nN (x∗; µ, Σ)\nx(i) ∼N (µ, Σ)\n1\nargmax p(|| · ||)\n2\nargmax p(|| · ||)\n3\nN (x∗; µ, Σ)\nFigure 3: Likelihood and norm insufficient. Column (1) of each panel shows an image generated\nusing a random sample from the associated Gaussian latent distribution for the diffusion model\nof Rombach et al. (2022) (left side) and the flow matching model of Esser et al. (2024) (right side).\nColumns (2) and (3) both show images generated from latents with the most likely norm according\nto their respective latent distribution. Columns (2) use the same Gaussian samples as in columns\n(1) but rescaled to have this norm, also yielding realistic images. Meanwhile, columns (3) show\nthe failed generation from constant vectors sI scaled to have the most likely norm according to the\nlatent distribution but lacking other characteristics (e.g. not having all-equal values) that the network\nwas trained to expect, even though its likelihood N(sI; µ, Σ) is typical of real samples. Moreover,\nthe distribution mode µ, which also lacks needed characteristics , has vastly higher log likelihood\nthan any realistic sample; -33875 and -135503 for the two models, respectively. See Table 2 for an\nexample of failed generation using the mode µ.\n3\nASSESSING VALIDITY OF LATENTS VIA DISTRIBUTION TESTING\nBefore introducing our proposed method, we first explore evidence for our central hypothesis that\ntackles a misconception underpinning current interpolation methods for Gaussian latents:\nHaving a latent vector with a norm that is likely for a sample from the latent distribution N(µ, Σ)\nis not a sufficient condition for plausible sample generation. Rather, plausible generation requires\n4\n"
    },
    {
      "page_number": 5,
      "text": "Published as a conference paper at ICLR 2025\na latent vector with characteristics that match those of a sample from N(µ, Σ) more generally (as\nevaluated by normality tests), with a likely norm being only one such characteristic.\nThe characteristics of the random variable x may be described by a collection of statistics, e.g. its\nmean, variance or norm. Above, we hypothesise that if a specified latent x∗has an extremely unlikely\nvalue for a characteristic for which the network has come to rely, then implausible generation will\nfollow. A norm is unlikely if it has a low likelihood under its sampling distribution; for unit Gaussians\nit is χ(D), see Section 1. While the norm has been shown to often be an important statistic (Samuel\net al., 2023), our Figure 3 illustrates on a popular diffusion model (Rombach et al., 2022) and a flow\nmatching model (Esser et al., 2024) that a latent with a likely norm — even the most likely — can\nstill induce failure in the generative process if other evidently critical characteristics are unmet.\nInversion budget\nOriginal\n1 step\n2 steps\n4 steps\n8 steps\n16 steps\n32 steps\n64 steps\n100 steps\n200 steps\n500 steps\n999 steps\nImage\nRec. error:\np-value\n0.340\n0\n0.390\n0\n0.184\n1.2e−58\n0.230\n2.5e−15\n0.0539\n2.3e−5\n0.0295\n7.4e−3\n0.0225\n1.9e−2\n0.0185\n8.4e−2\n0.0181\n1.2e−1\n0.0199\n1.4e−1\n0.0178\n1.6e−1\nFigure 4: Normality testing of latent vectors obtained from inversion. We show the LPIPS (Zhang\net al., 2018) reconstruction errors (second row, in red) of 200 inverted randomly selected images\nacross 50 random classes from ImageNet1k (Deng et al., 2009), the p-values of their inversions (third\nrow), and rejection rates (bottom row) of the Kolmogorov-Smirnov normality test applied to the\ncorresponding latent obtained from inversion under various step budgets. We use the diffusion model\nof Rombach et al. (2022), always using its maximum number of steps (999) for generation, and\ndenote the 10th, 50th and 90th percentiles with black lines. The first row shows image reconstructions\nusing its inversion at each budget, highlighted in red when the latent was rejected (p < 1e−3), with\nthe interpretation that the characteristics of the latent were unlikely for a real sample according to\nthe KS test. We note the strong correlation between inversion budgets providing low reconstruction\nerrors and those for which the p-values of the latents are realistic — taking values likely to occur\nby chance for real samples. However, as we will see in Figure 5, there are still many latents with\nlow reconstruction error yet extremely low p-value, and this often severely affects the quality of its\ninterpolants.\nIn general, neural networks have many degrees of freedom, and so it is difficult to determine\nand list the input characteristics that a specific model relies on. Therefore, rather than trying to\ndetermine all necessary statistics for each model, we propose instead to rely on standard methods\nfor distribution testing, a classic area of statistics (Razali et al., 2011; Kolmogorov, 1933; Shapiro\n& Wilk, 1965) — here exploring the (null) hypothesis that a latent vector x∗∈RD is drawn from\nN(µ, Σ). Popular normality tests consider broad statistics associated with normality. For example,\nthe Kolmogorov–Smirnov (Kolmogorov, 1933) test considers the distribution of the largest absolute\ndifference between the empirical and the theoretical cumulative density function across all values.\n5\n"
    },
    {
      "page_number": 6,
      "text": "Published as a conference paper at ICLR 2025\nSeeds\nInterpolations\nVisual quality ≈1\nVisual quality ≈2\nVisual quality ≈3\nVisual quality ≈4\nVisual quality ≈5\nFigure 5: Lack of normality is linked to failure of interpolants The left and middle panels shows\nLPIPS (Zhang et al., 2018) reconstruction error (after 999 generation steps) and the Kolmogorov-\nSmirnov p-value for all inversions presented in Figure 4, split into two plots due to the vast dynamic\nrange of p-values. Although latents with high (realistic) p-values tend to have low reconstruction\nerrors, there are many latents with low reconstruction errors that also have low p-values. The right\npanel shows Q-Align visual quality scores (Wu et al., 2023) for spherical interpolants between pairs\nof inversions selected from matching ImageNet1k (Deng et al., 2009) image classes, demonstrating\nthat choosing seed latents with both low reconstruction error (< 0.05) and high p-values (> 1e−3)\nallows us to avoid low-quality interpolants that would arise when choosing seeds by reconstruction\nerror alone. For reference, we include examples of interpolants at each visual quality level.\n3.1\nTHE SAMPLE CHARACTERISTICS OF SEED LATENTS AFFECT THEIR INTERPOLATIONS\nAn ability to identify latents that lack the necessary characteristics for good quality generation\nis critical when manipulating latents that are not acquired by explicitly sampling from the latent\ndistribution. Indeed, as we show below, using such deficient latents as seeds for interpolation can\nseverely affect the quality of interpolants, for example when interpolating between inverted natural\nimages (the most common way to obtain a seed latent, see Section 2). As inversion is subject to\na finite computation budget (i.e. finite inversion steps), numerical imprecision, and because the\nparticular data instance may not be drawn from the distribution of the model’s training data, the\nresulting latent vector x∗may not correspond to a sample from the latent distribution. We will now\nshow that distribution testing is helpful for identifying such deficient latents, providing an opportunity\nto apply corrections or review the examples or inversion setup, before performing interpolation.\nFigure 4 demonstrates that inversions having realistic p-values are strongly correlated with them\nreproducing their original image with good quality. The p-value indicates the probability of observing\nthe latent vector by chance given that it is drawn from the latent distribution N(µ, Σ). If this value\nis extremely low that is a strong indicator that the latent lacks characteristics expected of samples\nfrom N(µ, Σ). In the figure we see that at low inversion budgets (with high reconstruction errors)\nmost p-values are 1e−50 or lower, and then reach values you expect to see by chance for real samples\n(around 1e−3) at budgets where the reconstruction errors tend to be small. However, we will show\nthat the p-value provides additional information about the quality of the latents than provided by\nreconstruction error alone.\nWe now examine how the p-values of two inversions relate to our ability to interpolate them with good\nquality. Figure 5 shows that although latents with realistic p-values tend to have low reconstruction\nerrors, there are many latents with low reconstruction errors that have low p-values. Therefore, just\nbecause the process of going from object to latent and back reproduces the object, this does not\nnecessarily mean that the resulting latent is well characterised as a sample from N(µ, Σ). The lack\nof such characteristics can be inherited when manipulating the latent; in Appendix 8 we show that\ninterpolants typically inherit low p-values from their seed latents. Figure 5 also demonstrates that if\n6\n"
    },
    {
      "page_number": 7,
      "text": "Published as a conference paper at ICLR 2025\nwe beyond requiring inversions to reconstruct their original objects also require their p-values to be\nrealistic — at levels expected of real samples — we are able to avoid many low quality interpolants\ndownstream, by not using the rejected latents as seeds. In other words, the normality test helps us\ndiscover when the latents acquired from inversion lack characteristics of real samples, as this lack\nprevents them from being reliably used as seeds. Distribution testing the latents can be helpful for\ndebugging the inversion setup — which typically involves solving an ODE — and assessing the\nsettings used. In Appendix E we investigate the effectiveness of a range of other classic normality\ntests and tests based on the likelihood N(x∗; µ, Σ) as well as the likelihood of the norm statistic.\nWe assess these methods across a suite of test cases including vectors with unlikely characteristics,\nand for which we know, through failed generation, violate the assumptions of the generative model.\n4\nLINEAR COMBINATIONS OF LATENTS\nNow, equipped with the knowledge that matching broad characteristics of a sample from the latent\ndistribution is critical, we propose a simple scheme for forming linear combinations of seed latents\n— which we now will assume are sampled from the latent distribution — to maintain their sample\ncharacteristics. We will focus on Gaussian latents in this section, and present an extension to general\nlatent distributions in Appendix A. In this section we change the notation slightly, where a seed\nlatent xk ∈RD rather than being a realisation of a random variable — a vector of known values — it\nis a random variable following the latent distribution (here, N(µ, Σ)). We assume access to K such\nseed latent variables {xk}K\nk=1 and attempt to form new variables following the same distribution.\nLet y be a linear combination of the K Gaussian latent variables xk ∼N(µ, Σ)\ny :=\nK\nX\nk=1\nwkxk = wT X,\n(1)\nwhere wk ∈R, w = [w1, w2, . . . , wK] and X = [x1, x2, . . . , xK]. Then we have that y is also a\nGaussian random variable, with mean and covariance\ny ∼N(αµ, βΣ)\nα =\nK\nX\nk=1\nwk\nβ =\nK\nX\nk=1\nw2\nk.\n(2)\nIn other words, y is only distributed as N(µ, Σ) in the specific case where (a) αµ = µ and (b)\nβΣ = Σ — an observation which we now use to explain the empirically observed behaviour of\nexisting interpolation methods. Firstly, for linear interpolation, where w = [v, 1 −v], v ∈[0, 1], (b)\nholds only for the endpoints v = {0, 1}, and so leads to implausible generations for interpolants (as\nwe demonstrate empirically in Figure 11). In contrast, in the popular case of high-dimensional unit\nGaussian latent vectors, spherical interpolants have β ≈1, ∀v ∈[0, 1], as proven in Appendix C,\nand (a) is met as α0 = 0, which is consistent with plausible interpolations (see Figure 11).\nIn this work, we instead propose transforming linear combinations such that α = β = 1, for any\nw ∈RK, thus exactly meeting the criteria for any linear combination and any choice of µ and Σ.\nWe define a transformed random variable z to use as the latent instead of the linear combination y\nz := Tw,µ(y) = Tw,µ(\nK\nX\nk=1\nwkxk),\n(3)\nwhere Tw,µ(y) := (1 −\nα\n√β )µ +\ny\n√β , for which it holds that z ∼N(µ, Σ) given latent variables\nxk ∼N(µ, Σ). Here, Tw,µ : RD →RD is the map that transforms samples from the distribution of\na linear combination of N(µ, Σ)-variables with weights w into N(µ, Σ). In Appendix B we show\nthat this transport map is Monge optimal, and extend it to general distributions in Appendix A. The\nweights w, which via α and β together with the set of K seed latents specify the transformed linear\ncombination z, depend on the operation, represented as particular linear combinations. Below are a\nfew examples of popular operations (linear combinations) to form y; these are used as above to, for\nthe corresponding weights w, obtain z following the distribution expected by the generative model.\n• Interpolation: y = wT X, where X = [x1, x2], w = [w1, 1 −w1] and w1 ∈[0, 1].\n7\n"
    },
    {
      "page_number": 8,
      "text": "Published as a conference paper at ICLR 2025\n• Centroid Determination: y = wT X, where X = [x1, . . . , xK], w = [ 1\nK ]K.\n• Subspaces: Suppose we wish to build a navigable subspace spanned by linear combinations\nof K latent variables. By performing the QR decomposition of X := [x1, x2, . . . , xK] ∈\nRD×K to produce a semi-orthonormal matrix U ∈RD×K (as the Q-matrix), we can\nthen define a subspace projection of any new x into the desired subspace via s(x) :=\nUU T x = Uh ∈RD. The weights w for a given point in the subspace s(x) are given\nby w = X†s(x) = X†Uh ∈RK where X† is the Moore–Penrose inverse of X. See\nthe derivation of the weights and proof in Appendix D. One can directly pick coordinates\nh ∈RK, compute the subspace projection y = Uh, and then subsequently use the transport\nmap (defined by Equation 3) to the latent space to obtain z. In Figure 1 we use grids in RK\nto set h, used as above together with a basis (defined by U) from a set of latents.\n5\nEXPERIMENTS\nWe now assess our proposed transformation scheme LOL experimentally. To verify that LOL matches\nor exceeds current methods for Gaussian latents for currently available operations — interpolation\nand centroid determination — we perform qualitative and quantitative comparisons to their respective\nbaselines. We then demonstrate new capabilities with several examples of low-dimensional subspaces\non popular diffusion models and a popular flow matching model.\n5.1\nINTERPOLATION AND CENTROID DETERMINATION\nFor the application of interpolation, we compare our proposed LOL to linear interpolation (LERP),\nspherical linear interpolation (SLERP), and Norm-Aware Optimization (NAO) Samuel et al. (2023).\nIn contrast to the all the other considered\ninterpolation methods (including LOL)\nwhich only involve closed-form expres-\nsions, NAO requires numerical optimisa-\ntion.\nWe closely follow the evaluation\nprotocol in Samuel et al. (2023), basing\nthe experiments on Stable Diffusion (SD)\n2.1 (Rombach et al., 2022) and inversions\nof random images from 50 random classes\nfrom ImageNet1k (Deng et al., 2009), and\nassess visual quality and preservation of se-\nmantics using Fr´echet Inception Distance\n(FID) (Heusel et al., 2017) and class pre-\ndiction accuracy, respectively. For the in-\nterpolation we (randomly without replace-\nment) pair the 50 images per class into 25\npairs, forming 1250 image pairs in total.\nInterpolation\nMethod\nAccuracy\nFID ↓\nTime\nLERP\n3.92%\n199\n6e−3s\nSLERP\n64.6%\n42.6\n9e−3s\nNAO\n62.1%\n46.0\n30s\nLOL (ours)\n67.4%\n38.9\n6e−3s\nCentroid determination\nMethod\nAccuracy\nFID ↓\nTime\nEuclidean\n0.286%\n310\n4e−4s\nStandardised Euclidean\n44.6%\n88.8\n1e−3s\nMode norm Euclidean\n44.6%\n88.4\n1e−3s\nNAO\n44.0%\n93.0\n90s\nLOL (ours)\n46.3%\n87.7\n6e−4s\nTable 1: Quantitative comparisons of baselines.\nFor the centroid determination we compare to NAO, the Euclidean centroid ¯x =\n1\nK\nPK\nk=1 xk and\ntwo transformations thereof; “standardised Euclidean”, where ¯x is subsequently standardised to have\nmean zero and unit variance (as SD 2.1 assumes), and “mode norm Euclidean”, where ¯x is rescaled\nfor its norm to equal the maximum likelihood norm\n√\nD. For each class, we form 10 3-groups, 10\n5-groups, 4 10-groups and 1 25-group, sampled without replacement per group, for a total of 1250\ncentroids per method. For more details on the experiment setup and settings, see Appendix F.\nIn Table 1 we show that our method outperforms or maintains the performance of the baselines\nin terms of FID distance and accuracy, as calculated using a pre-trained classifier following the\nevaluation methodology of Samuel et al. (2023). For an illustration of centroids and interpolations,\nsee Figure 2 and Figure 16, respectively. The evaluation time of an interpolation path and centroid,\nshown with one digit of precision, illustrate that the closed-form expressions are significantly faster\nthan NAO. Surprisingly, NAO did not perform as well as spherical interpolation and several other\nbaselines, despite using their implementation, which was outperforming these methods in Samuel\net al. (2023). We note one discrepancy is that we report FID distances using (2048) high-level features,\nwhile in their work they are using (64) low-level features, which in Seitzer (2020) is recommended\n8\n"
    },
    {
      "page_number": 9,
      "text": "Published as a conference paper at ICLR 2025\nagainst as it does not necessarily correlate with visual quality. In the appendix we include FID\ndistances using all settings of features. We note that, in our setup, the baselines perform substantially\nbetter than reported in Samuel et al. (2023) — including NAO in terms of FID distances using the\n64 low-level feature setting (1.30 in our setup vs 6.78 in their setup), and class accuracy during\ninterpolation (62% vs 52%). See Section H in the appendix for more details and ablations.\nx1\nx2\nx3\nx4\nx5\nFigure 6: Low-dimensional subspaces. The latents x1, . . . , x5 (corresponding to images) are\nconverted into basis vectors and used to define a 5-dimensional subspace. The grids show generations\nfrom the flow matching model Stable Diffusion 3 (Esser et al., 2024) over uniform grid points in the\nsubspace coordinate system, where the left and right grids are for the dimensions {1, 2} and {3, 4},\nrespectively, centered around the coordinate for x1. Each coordinate in the subspace correspond to a\nlinear combination of the basis vectors, which through LOL all yield high-quality generations.\n5.2\nLOW-DIMENSIONAL SUBSPACES\nx1\nx2\nx3\nFigure 7: Model-agnostic subspace definitions. The latents x1, x2 and x3 — which via the\ngenerative model correspond to 3D designs — are converted into basis vectors and used to define a\nthree-dimensional subspace. The left plot shows the generated designs corresponding to a ten-by-ten\ngrid in a two-dimensional slice of the subspace, shown over a region. The right plot shows the\nwavedrag, as evaluated by simulation in OpenVSP (McDonald & Gloudemans, 2022) for the designs\nover a 100-by-100 grid in the same region, with the respective design coordinates (from the left plot)\nshown as black dots. We trained the SLIDE (Lyu et al., 2023) diffusion model on ShapeNet (Chang\net al., 2015). LOL allows subspaces to be defined without any model-specific treatment.\nIn Figure 1 and Figure 6 we illustrate slices of a 5-dimensional subspace of the latent space of a flow\nmatching model, indexing high-dimensional images of sports cars and rocking chairs, respectively.\n9\n"
    },
    {
      "page_number": 10,
      "text": "Published as a conference paper at ICLR 2025\nThe subspaces here are defined using five images (one per desired dimension), formed using our LOL\nmethod described in Section 4 to transform the linear combinations of the corresponding projections.\nOur approach is dimensionality and model-agnostic, which we illustrate in Figure 7, where the\nsame procedure is used on a completely different model without adaptations — where we define\na three-dimensional subspace based on three designs in a point cloud diffusion model with mesh\nreconstruction. We evaluate designs in this subspace using a simple computational fluid dynamics\nsimulation to illustrate that our approach allows objective functions to be defined over the space, in\nturn allowing off-the-shelf optimisation methods to be applied to search for designs. In Figure 9\nin the appendix we show corresponding grids to the sports cars in Figure 1 without the proposed\nLOL transformation, which either leads to implausible images or the same image for each coordinate,\ndepending on the model. In the appendix (Section G) we also include more slices and examples;\nincluding subspaces using the diffusion model Stable Diffusion 2.1 (Rombach et al., 2022).\n6\nRELATED WORK\nGenerative models with non-Gaussian priors. In Fadel et al. (2021) and Davidson et al. (2018),\nin the context of normalizing flows and VAEs, respectively, Dirichlet and von Mises-Fisher prior\ndistributions are explored with the goal of improving interpolation performance. However, these\nmethods require training the model with the new latent distribution, which is impractical and untested\nfor the large pretrained models we consider.\nConditional Diffusion. An additional way to control the generation process of a diffusion model is to\nguide the generative process with additional information, such as text and labels, to produce outputs\nthat meet specific requirements, such as where samples are guided towards a desired class (Dhariwal\n& Nichol, 2021; Ho & Salimans, 2022), or to align outputs with text descriptions (Radford et al.,\n2021). Conditioning is complementary to latent space manipulation. For example, when making\nFigure 1 we used conditioning (a prompt) to constrain the generation to sports cars, however, the\nvariation of images fulfilling this constraint is encoded in the latent space.\nLow-dimensional representations.\nWe have shown that LOL can provide expressive low-\ndimensional representations of latent spaces of generative models. To the best of our knowledge, the\nmost closely related line of work was initiated in Kwon et al. (2022), where it was shown that semantic\nedit directions can recovered from activations of a UNet (Ronneberger et al., 2015) denoiser network\nduring generation. Using a pretrained CLIP (Contrastive Language-Image Pre-Training) (Radford\net al., 2021) model to define directions within the inner-most feature map of the UNet architecture,\nnamed h-space, they show that some semantic edits, such as adding glasses to an image of a per-\nson, correspond to linear edits in the h-space. More recently, Haas et al. (2024) demonstrate that\nh-space edit directions can also be found through Principle Component Analysis, and Park et al.\n(2023) propose a pullback metric that transfers edits in the h-space into the original latent space.\nHowever, while these three approaches demonstrate impressive editing capabilities on images, they\nall require a modification to the generative process and are limited to diffusion models built with\nUNet architectures. In our work low-dimensional representations are instead formed in closed-form\nas linear subspaces based on a (free) choice of latent basis vectors. These basis vectors could be\nobtained through various methodologies, including via the pull-back metric in Park et al. (2023),\nwhich may be interesting to explore in future work.\n7\nCONCLUSION\nIn this paper we propose LOL, a general and simple scheme to define linear combinations of latents\nthat maintain a prespecified latent distribution and demonstrate its effectiveness for interpolation\nand defining subspaces within the latent spaces of generative models. Of course, these linear\ncombinations only follow the desired distribution if the original (seed) latents do. Therefore, we\npropose the adoption of distribution tests to assess the validity of latents obtained from e.g. inversions.\nStill, the existing methods for distribution testing may not align perfectly with a given network’s\nexpectations — e.g. many tests may be stricter than necessary for the network — and it may be\nan interesting direction of future work to develop tailored methods, along with more extensive\ncomparisons of existing tests in the context of generative models.\n10\n"
    },
    {
      "page_number": 11,
      "text": "Published as a conference paper at ICLR 2025\nACKNOWLEDGMENTS\nWe thank Samuel Willis and Daattavya Aggarwal (University of Cambridge) for their valuable\ncontributions. Samuel contributed important ideas related to computational fluid dynamics simulation\nand the SLIDE model, and constructed the setup and plots used in Figure 7. Daattavya assisted in\nverifying proofs and offered comments that greatly improved the manuscript.\nREFERENCES\nRobert B Ash. Information theory. Courier Corporation, 2012.\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\nSilvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\nmodel repository. arXiv preprint arXiv:1512.03012, 2015.\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\ndifferential equations. Advances in neural information processing systems, 31, 2018.\nHarald Cram´er. On the composition of elementary errors: First paper: Mathematical deductions.\nScandinavian Actuarial Journal, 1928(1):13–74, 1928.\nTim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspherical\nvariational auto-encoders. In 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI\n2018, pp. 856–865. Association For Uncertainty in Artificial Intelligence (AUAI), 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780–8794, 2021.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In Forty-first International Conference on Machine Learning,\n2024.\nSamuel G Fadel, Sebastian Mair, Ricardo da S. Torres, and Ulf Brefeld. Principled interpolation in\nnormalizing flows. In Machine Learning and Knowledge Discovery in Databases. Research Track:\nEuropean Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings,\nPart II 21, pp. 116–131. Springer, 2021.\nRafael G´omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos´e Miguel Hern´andez-Lobato,\nBenjam´ın S´anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,\nRyan P Adams, and Al´an Aspuru-Guzik. Automatic chemical design using a data-driven continuous\nrepresentation of molecules. ACS central science, 4(2):268–276, 2018.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.\nImproved training of wasserstein gans. Advances in neural information processing systems, 30,\n2017.\nRen´e Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Stella Graßhof, Sami S Brandt, and\nTomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion\nmodels. In 2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition\n(FG), pp. 1–9. IEEE, 2024.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural\ninformation processing systems, 30, 2017.\n11\n"
    },
    {
      "page_number": 12,
      "text": "Published as a conference paper at ICLR 2025\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840–6851, 2020.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633–8646,\n2022.\nDiederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nAn Kolmogorov. Sulla determinazione empirica di una legge didistribuzione. Giorn Dell’inst Ital\nDegli Att, 4:89–91, 1933.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\ndiffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020.\nMingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent\nspace. arXiv preprint arXiv:2210.10960, 2022.\nRichard J Larsen and Morris L Marx. An introduction to mathematical statistics. Prentice Hall\nHoboken, NJ, 2005.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching\nfor generative modeling. arXiv preprint arXiv:2210.02747, 2022.\nShitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2837–2845, 2021.\nZhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua Lin, and Bo Dai. Controllable mesh genera-\ntion through sparse latent point diffusion models. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 271–280, 2023.\nRobert A McDonald and James R Gloudemans. Open vehicle sketch pad: An open source parametric\ngeometry and analysis tool for conceptual aircraft design. In AIAA SciTech 2022 Forum, pp. 0004,\n2022.\nEric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-\ndistribution inputs to deep generative models using typicality. arXiv preprint arXiv:1906.02994,\n2019.\nYong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the\nlatent space of diffusion models through the lens of riemannian geometry. Advances in Neural\nInformation Processing Systems, 36:24129–24142, 2023.\nKarl Pearson. X. on the criterion that a given system of deviations from the probable in the case of\na correlated system of variables is such that it can be reasonably supposed to have arisen from\nrandom sampling. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of\nScience, 50(302):157–175, 1900.\nGabriel Peyr´e and Marco Cuturi. Computational optimal transport, 2020. URL https://arxiv.\norg/abs/1803.00567.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nNornadiah Mohd Razali, Yap Bee Wah, et al. Power comparisons of shapiro-wilk, kolmogorov-\nsmirnov, lilliefors and anderson-darling tests. Journal of statistical modeling and analytics, 2(1):\n21–33, 2011.\n12\n"
    },
    {
      "page_number": 13,
      "text": "Published as a conference paper at ICLR 2025\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684–10695, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical image computing and computer-assisted intervention–MICCAI\n2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III\n18, pp. 234–241. Springer, 2015.\nDvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, and Gal Chechik. Norm-guided latent space\nexploration for text-to-image generation. Advances in Neural Information Processing Systems, 36,\n2023.\nMaximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/\npytorch-fid, August 2020. Version 0.3.0.\nSamuel Sanford Shapiro and Martin B Wilk. An analysis of variance test for normality (complete\nsamples). Biometrika, 52(3-4):591–611, 1965.\nKen Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual\nconference on Computer graphics and interactive techniques, pp. 245–254, 1985.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020a.\nYang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020b.\nMichel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces.\nPublications Math´ematiques de l’Institut des Hautes Etudes Scientifiques, 81:73–205, 1995.\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao\nLi. Maxvit: Multi-axis vision transformer. In European conference on computer vision, pp.\n459–479. Springer, 2022.\nC´edric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.\nTom White. Sampling generative networks. arXiv preprint arXiv:1609.04468, 2016.\nHaoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao,\nAnnan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via\ndiscrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 586–595, 2018.\nPengFei Zheng, Yonggang Zhang, Zhen Fang, Tongliang Liu, Defu Lian, and Bo Han. Noisediffusion:\nCorrecting noise for image interpolation with diffusion models beyond spherical linear interpolation.\narXiv preprint arXiv:2403.08840, 2024.\nA\nOPTIMAL TRANSPORT MAP FOR GENERAL LATENT LINEAR COMBINATIONS\nWITH INDEPENDENT ELEMENTS\nIn Section 4 we introduced LOL for Gaussian latent variables, the most commonly used latent\ndistribution in modern generative models. We now extend LOL to general latent distributions with\nindependent real-valued elements, provided the cumulative distribution function (CDF) and its inverse\nis known for each respective element.\nLet p(x) be a distribution with independent components across elements, such that p(x) =\nQD\nd=1 pd(x(d)), where pd is the distribution of element x(d) ∈R with CDF Φd and inverse CDF Φ−1\nd .\n13\n"
    },
    {
      "page_number": 14,
      "text": "Published as a conference paper at ICLR 2025\nGiven linear combination weights {wk}K\nk=1, wk ∈R, and seeds {xk}K\nk=1, xk ∼p, we obtain z ∼p\nvia the transport map T{w} := {T (d)\n{w}}D\nd=1 defined per latent element z(d) as:\nz(d) := T (d)\n{w}({x(d)\nk }) = Φ−1\nd (Φϵ(ϵ(d)))\nϵ(d) = Tw,0 (PK\nk=1 wkϵ(d)\nk )\nϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )), (4)\nwhere Φϵ is the CDF of the standard Gaussian, and Tw,0 : R →R is the transport map for standard\nGaussians as defined in Equation 3. We will now show that the proposed map leads to the target\ndistribution p and then show that this map is Monge optimal, leveraging results from Section B.\nLemma 1. Let z be a transformed variable defined through the transport map T{w} applied to a\nlinear combination of independent latent variables xk ∼p(x), such that\nz(d) = T (d)\n{w}({x(d)\nk }) = Φ−1\nd (Φϵ(Tw,0(\nK\nX\nk=1\nwkϵ(d)\nk )))\nwhere\nϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )).\nThen, z ∼p(x), meaning that the transformed variable follows the target latent distribution.\nProof. We prove that z ∼p(x) by verifying that each element z(d) follows the corresponding\nmarginal distribution pd.\nSince x(d)\nk\n∼pd, its cumulative distribution function (CDF) is given by Φd(x(d)\nk ). Defining the\nstandard normal equivalent ϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )), we know that ϵ(d)\nk\n∼N(0, 1) because the\ntransformation via Φ−1\nϵ\npreserves uniformity.\nNow, consider the weighted sum:\nϵ(d) =\nK\nX\nk=1\nwkϵ(d)\nk .\nSince the ϵ(d)\nk\nare independent standard normal variables, the resulting sum follows:\nϵ(d) ∼N(α · 0, β · 1) = N(0, β),\nwhere α = PK\nk=1 wk and β = PK\nk=1 w2\nk.\nApplying the optimal Gaussian transport map Tw,0 (as proven in Section B) to ϵ(d), we obtain:\nTw,0(ϵ(d)) = ϵ(d)\n√β .\nSince ϵ(d) ∼N(0, β), it follows that:\nϵ(d)\n√β ∼N(0, 1).\nThus, applying the standard normal CDF, we get:\nΦϵ(Tw,0(ϵ(d))) ∼U(0, 1),\nwhich is a uniform distribution.\nFinally, applying the inverse CDF Φ−1\nd\nof the target distribution pd results in:\nz(d) = Φ−1\nd (Φϵ(Tw,0(ϵ(d)))) ∼pd.\nSince this holds for each d, we conclude that z ∼p(x), proving that the transport map correctly\ntransforms the linear combination into the desired latent distribution.\n14\n"
    },
    {
      "page_number": 15,
      "text": "Published as a conference paper at ICLR 2025\nLemma 2. The transport map T{w} defined element-wise as\nz(d) = T (d)\n{w}({x(d)\nk }) = Φ−1\nd (Φϵ(Tw,0(\nK\nX\nk=1\nwkϵ(d)\nk )))\nwhere\nϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )),\nis the Monge optimal transport map, minimizing the quadratic Wasserstein distance W2 between the\nsource distribution of the weighted sum of independent elements and the target latent distribution p.\nProof. We want to show that T{w} minimizes the quadratic Wasserstein distance W2 between the\nsource distribution (the weighted sum of independent seeds) and the target distribution.\nStep 1: General Theorem for One-Dimensional Optimal Transport\nFor two probability mea-\nsures P and Q on R with cumulative distribution functions (CDFs) FP and FQ, respectively, the\noptimal transport map in the W2 sense (which minimizes the expected squared Euclidean distance) is\ngiven by the monotone increasing function:\nT ∗= F −1\nQ\n◦FP .\nThis result follows from classical optimal transport theory for univariate distributions (see Remark\n2.30 in Peyr´e & Cuturi (2020)).\nStep 2: Applying to Our Case\nFor each element d, the source distribution is given by the weighted\nsum:\nϵ(d) =\nK\nX\nk=1\nwkϵ(d)\nk .\nEach ϵ(d)\nk\nis obtained by transforming the original variable x(d)\nk\nusing:\nϵ(d)\nk\n= Φ−1\nϵ (Φd(x(d)\nk )).\nSince the ϵ(d)\nk\nare independent standard normal variables, their weighted sum follows:\nϵ(d) ∼N(0, β),\nwhere\nβ =\nK\nX\nk=1\nw2\nk.\nTo map this distribution to the standard normal N(0, 1), the optimal transport map for Gaussians\n(derived in Section B) is:\nTw,0(ϵ(d)) = ϵ(d)\n√β .\nApplying the standard normal CDF Φϵ to this transformation ensures that:\nΦϵ(Tw,0(ϵ(d))) ∼U(0, 1).\nBy the general optimal transport theorem stated in Step 1, the optimal transport map to the target\ndistribution pd is then:\nz(d) = Φ−1\nd (Φϵ(Tw,0(ϵ(d)))).\n15\n"
    },
    {
      "page_number": 16,
      "text": "Published as a conference paper at ICLR 2025\nSince each element d is mapped independently using the element-wise Monge optimal transport map,\nthe full transformation T{w} is optimal in the Monge sense, minimizing the quadratic Wasserstein\ndistance W2.\nThus, T{w} is the Monge optimal transport map from the distribution of the weighted sum of\nindependent elements to the target latent distribution p, completing the proof.\nB\nOPTIMAL TRANSPORT MAP FOR GAUSSIAN LATENT LINEAR COMBINATIONS\nIn Section 4 we introduced LOL, which we use to transform random variables arising from linear com-\nbinations — for example interpolations or subspace projections — to the latent distribution N(µ, Σ)\nvia a transport map Tw,µ defined per set of linear combination weights w = [w1, . . . , wK}, wk ∈R.\nWe will now derive the distribution of a (uncorrected) linear combination y — which we show does\nnot match the latent distribution — followed by the distribution of the transformed variable z, which\nwe show does. We will then show that the proposed transport map Tw,µ is Monge optimal.\nLemma 3. Let y be a linear combination of K i.i.d. random variables xk ∼N(µ, Σ), where y is\ndefined as\ny :=\nK\nX\nk=1\nwkxk = wT X,\nwith wk ∈R, y ∈RD, xk ∈RD, w = [w1, w2, . . . , wK], K ∈N>0 and X = [x1, x2, . . . , xK].\nThen y is a Gaussian random variable with the distribution\ny ∼N(αµ, βΣ),\nwhere\nα =\nK\nX\nk=1\nwk\nand\nβ =\nK\nX\nk=1\nw2\nk.\nProof. Given xk ∼N(µ, Σ), we know that each xk has mean µ and covariance matrix Σ.\nDefine y = wT X = PK\nk=1 wkxk.\nFirst, we calculate the mean of y:\nE[y] = E\n\" K\nX\nk=1\nwkxk\n#\n=\nK\nX\nk=1\nwkE[xk] =\nK\nX\nk=1\nwkµ =\n K\nX\nk=1\nwk\n!\nµ = αµ.\nNext, we calculate the covariance of y:\nCov(y) = Cov\n K\nX\nk=1\nwkxk\n!\n=\nK\nX\nk=1\nw2\nkCov(xk),\nsince the xk are i.i.d. and thus Cov(x(i), x(j)) = 0 for i ̸= j.\nGiven that xk ∼N(µ, Σ), we have Cov(xk) = Σ. Therefore,\nCov(y) =\nK\nX\nk=1\nw2\nkΣ =\n K\nX\nk=1\nw2\nk\n!\nΣ = βΣ.\nHence, y ∼N(αµ, βΣ) with α = PK\nk=1 wk and β = PK\nk=1 w2\nk.\n16\n"
    },
    {
      "page_number": 17,
      "text": "Published as a conference paper at ICLR 2025\nLemma 4. Let z be defined as\nz = Tw,µ(y)\nTw,µ(y) = (1 −α\n√β )µ + y\n√β\nα =\nK\nX\nk=1\nwk\nβ =\nK\nX\nk=1\nw2\nk\nwith y ∼N(αµ, βΣ). Then z ∼N(µ, Σ).\nProof. Given y ∼N(αµ, βΣ), we need to show that z ∼N(µ, Σ).\nFirst, we calculate the mean of z:\nE[z] = E [a + By] = a + BE[y] =\n\u0012\n1 −α\n√β\n\u0013\nµ +\n1\n√β αµ = µ,\nwhere\na =\n\u0012\n1 −α\n√β\n\u0013\nµ\nand\nB =\n1\n√β I.\nNext, we calculate the covariance of z:\nCov(z) = Cov (a + By) = BCov(y)BT =\n1\n√β βΣ 1\n√β = Σ.\nSince z has mean µ and covariance Σ, we conclude that\nz ∼N(µ, Σ).\nLemma 5. The transport map Tw,µ defined by\nTw,µ(y) = (1 −α\n√β )µ + y\n√β\nis the Monge optimal transport map, minimizing the quadratic Wasserstein distance W2 between\nN(αµ, βΣ) and N(µ, Σ).\nProof. In optimal transport theory, the Monge problem seeks a transport map T : RD →RD\nthat pushes forward the source distribution N(µy, Σy) to the target distribution N(µz, Σz) while\nminimizing the expected squared Euclidean distance:\nW 2\n2 (N(µy, Σy), N(µz, Σz)) = inf\nT ∈M E[∥T (y) −y∥2].\nThe optimal transport map for Gaussian distributions is given by the affine transformation:\nT (y) = µz + A(y −µy),\nwhere A is defined as\nA := Σ−1/2\ny\n(Σ1/2\ny\nΣzΣ1/2\ny\n)1/2Σ−1/2\ny\n.\nSee Remark 2.31 in Peyr´e & Cuturi (2020).\nSetting the parameters\nµy = αµ,\nΣy = βΣ,\n(5)\nµz = µ,\nΣz = Σ,\n(6)\n17\n"
    },
    {
      "page_number": 18,
      "text": "Published as a conference paper at ICLR 2025\nwe compute A:\nA = (βΣ)−1/2((βΣ)1/2Σ(βΣ)1/2)1/2(βΣ)−1/2 =\n1\n√β I.\n(7)\nThus, the optimal transport map simplifies to\nz := T (y) = µ +\n1\n√β (y −αµ) = (1 −α\n√β )µ + y\n√β .\n(8)\nSince this matches exactly with Tw,µ, we conclude that Tw,µ is the Monge optimal transport map\nminimising the quadratic Wasserstein distance.\nC\nSPHERICAL INTERPOLANTS OF HIGH-DIMENSIONAL UNIT GAUSSIAN\nRANDOM VARIABLES ARE APPROXIMATELY UNIT GAUSSIAN\nSpherical linear interpolation (SLERP) (Shoemake, 1985) is defined as\ny = w1x1 + w2x2\n(9)\nwhere\nwi := sin viθ\nsin θ ,\n(10)\nvi ∈[0, 1] and v2 = 1 −v1 and cos θ =\n⟨x1,x2⟩\n||x1||||x2||, where cos θ is typically referred to as the cosine\nsimilarity of x1 and x2.\nAs such, using Equation 2, we obtain\nα =\nK\nX\nk=1\nwk = sin v1θ\nsin θ\n+ sin v2θ\nsin θ\nand\nβ = sin2 v1θ\nsin2 θ\n+ sin2 v2θ\nsin2 θ\n(11)\nAs discussed in Section 4, for a linear combination y to be a random variable following distribution\nN(µ, Σ), given that x1 and x2 do, it must be true that αµ = µ and βΣ = Σ.\nA common case is using unit Gaussian latents (as in e.g. the models Esser et al. (2024) and Rombach\net al. (2022) used in this paper), i.e. where µ = 0, Σ = I. In this case it trivially follows that\nαµ = µ since α0 = 0. We will now show that β ≈1 in this special (i.e. unit Gaussian) case.\nLemma 6. Let\nβ = sin2 vθ\nsin2 θ + sin2(1 −v)θ\nsin2 θ\n,\nwhere cos θ =\n⟨x1,x2⟩\n∥x1∥∥x2∥and x1, x2 ∼N(0, I). Then β ≈1 for large D, ∀v ∈[0, 1].\nProof. Since x1, x2 ∼N(0, I), each component x1j and x2j for j = 1, . . . , D are independent\nstandard normal random variables. The inner product ⟨x1, x2⟩is given by:\n⟨x1, x2⟩=\nD\nX\nj=1\nx1jx2j.\nThe product x1jx2j follows a distribution known as the standard normal product distribution. For\nlarge D, the sum of these products is approximately normal due to the Central Limit Theorem (CLT),\nwith:\n⟨x1, x2⟩∼N(0, D).\n18\n"
    },
    {
      "page_number": 19,
      "text": "Published as a conference paper at ICLR 2025\nNext, consider the norms ∥x1∥and ∥x2∥. Each ∥xi∥2 = PD\nj=1 x2\nij is a chi-squared random variable\nwith D degrees of freedom. For large D, by the central limit theorem, ∥xi∥2 ∼N(D, 2D), and\ntherefore ∥xi∥is approximately\n√\nD.\nThus, for large D,\ncos(θ) = ⟨x1, x2⟩\n∥x1∥∥x2∥≈N(0, D)\n√\nD ·\n√\nD\n= N(0, D)\nD\n= N\n\u0012\n0, 1\nD\n\u0013\n.\nThus, θ ≈π/2, which implies sin(θ) ≈1. Therefore:\nβ = sin2(vθ)\nsin2 θ\n+ sin2((1 −v)θ)\nsin2 θ\n≈sin2(vθ) + sin2((1 −v)θ).\nUsing the identity sin2(a) + sin2(b) = 1 −cos2(a −b),\nβ ≈1 −cos2(vθ −(1 −v)θ) = 1 −cos2(θ)\nGiven v ∈[0, 1] and θ ≈π/2 for large D, the argument of cos remains small, leading to cos(·) ≈0.\nHence,\nβ ≈1.\nTherefore, for large D, it follows that β ≈1.\nEmpirical verification\nTo confirm the effectiveness of the approximations used above for values\nof D which are typical for popular generative models that use unit Gaussian latents, we estimate the\nconfidence interval of β using 10k samples of x1 and x2, respectively, where xi ∼N(0, I). For\nStable Diffusion 3 (Esser et al., 2024), a flow matching model with D = 147456, the estimated 99%\nconfidence interval of β is [0.9934, 1.0067] for v = 0.5 where the error is largest. For Stable Diffusion\n2.1 (Rombach et al., 2022), a diffusion model with D = 36864, the corresponding confidence interval\nof β is [0.9868, 1.014].\nD\nLINEAR COMBINATION WEIGHTS OF SUBSPACE PROJECTIONS\nIn Section 4 we introduced LOL, which we use to transform latent variables arising from linear\ncombinations such that they maintain the the latent distribution. This transformation depends on the\nweights w which specify the linear combination. We will now derive the linear combination weights\nfor subspace projections.\nLemma 7. Let U ∈RD×K be a semi-orthonormal matrix. For a given point x ∈RD, the subspace\nprojection is s(x) = UU T x. The weights w ∈RK such that s(x) is a linear combination of\nx1, x2, . . . , xK (columns of A = [x1, x2, . . . , xK] ∈RD×K) can be expressed as w = A†s(x),\nwhere A† is the Moore-Penrose inverse of A.\nProof. The subspace projection s(x) of x ∈RD is defined as:\ns(x) = UU T x.\nWe aim to express s(x) as a linear combination of the columns of A = [x1, x2, . . . , xK] ∈RD×K.\nThat is, we seek w ∈RK such that:\ns(x) = Aw.\nBy definition, the Moore-Penrose inverse A† of A satisfies the following properties:\n1. AA†A = A\n2. A†AA† = A†\n19\n"
    },
    {
      "page_number": 20,
      "text": "Published as a conference paper at ICLR 2025\n3. (AA†)T = AA†\n4. (A†A)T = A†A\nSince s(x) is in the subspace spanned by the columns of A, there exists a w such that:\ns(x) = Aw.\nConsider a w′ ∈RK constructed using the Moore-Penrose inverse A†:\nw′ = A†s(x).\nWe now verify that this w′ satisfies the required equation. Substituting back\nAw′ = A(A†s(x))\nand using the property of the Moore-Penrose inverse AA†A = A, we get:\nAA†s(x) = s(x).\nThus:\nAw′ = s(x),\nwhich shows that w′ = A†s(x) is indeed the correct expression for the weights.\nFrom uniqueness of w for a given set of columns x1, x2, . . . , xK (see Lemma 8), we have proven\nthat the weights w for a given point in the subspace s(x) are given by:\nw = A†s(x).\nLemma 8. The weights w ∈RK such that s(x) is a linear combination of x1, x2, . . . , xK (columns\nof A = [x1, x2, . . . , xK] ∈RD×K) are unique.\nProof. Suppose there exist two different weight vectors w1 and w2 such that both satisfy the equation:\ns(x) = Aw1 = Aw2.\nThen, subtracting these two equations gives:\nAw1 −Aw2 = 0.\nThis simplifies to:\nA(w1 −w2) = 0.\nLet v = w1 −w2. Then:\nAv = 0.\nSince v ∈RK, this equation implies that v lies in the null space of A. However, the assumption that\nA has full column rank (since A is used to represent a linear combination for s(x)) implies that A\nhas no non-zero vector in its null space, i.e., Av = 0 only when v = 0.\nTherefore:\nv = 0 =⇒w1 = w2.\nThis shows that the weights w are unique, and there cannot be two distinct sets of weights w1 and\nw2 that satisfy the equation s(x) = Aw.\nHence, we conclude that the weights w such that s(x) is a linear combination of x1, x2, . . . , xK are\nunique.\n20\n"
    },
    {
      "page_number": 21,
      "text": "Published as a conference paper at ICLR 2025\nE\nNORMALITY TEST COMPARISON\nWe will now investigate the effectiveness of a range of classic normality tests and tests based on the\nlikelihood N(x∗; µ, Σ) as well as the likelihood of the norm statistic. We assess these methods across\na suite of test cases including for vectors with unlikely characteristics and which we know, through\nfailed generation, violate the assumptions of the model. In order to improve statistical power and\ndetect even small deviations from normality of a single instance x∗∈RD, we transform the single\nhigh-dimensional latent into a large collection of (D) i.i.d. unit Gaussian samples {ϵi}D\ni=1, and test\nthe hypothesis ϵ1, . . . , ϵd ∼N(0, 1) where ϵ = Σ−1(x −µ), ϵ = [ϵ1, . . . , ϵd]. Note that rejecting\nthe hypothesis ϵ1, . . . , ϵd ∼N(0, 1) corresponds to rejecting the hypothesis that x∗∼N(µ, Σ).\nWe compare popular alternatives for normality testing that assume a known mean and variance and\nthat are applicable to very large sample sizes, including Kolmogorov-Smirnov (Kolmogorov, 1933),\nShapiro-Wilk (Shapiro & Wilk, 1965), Chi-square (Pearson, 1900), and Cram´er–von Mises (Cram´er,\n1928). We assess these methods on a suite of test cases of vectors which are extremely unlikely\nunder the model’s latent distribution and which we know through failed generation violates the\ncharacteristic assumptions of the diffusion model, as well as assess the ”positive case” where the\nsample does follow the latent distribution, x∗∼N(µ, Σ). We report the [0.1%, 99.9%]-confidence\ninterval of the p-value produced by each respective method on 1e4 random samples in the stochastic\ntest cases. We also include the likelihood N(x∗; µ, Σ) and the likelihood of the norm statistic.\nThe results are reported in Table 2. We find that Kolmogorov-Smirnov and Cram´er–von Mises both\nsucceed in reporting a lower 99.9th percentile p-value for each tested failure case than the 0.1th\npercentile assigned to real samples. Kolmogorov-Smirnov did so with the largest gap, reporting\nextremely low p-values for all failure cases while reporting calibrated p-values for the positive case.\nShapiro-Wilk, which in the literature often is regarded as one of the most powerful tests (Razali\net al., 2011), did not work well in our context in contrast to the other normality testing methods, as\nit produced high p-values also for several of the failure cases. An explanation may be that in our\ncontext we have sample sizes of tens of thousands (same as the dimensionality of the latent D), while\ncomparisons in the literature typically focuses on smaller sample sizes, such as up to hundreds or a\nfew thousand (Razali et al., 2011). The Chi-square test is a test for discrete distributions. However, it\nis commonly used on binned data (Larsen & Marx, 2005) to approximate a continuous distribution as\ndiscrete. We do this, using histograms of 30 bins. This test successfully produced very low p-values\nfor each failure case, but also did so for many valid samples, yielding a 0.1% of 0. The likelihood\nN(x∗; µ, Σ) assigns high likelihoods to vectors x∗near the mode irrespective of its characteristics.\nWe note that it fails to distinguish to failure cases from real samples, and in some cases assigns\nmuch higher likelihood to the failure cases than real data. The insufficiency of the likelihood to\ndescribe the feasibility of samples is a phenomenon present in cases of high-dimensionality and\nlow joint dependence between these dimensions; this is due to the quickly shrinking concentration\nof points near the centre (or close to µ, where the likelihood is highest) as the dimensionality is\nincreased (Talagrand, 1995; Ash, 2012; Nalisnick et al., 2019). The norm statistic we note successfully\nassigns low likelihoods to some failure cases, where the norm is atypical, but fails to do so in most of\nthe tested cases.\nIn summary, we find that Kolmogorov-Smirnov and Cram´er–von Mises both correctly rejects the\nfailure cases (by assigning low p-values) while reporting calibrated p-values for real samples, with\nKolmogorov-Smirnov doing so with the lowest p-values for all failure cases.\nF\nINTERPOLATION AND CENTROID DETERMINATION SETUP DETAILS\nBaselines\nFor the application of interpolation, we compare to linear interpolation (LERP), spherical\nlinear interpolation (SLERP), and Norm-Aware Optimization (NAO) (Samuel et al., 2023), a recent\napproach which considers the norm of the noise vectors. In contrast to the other approaches which\nonly involve closed-form expressions, NAO involves a numerical optimization scheme based on a\ndiscretion of a line integral. For the application of centroid determination we compare to NAO, the\nEuclidean centroid ¯x = 1\nK\nPK\nk=1 xk, and two transformations of the Euclidean centroid; ”standard-\nised Euclidean”, where ¯x is subsequently standardised to have mean zero and unit variance, and\n”mode norm Euclidean”, where ¯x is rescaled to have the norm equal to the (square root of the) mode\n21\n"
    },
    {
      "page_number": 22,
      "text": "Published as a conference paper at ICLR 2025\nTest case:\nx(i) ∼N(0, I)\n0\ns1\nn(1)\nn(ramp)\n0.1x(i)\nx(i) + 0.1\nn(duplicates)\nn(5% rescaled) n(1% rescaled)\nMethod/value\nlog N(x(i); 0, I)\n[−5e4, −5e4]\n−3e4\n−5e4\n−3e4\n−5e4\n[−3e4, −3e4]\n[−5e4, −5e4]\n[−5e4, −5e4]\n[−5e4, −5e4]\n[−5e4, −5e4]\nlog χ2(||x(i)||2; D)\n[−1e1, −7]\n−∞\n−7\n−∞\n−7\n[−7e4, −7e4]\n[−2e1, −7]\n[−7, −7]\n[−7, −7]\n[−7, −7]\nShapiro-Wilk\n[2e−4, 1]\n1\n1\n1\n5e−72\n[8e−3, 1]\n[8e−3, 1]\n[1e−50, 8e−19]\n[3e−105, 1e−99]\n[2e−85, 2e−69]\nChi-square\n[0, 1]\n0\n0\n0\n0\n[0, 0]\n[6e−149, 9e−45]\n[0, 1e−216]\n[0, 0]\n[0, 0]\nCramer-von-Mises\n[2e−4, 1]\n5e−7\n0.0\n5e−7\n1e−8\n[4e−7, 4e−7]\n[2e−9, 2e−8]\n[4e−12, 7e−4]\n[2e−8, 6e−8]\n[2e−11, 5e−9]\nKolmogorov-Smirnov\n[5e−4, 1]\n0\n0\n0\n2e−105\n[0, 0]\n[9e−77, 9e−37]\n[1e−129, 7e−10]\n[8e−252, 2e−177]\n[1e−31, 2e−12]\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTable 2: Normality testing of latents Reported is the [0.1%, 99.9%]-confidence interval of the p-\nvalue produced by each respective normality testing method on 1e4 random samples in the stochastic\ntest cases. We also report the log likelihood of the vector x(i) under the latent distribution and the\nlikelihood of the norm statistic. The diffusion model in Rombach et al. (2022) is used in this example.\nGreen colour for the respective failure test case indicate the method assigning a lower p-value or\nlikelihood to the failure case to at least 99.9% of the failure samples than the lowest 0.1% of real\nsamples, to illustrate that it was successful in distinguishing the failure samples. Chi-square assigns\nlow p-values to many real samples as well, which we indicate in red. The images show the generated\nimage from a random failure sample of the test case. In (failure) Test 1 in column 1 the latent is 0,\nthe mode of the latent distribution for this model N(0, I). In Test 2 it is the constant vector with\nthe maximum likelihood norm of the norm sampling distribution. In Test 3 the constant vector is\ninstead normalised to have its first two moments matching the latent’s marginal distribution. n(·)\nindicate that the vector(s) of the test case is normalised to have zero mean and unit variance. In Test 4\na linspace vector [−1, . . . , 1]D have been similarly normalised. In Test 5 and 6 the normal samples\n(from the left most column) have been transformed with scaling and a bias, respectively. In Test 7\nrandom selections of 1% of the dimensions [1, · · · , D] of the normal samples have been repeated\n(100 times) and subsequently normalised by n(·). In Test 8 and 9 a proportion of the dimensions of\nthe normal samples have been multiplied by 5 (with indices selected randomly) and the whole vectors\nare subsequently normalised by n(·), where the proportion is 5% and 1%, respectively.\nof χ2(D), the chi-squared distribution with D degrees of freedom, which is the maximum likelihood\nnorm given that x has been generated from a unit Gaussian with D dimensions.\nEvaluation sets\nWe closely follow the evaluation protocol in Samuel et al. (2023), where we base\nthe experiments on Stable Diffusion 2 (Rombach et al., 2022) and inversions of random images from\nImageNet1k (Deng et al., 2009). We (uniformly) randomly select 50 classes, each from which we\nrandomly select 50 unique images, and find their corresponding latents through DDIM inversion (Song\net al., 2020a) using the class name as prompt. We note that the DDIM inversion can be sensitive to\nthe number of steps. Therefore, we made sure to use a sufficient number of steps for the inversion\n(we used 400 steps), which we then matched for the generation; see Figure 15 for an illustration\nof the importance of using a sufficient number of steps. We used the guidance scale 1.0 (i.e. no\nguidance) for the inversion, which was then matched during generation. Note that using no guidance\nis important both for accurate inversion as well as to not introduce a factor (the dynamics of prompt\nguidance) which would be specific to the Stable Diffusion 2.1 model.\nFor the interpolation setup we randomly (without replacement) pair the 50 images per class into 25\npairs, forming 1250 image pairs in total. In between the ends of each respective pair, each method then\nis to produce three interpolation points (and images). For the NAO method, which needs additional\ninterpolation points to approximate the line integral, we used 11 interpolation points and selected\nthree points from these at uniform (index) distance, similar to in Samuel et al. (2023).\nFor the centroid determination setup we for each class form 10 3-groups, 10 5-groups, 4 10-groups\nand 1 25-group, sampled without replacement per group1; i.e. each method is to form 25 centroids\nper class total, for an overall total of 1250 centroids per method. Similarly to the interpolation setup,\nfor NAO we used 11 interpolations points per path, which for their centroid determination method\nentails K paths per centroid.\n1But with replacement across groups, i.e. the groups are sampled independently from the same collection of\nimages.\n22\n"
    },
    {
      "page_number": 23,
      "text": "Published as a conference paper at ICLR 2025\nFigure 8: Low p-values are inherited by interpolants The x-axis show the lower Kolmogorov-\nSmirnov p-value of the two seed latents for each corresponding spherical interpolation in Figure 5,\nand the y-axis show the p-value of the resulting interpolant. Due to the vast dynamic range of the\np-values, the scatter plot is shown in full on the left, and zoomed in on the upper-right quadrant on\nthe right. We note that when any of the two seed latents have a small p-value this largely tend to be\ninherited by the interpolant. The Pearson correlation coefficient is 0.79 for the indicator of acceptance\n(defined as 1 if the p-value is greater than 1e−3 and 0 otherwise) and 0.66 for the log10 p-values.\nEvaluation\nWe, as in Samuel et al. (2023), assess the methods quantitatively based on visual quality\nand preservation of semantics using FID distances and class prediction accuracy, respectively.\nThe FID distances are computed using the pytorch-fid library (Seitzer, 2020), using all evaluation im-\nages produced per method for the interpolation and centroid determination respectively, to maximize\nthe FID distance estimation accuracy.\nFor the classification accuracy, we used a pre-trained classifier, the MaxViT image classification\nmodel (Tu et al., 2022) as in Samuel et al. (2023), which achieves a top-1 of 88.53% and 98.64%\ntop-5 accuracy on the test-set of ImageNet.\nSee results in Section H.\nG\nADDITIONAL QUALITATIVE RESULTS\nSee Figure 9 for a demonstration of subspaces of latents without the LOL transformation introduced\nin Equation 3, using a diffusion model (Rombach et al., 2022) and a flow matching model (Esser\net al., 2024), respectively. The setup is identical (with the same original latents) as in Figure 12 and\nFigure 1, respectively, except without applying the proposed (LOL) transformation.\nSee Figure 10 for additional slices of the sports car subspace shown in Figure 1.\nSee Figure 12 and Figure 13 for Stable Diffusion 2.1 (SD2.1) versions of the Stable Diffusion 3 (SD3)\nexamples in Figure 1 and Figure 6, with an otherwise identical setup including the prompts. Just like\nin the SD3 examples LOL defines working subspaces. However, as expected since SD2.1 is an older\nmodel than SD3, the visual quality of the generations is better using SD3.\nSee Figure 11 for an interpolation example.\nIn the examples above the SD2.1 and SD3 models are provided with a text prompt during the\ngeneration. See Figure 14 for an example where the original latents ({xi}) were obtained using\nDDIM inversion (from images) without a prompt (and guidance scale 1.0, i.e. no guidance), allowing\ngeneration without a prompt. This allows us to also interpolate without conditioning on a prompt.\nWe note that, as expected without a prompt, the intermediates are not necessarily related to the end\npoints (the original images) but still realistic images are obtained as expected (except for using linear\ninterpolation, see discussion in Section 4) and the interpolations yield smooth gradual changes.\n23\n"
    },
    {
      "page_number": 24,
      "text": "Published as a conference paper at ICLR 2025\nx1\nx2\nx3\nx4\nx5\nx1\nx2\nx3\nx4\nx5\nFigure 9: Without LOL transformation. The setup here is exactly the same as in Figure 12 and\nFigure 1, respectively, except without the proposed (LOL) transformation (see Equation 3). The\nprompt used is ”A high-quality photo of a parked, colored sports car taken with a DLSR camera with\na 45.7MP sensor. The entire sports car is visible in the centre of the image. The background is simple\nand uncluttered to keep the focus on the sports car, with natural lighting enhancing its features.”.\nWe note that the diffusion model does not produce images of a car without the transformation, and\nneither model produce anything else than visually the same image for all coordinates.\nx1\nx2\nx3\nx4\nx5\nFigure 10: Additional slices of a sports car subspace. The latents x1, . . . , x5 (corresponding to\nimages) are converted into basis vectors and used to define a 5-dimensional subspace. The grids\nshow generations from uniform grid points in the subspace coordinate system, where the left and\nright grids are for the dimensions {1, 2} and {3, 4}, respectively, centered around the coordinate for\nx1. Each coordinate in the subspace correspond to a linear combination of the basis vectors. The\nflow matching model Stable Diffusion 3 (Esser et al., 2024) is used in this example. See Figure 1 for\nanother slice of the latent subspace.\nH\nADDITIONAL QUANTITATIVE RESULTS AND ANALYSIS\nIn Table 1 we show that our method outperforms or maintains the performance of the baselines\nin terms of FID distances and accuracy, as calculated using a pre-trained classifier following the\nevaluation methodology of Samuel et al. (2023). For an illustration of centroids and interpolations,\nsee Figure 2 and Figure 16, respectively. The evaluation time of an interpolation path and centroid,\nshown with one digit of precision, illustrate that the closed-form expressions are significantly faster\nthan NAO. Surprisingly, NAO did not perform as well as spherical interpolation and several other\nbaselines, despite using their implementation, which was outperforming these methods in Samuel\n24\n"
    },
    {
      "page_number": 25,
      "text": "Published as a conference paper at ICLR 2025\nx1\ninterpolations\nx2\nLERP\nSLERP\nNAO\nLOL\nFigure 11: Interpolation. Shown are generations from equidistant interpolations of latents x1 and\nx2 (in v ∈[0, 1]) using the respective method. The diffusion model Stable Diffusion 2.1 (Rombach\net al., 2022) is used in this example.\nx1\nx2\nx3\nx4\nx5\nFigure 12: Low-dimensional subspaces. The latents x1, . . . , x5 (corresponding to images) are\nconverted into basis vectors and used to define a 5-dimensional subspace. The grids show generations\nfrom uniform grid points in the subspace coordinate system, where the left and right grids are for the\ndimensions {1, 3} and {2, 4}, respectively, centered around the coordinate for x1. Each coordinate\nin the subspace correspond to a linear combination of the basis vectors. The diffusion model Stable\nDiffusion 2.1 (Rombach et al., 2022) is used in this example.\net al. (2023). We note one discrepancy is that we report FID distances using (2048) high-level features,\nwhile in their work they are using (64) low-level features, which in Seitzer (2020) is recommended\nagainst as it does not necessarily correlate with visual quality. In Table 3 we report results with all\nfeature settings. We note that, in our setup, the baselines perform substantially better than reported\nin Samuel et al. (2023) — including NAO in terms of FID distances using the 64 low-level feature\nsetting (1.30 in our setup vs 6.78 in their setup), and class accuracy during interpolation (62% vs\n52%), which we study below.\nThe number of DDIM inversion steps used for the ImageNet images is not reported in the NAO\nsetup (Samuel et al., 2023), but if we use 1 step instead of 400 (400 is what we use in all our\nexperiments) the class preserving accuracy of NAO for centroid determination, shown in Table 4,\nresemble theirs more, yielding 58%. As we illustrate in Figure 5 and Figure 15, a sufficient number of\ninversion steps is critical to acquire latents which not only reconstructs the original image but which\ncan be successfully interpolated. We hypothesise that a much lower setting of DDIM inversions\n25\n"
    },
    {
      "page_number": 26,
      "text": "Published as a conference paper at ICLR 2025\nx1\nx2\nx3\nx4\nx5\nFigure 13: Low-dimensional subspaces. The latents x1, . . . , x5 (corresponding to images) are\nconverted into basis vectors and used to define a 5-dimensional subspace. The grids show generations\nfrom uniform grid points in the subspace coordinate system, where the left and right grids are for the\ndimensions {1, 3} and {2, 4}, respectively, centered around the coordinate for x1. Each coordinate\nin the subspace correspond to a linear combination of the basis vectors. The diffusion model Stable\nDiffusion 2.1 (Rombach et al., 2022) is used in this example.\nsteps were used in their setup than in ours, which would be consistent with their norm-correcting\noptimization method having a large effect, making otherwise blurry/superimposed images intelligible\nfor the classification model.\nInterpolation\nMethod\nAccuracy\nFID 64\nFID 192\nFID 768\nFID 2048\nTime\nLERP\n3.92%\n63.4\n278\n2.18\n199\n6e−3s\nSLERP\n64.6%\n2.28\n4.95\n0.173\n42.6\n9e−3s\nNAO\n62.1%\n1.30\n4.11\n0.195\n46.0\n30s\nLOL (ours)\n67.4%\n1.72\n3.62\n0.156\n38.9\n6e−3s\nCentroid determination\nMethod\nAccuracy\nFID 64\nFID 192\nFID 768\nFID 2048\nTime\nEuclidean\n0.286%\n67.7\n317\n3.68\n310\n4e−4s\nStandardized Euclidean\n44.6%\n5.92\n21.0\n0.423\n88.8\n1e−3s\nMode norm Euclidean\n44.6%\n6.91\n21.6\n0.455\n88.4\n1e−3s\nNAO\n44.0%\n4.16\n15.6\n0.466\n93.0\n90s\nLOL (ours)\n46.3%\n9.38\n25.5\n0.455\n87.7\n6e−4s\nTable 3: Full table with class accuracy and FID distances using each setting in Seitzer (2020). Note\nthat FID 64, 192, and 768 is not recommended by Seitzer (2020), as it does not necessarily correlate\nwith visual quality. FID 2048 is based on the final features of InceptionV3 (as in Heusel et al. (2017)),\nwhile FID 64, 192 and 768 are based on earlier layers; FID 64 being the first layer.\nI\nADDITIONAL IMAGENET INTERPOLATIONS AND CENTROIDS\n26\n"
    },
    {
      "page_number": 27,
      "text": "Published as a conference paper at ICLR 2025\nx1\ninterpolations\nx2\nLERP\nSLERP\nNAO\nLOL\nFigure 14: Interpolation during unconditional generation. Shown are generations from equidistant\ninterpolations of latents x1 and x2 using the respective method. The latents x1 and x2 were obtained\nfrom DDIM inversion (Song et al., 2020a) with an empty prompt, and all generations in this example\nare then carried out with an empty prompt. The larger images show interpolation index 8 for SLERP,\nNAO and LOL, respectively. The diffusion model Stable Diffusion 2.1 (Rombach et al., 2022) is used\nin this example.\nCentroid determination of invalid latents\nMethod\nAccuracy\nFID 64\nFID 192\nFID 768\nFID 2048\nTime\nEuclidean\n42.3%\n25.7\n59.3\n1.23\n170\n4e−4s\nStandardized Euclidean\n46.6%\n3.35\n16.6\n1.23\n175\n1e−3s\nMode norm Euclidean\n50.6%\n2.08\n8.74\n1.19\n173\n1e−3s\nNAO\n57.7%\n3.77\n13.3\n1.05\n150\n90s\nLOL (ours)\n48.6%\n2.96\n12.5\n1.22\n171\n6e−4s\nTable 4: Centroid determination results if we would use a single step for both the DDIM inversion\nand generation (not recommended). This results in latents which do not follow the correct distribution\nN(µ, Σ) and which, despite reconstructing the original image exactly if generating also using a\nsingle step, cannot be used successfully for interpolation, see Figure 5. As illustrated in the figure,\ninterpolations of latents obtained through a single step of DDIM inversion merely results in the\ntwo images being superimposed. Note that the class accuracy is much higher for all methods on\nsuch superimposed images, compared to using DDIM inversion settings which allow for realistic\ninterpolations (see Table 3). Moreover, the FID distances set to use fewer features are lower for these\nsuperimposed images, while with recommended setting for FID distances (with 2048 features) they\nare higher (i.e. worse), as one would expect by visual inspection. The NAO method, initialized by the\nlatent yielding the superimposed image, then optimises the latent yielding a norm close to norms of\ntypical latents, which as indicated in the table leads to better ”class preservation accuracy”. However,\nthe images produced with this setting are clearly unusable as they do not look like real images.\n27\n"
    },
    {
      "page_number": 28,
      "text": "Published as a conference paper at ICLR 2025\n1 step\n2 steps\n16 steps\n100 steps\n500 steps\n999 steps\nImage 1\nLERP\nSLERP\nNAO\nLOL\nImage 2\nFigure 15: Accurate DDIM inversions are critical for interpolation. Shown is the interpolation\n(center) of Image 1 and Image 2 using each respective method, after a varying number of budgets\nof DDIM inversion steps (Song et al., 2020a). For each budget setting, the inversion was run\nfrom the beginning. We note that although a single step of DDIM inversion yields latents which\nperfectly reconstructs the original image such latents do not lead to realistic images, but merely visual\n”superpositions” of the images being interpolated.\n28\n"
    },
    {
      "page_number": 29,
      "text": "Published as a conference paper at ICLR 2025\nx1\ninterpolations\nx2\nLERP\nSLERP\nNAO\nLOL\nFigure 16: Interpolation. Shown are generations from equidistant interpolations of latents x1 and\nx2 using the respective method. The latents x1 and x2 were obtained from DDIM inversion (Song\net al., 2020a) of two random image examples from one of the randomly selected ImageNet classes\n(”Chesapeake Bay retriever”) described in Section 5. The larger images show interpolation index 6\nfor SLERP, NAO and LOL, respectively. The diffusion model Stable Diffusion 2.1 (Rombach et al.,\n2022) is used in this example.\nx1\nx2\nx3\nx4\nx5\nEuclidean\nS. Euclidean\nM.n. Euclidean\nNAO\nLOL\nFigure 17: ImageNet centroid determination. The centroid of the latents x1, x2, x3 as determined\nusing the different methods, with the result shown in the respective (right-most) plot. The diffusion\nmodel Stable Diffusion 2.1 (Rombach et al., 2022) is used in this example.\n29\n"
    },
    {
      "page_number": 30,
      "text": "Published as a conference paper at ICLR 2025\nx1\nx3\nx5\nx7\nx10\nEuclidean\nS. Euclidean\nM.n. Euclidean\nNAO\nLOL\nx1\nx5\nx10\nx15\nx25\nEuclidean\nS. Euclidean\nM.n. Euclidean\nNAO\nLOL\nFigure 18: Centroid determination of many latents. The centroid of the latents as determined\nusing the different methods, with the result shown in the respective (right-most) plot. Ten and 25\nlatents are used in the first two and second two examples (rows), respectively, and the plots show a\nsubset of these in the left-most plots. We noted during centroid determination of many latents (such\nas 10 and 25) that the centroids were often unsatisfactory using all methods; blurry, distorted etc. For\napplications needing to find meaningful centroids of a large number of latents, future work is needed.\nThe diffusion model Stable Diffusion 2.1 (Rombach et al., 2022) is used in this example.\n30\n"
    }
  ]
}