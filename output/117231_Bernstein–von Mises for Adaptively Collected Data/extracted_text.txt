=== Page 1 ===
Bernstein–von Mises for Adaptively Collected Data
Kevin Du
Department of Statistics
Harvard University
kevindu@college.harvard.edu
Yash Nair
Department of Statistics
Stanford University
yashnair@stanford.edu
Lucas Janson
Department of Statistics
Harvard University
ljanson@fas.harvard.edu
Abstract
Uncertainty quantification (UQ) for adaptively collected data, such as that coming
from adaptive experiments, bandits, or reinforcement learning, is necessary for
critical elements of data collection such as ensuring safety and conducting after-
study inference. The data’s adaptivity creates significant challenges for frequentist
UQ, yet Bayesian UQ remains the same as if the data were independent and
identically distributed (i.i.d.), making it an appealing and commonly used approach.
Bayesian UQ requires the (correct) specification of a prior distribution while
frequentist UQ does not, but for i.i.d. data the celebrated Bernstein–von Mises
theorem shows that as the sample size grows, the prior ‘washes out’ and Bayesian
UQ becomes frequentist-valid, implying that the choice of prior need not be a
major impediment to Bayesian UQ as it makes no difference asymptotically. This
paper for the first time extends the Bernstein–von Mises theorem to adaptively
collected data, proving asymptotic equivalence between Bayesian UQ and Wald-
type frequentist UQ in this challenging setting. Our result showing this asymptotic
agreement does not require the standard stability condition required by works
studying validity of Wald-type frequentist UQ; in cases where stability is satisfied,
our results combined with these prior studies of frequentist UQ imply frequentist
validity of Bayesian UQ. Counterintuitively however, they also provide a negative
result that Bayesian UQ is not asymptotically frequentist valid when stability fails,
despite the fact that the prior washes out and Bayesian UQ asymptotically matches
standard Wald-type frequentist UQ. We empirically validate our theory (positive
and negative) via a range of simulations.
1
Introduction
Data in applications such as robotics [19], healthcare [32], clinical trials [5], online education [27],
mobile health [2, 31], and online advertising [22, 10] is routinely being collected adaptively. This
means that the data is collected sequentially, with decisions about the data collection itself being
made online based on all the data observed up to that time point. In particular, the online decisions
often try to focus on actions or interventions that prior data indicates will produce large values of
some reward function, and this type of data collection includes adaptive experiments, multi-armed
bandits, and reinforcement learning.
Two important elements of adaptive data collection are (1) assessing at the end of the data collection
what has been learned (e.g., to assess confidence in a putatively optimal clinical treatment or inform
a future online advertising campaign) [33, 14] and (2) making sure to avoid certain bad outcomes
39th Conference on Neural Information Processing Systems


=== Page 2 ===
during the data collection process (e.g., avoiding crashing a robot while it is reinforcement learning)
[4]. Both of these elements, as well as others, rely critically on uncertainty quantification (UQ) of
the parameters of the data-generating environment, yet standard frequentist UQ, such as Wald-type
inference based on the maximum likelihood estimator (MLE), is made far more complicated by the
adaptivity of the data collection [21] which results in data which is not i.i.d., Markovian, or even
stationary.
On the other hand, the Bayesian statistical paradigm translates seamlessly to the adaptive setting, so
that the vast wealth of Bayesian UQ methodology designed for i.i.d. data can be directly applied to
adaptively collected data to provide strong Bayesian probabilistic guarantees. However, the validity
of those guarantees is predicated on correctly specifying a prior distribution for the parameters, which
can be challenging to do or justify in practice. For i.i.d. data, the celebrated Bernstein–von Mises
(BvM) theorem [29, Theorem 10.1] proves that for any reasonable choice of prior, Bayesian UQ
asymptotically matches that of prior-free Wald-type frequentist UQ, providing strong Bayesian and
frequentist justification for Bayesian UQ in large samples without the analyst having to worry too
much about the choice of prior. Although BvM has been extended beyond i.i.d. data in a number of
ways (see Section 2 for more details), it has not been extended to the setting of adaptively collected
data.
Contributions
This paper for the first time proves BvM results for adaptively collected data,
showing that Bayesian UQ asymptotically matches (asymptotically normal) Wald-type frequentist
UQ under certain mild conditions. Our first result, in Section 3, applies to a very general class of
adaptive linear Gaussian settings that include Gaussian multi-armed bandits, adaptive Gaussian linear
bandits (a form of contextual bandit), and the linear quadratic regulator (a form of reinforcement
learning on a Markov decision process). We then show in Section 4 that in the special case of
multi-armed bandits, the conditions of our first result can be weakened, and the Gaussian assumption
can also be generalized to any exponential family. Finally, we show in Section 5 that the conditions
of the first result can also be weakened for linear (contextual) bandits. Section 6 empirically validates
our theoretical results. A surprising aspect of our BvM results is that they do not require the key
stability condition used in frequentist validity results; however, it is known that Wald-type frequentist
inference may fail to be asymptotically valid in cases when the stability condition does not hold.
The counterintuitive implication is that in some adaptive settings, Bayesian UQ asymptotically
matches frequentist UQ (and the prior becomes irrelevant), yet Bayesian UQ is not asymptotically
frequentist-valid.
Notation and Terminology.
Throughout this paper, we refer to a confidence set as asymptotically
frequentist-valid at level α if for any parameter value, the limit inferior of the frequentist coverage
is at least 1 −α. Similarly, we refer to a credible set as asymptotically Bayesian-valid at level α if
the limit inferior of the Bayesian coverage is at least 1 −α. We will use the term frequentist (resp.
Bayesian) UQ generically to refer to any of the many forms of frequentist (resp. Bayesian) statistical
inference such as hypothesis tests, confidence intervals, or prediction intervals (resp. such as Bayesian
hypothesis tests, credible intervals, or posterior predictive intervals). By Wald-type frequentist UQ,
we refer to standard asymptotic hypothesis testing or confidence interval construction using the MLE
and its asymptotic Normality—for instance, as demonstrated in Example 15.6 of [29]. Let λmin(A)
and λmax(A) denote the minimum and maximum eigenvalues of the matrix A respectively.
2
Background
As mentioned in the previous section, Bayesian statistical inference requires no adjustment for the
adaptivity of the data. This both makes the Bayesian approach an appealing and commonly used
tool for UQ in adaptively collected data and means there is no need for (and thus a lack of) prior
work extending it to the adaptive setting. Thus, prior work on UQ for adaptively collected data has
focused on frequentist UQ, with the primary approach being to make assumptions on both the data
and the adaptive assignment algorithm by which it is collected. These assumptions either (1) enable
the use of martingale central limit theorems to establish asymptotic normality of common frequentist
estimators like the MLE (see, e.g., [21, 33, 14, 6, 12, 23]) or (2) allow for conservative finite-sample
inference via martingale-based concentration bounds [15, 16, 1]. These conditions are often highly
technical and hard to check, and reduce to or are related to a stability condition introduced in [21];
see Section 3 for more details. Another line of work uses randomization testing for frequentist UQ
2


=== Page 3 ===
for adaptively collected data, providing non-asymptotic and non-conservative guarantees but relying
on sampling procedures that can be computationally prohibitive to use [25].
The classical BvM theorem [29, Theorem 10.1] proves that under minimal conditions, for i.i.d. data
drawn from a parametric model, the posterior distribution is asymptotically normal centered at the
MLE with variance equal to the inverse Fisher information, and hence agrees asymptotically with
(and inherits the asymptotic frequentist validity of) Wald-style frequentist inference. Prior works
generalizing BvM to non-i.i.d. data only apply in settings where the ratio of the maximum and
minimum eigenvalues of the Fisher information matrix is bounded [18, 11, 8, 20, 7, 9]. This condition
fails to hold in most adaptive settings where, over time, certain actions are learned to be better than
others and are asymptotically sampled infinitely more often than the worse actions, resulting in
an unbounded ratio (e.g., regret-optimal multi-armed bandit algorithms sample suboptimal arms
logarithmically often, leading to an eigenvalue ratio that grows like n/ log(n) [3]).
3
BvM on Adaptive Linear Gaussian Data
This paper will primarily consider the adaptive data collection setting, laid out in Algorithm 1, that
at time step j allows the data collector to choose via an arbitrary function Λ a covariate vector
xj based on all data observed so far (Hj−1 = ((x1, y1), . . . , (xj−1, yj−1))), and then observe
yj ∼N(x⊤
j β, σ2). We assume the only unknown in this sampling procedure is β, and hence the task
at hand is to perform UQ for β based on the full data trajectory Hn = (Xn, yn), where Xn is the
n × p matrix with jth row given by x⊤
j and yn is the n-vector with jth entry given by yj.
Algorithm 1 Adaptive Linear Gaussian Sampling Procedure
Input Covariate sampling rule Λ : (Rp × R)∗→∆(Rp), coefficient vector β ∈Rp, variance
σ2 ∈R+
Output Sampling trajectory Hn
H0 ←∅
for j = 1, . . . , n do
Sample xj ∼Λ(·|Hj−1)
Sample yj ∼N(x⊤
j β, σ2)
Hj ←((x1, y1), . . . , (xj, yj))
end for
Here, Λ denotes a placeholder that can represent any sampling algorithm including UCB, Thompson
sampling, autoregressive models, etc. We now present our first result, which states that the posterior
distribution is asymptotically normal, centered at the MLE with variance equal to the inverse empirical
Fisher information.
Theorem 1. Suppose π(β) is the prior distribution for β and π(β|Hn) is the posterior after observing
the trajectory Hn. Let ˆβn = (X⊤
n Xn)−1X⊤
n yn be the MLE for β and let β0 be the true value of β.
Assume the sampling procedure in Algorithm 1 satisfies the following conditions:
(i) λmin(X⊤
n Xn)
p→∞and log λmax(X⊤
n Xn) = op(λmin(X⊤
n Xn)).
(ii) π(·) is continuous with positive density at β0.
Then, the posterior distribution π(β|Hn) satisfies
∥π(β|Hn) −N(ˆβn, σ2(X⊤
n Xn)−1)∥TV
p→0.
Theorem 1’s proof, given in Appendix B.1, first upper bounds the key total variation (TV) distance
in terms of the L1 distances of unnormalized densities, allowing us to ignore normalizing constants
in our analysis. Our proof then uses the fact that the posterior distribution is not affected by the
adaptivity of the algorithm, since terms in the likelihood involving the covariate sampling process Λ
do not depend on the parameter and can thus be absorbed into the normalizing constant. We also use
a common trick in BvM-style proofs [18, 29] of truncating the posterior to local ellipsoids defined as
3


=== Page 4 ===
{β : ∥(X⊤
n Xn)1/2(β −β0)∥≤Mn} for some local radius Mn. This truncation, however, differs
from those used in previous BvM-style proofs, as here the empirical Fisher information X⊤
n Xn/σ2
does not necessarily have bounded condition number, implying that the resulting ellipsoids may be
highly anisotropic, stretching much wider in some directions than in others. This step of our proof
relies critically on the second part of condition (i) of Theorem 1 which allows us to show that the
density of the representative normal distribution outside of local ellipsoids converges to zero. We
furthermore show that the posterior can be written as proportional to the prior multiplied by the
density of the representative normal, meaning we can truncate the distribution to a local ellipsoid
even if the prior is unbounded.
Our proof requires condition (i) for a couple of reasons. If the maximum eigenvalue of the observation
matrix grows exponentially faster than the minimum eigenvalue, there are two potential pathological
consequences. First, Lai and Wei showed that condition (i) of Theorem 1 is nearly necessary for
MLE consistency; in particular, if condition (i) of Theorem 1 is removed, the MLE is not necessarily
consistent [21, Example 1]. Second, even if ˆβn is consistent but condition (i) fails, it may no longer
be the case that the density of the distribution N(ˆβn, σ2(X⊤
n Xn)−1) at some other point β′ ̸= β0
converges to 0. One can see this by writing the density as
N(β′; ˆβn, σ2(X⊤
n Xn)−1) =
s
|X⊤
n Xn|
(2πσ2)p exp(−1
2σ2 (β′ −ˆβn)⊤X⊤
n Xn(β′ −ˆβn)),
where N(x; µ, σ2) denotes the density value at x of the N(µ, σ2) distribution. While the exponential
term in the above expression necessarily converges to zero if β′ ̸= β0, ˆβn is consistent, and
λmin(X⊤
n Xn)
p→∞, it is not necessarily true that the entire expression converges to zero or is
even bounded. This is because the quadratic term inside the exponential may only be of order
O(λmin(X⊤
n Xn)) whereas the term |X⊤
n Xn| grows at least as quickly as λmax(X⊤
n Xn). Thus,
although the measure of a neighborhood of β′ under the probability measure N(ˆβn, σ2(X⊤
n Xn)−1)
must converge to zero, without condition (i), it is possible that the density of this distribution diverges
at β′. We show in the proof of Theorem 1 that the likelihood function is proportional to the density
of N(ˆβn, σ2(X⊤
n Xn)−1), meaning that it might be possible that the density of both the prior and
the normalized likelihood function diverge at β′. This could potentially cause the posterior to have
asymptotically non-negligible measure in a neighborhood of β′, which would violate the BvM
statement as the measure of the normal distribution N(ˆβn, σ2(X⊤
n Xn)−1) converges to zero in a
neighborhood of β′.
Notably, Theorem 1 does not require the key stability condition, originally from [21] but used in
many works since then for frequentist UQ for adaptively collected data, which assumes the existence
of a deterministic sequence Bn for which B−1
n (X⊤
n Xn)1/2
p→Ip×p. Without this condition, ˆβn may
fail to be asymptotically normal [21, Example 3]. In Proposition 2 in Appendix D we show that their
example does, however, satisfy the conditions of our Theorem 1. This leads to the rather surprising
result that there are settings where Bayesian UQ is asymptotically equivalent to standard Wald-style
frequentist UQ, yet the latter (and therefore also the former) is asymptotically frequentist-invalid. We
will describe another such example in the next section, which requires the triangular array version of
BvM we prove there.
The previous paragraph notes that Theorem 1 can indicate either asymptotic frequentist validity
or invalidity of Bayesian UQ, depending on the situation. However, in terms of Bayesian validity,
Theorem 1 (and indeed all the theorems in the paper) provide a strong positive result regarding
misspecified priors: they say that the prior distribution π is ‘washed out’ as n →∞. Thus, from a
Bayesian perspective, a credible interval asymptotically has the correct coverage even if the prior
is misspecified, as long as both the correct prior and the misspecified prior used for the inference
are continuous and bounded, with the misspecified prior’s support containing that of the correct
prior. Note, however, that the rate at which the “wash out” effect occurs depends on the rate at
which the posterior distribution converges in TV distance to a normal distribution, which may be
logarithmically slow for optimal bandit algorithms. Thus, it may not be accurate in finite samples to
treat a misspecified prior as having “washed out”.
4


=== Page 5 ===
4
BvM on Multi-armed Bandits
A notable special case of Algorithm 1 is the multi-armed bandit setting, which corresponds to the case
when Λ’s output is supported only on the basis vectors. The multi-armed bandit setting can model
experiments involving adaptively chosen treatment arms—typically by some optimization algorithm
like UCB [3] or Thompson sampling [17]—where we want to estimate the mean outcome of each
arm. For the bandit setting, we will use the notation Ni,n, ˆµ(n)
i
to denote the count and sample mean
respectively of the pulls of arm i in the first n steps.
As discussed in the prior section, our BvM proof requires the assumption that the maximum eigenvalue
of the data matrix grows subexponentially with respect to the minimum eigenvalue. However, this
assumption is violated by many popular bandit algorithms such as UCB, where suboptimal arms are
pulled only Op(log n) times. Thus, we require a modification of the above proof to specifically the
case of bandits to include these existing algorithms. We also show that we can generalize our result to
triangular arrays of data—that is, we now allow the adaptive decision rule to depend on the sequence
length n and superscript it by n to make this dependence explicit: Λn. However, we assume that the
parameter β0, variance σ2, and prior π remain the same throughout the array. The triangular array
formulation enables us to extend our results to sampling procedures which have policies depending
on the length of the overall experiment such as is the case in the batched bandit setting [33]. First, we
show a consistency result for the case of triangular array bandits.
Lemma 1. Suppose independent length-mn trajectories Hn
mn = ((xn
1, yn
1 ), . . . , (xn
mn, yn
mn)) are
drawn via Algorithm 1 using sampling rules Λn for each n, where β and σ2 remain the same for all
trajectories. Let Xn =
 xn
1
· · ·
xn
mn
⊤and yn =
 yn
1
· · ·
yn
mn
⊤. Assume the triangular
array version of Algorithm 1 satisfies the following conditions:
(i) Each xn
j is a basis vector, i.e. xn
j ∈{e1, . . . , ep}.
(ii) λmin(X⊤
n Xn)
p→∞.
Then, the MLE ˆβn = (X⊤
n Xn)−1X⊤
n yn is consistent, i.e. ˆβn
p→β0.
The proof of Lemma 1 appears in Appendix C. In addition to this consistency result, we also need a
condition that allows us to truncate the posterior distribution. Note that the theorem below assumes
that the prior density π is bounded which we did not need in the proof of Theorem 1; this condition
guarantees that the posterior can be asymptotically approximated as proportional to the likelihood
when the regularity condition on the maximum eigenvalue is not met. With these changes, we no
longer need the condition that log λmax(X⊤
n Xn) = op(λmin(X⊤
n Xn)), making Theorem 2 very
broadly applicable to standard bandit algorithms.
Theorem 2. Assume the triangular array version of Algorithm 1 satisfies the following conditions:
(i) Each xn
j is a basis vector, i.e. xn
j ∈{e1, . . . , ep}.
(ii) λmin(X⊤
n Xn)
p→∞.
(iii) π(·) has continuous and bounded density on Rp which is positive at β0.
Then, the posterior distribution π(β|Hn
mn) satisfies
∥π(β|Hn
mn) −N(ˆβn, σ2(X⊤
n Xn)−1)∥TV
p→0.
Remark 1. This result assumes homoskedasticity of the bandits, but it also holds if each arm has a
different (but still known) variance σ2
1, . . . , σ2
p > 0, as shown in Theorem 5 in the Appendix E.
The proof of Theorem 2 appears in Appendix B.2. Note that the rate of convergence of the TV distance
in Theorem 2 depends on the growth rate of λmin(X⊤
n Xn), which can be logarithmically slow for
optimal bandit algorithms as seen in Section 6. Theorem 2 gives the following corollary (proven
in Appendix D) in the setting of non-triangular-array bandits, showing that the BvM convergence
statement is uniform over sampling rules as long as λmin(X⊤
n Xn) grows arbitrarily large among
these sampling rules.
5


=== Page 6 ===
Corollary 1. For any sequences (rn, ϵn) for which rn →∞and ϵn →0, let Pn be the sequence
of sets of distributions P of trajectories induced by sampling rules Λ such that for all n and for all
P ∈Pn,
(i) Each xn
j is a basis vector, i.e. xn
j ∈{e1, . . . , ep}.
(ii) P(λmin(X⊤
n Xn) > rn) > 1 −ϵn.
Then, we have for any c > 0,
lim sup
n→∞
sup
P ∈Pn
P(∥π(β|Hn) −N(ˆβn, σ2(X⊤
n Xn)−1)∥TV > c) = 0.
Theorem 2 implies that the interval [ˆµ(n)
i
± z1−α/2σN −1/2
i,n
] is an asymptotically valid Bayesian
credible interval. Note that this matches the Wald-type frequentist interval which one uses in i.i.d.
settings where the MLE is indeed normal. But [33] shows that for Thompson sampling in the two-arm
batched bandit setting, the distribution of the sample means is not asymptotically normal in the case
when the true arm means are equal. Thus, Theorem 2 implies that in this setting the credible interval
will fail to be asymptotically frequentist-valid despite asymptotically matching the usual Wald-type
frequentist confidence interval.
We can generalize this result beyond Gaussian bandits to exponential family bandits, i.e., where we
still constrain xj in Algorithm 1 to be a basis vector, but now instead of yj being sampled from a
Gaussian with mean x⊤
j β, it is sampled from a exponential family (with density exp(ηyj−b(η))h(yj))
with parameter η = x⊤
j β. Exponential family models include many common distribution types such
as Bernoulli, Poisson, Gamma, Beta, and Geometric distributions. For the remainder of our results in
this paper, we return to the non-triangular array setting where Λ is not allowed to depend on n.
Theorem 3. Let β0 ∈Rp be the true parameter value and β0,i be the i-th coordinate of β0. Let Nn,i
be the number of times arm i was pulled and ¯Yn,i =
1
Nn,i
Pn
j=1 I[xj = i]yj. Let the local MLE
be ˆβn,i = β0,i +
¯Yn,i−b′(β0,i)
b′′(β0,i)
and the empirical Fisher information be In = diag{Ni,nb′′(β0,i)}.
Suppose the exponential family version of Algorithm 1 satisfies the following properties:
(i) β0,1, . . . , β0,p are in the interior of the natural parameter space.
(ii) mini Nn,i
p→∞.
(iii) π(·) has continuous and bounded density on Rp which is positive at β0.
Then
∥π(β|Hn) −N(ˆβn, I−1
n )∥TV
p→0.
The proof of Theorem 3 appears in Appendix B.3. The local MLE defined above is the maximizer of
the second order Taylor expansion of the log-likelihood around β0, and it is asymptotically equivalent
to the true MLE by the asymptotic equivalence of the log-likelihood to its second-order Taylor
expansion in a local neighborhood of the true parameter value. Note that the above theorem requires
the true parameters to be in the interior of the natural parameter space, meaning for extremes of
common models such as Poisson with rate near 0 or Bernoulli with probability near 1, the conclusion
of the theorem may be a bad approximation for finite n.
The proof of this theorem is similar to that of Theorem 1, but here we require a Taylor expansion
to express the likelihood as a function proportional to a Gaussian density. The bandit setting is
particularly nice for performing this expansion since the likelihood factors into the product of
the likelihoods for each individual arm. Thus, even if the eigenvalues of In grow at asymptotically
different rates, the likelihood is still well-approximated by the second-order expansion. This argument
is harder when generalizing beyond bandits, as argued in Appendix F. Additionally, to argue that we
can truncate the posterior distribution to a local neighborhood, we use the convexity of b(·) to bound
the tails of the likelihood function. Our proof does not necessarily generalize to triangular arrays as it
relies on uniform bounds on a serialized sequence of samples, such as the one provided by the law of
iterated logarithms.
6


=== Page 7 ===
5
BvM on Gaussian Linear Bandits
Another special case for our BvM theorem is the case of Gaussian linear bandits, in which a
context xj is observed for each arm pull aj, impacting the resulting reward distribution through a
linear transformation, i.e. yj ∼N(x⊤
j θaj, σ2) where θ1, . . . , θm are parameter vectors. To model
contextual bandits with our adaptive linear Gaussian data process in Algorithm 1, we will let the
parameter be β ∈Rmd where the (id −d + 1)–(id)th indices of β represent θi; in other words, we
stack the parameter vectors θ1, . . . , θm vertically. Then, when we observe context x′ ∈Rd and action
i ∈{1, . . . , m}, we sample the outcome from N(β⊤x, σ2) where the (id −d + 1)–(id)th index of
x ∈Rmd represents x′. Then, we can state BvM in the contextual bandit setting as follows.
Theorem 4. Suppose p = md and we decompose the covariate space as
X :=
 Rd × {0d} × · · · × {0d}

∪
 {0d} × Rd × · · · × {0d}

∪· · · ∪
 {0d} × {0d} × · · · × Rd
Assume the sampling procedure in Algorithm 1 satisfies the following conditions:
(i) Each action xj falls in X.
(ii) For all i = 1, . . . , m, let Ti : X →Rd be a projection onto the (id −d + 1)–(id)th coordinates.
Letting In,i = Pn
j=1 Ti(xj)Ti(xj)⊤∈Rd×d, for all i we have
λmin(In,i)
p→∞and log λmax(In,i) = op(λmin(In,i)).
(iii) π(·) is continuous with bounded density on Rp and positive density at β0.
Then, the posterior distribution π(β|Hn) satisfies
∥π(β|Hn) −N(ˆβn, σ2(X⊤
n Xn)−1)∥TV
p→0.
The proof of Theorem 4 appears in Appendix B.4. Note that condition (ii) in Theorem 4 is different
from condition (i) in Theorem 1 because it concerns the distribution of contexts conditioned on a
particular arm, rather than the distribution of arm pulls. Thus, it is possible for a contextual bandit
algorithm to satisfy condition (ii) when it pulls different arms at exponentially different rates. This
result does not immediately generalize to triangular arrays, since we rely on consistency which may
not hold in the triangular array setting for contextual bandits. However, the non-triangular array
version of Theorem 2 is a special case of Theorem 4.
6
Numerical Experiments
We discuss the empirical validity of the statement of Theorem 1 in the setting of multi-armed ban-
dits, contextual bandits, and the linear quadratic regulator (LQR). As seen in the previous section,
the posterior distribution is asymptotically equivalent in TV distance to the normal distribution
N(ˆβn, σ2(X⊤
n Xn)−1), which we call the representative normal distribution. Although this equiva-
lence holds asymptotically, the convergence rate to this representative normal distribution may be
quite slow depending on the rate of growth of λmin(X⊤
n Xn). We empirically perform posterior
inference in three common adaptive settings and show that the posterior does empirically converge to
the representative normal.
We use Monte Carlo and the relation ∥P −Q∥TV = EX∼P
h
max

0, 1 −Q(X)
P (X)
i
to approximate
the TV distance between the posterior distribution and the representative normal distribution. As seen
in Figures 1, 2, and 3, the convergence rate of the TV distance depends on the configuration of the
true parameters. Note that in the multi-armed bandit setting, the convergence seems fastest when
the arm means are equal and becomes slower as the margin increases. One explanation for this is
that the result of Theorem 2 requires the condition λmin(X⊤
n Xn)
p→∞, suggesting that the rate at
which this minimum eigenvalue grows determines the rate of convergence of the TV distance. More
specifically, we see that in our proof of Theorem 2, we use the expression E

π(κn)
π(β0) −1

+

as an
7


=== Page 8 ===
Figure 1: (Left) Average TV distance measured in the BvM statement for UCB in two-arm Gaussian
bandits over horizon T = 104 using 104 replicates under five different true parameter configurations
labelled by [µ1, µ2] where µ1, µ2 are the true means. (Right) Average TV distance measured in the
BvM statement for lin-UCB on three-arm Gaussian linear contextual bandits with context distribution
N(0, I2×2) under three different true parameter configurations. Standard Gaussian priors are used
for all arms. TV estimates shown have standard error at most 0.1 times the TV estimate.
Figure 2: Average BvM TV distance for UCB on Bernoulli bandits and Poisson bandits, under the
same configurations as Figure 1. Beta(1, 1) priors are used for the Bernoulli bandit and Gamma(1, 1)
priors for the Poisson bandit. The representative normal is centered at the true MLE, which is
asymptotically equivalent to the local MLE used in Theorem 3. TV estimates shown have standard
error at most 0.1 times the TV estimate.
upper bound on the TV distance where κn|Hn
mn ∼N(ˆβn, σ2(X⊤
n Xn)−1). Thus, we see that the
rate at which λmin(X⊤
n Xn) diverges affects the convergence rate of κn
p→β0, which in turn affects
the convergence π(κn)
π(β0) −1
p→0. This explains why the TV distance converges fastest for the zero
margin setting (even though that setting does not satisfy stability).
We simulate the algorithm Lin-UCB in the setting of contextual bandits under three different parameter
configurations [22]. The “undominated” configuration represents a case where each arm is optimal
for some choice of context. In the “dominated” case, one arm is never optimal for any choice of
context, and in the “duplicate” case, two arms share the same parameters. In Figure 1, we empirically
see that the convergence of the posterior to the representative normal is fastest for the undominated
configuration, possibly because in this case, the optimal policy samples each arm with probability
bounded away from zero. In the configuration where two arms are duplicate, there may be substantial
bias in the estimation of the arm parameters [26], leading the “duplicate” case to have the slowest
convergence as seen in Figure 1.
We can also model the Linear Quadratic Regulator (LQR) [24], a common setting used in control
theory, using our adaptive linear Gaussian framework. In LQR, we control a state transition of the
8


=== Page 9 ===
form xj+1 = Axj + Buj + ϵj where A ∈Rk×k, B ∈Rk×d, uj are adaptively chosen actions,
ϵj ∼N(0, σ2Ik) for j = 1, . . . , n, and we aim to estimate the transition matrices A, B. To
represent this as an instance of Algorithm 1, we serialize the observed state vectors xj into the
sequence of observed outcome variables—that is, we use a trajectory of length ˜n = nk where
˜y(i−1)k+1, . . . , ˜yik represents xi. We let the parameter value β be (A
B) in row major order. Then,
the ((i −1)(k + d) + 1)–(i(k + d)) indices of the covariate ˜x(j−1)k+i ∈Rk(k+d) represent

xj
uj

and all other entries are zero.
We simulate the Noisy Certainty Equivalent Control (NCEC) algorithm on LQR under three different
configurations [30]. The “determined” configuration represents a case where the action space has the
same dimensionality as the state space and the action transition matrix is full rank. The “stabilizable”
case is underdetermined with fewer action dimensions than state dimensions but where the optimal
policy allows the system to be stable. The “unstabilizable” case is underdetermined where no policy
makes the system stable. In all settings, we empirically see that the BvM TV distance decreases over
time, with the convergence rate being the fastest for the unstabilizable case as it has the highest growth
rate of λmin(X⊤
n Xn). Finally, as we show in Proposition 3 in Appendix D, the NCEC algorithm
satisfies condition (i) in Theorem 1 so long as a certain stability condition holds (see Assumption 1
in the same appendix section for a precise definition); both our “determined” and “stabilizable”
simulation settings satisfy this condition.
We also simulate Thompson Sampling in the two-stage Gaussian batched bandit with two arms,
which is the setting analyzed in Theorem 2 of [33]. We compute the TV distance between the
posterior and the representative normal in the same way for different values of the margin. As shown
in Figure 3, we again see that the average BvM distance is smallest when the margin is zero. We
also plot the empirical coverage of the 95% credible interval for the margin, which matches the
coverage level except near the zero-margin case, as suggested by [33]. This is an example of a fairly
well-behaved sampling process where our BvM result applies but Bayesian credible intervals are
still not asymptotically frequentist-valid. Note that this setting satisfies the stability condition if and
only if the margin is nonzero. This suggests that the asymptotic frequentist invalidity of Bayesian
credible intervals is a local phenomenon around the zero-margin case. Figure 3 supports this analysis
and also reveals that in this local region, some parameter values lead to overcoverage and some to
undercoverage from a frequentist perspective. Our BvM result may provide an explanation for this, as
the Bayesian coverage under a prior containing this local zero-margin region must be correct, meaning
the coverage probability aggregated over the erroneous local region should match the coverage level.
Appendix G contains a demonstration of our main results on a simple real-world dataset. Coverage
plots for the other simulations are shown in Appendix H.
7
Discussion and Future Work
This paper has shown that, for a number of important classes of adaptively collected data, a Bernstein–
von Mises theorem applies, linking Bayesian UQ and Wald-type frequentist UQ. This ensures that
under extremely mild conditions, Bayesian UQ is asymptotically Bayesian-valid even when the
prior is misspecified, and when the stability condition of [21] holds, it also ensures Bayesian UQ is
asymptotically frequentist-valid.
One of the surprising takeaways from our work is that when the stability condition of [21] fails, BvM
holds but Bayesian UQ is asymptotically frequentist invalid. We note that our work, however, only
considers the case when a fixed, nonrandom prior is used for Bayesian UQ. This raises the question of
whether there is a data-dependent way to set the prior so that Bayesian UQ is always asymptotically
frequentist valid; we could think of such a method as empirical Bayesian UQ.
Another direction of inquiry would be to extend the BvM result to general parametric models beyond
just the Gaussian and exponential family cases. However, there are several obstacles to showing the
adaptive BvM result in more general parametric models which we discuss in Appendix F.
Note that this paper does not suggest a method of asymptotically frequentist-valid Bayesian inference;
in fact, we would like to warn practitioners against using either Wald-type frequentist UQ or Bayesian
UQ as if it were asymptotically frequentist-valid.
9


=== Page 10 ===
Figure 3: (Left) Average BvM TV distance and empirical coverage of the 95% credible interval
for the margin for Thompson Sampling in the two-batch two-arm Gaussian bandit setting with 104
samples per batch. Error bars are 95% confidence intervals over 2 × 105 replicates. Blacked dotted
line is the correct coverage level. N(0, 1) priors are used. (Right) Average TV distance for Noisy
Certainty Equivalent Control on LQR [30] under three different parameter configurations. Standard
Gaussian priors are used for all arms. TV estimates shown have standard error at most 0.2 times the
TV estimate.
Declaration of LLM usage:
ChatGPT-4o was used to create code templates for a Python implementation of the lin-UCB and
Stepwise Noisy Certainty Equivalent Control algorithms. The authors revised the templates to ensure
correct implementation and modified them to verify the BvM statement.
Broader Impact:
The results presented here have the potential to guide the design of safer systems and more reliable
hypothesis testing in adaptive experiments. However, it should be noted that the Bernstein–von Mises
theorem does not immediately imply frequentist-valid inference as discussed. Thus, Bayesian UQ
should not in general be treated as frequentist-valid. Instead, our result contributes to a more complete
understanding of the differences between frequentist and Bayesian approaches in adaptively collected
data.
References
[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear
stochastic bandits. Advances in neural information processing systems, 24, 2011.
[2] Adrian Aguilera, Caroline A Figueroa, Rosa Hernandez-Ramos, Urmimala Sarkar, Anupama
Cemballi, Laura Gomez-Pathak, Jose Miramontes, Elad Yom-Tov, Bibhas Chakraborty, Xiaoxi
Yan, et al. mhealth app using machine learning to increase physical activity in diabetes and
depression: clinical trial protocol for the diamante study. BMJ open, 10(8):e034723, 2020.
[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine learning, 47:235–256, 2002.
[4] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. Advances in neural information processing
systems, 30, 2017.
[5] Deepak L Bhatt and Cyrus Mehta. Adaptive designs for clinical trials. New England Journal of
Medicine, 375(1):65–74, 2016.
[6] Aurélien Bibaut, Maria Dimakopoulou, Nathan Kallus, Antoine Chambaz, and Mark van
Der Laan. Post-contextual-bandit inference. Advances in neural information processing systems,
34:28548–28559, 2021.
10


=== Page 11 ===
[7] Natalia Bochkina. Bernstein–von mises theorem and misspecified models: A review. Founda-
tions of modern statistics, pages 355–380, 2019.
[8] Natalia A Bochkina and Peter J Green. The bernstein–von mises theorem and nonregular
models. 2014.
[9] Ismaël Castillo and Judith Rousseau. A bernstein–von mises theorem for smooth functionals in
semiparametric models. 2015.
[10] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in
neural information processing systems, 24, 2011.
[11] Benjamin Connault. A weakly dependent bernstein–von mises theorem. Technical report,
Working Paper, 2014.
[12] Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, and Matt Taddy. Accurate inference for
adaptive linear models. In International Conference on Machine Learning, pages 1194–1203.
PMLR, 2018.
[13] Allan Gut. Stopped random walks. Springer, 2009.
[14] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence
intervals for policy evaluation in adaptive experiments. Proceedings of the national academy of
sciences, 118(15):e2014602118, 2021.
[15] Steven R Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon.
Time-uniform,
nonparametric, nonasymptotic confidence sequences. The Annals of Statistics, 49(2):1055–1080,
2021.
[16] Emilie Kaufmann and Wouter M Koolen. Mixture martingales revisited with applications to
sequential tests and confidence intervals. Journal of Machine Learning Research, 22(246):1–44,
2021.
[17] Emilie Kaufmann, Nathaniel Korda, and Rémi Munos. Thompson sampling: An asymptotically
optimal finite-time analysis. In International conference on algorithmic learning theory, pages
199–213. Springer, 2012.
[18] Bas JK Kleijn and Aad W Van der Vaart. The bernstein-von-mises theorem under misspecifica-
tion. 2012.
[19] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.
The International Journal of Robotics Research, 32(11):1238–1274, 2013.
[20] Geerten Koers, Botond Szabó, and Aad van der Vaart. Misspecified bernstein-von mises theorem
for hierarchical models. arXiv preprint arXiv:2308.07803, 2023.
[21] Tze Leung Lai and Ching Zong Wei. Least squares estimates in stochastic regression models
with applications to identification and control of dynamic systems. The Annals of Statistics,
pages 154–166, 1982.
[22] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pages 661–670, 2010.
[23] Alexander R Luedtke and Mark J Van Der Laan. Statistical inference for the mean outcome
under a possibly non-unique optimal treatment strategy. Annals of statistics, 44(2):713, 2016.
[24] Volker Ludwig Mehrmann. The autonomous linear quadratic control problem: theory and
numerical solution. Springer, 1991.
[25] Yash Nair and Lucas Janson. Randomization tests for adaptively collected data. arXiv preprint
arXiv:2301.05365, 2023.
11


=== Page 12 ===
[26] Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou. Why adaptively collected data
have negative bias and how to correct for it. In International Conference on Artificial Intelligence
and Statistics, pages 1261–1269. PMLR, 2018.
[27] Anna Rafferty, Huiji Ying, Joseph Williams, et al. Statistical consequences of using multi-armed
bandits to conduct adaptive educational experiments. Journal of Educational Data Mining,
11(1):47–79, 2019.
[28] Yuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. Large-scale open dataset,
pipeline, and benchmark for bandit algorithms. arXiv preprint arXiv:2008.07146, 2020.
[29] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
[30] Feicheng Wang and Lucas Janson. Exact asymptotics for linear quadratic adaptive control.
Journal of Machine Learning Research, 22(265):1–112, 2021.
[31] Elad Yom-Tov, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, and Irit
Hochberg. Encouraging physical activity in patients with diabetes: intervention using a rein-
forcement learning system. Journal of medical Internet research, 19(10):e338, 2017.
[32] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare:
A survey. ACM Computing Surveys (CSUR), 55(1):1–36, 2021.
[33] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in
neural information processing systems, 33:9818–9829, 2020.
12


=== Page 13 ===
NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In the abstract and motivation, we claim that the BvM theorem generalizes to
adaptive data without assuming a stability assumption on the data. This claim is shown in
the proof of Theorem 1 and intuition for this result is given in section 3 of the main text.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In section 7 of the text, we discuss the phenomenon that Bayesian credible
intervals may fail to be frequentist-valid and how our work does not explain how to charac-
terize this discrepancy. We also discuss how our BvM result requires the data to be generated
from an exponential family model which restricts the generality of the result.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory assumptions and proofs
13


=== Page 14 ===
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Our main results, Theorems 1, 2, 3, 4 are all proven in Appendix B, with the
supporting lemmas also proven in Appendix C.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental result reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The configurations for experiments are given in the figure captions of Figures 1,
2, 3 with the algorithms (lin-UCB and NCEC) cited. We also describe how the TV distance
was estimated in the text.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
14


=== Page 15 ===
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Access to the source code is provided in Appendix A.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental setting/details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Details for the experiment parameters are included in the captions of Figures 1,
2, 3 with full details included in the source code in Appendix A.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment statistical significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The caption of figures describes the standard error of estimates provided in the
experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
15


=== Page 16 ===
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments compute resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We mention in Appendix A that the data in each figure require under 3 hours
in CPU time to generate.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code of ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: Our work does not involve human subjects and our data do not contain any
private or personal information. We hope that the statistical inference methods provided in
this paper guides more reliable experimentation methods in adaptive design.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We briefly discuss the broader impacts of this work at the end of the main text.
Guidelines:
16


=== Page 17 ===
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper does not contain any results that can be misused.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This paper does not use existing code or data. The existing algorithms used in
the paper are implemented independently.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
17


=== Page 18 ===
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and research with human subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing or research with human subjects was done in this paper.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional review board (IRB) approvals or equivalent for research with human
subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing or research with human subjects was done in this paper.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
18


=== Page 19 ===
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
16. Declaration of LLM usage
Question: Does the paper describe the usage of LLMs if it is an important, original, or
non-standard component of the core methods in this research? Note that if the LLM is used
only for writing, editing, or formatting purposes and does not impact the core methodology,
scientific rigorousness, or originality of the research, declaration is not required.
Answer: [Yes]
Justification: The usage of LLMs for this project is described under the “Declaration of
LLM usage” section at the end of the main text.
Guidelines:
• The answer NA means that the core method development in this research does not
involve LLMs as any important, original, or non-standard components.
• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)
for what should or should not be described.
19


=== Page 20 ===
A
Source Code
The code for this project can be accessed at https://github.com/TheDukeVin/BvM/tree/main. Each
figure’s data takes under three hours of CPU time to generate.
B
Main Theorems
B.1
Proof of Theorem 1
We first write an explicit expression for the posterior. Note that the likelihood can be written as, after
removing constant factors that don’t depend on β,
L(β; Hn) =


n
Y
j=1
Λ(xj|Hj−1)




n
Y
j=1
P(yj|xj, β)


∝
n
Y
j=1
P(yj|xj, β) ∝
n
Y
j=1
exp
 
−(yj −x⊤
j β)2
2σ2
!
=
n
Y
j=1
exp
 
−(yj −ˆβ⊤
n xj)2
2σ2
−(ˆβ⊤
n xj −β⊤xj)2
2σ2
!
∝exp(−1
2σ2 (ˆβn −β)⊤X⊤
n Xn(ˆβn −β)).
Thus, by Bayes’ rule, the posterior can be written as
π(β|Hn) ∝π(β) exp(−1
2σ2 (ˆβn −β)⊤X⊤
n Xn(ˆβn −β)).
Let d(Hn) = ∥π(β|Hn) −N(ˆβn, σ2(X⊤
n Xn)−1)∥TV. By assumption, π(β0) > 0, so using Lemma
2, we have
d(Hn) ≤
Z  π(β)
π(β0)
1
p
(2πσ2)p|X⊤
n Xn|−1 exp(−1
2σ2 (β −ˆβn)⊤X⊤
n Xn(β −ˆβn))
−
1
p
(2πσ2)p|X⊤
n Xn|−1 exp(−1
2σ2 (β −ˆβn)⊤X⊤
n Xn(β −ˆβn))

+dβ
=
Z  π(β)
π(β0) −1

+
1
p
(2πσ2)p|X⊤
n Xn|−1 exp(−1
2σ2 (β −ˆβn)⊤X⊤
n Xn(β −ˆβn))dβ.
= E
"π(κn)
π(β0) −1

+
Hn
#
where κn is a random variable marginally distributed as κn|Hn ∼N(ˆβn, σ2(X⊤
n Xn)−1). Let cn
be a sequence of random variables satisfying
cn
log λmax(X⊤
n Xn)
p→∞and
cn
λmin(X⊤
n Xn)
p→0, which
must exist by condition (i), i.e. we can let cn =
p
λmin(X⊤
n Xn) log λmax(X⊤
n Xn). Then, to show
d(Hn)
p→0, we use a common trick when showing BvM-style results [29, 11, 18]—that is, we will
partition the expectation into parts inside and outside a local ellipsoid. We will show the following
two statements.
20


=== Page 21 ===
E
"π(κn)
π(β0) −1

+
1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 ≤cn]
Hn
#
p→0
(1)
E
"π(κn)
π(β0) −1

+
1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]
Hn
#
p→0
(2)
To show (1), note that we have
E
"π(κn)
π(β0) −1

+
1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 ≤cn]
Hn
#
≤
1
π(β0)E

(π(κn) −π(β0))+ 1[∥(κn −ˆβn)∥2 ≤
cn
λmin(X⊤
n Xn)]
Hn

≤
1
π(β0)
sup
∥κn−ˆβn∥≤
c1/2
n
λmin(X⊤
n Xn)1/2
(π(κn) −π(β0))+
≤
1
π(β0)
sup
∥κn−β0∥≤
c1/2
n
λmin(X⊤
n Xn)1/2 +∥ˆβn−β0∥
(π(κn) −π(β0))+
Lai and Wei showed that ˆβn is consistent if condition (i) is satisfied [21].
Thus, we have
c1/2
n
λmin(X⊤
n Xn)1/2 + ∥ˆβn −β0∥
p→0, meaning the above expression does indeed converge to zero
in probability if π is continuous at β0.
To show (2), note that by the triangle inequality, we have
E
"π(κn)
π(β0) −1

+
1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]
Hn
#
≤
1
π(β0)E
h
π(κn)1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]
Hn
i
+ E
h
1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]
Hn
i
The first term can then be bounded as
1
π(β0)E
h
π(κn)1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]
Hn
i
=
1
π(β0)
Z
π(κn)1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]P(κn|Hn)dκn
≤
1
π(β0)
Z
π(κn)1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]
1
p
(2πσ2)p|X⊤
n Xn|−1 exp(−cn
2σ2 )dκn
≤(2πσ2)−p/2
π(β0)
exp(−cn
2σ2 )λmax(X⊤
n Xn)p/2
Z
π(κn)1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]dκn
≤(2πσ2)−p/2
π(β0)
exp(−cn
2σ2 + p
2 log λmax(X⊤
n Xn))
Z
π(κn)dκn
= (2πσ2)−p/2
π(β0)
exp(−cn
2σ2 + p
2 log λmax(X⊤
n Xn))
21


=== Page 22 ===
If
cn
log λmax(X⊤
n Xn)
p→∞, the above expression converges to zero in probability. To compute the
second term, note that (X⊤
n Xn)1/2(κn −ˆβn)|Hn is distributed as N(0, σ2Ip). Thus, if cn
p→∞,
then E
h
1[∥(X⊤
n Xn)1/2(κn −ˆβn)∥2 > cn]
Hn
i
p→0. We have shown that each term (1) and (2) in
the decomposition of the upper bound for d(Hn) converges to zero in probability, meaning d(Hn)
converges to zero in probability as well.
B.2
Proof of Theorem 2
As in the proof of Theorem 1, the posterior distribution can be expressed as
π(β|Hn
mn) ∝π(β) exp(−1
2σ2 (ˆβn −β)⊤X⊤
n Xn(ˆβn −β)).
Letting d(Hn
mn) = ∥π(β|Hn
mn) −N(ˆβn, σ2(X⊤
n Xn)−1)∥TV, we also have
d(Hn
mn) ≤E
"π(κn)
π(β0) −1

+
Hn
mn
#
where κn|Hn
mn ∼N(ˆβn, σ2(X⊤
n Xn)−1).
By Lemma 1, we have that ˆβn is consistent.
If
λmin(X⊤
n Xn)
p→∞, then κn
p→β0 as we can write κn = β0 + (ˆβn −β0) + Zn where
Zn|Hn
mn ∼N(0, σ2(X⊤
n Xn)−1) and both the terms ˆβn −β0 and Zn converge to zero in prob-
ability. If π is continuous and bounded, by Vitali’s convergence theorem,
E[d(Hn
mn)] ≤E
"π(κn)
π(β0) −1

+
#
→0,
meaning d(Hn
mn)
p→0.
B.3
Proof of Theorem 3
Suppose Yi,(1), Yi,(2), . . . are the serialized arm pulls from arm i. Let ¯Yi,(N) = 1
N
PN
j=1 yi,(j) be the
sequence of sample means. By Lemma 3, we have
¯Yi,(N) −µ
σ
= o(N −1/2+ϵ) almost surely as N →∞.
(3)
for some small ϵ, say ϵ = 0.01. Since ¯Yn,i = ¯Yi,(Nn,i), we have that ¯Yn,i −µi = Op(N −1/2+ϵ
n,i
).
Thus, ˆβn,i −β0,i = Op(N −1/2+ϵ
n,i
), meaning ˆβn,i is consistent for β0,i.
We prove the result in three steps by truncating both the posterior and normal distribution to the set
Cn = {β : ∀i, |βi−β0,i| ≤cn,i} for some sequence of random variables cn,i such that N 1/3
n,i cn,i
p→0
and N 1/2−ϵ
n,i
cn,i
p→∞as n →∞. We then show that
∥π(β|Hn) −πCn(β|Hn)∥TV
p→0
(4)
∥πCn(β|Hn) −N Cn(β0, I−1
n )∥TV
p→0
(5)
∥N Cn(β0, I−1
n ) −N(β0, I−1
n )∥TV
p→0.
(6)
We first show (5). The likelihood function can be calculated as
L(β; Hn) ∝
p
Y
i=1
exp(Nn,i( ¯Yn,iβi −b(βi))).
22


=== Page 23 ===
By Taylor’s Theorem, for any β ∈Cn, we have
L(β; Hn) ∝exp
 p
X
i=1
Nn,i[ ¯Yn,iβi −b(β0,i) −b′(β0,i)(βi −β0,i)
−1
2b′′(β0,i)(βi −β0,i)2 −1
6b′′′(β∗
n,β,i)(βi −β0,i)3]

for some β∗
n,β ∈Cn. However, if b′′′ is bounded on a neighborhood of β0,i for each i and N 1/3
n,i cn,i
p→
0, we have that supβ∈Cn Nn,ib′′′(β∗
n,β,i)(βi −β0,i)3
p→0. Thus, we can write
L(β; Hn) ∝exp
 p
X
i=1
Nn,i[ ¯Yn,iβi −b(β0,i) −b′(β0,i)(βi −β0,i) −1
2b′′(β0,i)(βi −β0,i)2] + op(1)
!
∝exp
 p
X
i=1
Nn,i[−1
2b′′(β0,i)β2
i + ( ¯Yn,i −b′(β0,i) + b′′(β0,i)β0,i)βi]
!
(1 + op(1))
= exp
 p
X
i=1
Nn,i[−1
2b′′(β0,i)β2
i + b′′(β0,i)ˆβn,iβi]
!
(1 + op(1))
∝exp
 
−1
2
p
X
i=1
Nn,ib′′(β0,i)(βi −ˆβn,i)2
!
(1 + op(1))
(7)
= exp(−1
2(β −ˆβn)⊤In(β −ˆβn))(1 + op(1))
Let the density of the truncated normal distribution N Cn(ˆβn,i, I−1
n ) be expressed as
An1β∈Cn exp(−1
2(β −ˆβn,i)⊤In(β −ˆβn,i)).
where An is chosen such that the density is suitably normalized. Let dn(Hn) = ∥πCn(β|Hn) −
N Cn(β0, I−1
n )∥TV. By Lemma 2, we have
dn(Hn) ≤
Z  
An1β∈Cn
π(β)L(β; Hn)
π(ˆβn)L(ˆβn; Hn)
−An1β∈Cn exp(−1
2(β −ˆβn)⊤In(β −ˆβn))
!
+
dβ
≤
Z
An1β∈Cn
 
π(β)
π(ˆβn)
exp(−1
2(β −ˆβn)⊤In(β −ˆβn))(1 + ¯op(1))
−exp(−1
2(β −ˆβn)⊤In(β −ˆβn))

+
dβ
where we say a sequence of random variables Wn(β) indexed by β is ¯op(1) if supβ∈Cn Wn(β)
p→0.
Thus, we have
dn(Hn) ≤
Z   
π(β)
π(ˆβn)
−1 + ¯op(1)
!
An1β∈Cn exp(−1
2(β −ˆβn)⊤In(β −ˆβn))
!
+
dβ.
= Eηn∼N Cn( ˆβn,i,I−1
n )
 
π(ηn)
π(ˆβn)
−1 + ¯op(1)
!
+
≤
supβ∈Cn π(β)
infβ∈Cn π(β) −1 + ¯op(1)

+
23


=== Page 24 ===
Then, since cn,i
p→0 for each i and π is continuous, we know
supβ∈Cn π(β)
infβ∈Cn π(β)
p→1, meaning the above
expression indeed converges to 0 in probability. Next, we show (4). Note that
∥π(β|Hn) −πCn(β|Hn)∥TV = Pπ(β ∈CC
n |Hn)
=
R
CC
n π(β)L(β; Hn)dβ
R
Rp π(β)L(β; Hn)dβ
≤πmax
πmin
n
R
CC
n L(β; Hn)dβ
R
Cn L(β; Hn)dβ
where πmax = supβ π(β) and πmin
n
= infβ∈Cn π(β). Note that if π is continuous and positive at β0,
then πmin
n
converges to some positive constant. Then, since the ancillary function b of the exponential
family is convex, the likelihood function is log-concave in β, meaning we have
R
CC
n L(β; Hn)dβ
R
Cn L(β; Hn)dβ ≤sup
v∈∂Cn
R ∞
1
L(β0 + (v −β0)t; Hn)tp−1dt
R 1
0 L(β0 + (v −β0)t; Hn)tp−1dt
≤sup
v∈∂Cn
R ∞
1
exp(ℓn(β0) + (ℓn(v) −ℓn(β0))t)tp−1dt
R 1
0 exp(ℓn(β0) + (ℓn(v) −ℓn(β0))t)tp−1dt
= sup
v∈∂Cn
R ∞
ℓn(β0)−ℓn(v) exp(−x)xp−1dx
R ℓn(β0)−ℓn(v)
0
exp(−x)xp−1dx
=
R ∞
an exp(−x)xp−1dx
R an
0
exp(−x)xp−1dx
where an = infv∈∂Cn ℓn(β0) −ℓn(v). Note that the function a 7→
R ∞
a
exp(−x)xp−1dx
R a
0 exp(−x)xp−1dx goes to zero as
a →∞. Thus, it suffices to show that an
p→∞. To do this, note that by expression 7, we have
an =
inf
v∈∂Cn −1
2
p
X
i=1
Nn,ib′′(β0,i)[(β0,i −ˆβn,i)2 −(vi −ˆβn,i)2] + ¯op(1).
Since |vi −ˆβn,i| ≥∥vi −β0,i| −|β0,i −ˆβn,i∥= |cn,i −|β0,i −ˆβn,i∥, we have
an ≥
inf
v∈∂Cn −1
2
p
X
i=1
Nn,ib′′(β0,i)[|β0,i −ˆβn,i|2 −(cn,i −|β0,i −ˆβn,i|)2] + ¯op(1)
=
inf
v∈∂Cn −1
2
p
X
i=1
Nn,ib′′(β0,i)[|β0,i −ˆβn,i|2 −c2
n,i + 2cn,i|β0,i −ˆβn,i| −|β0,i −ˆβn,i|2] + ¯op(1)
=
inf
v∈∂Cn
1
2
p
X
i=1
c2
n,iNn,ib′′(β0,i)[1 −2|β0,i −ˆβn,i|
cn,i
] + ¯op(1)
By the definition of cn,i as well as Equation (3), we know c2
n,iNn,i
p→∞and β0,i−ˆβn,i
cn,i
p→0. Thus,
we indeed have an
p→∞. Lastly, we show (6). We have
∥N(β0, I−1
n ) −N Cn(β0, I−1
n )∥TV = PX∼N (β0,I−1
n )(∃i, |Xi −β0,i| > cn,i)
= PX∼N (0,Ip×p)(∃i, |Xi| > cn,iN 1/2
n,i b′′(β0,i)1/2).
If cn,iN 1/2
n,i
p→∞, then the above expression indeed coverges to zero in probability.
24


=== Page 25 ===
B.4
Proof of Theorem 4
We only need to establish the consistency of ˆβn, after which the result follows by a similar argument
to the proof of Theorem 2. Consistency can be shown from condition (ii) by Theorem 1 of [21].
C
Technical Lemmas
Proof of Lemma 1:
We translate this directly to the bandit setting, after which this result follows
from a similar version of Theorem 2.2 of [13]. For each i = 1, . . . , p, let µi = βi be the i-th
coordinate of β which represents the true mean of arm i. Let Ni,n = (Xn)ii be the number of times
arm i is pulled in the trajectory Hn
mn. We serialize all arm pulls from each arm i—that is, suppose
yn
i,(1), yn
i,(2), . . .
ind
∼N(µi, σ2). Then, when drawing yn
j |xn
j ∼N(µxj, σ2), we look up the next
unused sample in the serialized sequence yn
xj,(1), . . . and use that as our observation.
Under this formulation of the data-generating process, the sample mean ˆβn,i is simply the mean
of the first Ni,n samples from the serialized sequence yn
i,(1), . . .. Note that the serialized sample
means ¯yn
i,t = 1
t
Pt
j=1 yn
i,(j) satisfy ¯yn
i,t
a.s.
→µi as t →∞for each i, n by the strong law of large
numbers. Then, Proposition 1 implies that for each i, n, there exists some function ϵn
i (·) satisfying
limB→∞ϵn
i (B) = 0 such that for any B,
P[N ≥B] ≥1 −1
B =⇒P[|¯yn
i,(N) −µi| ≤ϵn
i (B)] ≥1 −ϵn
i (B)
Crucially, note that the functions ϵn
i (·) can be chosen such that ϵn
i (·) does not depend on n. This is
because the distribution of serialized samples {yn
i,(j)}i∈{1,...,p},j≥1 for each trajectory is identical,
and in Proposition 1, the bound translation function ϵ is chosen solely based on the distribution of
the almost-surely convergent sequence. Finally, by condition (ii), we indeed have that Ni,n
p→∞
for each i, meaning the condition P[Ni,n ≥Bn] ≥1 −
1
Bn is indeed satisfied for some sequence
Bn →∞- to see this, note that letting Bn = sup{B : P[Ni,n ≥B] ≥1 −1
B }, we must have
Bn →∞as otherwise there would be some constant C where P[Ni,n ≤C] >
1
C for infinitely
many n which would contradict the statement that Ni,n diverges in probability. Thus, we have
P[|¯yn
i,(Ni,n) −µi| ≤ϵn
i (Bn)] ≥1 −ϵn
i (Bn) for each n meaning the sample mean ˆβn,i = ¯yn
i,(Ni,n) is
indeed consistent for µi.
Proposition 1. (Bound translation) Suppose Yn is a sequence of random variables such that Yn
a.s.
→Y
for some real number Y . Then there exists a function ϵ(B) with ϵ(B) →0 as B →∞such that for
any random variable N and positive integer B, we have
P[N ≥B] ≥1 −1
B =⇒P[|YN −Y | ≤ϵ(B)] ≥1 −ϵ(B).
Proof. Note that it is sufficient to show that for any fixed η, there is some function ϵη(B) such that
limB→∞ϵη(B) = 0 and
P[N ≥B] ≥1 −1
B =⇒P[|YN −Y | ≤η] ≥1 −ϵη(B).
To show this, we simply let ϵη(B) = 1
B + P[∃n ≥B, |Yn −Y | > η]. If Yn converges almost surely
to Y , then limB→∞ϵη(B) = 0. Also, we indeed have
P[|YN −Y | ≤η] ≥P[N ≥B and ∀n ≥B, |Yn −Y | ≤η] ≥1 −ϵη(B).
Since we can do this for any η, the statement of the result is indeed true.
Lemma 2. Let P(·) and Q(·) be continuous distributions and c ∈R be any constant. Then,
∥P −Q∥TV = 1
2
R
(P(x) −Q(x))+dx ≤
R
(cP(x) −Q(x))+dx.
25


=== Page 26 ===
Proof. Let A = {x : P(x) > Q(x)}. Let a1 =
R
A P(x) −Q(x)dx and a2 =
R
AC Q(x) −P(x)dx.
Then, note that
∥P −Q∥TV = 1
2
Z
(P(x) −Q(x))+dx = 1
2a1 + 1
2a2
and
0 =
Z
P(x) −Q(x)dx = a1 −a2
Solving for a1 and a2, we get
a1 = a2 = ∥P −Q∥TV.
To show the desired result, if c ≥1, we have
∥P −Q∥TV = a1 ≤
Z
A
cP(x) −Q(x)dx ≤
Z
(cP(x) −Q(x))+dx.
Similarly, if c ≤1, then
∥P −Q∥TV = a2 ≤
Z
AC Q(x) −cP(x)dx ≤
Z
(cP(x) −Q(x))+dx.
Lemma 3. Let X1, X2, . . . be i.i.d. samples from a distribution P with mean µ and finite variance
σ2. Let ˆµn = 1
n
Pn
i=1 Xi. Then,
n1/2−ϵ(ˆµn −µ)
a.s.
→0.
for any ϵ > 0.
Proof. By the law of iterated logarithms, we have
lim sup
n→∞
√n|ˆµn −µ|
p
2σ2 log log n
= 1
with probability 1. Then, we have
lim sup
n→∞n1/2−ϵ|ˆµn −µ| ≤
 
lim sup
n→∞
√n|ˆµn −µ|
p
2σ2 log log n
!  
lim sup
n→∞
p
2σ2 log log n
nϵ
!
= 0
with probability 1.
D
Auxilliary results
Proposition 2. Let x1 be deterministic and take any real value. Let the adaptive sampling procedure
simply deterministically set xi = yi−1 for i > 1. Then, if β = 1, the MLE is asymptotically
non-Normal. However, condition (i) of Theorem 1 is satisfied by this sampling design.
Proof. The asymptotic non-Normality is established by Lai and Wei in [21, Example 3]. The second
part of condition (i) is satisfied because the covariates are 1-dimensional and the first part is satisfied
because Pn
i=1 x2
i
p→∞, which is a direct consequence of [21, Equation (4.9)].
26


=== Page 27 ===
We now recall an assumption from [30]:
Assumption 1 (Stability, [30, Assumption 1]). There exists a matrix K0 for which the maximum
absolute eigenvalue of A + BK0 is less than 1.
Proposition 3. Suppose we run the stepwise NCEC algorithm (Algorithm 1 in [30]) on an instance
of LQR satisfying Assumption 1 using any configuration of the hyperparameters τ 2 > 0, β ∈[1/2, 1)
and α > 0 permitted therein. Then, this satisfies condition (i) of Theorem 1.
Proof. First, observe that our Gram matrix ˜X⊤˜X is more simply expressed as Ik×k ⊗Gn where
⊗denotes Kronecker product and Gn := Pn
i=1

xi
ui
 
xi
ui
⊤
denotes the Gram matrix. Because the
largest and smallest eigenvalues of Ik×k ⊗Gn are the same as that of Gn it suffices to study the
spectrum of the latter. Theorem 3 of [30] shows, in the stabilizable regime, that
D−1
n Gn
 D−1
n
⊤
p→Ik+d,
where
Dn = nβ/2 logα/2(n)

Ik
0
K
Id
 
C1/2
n
0
0
p
τ 2/βId

,
(8)
Cn = n1−β log−α(n)
∞
X
p=0
(A+BK)p((A+BK)p)⊤σ2 + τ 2
β
∞
X
q=0
(A+BK)qBB⊤((A+BK)q)⊤,
(9)
and K is a matrix that does not depend on n (see equation (3) of [30] for a precise definition).
from which it follows that
Gn −DnD⊤
n
 = op(1).
(10)
Below, we show that λmax
 DnD⊤
n

≤O(n) and λmin
 DnD⊤
n

≥Ω(nβ logα(n)). Because Equa-
tion (10) implies both that λmin(Gn) −λmin
 DnD⊤
n

= op(1) and λmin(Gn) −λmin
 DnD⊤
n

=
op(1), we indeed have that condition (i) of Theorem 1 is met: log (λmax(Gn)) = op(λmin(Gn)) and
λmin(Gn)
p→∞.
Upper bound on λmax(DnD⊤
n )
By submultiplicativity of the operator norm, we have, using
equation (8), that the largest eigenvalue of DnD⊤
n is at most
nβ logα(n)


Ik
0
K
Id

2 

C1/2
n
0
0
p
τ 2/βId

2
The middle term is of constant order and therefore it suffices to upper-bound the last term. Because


C1/2
n
0
0
p
τ 2/βId
 ≤max

∥C1/2
n
∥, ∥
p
τ 2/βId∥

and since ∥
p
τ 2/βId∥is again of constant
order it suffices to simply bound ∥C1/2
n
∥. Using equation (9) and triangle inequality, this is at most
O(n(1−β)/2 log−α/2(n)). Consequently, the maximum eigenvalue of DnD⊤
n is O (n).
Lower bound on λmin(DnD⊤
n )
To lower bound the minimum eigenvalue of DnD⊤
n , we apply the
same argument to the maximum eigenvalue of (D⊤
n )−1D−1
n . The largest eigenvalue of (D⊤
n )−1D−1
n
is at most
n−β log−α(n)


Ik
0
K
Id
−1
2 

C1/2
n
0
0
p
τ 2/βId
−1
2
Then,


C1/2
n
0
0
p
τ 2/βId
−1 ≤max(||C−1
n ||1/2,
p
β/τ 2) = max(λmin(Cn)−1/2,
p
β/τ 2).
Again using equation (9) and the triangle inequality, this is at most O(1), meaning the minimum
eigenvalue of DnD⊤
n is at least Ω(nβ logα(n)).
27


=== Page 28 ===
Proof of Corollary 1: Suppose by contradiction that there is some k where
sup
P ∈Pn
P(||π(β|Hn) −N(ˆβn, σ2(X⊤
n Xn)−1)||T V > c) > k
for infinitely many n. Then, for all such n, there is some sampling rule Λn whose corresponding
distribution trajectory P satisfies
P(||π(β|Hn) −N(ˆβn, σ2(X⊤
n Xn)−1)||T V > c) ≥k.
However, this contradicts the statement of Theorem 2.
E
Heteroskedastic Bandits
Algorithm 2 Heteroskedastic Gaussian bandits
Input Action set A = {1, . . . , p}, Arm sampling rule Λ : (A×R)∗→∆(A), Reward distributions
Bi = N(βi, σ2
i ).
Output Sampling trajectory Hn
H0 ←∅
for j = 1, . . . , n do
Sample aj ∼Λ(·|Hj−1)
Sample yj ∼N(βaj, σ2
aj)
Hj ←{(x1, y1), . . . , (xj, yj)}
end for
Theorem 5. Suppose we generate a length-mn trajectory Hn
mn = ((an
1, yn
1 ), . . . , (an
n, yn
n)) for each
n according to decision rules Λn in the case of Heteroskedastic Gaussian bandits. Assume the
sampling procedure satisfies the following conditions:
(i) mini Ni,n
p→∞where Ni,n = Pn
j=1 I[an
j = i].
(ii) π(·) is continuous and bounded on Rp with positive density at β0.
Then, the posterior distribution π(β|Hn
mn) satisfies
∥π(β|Hn
mn) −N(ˆβn, diag(σ2
i N −1
i,n ))∥TV
p→0.
Proof. We rescale this problem to an instance of the homoskedastic bandit problem and apply
Theorem 2. Let the covariate sampling rule be ˜Λn(Hj−1) = eΛn(Hj−1) and the rescaled parameters
be ˜βi = βi
σi with the homoskedastic variance ˜σ = 1. Let ˜π(˜β) ∝π(σ ∗˜β) where ∗represents
element-wise multiplication where σ is the vector of (σi)p
i=1. Then, by Theorem 2,
∥˜π(˜β| ˜Hn
mn) −N(˜ˆβn, ( ˜X⊤
n ˜Xn)−1)∥TV
p→0.
Rescaling the posterior back to the original parameters gives the desired result.
F
Parametric BvM
There are considerable challenges when extending the BvM result to parametric models satisfying
only weak regularity conditions such as differentiability in quadratic mean. Consider replacing the
normal distribution in Algorithm 1 with a general parametric model Pθ(·|xj) with θ = β⊤xj. We
may want to show that in this setting, the posterior distribution is asymptotically normal similar to
the BvM statement, i.e.
28


=== Page 29 ===
∥π(θ|Hn) −N(ˆθn, J−1
n )∥TV
p→0
where Jn = −Pn
j=1 ¨ℓˆθn(yj|xj) is the empirical Fisher Information and ℓθ = log Pθ is the log-
likelihood. Suppose we follow the steps of the classical BvM proof. Letting πCn(θ|Hn) and
N Cn(ˆθn, J−1
n ) denote truncations of these distributions to the set Cn, the classical proof proceeds in
the following three steps—we show each of the following convergences:
∥π(θ|Y ) −πCn(θ|Y )∥TV
p→0
∥πCn(θ|Y ) −N Cn(ˆθn, J−1
n )∥TV
p→0
∥N Cn(ˆθn, J−1
n ) −N(ˆθn, J−1
n )∥TV
p→0.
The first step requires a method of truncating the posterior distribution, which is in general quite
difficult without strong assumptions on the behavior of the adaptive system similar to the results
of [18] and [11]. With independent data, we could truncate the posterior due to Hoeffding bounds
on the likelihood, but these bounds do not apply in adaptively collected data. The second step was
implied by a second-order Taylor expansion of the log-likelihood in the classical proof, but in adaptive
settings, the Fisher information matrix Jn may grow at different rates in different directions, i.e.
λmin(Jn) may grow at a different rate than λmax(Jn). This would make an argument based on a
Taylor expansion more complicated and a valid proof seems to require further assumptions on the
growth rate of Jn. Finally, the third step was implied in the i.i.d. setting by the local consistency of
ˆθn, i.e. J1/2
n
(ˆθn −θ0) = Op(1) but this condition may be violated in adaptive experiments. In fact,
as Lai and Wei showed, we require some nontrivial assumptions on the empirical Fisher Information
to guarentee consistency, even in the case of Gaussian regression.
G
Real-world example
We compute the BvM total variation distance on a real-world instance of Bernoulli Thompson
sampling provided in [28]. The dataset consists of the interaction of a fashion recommendation
algorithm with users, where the algorithm is optimizing for the number of clicks on fashion items.
Although the original dataset is modeled with a contextual bandit setting with a batch size of 3, we
simplify their environment to an 80-armed bandit problem by disregarding the context and flattening
the batch size. These simplifications were made solely for computational convenience. The dataset
consists of 12m impressions or time steps, during which we update the posterior distribution with a
Beta(1, 1) prior. The BvM total variation distance throughout this rollout is shown below.
Figure 4: BvM TV distance for the Zozo dataset with prior Beta(1, 1). TV estimates computed with
104 samples and have standard error at most 0.004.
As seen in Figure 4, the convergence of the total variation distance is quite slow. This is perhaps
due to the sparse success rates in the Bernoulli bandit and the larger dimensionality of the parameter
29


=== Page 30 ===
space. It should be noted that the original dataset contained missing data within batches, i.e. not all
batches had size 3.
H
Plots
Figure 5: Frequentist coverage of the Bayesian credible interval for the contextual bandit and LQR,
under the same configurations as Section 6. Coverage estimates shown have standard error at most
0.004.
Figure 6: Frequentist coverage of the Bayesian credible interval for UCB on Gaussian bandits,
Bernoulli bandits and Poisson bandits, under the same configurations as Section 6. Coverage
estimates shown have standard error at most 0.004.
30


