{
  "filename": "117231_Bernstein–von Mises for Adaptively Collected Data.pdf",
  "total_pages": 30,
  "full_text": "Bernstein–von Mises for Adaptively Collected Data\nKevin Du\nDepartment of Statistics\nHarvard University\nkevindu@college.harvard.edu\nYash Nair\nDepartment of Statistics\nStanford University\nyashnair@stanford.edu\nLucas Janson\nDepartment of Statistics\nHarvard University\nljanson@fas.harvard.edu\nAbstract\nUncertainty quantification (UQ) for adaptively collected data, such as that coming\nfrom adaptive experiments, bandits, or reinforcement learning, is necessary for\ncritical elements of data collection such as ensuring safety and conducting after-\nstudy inference. The data’s adaptivity creates significant challenges for frequentist\nUQ, yet Bayesian UQ remains the same as if the data were independent and\nidentically distributed (i.i.d.), making it an appealing and commonly used approach.\nBayesian UQ requires the (correct) specification of a prior distribution while\nfrequentist UQ does not, but for i.i.d. data the celebrated Bernstein–von Mises\ntheorem shows that as the sample size grows, the prior ‘washes out’ and Bayesian\nUQ becomes frequentist-valid, implying that the choice of prior need not be a\nmajor impediment to Bayesian UQ as it makes no difference asymptotically. This\npaper for the first time extends the Bernstein–von Mises theorem to adaptively\ncollected data, proving asymptotic equivalence between Bayesian UQ and Wald-\ntype frequentist UQ in this challenging setting. Our result showing this asymptotic\nagreement does not require the standard stability condition required by works\nstudying validity of Wald-type frequentist UQ; in cases where stability is satisfied,\nour results combined with these prior studies of frequentist UQ imply frequentist\nvalidity of Bayesian UQ. Counterintuitively however, they also provide a negative\nresult that Bayesian UQ is not asymptotically frequentist valid when stability fails,\ndespite the fact that the prior washes out and Bayesian UQ asymptotically matches\nstandard Wald-type frequentist UQ. We empirically validate our theory (positive\nand negative) via a range of simulations.\n1\nIntroduction\nData in applications such as robotics [19], healthcare [32], clinical trials [5], online education [27],\nmobile health [2, 31], and online advertising [22, 10] is routinely being collected adaptively. This\nmeans that the data is collected sequentially, with decisions about the data collection itself being\nmade online based on all the data observed up to that time point. In particular, the online decisions\noften try to focus on actions or interventions that prior data indicates will produce large values of\nsome reward function, and this type of data collection includes adaptive experiments, multi-armed\nbandits, and reinforcement learning.\nTwo important elements of adaptive data collection are (1) assessing at the end of the data collection\nwhat has been learned (e.g., to assess confidence in a putatively optimal clinical treatment or inform\na future online advertising campaign) [33, 14] and (2) making sure to avoid certain bad outcomes\n39th Conference on Neural Information Processing Systems\nduring the data collection process (e.g., avoiding crashing a robot while it is reinforcement learning)\n[4]. Both of these elements, as well as others, rely critically on uncertainty quantification (UQ) of\nthe parameters of the data-generating environment, yet standard frequentist UQ, such as Wald-type\ninference based on the maximum likelihood estimator (MLE), is made far more complicated by the\nadaptivity of the data collection [21] which results in data which is not i.i.d., Markovian, or even\nstationary.\nOn the other hand, the Bayesian statistical paradigm translates seamlessly to the adaptive setting, so\nthat the vast wealth of Bayesian UQ methodology designed for i.i.d. data can be directly applied to\nadaptively collected data to provide strong Bayesian probabilistic guarantees. However, the validity\nof those guarantees is predicated on correctly specifying a prior distribution for the parameters, which\ncan be challenging to do or justify in practice. For i.i.d. data, the celebrated Bernstein–von Mises\n(BvM) theorem [29, Theorem 10.1] proves that for any reasonable choice of prior, Bayesian UQ\nasymptotically matches that of prior-free Wald-type frequentist UQ, providing strong Bayesian and\nfrequentist justification for Bayesian UQ in large samples without the analyst having to worry too\nmuch about the choice of prior. Although BvM has been extended beyond i.i.d. data in a number of\nways (see Section 2 for more details), it has not been extended to the setting of adaptively collected\ndata.\nContributions\nThis paper for the first time proves BvM results for adaptively collected data,\nshowing that Bayesian UQ asymptotically matches (asymptotically normal) Wald-type frequentist\nUQ under certain mild conditions. Our first result, in Section 3, applies to a very general class of\nadaptive linear Gaussian settings that include Gaussian multi-armed bandits, adaptive Gaussian linear\nbandits (a form of contextual bandit), and the linear quadratic regulator (a form of reinforcement\nlearning on a Markov decision process). We then show in Section 4 that in the special case of\nmulti-armed bandits, the conditions of our first result can be weakened, and the Gaussian assumption\ncan also be generalized to any exponential family. Finally, we show in Section 5 that the conditions\nof the first result can also be weakened for linear (contextual) bandits. Section 6 empirically validates\nour theoretical results. A surprising aspect of our BvM results is that they do not require the key\nstability condition used in frequentist validity results; however, it is known that Wald-type frequentist\ninference may fail to be asymptotically valid in cases when the stability condition does not hold.\nThe counterintuitive implication is that in some adaptive settings, Bayesian UQ asymptotically\nmatches frequentist UQ (and the prior becomes irrelevant), yet Bayesian UQ is not asymptotically\nfrequentist-valid.\nNotation and Terminology.\nThroughout this paper, we refer to a confidence set as asymptotically\nfrequentist-valid at level α if for any parameter value, the limit inferior of the frequentist coverage\nis at least 1 −α. Similarly, we refer to a credible set as asymptotically Bayesian-valid at level α if\nthe limit inferior of the Bayesian coverage is at least 1 −α. We will use the term frequentist (resp.\nBayesian) UQ generically to refer to any of the many forms of frequentist (resp. Bayesian) statistical\ninference such as hypothesis tests, confidence intervals, or prediction intervals (resp. such as Bayesian\nhypothesis tests, credible intervals, or posterior predictive intervals). By Wald-type frequentist UQ,\nwe refer to standard asymptotic hypothesis testing or confidence interval construction using the MLE\nand its asymptotic Normality—for instance, as demonstrated in Example 15.6 of [29]. Let λmin(A)\nand λmax(A) denote the minimum and maximum eigenvalues of the matrix A respectively.\n2\nBackground\nAs mentioned in the previous section, Bayesian statistical inference requires no adjustment for the\nadaptivity of the data. This both makes the Bayesian approach an appealing and commonly used\ntool for UQ in adaptively collected data and means there is no need for (and thus a lack of) prior\nwork extending it to the adaptive setting. Thus, prior work on UQ for adaptively collected data has\nfocused on frequentist UQ, with the primary approach being to make assumptions on both the data\nand the adaptive assignment algorithm by which it is collected. These assumptions either (1) enable\nthe use of martingale central limit theorems to establish asymptotic normality of common frequentist\nestimators like the MLE (see, e.g., [21, 33, 14, 6, 12, 23]) or (2) allow for conservative finite-sample\ninference via martingale-based concentration bounds [15, 16, 1]. These conditions are often highly\ntechnical and hard to check, and reduce to or are related to a stability condition introduced in [21];\nsee Section 3 for more details. Another line of work uses randomization testing for frequentist UQ\n2\nfor adaptively collected data, providing non-asymptotic and non-conservative guarantees but relying\non sampling procedures that can be computationally prohibitive to use [25].\nThe classical BvM theorem [29, Theorem 10.1] proves that under minimal conditions, for i.i.d. data\ndrawn from a parametric model, the posterior distribution is asymptotically normal centered at the\nMLE with variance equal to the inverse Fisher information, and hence agrees asymptotically with\n(and inherits the asymptotic frequentist validity of) Wald-style frequentist inference. Prior works\ngeneralizing BvM to non-i.i.d. data only apply in settings where the ratio of the maximum and\nminimum eigenvalues of the Fisher information matrix is bounded [18, 11, 8, 20, 7, 9]. This condition\nfails to hold in most adaptive settings where, over time, certain actions are learned to be better than\nothers and are asymptotically sampled infinitely more often than the worse actions, resulting in\nan unbounded ratio (e.g., regret-optimal multi-armed bandit algorithms sample suboptimal arms\nlogarithmically often, leading to an eigenvalue ratio that grows like n/ log(n) [3]).\n3\nBvM on Adaptive Linear Gaussian Data\nThis paper will primarily consider the adaptive data collection setting, laid out in Algorithm 1, that\nat time step j allows the data collector to choose via an arbitrary function Λ a covariate vector\nxj based on all data observed so far (Hj−1 = ((x1, y1), . . . , (xj−1, yj−1))), and then observe\nyj ∼N(x⊤\nj β, σ2). We assume the only unknown in this sampling procedure is β, and hence the task\nat hand is to perform UQ for β based on the full data trajectory Hn = (Xn, yn), where Xn is the\nn × p matrix with jth row given by x⊤\nj and yn is the n-vector with jth entry given by yj.\nAlgorithm 1 Adaptive Linear Gaussian Sampling Procedure\nInput Covariate sampling rule Λ : (Rp × R)∗→∆(Rp), coefficient vector β ∈Rp, variance\nσ2 ∈R+\nOutput Sampling trajectory Hn\nH0 ←∅\nfor j = 1, . . . , n do\nSample xj ∼Λ(·|Hj−1)\nSample yj ∼N(x⊤\nj β, σ2)\nHj ←((x1, y1), . . . , (xj, yj))\nend for\nHere, Λ denotes a placeholder that can represent any sampling algorithm including UCB, Thompson\nsampling, autoregressive models, etc. We now present our first result, which states that the posterior\ndistribution is asymptotically normal, centered at the MLE with variance equal to the inverse empirical\nFisher information.\nTheorem 1. Suppose π(β) is the prior distribution for β and π(β|Hn) is the posterior after observing\nthe trajectory Hn. Let ˆβn = (X⊤\nn Xn)−1X⊤\nn yn be the MLE for β and let β0 be the true value of β.\nAssume the sampling procedure in Algorithm 1 satisfies the following conditions:\n(i) λmin(X⊤\nn Xn)\np→∞and log λmax(X⊤\nn Xn) = op(λmin(X⊤\nn Xn)).\n(ii) π(·) is continuous with positive density at β0.\nThen, the posterior distribution π(β|Hn) satisfies\n∥π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV\np→0.\nTheorem 1’s proof, given in Appendix B.1, first upper bounds the key total variation (TV) distance\nin terms of the L1 distances of unnormalized densities, allowing us to ignore normalizing constants\nin our analysis. Our proof then uses the fact that the posterior distribution is not affected by the\nadaptivity of the algorithm, since terms in the likelihood involving the covariate sampling process Λ\ndo not depend on the parameter and can thus be absorbed into the normalizing constant. We also use\na common trick in BvM-style proofs [18, 29] of truncating the posterior to local ellipsoids defined as\n3\n{β : ∥(X⊤\nn Xn)1/2(β −β0)∥≤Mn} for some local radius Mn. This truncation, however, differs\nfrom those used in previous BvM-style proofs, as here the empirical Fisher information X⊤\nn Xn/σ2\ndoes not necessarily have bounded condition number, implying that the resulting ellipsoids may be\nhighly anisotropic, stretching much wider in some directions than in others. This step of our proof\nrelies critically on the second part of condition (i) of Theorem 1 which allows us to show that the\ndensity of the representative normal distribution outside of local ellipsoids converges to zero. We\nfurthermore show that the posterior can be written as proportional to the prior multiplied by the\ndensity of the representative normal, meaning we can truncate the distribution to a local ellipsoid\neven if the prior is unbounded.\nOur proof requires condition (i) for a couple of reasons. If the maximum eigenvalue of the observation\nmatrix grows exponentially faster than the minimum eigenvalue, there are two potential pathological\nconsequences. First, Lai and Wei showed that condition (i) of Theorem 1 is nearly necessary for\nMLE consistency; in particular, if condition (i) of Theorem 1 is removed, the MLE is not necessarily\nconsistent [21, Example 1]. Second, even if ˆβn is consistent but condition (i) fails, it may no longer\nbe the case that the density of the distribution N(ˆβn, σ2(X⊤\nn Xn)−1) at some other point β′ ̸= β0\nconverges to 0. One can see this by writing the density as\nN(β′; ˆβn, σ2(X⊤\nn Xn)−1) =\ns\n|X⊤\nn Xn|\n(2πσ2)p exp(−1\n2σ2 (β′ −ˆβn)⊤X⊤\nn Xn(β′ −ˆβn)),\nwhere N(x; µ, σ2) denotes the density value at x of the N(µ, σ2) distribution. While the exponential\nterm in the above expression necessarily converges to zero if β′ ̸= β0, ˆβn is consistent, and\nλmin(X⊤\nn Xn)\np→∞, it is not necessarily true that the entire expression converges to zero or is\neven bounded. This is because the quadratic term inside the exponential may only be of order\nO(λmin(X⊤\nn Xn)) whereas the term |X⊤\nn Xn| grows at least as quickly as λmax(X⊤\nn Xn). Thus,\nalthough the measure of a neighborhood of β′ under the probability measure N(ˆβn, σ2(X⊤\nn Xn)−1)\nmust converge to zero, without condition (i), it is possible that the density of this distribution diverges\nat β′. We show in the proof of Theorem 1 that the likelihood function is proportional to the density\nof N(ˆβn, σ2(X⊤\nn Xn)−1), meaning that it might be possible that the density of both the prior and\nthe normalized likelihood function diverge at β′. This could potentially cause the posterior to have\nasymptotically non-negligible measure in a neighborhood of β′, which would violate the BvM\nstatement as the measure of the normal distribution N(ˆβn, σ2(X⊤\nn Xn)−1) converges to zero in a\nneighborhood of β′.\nNotably, Theorem 1 does not require the key stability condition, originally from [21] but used in\nmany works since then for frequentist UQ for adaptively collected data, which assumes the existence\nof a deterministic sequence Bn for which B−1\nn (X⊤\nn Xn)1/2\np→Ip×p. Without this condition, ˆβn may\nfail to be asymptotically normal [21, Example 3]. In Proposition 2 in Appendix D we show that their\nexample does, however, satisfy the conditions of our Theorem 1. This leads to the rather surprising\nresult that there are settings where Bayesian UQ is asymptotically equivalent to standard Wald-style\nfrequentist UQ, yet the latter (and therefore also the former) is asymptotically frequentist-invalid. We\nwill describe another such example in the next section, which requires the triangular array version of\nBvM we prove there.\nThe previous paragraph notes that Theorem 1 can indicate either asymptotic frequentist validity\nor invalidity of Bayesian UQ, depending on the situation. However, in terms of Bayesian validity,\nTheorem 1 (and indeed all the theorems in the paper) provide a strong positive result regarding\nmisspecified priors: they say that the prior distribution π is ‘washed out’ as n →∞. Thus, from a\nBayesian perspective, a credible interval asymptotically has the correct coverage even if the prior\nis misspecified, as long as both the correct prior and the misspecified prior used for the inference\nare continuous and bounded, with the misspecified prior’s support containing that of the correct\nprior. Note, however, that the rate at which the “wash out” effect occurs depends on the rate at\nwhich the posterior distribution converges in TV distance to a normal distribution, which may be\nlogarithmically slow for optimal bandit algorithms. Thus, it may not be accurate in finite samples to\ntreat a misspecified prior as having “washed out”.\n4\n4\nBvM on Multi-armed Bandits\nA notable special case of Algorithm 1 is the multi-armed bandit setting, which corresponds to the case\nwhen Λ’s output is supported only on the basis vectors. The multi-armed bandit setting can model\nexperiments involving adaptively chosen treatment arms—typically by some optimization algorithm\nlike UCB [3] or Thompson sampling [17]—where we want to estimate the mean outcome of each\narm. For the bandit setting, we will use the notation Ni,n, ˆµ(n)\ni\nto denote the count and sample mean\nrespectively of the pulls of arm i in the first n steps.\nAs discussed in the prior section, our BvM proof requires the assumption that the maximum eigenvalue\nof the data matrix grows subexponentially with respect to the minimum eigenvalue. However, this\nassumption is violated by many popular bandit algorithms such as UCB, where suboptimal arms are\npulled only Op(log n) times. Thus, we require a modification of the above proof to specifically the\ncase of bandits to include these existing algorithms. We also show that we can generalize our result to\ntriangular arrays of data—that is, we now allow the adaptive decision rule to depend on the sequence\nlength n and superscript it by n to make this dependence explicit: Λn. However, we assume that the\nparameter β0, variance σ2, and prior π remain the same throughout the array. The triangular array\nformulation enables us to extend our results to sampling procedures which have policies depending\non the length of the overall experiment such as is the case in the batched bandit setting [33]. First, we\nshow a consistency result for the case of triangular array bandits.\nLemma 1. Suppose independent length-mn trajectories Hn\nmn = ((xn\n1, yn\n1 ), . . . , (xn\nmn, yn\nmn)) are\ndrawn via Algorithm 1 using sampling rules Λn for each n, where β and σ2 remain the same for all\ntrajectories. Let Xn =\n\u0000xn\n1\n· · ·\nxn\nmn\n\u0001⊤and yn =\n\u0000yn\n1\n· · ·\nyn\nmn\n\u0001⊤. Assume the triangular\narray version of Algorithm 1 satisfies the following conditions:\n(i) Each xn\nj is a basis vector, i.e. xn\nj ∈{e1, . . . , ep}.\n(ii) λmin(X⊤\nn Xn)\np→∞.\nThen, the MLE ˆβn = (X⊤\nn Xn)−1X⊤\nn yn is consistent, i.e. ˆβn\np→β0.\nThe proof of Lemma 1 appears in Appendix C. In addition to this consistency result, we also need a\ncondition that allows us to truncate the posterior distribution. Note that the theorem below assumes\nthat the prior density π is bounded which we did not need in the proof of Theorem 1; this condition\nguarantees that the posterior can be asymptotically approximated as proportional to the likelihood\nwhen the regularity condition on the maximum eigenvalue is not met. With these changes, we no\nlonger need the condition that log λmax(X⊤\nn Xn) = op(λmin(X⊤\nn Xn)), making Theorem 2 very\nbroadly applicable to standard bandit algorithms.\nTheorem 2. Assume the triangular array version of Algorithm 1 satisfies the following conditions:\n(i) Each xn\nj is a basis vector, i.e. xn\nj ∈{e1, . . . , ep}.\n(ii) λmin(X⊤\nn Xn)\np→∞.\n(iii) π(·) has continuous and bounded density on Rp which is positive at β0.\nThen, the posterior distribution π(β|Hn\nmn) satisfies\n∥π(β|Hn\nmn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV\np→0.\nRemark 1. This result assumes homoskedasticity of the bandits, but it also holds if each arm has a\ndifferent (but still known) variance σ2\n1, . . . , σ2\np > 0, as shown in Theorem 5 in the Appendix E.\nThe proof of Theorem 2 appears in Appendix B.2. Note that the rate of convergence of the TV distance\nin Theorem 2 depends on the growth rate of λmin(X⊤\nn Xn), which can be logarithmically slow for\noptimal bandit algorithms as seen in Section 6. Theorem 2 gives the following corollary (proven\nin Appendix D) in the setting of non-triangular-array bandits, showing that the BvM convergence\nstatement is uniform over sampling rules as long as λmin(X⊤\nn Xn) grows arbitrarily large among\nthese sampling rules.\n5\nCorollary 1. For any sequences (rn, ϵn) for which rn →∞and ϵn →0, let Pn be the sequence\nof sets of distributions P of trajectories induced by sampling rules Λ such that for all n and for all\nP ∈Pn,\n(i) Each xn\nj is a basis vector, i.e. xn\nj ∈{e1, . . . , ep}.\n(ii) P(λmin(X⊤\nn Xn) > rn) > 1 −ϵn.\nThen, we have for any c > 0,\nlim sup\nn→∞\nsup\nP ∈Pn\nP(∥π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV > c) = 0.\nTheorem 2 implies that the interval [ˆµ(n)\ni\n± z1−α/2σN −1/2\ni,n\n] is an asymptotically valid Bayesian\ncredible interval. Note that this matches the Wald-type frequentist interval which one uses in i.i.d.\nsettings where the MLE is indeed normal. But [33] shows that for Thompson sampling in the two-arm\nbatched bandit setting, the distribution of the sample means is not asymptotically normal in the case\nwhen the true arm means are equal. Thus, Theorem 2 implies that in this setting the credible interval\nwill fail to be asymptotically frequentist-valid despite asymptotically matching the usual Wald-type\nfrequentist confidence interval.\nWe can generalize this result beyond Gaussian bandits to exponential family bandits, i.e., where we\nstill constrain xj in Algorithm 1 to be a basis vector, but now instead of yj being sampled from a\nGaussian with mean x⊤\nj β, it is sampled from a exponential family (with density exp(ηyj−b(η))h(yj))\nwith parameter η = x⊤\nj β. Exponential family models include many common distribution types such\nas Bernoulli, Poisson, Gamma, Beta, and Geometric distributions. For the remainder of our results in\nthis paper, we return to the non-triangular array setting where Λ is not allowed to depend on n.\nTheorem 3. Let β0 ∈Rp be the true parameter value and β0,i be the i-th coordinate of β0. Let Nn,i\nbe the number of times arm i was pulled and ¯Yn,i =\n1\nNn,i\nPn\nj=1 I[xj = i]yj. Let the local MLE\nbe ˆβn,i = β0,i +\n¯Yn,i−b′(β0,i)\nb′′(β0,i)\nand the empirical Fisher information be In = diag{Ni,nb′′(β0,i)}.\nSuppose the exponential family version of Algorithm 1 satisfies the following properties:\n(i) β0,1, . . . , β0,p are in the interior of the natural parameter space.\n(ii) mini Nn,i\np→∞.\n(iii) π(·) has continuous and bounded density on Rp which is positive at β0.\nThen\n∥π(β|Hn) −N(ˆβn, I−1\nn )∥TV\np→0.\nThe proof of Theorem 3 appears in Appendix B.3. The local MLE defined above is the maximizer of\nthe second order Taylor expansion of the log-likelihood around β0, and it is asymptotically equivalent\nto the true MLE by the asymptotic equivalence of the log-likelihood to its second-order Taylor\nexpansion in a local neighborhood of the true parameter value. Note that the above theorem requires\nthe true parameters to be in the interior of the natural parameter space, meaning for extremes of\ncommon models such as Poisson with rate near 0 or Bernoulli with probability near 1, the conclusion\nof the theorem may be a bad approximation for finite n.\nThe proof of this theorem is similar to that of Theorem 1, but here we require a Taylor expansion\nto express the likelihood as a function proportional to a Gaussian density. The bandit setting is\nparticularly nice for performing this expansion since the likelihood factors into the product of\nthe likelihoods for each individual arm. Thus, even if the eigenvalues of In grow at asymptotically\ndifferent rates, the likelihood is still well-approximated by the second-order expansion. This argument\nis harder when generalizing beyond bandits, as argued in Appendix F. Additionally, to argue that we\ncan truncate the posterior distribution to a local neighborhood, we use the convexity of b(·) to bound\nthe tails of the likelihood function. Our proof does not necessarily generalize to triangular arrays as it\nrelies on uniform bounds on a serialized sequence of samples, such as the one provided by the law of\niterated logarithms.\n6\n5\nBvM on Gaussian Linear Bandits\nAnother special case for our BvM theorem is the case of Gaussian linear bandits, in which a\ncontext xj is observed for each arm pull aj, impacting the resulting reward distribution through a\nlinear transformation, i.e. yj ∼N(x⊤\nj θaj, σ2) where θ1, . . . , θm are parameter vectors. To model\ncontextual bandits with our adaptive linear Gaussian data process in Algorithm 1, we will let the\nparameter be β ∈Rmd where the (id −d + 1)–(id)th indices of β represent θi; in other words, we\nstack the parameter vectors θ1, . . . , θm vertically. Then, when we observe context x′ ∈Rd and action\ni ∈{1, . . . , m}, we sample the outcome from N(β⊤x, σ2) where the (id −d + 1)–(id)th index of\nx ∈Rmd represents x′. Then, we can state BvM in the contextual bandit setting as follows.\nTheorem 4. Suppose p = md and we decompose the covariate space as\nX :=\n\u0000Rd × {0d} × · · · × {0d}\n\u0001\n∪\n\u0000{0d} × Rd × · · · × {0d}\n\u0001\n∪· · · ∪\n\u0000{0d} × {0d} × · · · × Rd\u0001\nAssume the sampling procedure in Algorithm 1 satisfies the following conditions:\n(i) Each action xj falls in X.\n(ii) For all i = 1, . . . , m, let Ti : X →Rd be a projection onto the (id −d + 1)–(id)th coordinates.\nLetting In,i = Pn\nj=1 Ti(xj)Ti(xj)⊤∈Rd×d, for all i we have\nλmin(In,i)\np→∞and log λmax(In,i) = op(λmin(In,i)).\n(iii) π(·) is continuous with bounded density on Rp and positive density at β0.\nThen, the posterior distribution π(β|Hn) satisfies\n∥π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV\np→0.\nThe proof of Theorem 4 appears in Appendix B.4. Note that condition (ii) in Theorem 4 is different\nfrom condition (i) in Theorem 1 because it concerns the distribution of contexts conditioned on a\nparticular arm, rather than the distribution of arm pulls. Thus, it is possible for a contextual bandit\nalgorithm to satisfy condition (ii) when it pulls different arms at exponentially different rates. This\nresult does not immediately generalize to triangular arrays, since we rely on consistency which may\nnot hold in the triangular array setting for contextual bandits. However, the non-triangular array\nversion of Theorem 2 is a special case of Theorem 4.\n6\nNumerical Experiments\nWe discuss the empirical validity of the statement of Theorem 1 in the setting of multi-armed ban-\ndits, contextual bandits, and the linear quadratic regulator (LQR). As seen in the previous section,\nthe posterior distribution is asymptotically equivalent in TV distance to the normal distribution\nN(ˆβn, σ2(X⊤\nn Xn)−1), which we call the representative normal distribution. Although this equiva-\nlence holds asymptotically, the convergence rate to this representative normal distribution may be\nquite slow depending on the rate of growth of λmin(X⊤\nn Xn). We empirically perform posterior\ninference in three common adaptive settings and show that the posterior does empirically converge to\nthe representative normal.\nWe use Monte Carlo and the relation ∥P −Q∥TV = EX∼P\nh\nmax\n\u0010\n0, 1 −Q(X)\nP (X)\n\u0011i\nto approximate\nthe TV distance between the posterior distribution and the representative normal distribution. As seen\nin Figures 1, 2, and 3, the convergence rate of the TV distance depends on the configuration of the\ntrue parameters. Note that in the multi-armed bandit setting, the convergence seems fastest when\nthe arm means are equal and becomes slower as the margin increases. One explanation for this is\nthat the result of Theorem 2 requires the condition λmin(X⊤\nn Xn)\np→∞, suggesting that the rate at\nwhich this minimum eigenvalue grows determines the rate of convergence of the TV distance. More\nspecifically, we see that in our proof of Theorem 2, we use the expression E\n\u0014\u0010\nπ(κn)\nπ(β0) −1\n\u0011\n+\n\u0015\nas an\n7\nFigure 1: (Left) Average TV distance measured in the BvM statement for UCB in two-arm Gaussian\nbandits over horizon T = 104 using 104 replicates under five different true parameter configurations\nlabelled by [µ1, µ2] where µ1, µ2 are the true means. (Right) Average TV distance measured in the\nBvM statement for lin-UCB on three-arm Gaussian linear contextual bandits with context distribution\nN(0, I2×2) under three different true parameter configurations. Standard Gaussian priors are used\nfor all arms. TV estimates shown have standard error at most 0.1 times the TV estimate.\nFigure 2: Average BvM TV distance for UCB on Bernoulli bandits and Poisson bandits, under the\nsame configurations as Figure 1. Beta(1, 1) priors are used for the Bernoulli bandit and Gamma(1, 1)\npriors for the Poisson bandit. The representative normal is centered at the true MLE, which is\nasymptotically equivalent to the local MLE used in Theorem 3. TV estimates shown have standard\nerror at most 0.1 times the TV estimate.\nupper bound on the TV distance where κn|Hn\nmn ∼N(ˆβn, σ2(X⊤\nn Xn)−1). Thus, we see that the\nrate at which λmin(X⊤\nn Xn) diverges affects the convergence rate of κn\np→β0, which in turn affects\nthe convergence π(κn)\nπ(β0) −1\np→0. This explains why the TV distance converges fastest for the zero\nmargin setting (even though that setting does not satisfy stability).\nWe simulate the algorithm Lin-UCB in the setting of contextual bandits under three different parameter\nconfigurations [22]. The “undominated” configuration represents a case where each arm is optimal\nfor some choice of context. In the “dominated” case, one arm is never optimal for any choice of\ncontext, and in the “duplicate” case, two arms share the same parameters. In Figure 1, we empirically\nsee that the convergence of the posterior to the representative normal is fastest for the undominated\nconfiguration, possibly because in this case, the optimal policy samples each arm with probability\nbounded away from zero. In the configuration where two arms are duplicate, there may be substantial\nbias in the estimation of the arm parameters [26], leading the “duplicate” case to have the slowest\nconvergence as seen in Figure 1.\nWe can also model the Linear Quadratic Regulator (LQR) [24], a common setting used in control\ntheory, using our adaptive linear Gaussian framework. In LQR, we control a state transition of the\n8\nform xj+1 = Axj + Buj + ϵj where A ∈Rk×k, B ∈Rk×d, uj are adaptively chosen actions,\nϵj ∼N(0, σ2Ik) for j = 1, . . . , n, and we aim to estimate the transition matrices A, B. To\nrepresent this as an instance of Algorithm 1, we serialize the observed state vectors xj into the\nsequence of observed outcome variables—that is, we use a trajectory of length ˜n = nk where\n˜y(i−1)k+1, . . . , ˜yik represents xi. We let the parameter value β be (A\nB) in row major order. Then,\nthe ((i −1)(k + d) + 1)–(i(k + d)) indices of the covariate ˜x(j−1)k+i ∈Rk(k+d) represent\n\u0012\nxj\nuj\n\u0013\nand all other entries are zero.\nWe simulate the Noisy Certainty Equivalent Control (NCEC) algorithm on LQR under three different\nconfigurations [30]. The “determined” configuration represents a case where the action space has the\nsame dimensionality as the state space and the action transition matrix is full rank. The “stabilizable”\ncase is underdetermined with fewer action dimensions than state dimensions but where the optimal\npolicy allows the system to be stable. The “unstabilizable” case is underdetermined where no policy\nmakes the system stable. In all settings, we empirically see that the BvM TV distance decreases over\ntime, with the convergence rate being the fastest for the unstabilizable case as it has the highest growth\nrate of λmin(X⊤\nn Xn). Finally, as we show in Proposition 3 in Appendix D, the NCEC algorithm\nsatisfies condition (i) in Theorem 1 so long as a certain stability condition holds (see Assumption 1\nin the same appendix section for a precise definition); both our “determined” and “stabilizable”\nsimulation settings satisfy this condition.\nWe also simulate Thompson Sampling in the two-stage Gaussian batched bandit with two arms,\nwhich is the setting analyzed in Theorem 2 of [33]. We compute the TV distance between the\nposterior and the representative normal in the same way for different values of the margin. As shown\nin Figure 3, we again see that the average BvM distance is smallest when the margin is zero. We\nalso plot the empirical coverage of the 95% credible interval for the margin, which matches the\ncoverage level except near the zero-margin case, as suggested by [33]. This is an example of a fairly\nwell-behaved sampling process where our BvM result applies but Bayesian credible intervals are\nstill not asymptotically frequentist-valid. Note that this setting satisfies the stability condition if and\nonly if the margin is nonzero. This suggests that the asymptotic frequentist invalidity of Bayesian\ncredible intervals is a local phenomenon around the zero-margin case. Figure 3 supports this analysis\nand also reveals that in this local region, some parameter values lead to overcoverage and some to\nundercoverage from a frequentist perspective. Our BvM result may provide an explanation for this, as\nthe Bayesian coverage under a prior containing this local zero-margin region must be correct, meaning\nthe coverage probability aggregated over the erroneous local region should match the coverage level.\nAppendix G contains a demonstration of our main results on a simple real-world dataset. Coverage\nplots for the other simulations are shown in Appendix H.\n7\nDiscussion and Future Work\nThis paper has shown that, for a number of important classes of adaptively collected data, a Bernstein–\nvon Mises theorem applies, linking Bayesian UQ and Wald-type frequentist UQ. This ensures that\nunder extremely mild conditions, Bayesian UQ is asymptotically Bayesian-valid even when the\nprior is misspecified, and when the stability condition of [21] holds, it also ensures Bayesian UQ is\nasymptotically frequentist-valid.\nOne of the surprising takeaways from our work is that when the stability condition of [21] fails, BvM\nholds but Bayesian UQ is asymptotically frequentist invalid. We note that our work, however, only\nconsiders the case when a fixed, nonrandom prior is used for Bayesian UQ. This raises the question of\nwhether there is a data-dependent way to set the prior so that Bayesian UQ is always asymptotically\nfrequentist valid; we could think of such a method as empirical Bayesian UQ.\nAnother direction of inquiry would be to extend the BvM result to general parametric models beyond\njust the Gaussian and exponential family cases. However, there are several obstacles to showing the\nadaptive BvM result in more general parametric models which we discuss in Appendix F.\nNote that this paper does not suggest a method of asymptotically frequentist-valid Bayesian inference;\nin fact, we would like to warn practitioners against using either Wald-type frequentist UQ or Bayesian\nUQ as if it were asymptotically frequentist-valid.\n9\nFigure 3: (Left) Average BvM TV distance and empirical coverage of the 95% credible interval\nfor the margin for Thompson Sampling in the two-batch two-arm Gaussian bandit setting with 104\nsamples per batch. Error bars are 95% confidence intervals over 2 × 105 replicates. Blacked dotted\nline is the correct coverage level. N(0, 1) priors are used. (Right) Average TV distance for Noisy\nCertainty Equivalent Control on LQR [30] under three different parameter configurations. Standard\nGaussian priors are used for all arms. TV estimates shown have standard error at most 0.2 times the\nTV estimate.\nDeclaration of LLM usage:\nChatGPT-4o was used to create code templates for a Python implementation of the lin-UCB and\nStepwise Noisy Certainty Equivalent Control algorithms. The authors revised the templates to ensure\ncorrect implementation and modified them to verify the BvM statement.\nBroader Impact:\nThe results presented here have the potential to guide the design of safer systems and more reliable\nhypothesis testing in adaptive experiments. However, it should be noted that the Bernstein–von Mises\ntheorem does not immediately imply frequentist-valid inference as discussed. Thus, Bayesian UQ\nshould not in general be treated as frequentist-valid. Instead, our result contributes to a more complete\nunderstanding of the differences between frequentist and Bayesian approaches in adaptively collected\ndata.\nReferences\n[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear\nstochastic bandits. Advances in neural information processing systems, 24, 2011.\n[2] Adrian Aguilera, Caroline A Figueroa, Rosa Hernandez-Ramos, Urmimala Sarkar, Anupama\nCemballi, Laura Gomez-Pathak, Jose Miramontes, Elad Yom-Tov, Bibhas Chakraborty, Xiaoxi\nYan, et al. mhealth app using machine learning to increase physical activity in diabetes and\ndepression: clinical trial protocol for the diamante study. BMJ open, 10(8):e034723, 2020.\n[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed\nbandit problem. Machine learning, 47:235–256, 2002.\n[4] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based\nreinforcement learning with stability guarantees. Advances in neural information processing\nsystems, 30, 2017.\n[5] Deepak L Bhatt and Cyrus Mehta. Adaptive designs for clinical trials. New England Journal of\nMedicine, 375(1):65–74, 2016.\n[6] Aurélien Bibaut, Maria Dimakopoulou, Nathan Kallus, Antoine Chambaz, and Mark van\nDer Laan. Post-contextual-bandit inference. Advances in neural information processing systems,\n34:28548–28559, 2021.\n10\n[7] Natalia Bochkina. Bernstein–von mises theorem and misspecified models: A review. Founda-\ntions of modern statistics, pages 355–380, 2019.\n[8] Natalia A Bochkina and Peter J Green. The bernstein–von mises theorem and nonregular\nmodels. 2014.\n[9] Ismaël Castillo and Judith Rousseau. A bernstein–von mises theorem for smooth functionals in\nsemiparametric models. 2015.\n[10] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in\nneural information processing systems, 24, 2011.\n[11] Benjamin Connault. A weakly dependent bernstein–von mises theorem. Technical report,\nWorking Paper, 2014.\n[12] Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, and Matt Taddy. Accurate inference for\nadaptive linear models. In International Conference on Machine Learning, pages 1194–1203.\nPMLR, 2018.\n[13] Allan Gut. Stopped random walks. Springer, 2009.\n[14] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence\nintervals for policy evaluation in adaptive experiments. Proceedings of the national academy of\nsciences, 118(15):e2014602118, 2021.\n[15] Steven R Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon.\nTime-uniform,\nnonparametric, nonasymptotic confidence sequences. The Annals of Statistics, 49(2):1055–1080,\n2021.\n[16] Emilie Kaufmann and Wouter M Koolen. Mixture martingales revisited with applications to\nsequential tests and confidence intervals. Journal of Machine Learning Research, 22(246):1–44,\n2021.\n[17] Emilie Kaufmann, Nathaniel Korda, and Rémi Munos. Thompson sampling: An asymptotically\noptimal finite-time analysis. In International conference on algorithmic learning theory, pages\n199–213. Springer, 2012.\n[18] Bas JK Kleijn and Aad W Van der Vaart. The bernstein-von-mises theorem under misspecifica-\ntion. 2012.\n[19] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238–1274, 2013.\n[20] Geerten Koers, Botond Szabó, and Aad van der Vaart. Misspecified bernstein-von mises theorem\nfor hierarchical models. arXiv preprint arXiv:2308.07803, 2023.\n[21] Tze Leung Lai and Ching Zong Wei. Least squares estimates in stochastic regression models\nwith applications to identification and control of dynamic systems. The Annals of Statistics,\npages 154–166, 1982.\n[22] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to\npersonalized news article recommendation. In Proceedings of the 19th international conference\non World wide web, pages 661–670, 2010.\n[23] Alexander R Luedtke and Mark J Van Der Laan. Statistical inference for the mean outcome\nunder a possibly non-unique optimal treatment strategy. Annals of statistics, 44(2):713, 2016.\n[24] Volker Ludwig Mehrmann. The autonomous linear quadratic control problem: theory and\nnumerical solution. Springer, 1991.\n[25] Yash Nair and Lucas Janson. Randomization tests for adaptively collected data. arXiv preprint\narXiv:2301.05365, 2023.\n11\n[26] Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou. Why adaptively collected data\nhave negative bias and how to correct for it. In International Conference on Artificial Intelligence\nand Statistics, pages 1261–1269. PMLR, 2018.\n[27] Anna Rafferty, Huiji Ying, Joseph Williams, et al. Statistical consequences of using multi-armed\nbandits to conduct adaptive educational experiments. Journal of Educational Data Mining,\n11(1):47–79, 2019.\n[28] Yuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. Large-scale open dataset,\npipeline, and benchmark for bandit algorithms. arXiv preprint arXiv:2008.07146, 2020.\n[29] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.\n[30] Feicheng Wang and Lucas Janson. Exact asymptotics for linear quadratic adaptive control.\nJournal of Machine Learning Research, 22(265):1–112, 2021.\n[31] Elad Yom-Tov, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, and Irit\nHochberg. Encouraging physical activity in patients with diabetes: intervention using a rein-\nforcement learning system. Journal of medical Internet research, 19(10):e338, 2017.\n[32] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare:\nA survey. ACM Computing Surveys (CSUR), 55(1):1–36, 2021.\n[33] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in\nneural information processing systems, 33:9818–9829, 2020.\n12\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: In the abstract and motivation, we claim that the BvM theorem generalizes to\nadaptive data without assuming a stability assumption on the data. This claim is shown in\nthe proof of Theorem 1 and intuition for this result is given in section 3 of the main text.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: In section 7 of the text, we discuss the phenomenon that Bayesian credible\nintervals may fail to be frequentist-valid and how our work does not explain how to charac-\nterize this discrepancy. We also discuss how our BvM result requires the data to be generated\nfrom an exponential family model which restricts the generality of the result.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\n13\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: Our main results, Theorems 1, 2, 3, 4 are all proven in Appendix B, with the\nsupporting lemmas also proven in Appendix C.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: The configurations for experiments are given in the figure captions of Figures 1,\n2, 3 with the algorithms (lin-UCB and NCEC) cited. We also describe how the TV distance\nwas estimated in the text.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\n14\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: Access to the source code is provided in Appendix A.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Details for the experiment parameters are included in the captions of Figures 1,\n2, 3 with full details included in the source code in Appendix A.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: The caption of figures describes the standard error of estimates provided in the\nexperiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n15\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We mention in Appendix A that the data in each figure require under 3 hours\nin CPU time to generate.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: Our work does not involve human subjects and our data do not contain any\nprivate or personal information. We hope that the statistical inference methods provided in\nthis paper guides more reliable experimentation methods in adaptive design.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We briefly discuss the broader impacts of this work at the end of the main text.\nGuidelines:\n16\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This paper does not contain any results that can be misused.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: This paper does not use existing code or data. The existing algorithms used in\nthe paper are implemented independently.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n17\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: This paper does not release new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: No crowdsourcing or research with human subjects was done in this paper.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: No crowdsourcing or research with human subjects was done in this paper.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n18\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [Yes]\nJustification: The usage of LLMs for this project is described under the “Declaration of\nLLM usage” section at the end of the main text.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n19\nA\nSource Code\nThe code for this project can be accessed at https://github.com/TheDukeVin/BvM/tree/main. Each\nfigure’s data takes under three hours of CPU time to generate.\nB\nMain Theorems\nB.1\nProof of Theorem 1\nWe first write an explicit expression for the posterior. Note that the likelihood can be written as, after\nremoving constant factors that don’t depend on β,\nL(β; Hn) =\n\n\nn\nY\nj=1\nΛ(xj|Hj−1)\n\n\n\n\nn\nY\nj=1\nP(yj|xj, β)\n\n\n∝\nn\nY\nj=1\nP(yj|xj, β) ∝\nn\nY\nj=1\nexp\n \n−(yj −x⊤\nj β)2\n2σ2\n!\n=\nn\nY\nj=1\nexp\n \n−(yj −ˆβ⊤\nn xj)2\n2σ2\n−(ˆβ⊤\nn xj −β⊤xj)2\n2σ2\n!\n∝exp(−1\n2σ2 (ˆβn −β)⊤X⊤\nn Xn(ˆβn −β)).\nThus, by Bayes’ rule, the posterior can be written as\nπ(β|Hn) ∝π(β) exp(−1\n2σ2 (ˆβn −β)⊤X⊤\nn Xn(ˆβn −β)).\nLet d(Hn) = ∥π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV. By assumption, π(β0) > 0, so using Lemma\n2, we have\nd(Hn) ≤\nZ \u0010 π(β)\nπ(β0)\n1\np\n(2πσ2)p|X⊤\nn Xn|−1 exp(−1\n2σ2 (β −ˆβn)⊤X⊤\nn Xn(β −ˆβn))\n−\n1\np\n(2πσ2)p|X⊤\nn Xn|−1 exp(−1\n2σ2 (β −ˆβn)⊤X⊤\nn Xn(β −ˆβn))\n\u0011\n+dβ\n=\nZ \u0012 π(β)\nπ(β0) −1\n\u0013\n+\n1\np\n(2πσ2)p|X⊤\nn Xn|−1 exp(−1\n2σ2 (β −ˆβn)⊤X⊤\nn Xn(β −ˆβn))dβ.\n= E\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n\f\f\fHn\n#\nwhere κn is a random variable marginally distributed as κn|Hn ∼N(ˆβn, σ2(X⊤\nn Xn)−1). Let cn\nbe a sequence of random variables satisfying\ncn\nlog λmax(X⊤\nn Xn)\np→∞and\ncn\nλmin(X⊤\nn Xn)\np→0, which\nmust exist by condition (i), i.e. we can let cn =\np\nλmin(X⊤\nn Xn) log λmax(X⊤\nn Xn). Then, to show\nd(Hn)\np→0, we use a common trick when showing BvM-style results [29, 11, 18]—that is, we will\npartition the expectation into parts inside and outside a local ellipsoid. We will show the following\ntwo statements.\n20\nE\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 ≤cn]\n\f\f\fHn\n#\np→0\n(1)\nE\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\n#\np→0\n(2)\nTo show (1), note that we have\nE\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 ≤cn]\n\f\f\fHn\n#\n≤\n1\nπ(β0)E\n\u0014\n(π(κn) −π(β0))+ 1[∥(κn −ˆβn)∥2 ≤\ncn\nλmin(X⊤\nn Xn)]\n\f\f\fHn\n\u0015\n≤\n1\nπ(β0)\nsup\n∥κn−ˆβn∥≤\nc1/2\nn\nλmin(X⊤\nn Xn)1/2\n(π(κn) −π(β0))+\n≤\n1\nπ(β0)\nsup\n∥κn−β0∥≤\nc1/2\nn\nλmin(X⊤\nn Xn)1/2 +∥ˆβn−β0∥\n(π(κn) −π(β0))+\nLai and Wei showed that ˆβn is consistent if condition (i) is satisfied [21].\nThus, we have\nc1/2\nn\nλmin(X⊤\nn Xn)1/2 + ∥ˆβn −β0∥\np→0, meaning the above expression does indeed converge to zero\nin probability if π is continuous at β0.\nTo show (2), note that by the triangle inequality, we have\nE\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\n#\n≤\n1\nπ(β0)E\nh\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\ni\n+ E\nh\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\ni\nThe first term can then be bounded as\n1\nπ(β0)E\nh\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\ni\n=\n1\nπ(β0)\nZ\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]P(κn|Hn)dκn\n≤\n1\nπ(β0)\nZ\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n1\np\n(2πσ2)p|X⊤\nn Xn|−1 exp(−cn\n2σ2 )dκn\n≤(2πσ2)−p/2\nπ(β0)\nexp(−cn\n2σ2 )λmax(X⊤\nn Xn)p/2\nZ\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]dκn\n≤(2πσ2)−p/2\nπ(β0)\nexp(−cn\n2σ2 + p\n2 log λmax(X⊤\nn Xn))\nZ\nπ(κn)dκn\n= (2πσ2)−p/2\nπ(β0)\nexp(−cn\n2σ2 + p\n2 log λmax(X⊤\nn Xn))\n21\nIf\ncn\nlog λmax(X⊤\nn Xn)\np→∞, the above expression converges to zero in probability. To compute the\nsecond term, note that (X⊤\nn Xn)1/2(κn −ˆβn)|Hn is distributed as N(0, σ2Ip). Thus, if cn\np→∞,\nthen E\nh\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\ni\np→0. We have shown that each term (1) and (2) in\nthe decomposition of the upper bound for d(Hn) converges to zero in probability, meaning d(Hn)\nconverges to zero in probability as well.\nB.2\nProof of Theorem 2\nAs in the proof of Theorem 1, the posterior distribution can be expressed as\nπ(β|Hn\nmn) ∝π(β) exp(−1\n2σ2 (ˆβn −β)⊤X⊤\nn Xn(ˆβn −β)).\nLetting d(Hn\nmn) = ∥π(β|Hn\nmn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV, we also have\nd(Hn\nmn) ≤E\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n\f\f\fHn\nmn\n#\nwhere κn|Hn\nmn ∼N(ˆβn, σ2(X⊤\nn Xn)−1).\nBy Lemma 1, we have that ˆβn is consistent.\nIf\nλmin(X⊤\nn Xn)\np→∞, then κn\np→β0 as we can write κn = β0 + (ˆβn −β0) + Zn where\nZn|Hn\nmn ∼N(0, σ2(X⊤\nn Xn)−1) and both the terms ˆβn −β0 and Zn converge to zero in prob-\nability. If π is continuous and bounded, by Vitali’s convergence theorem,\nE[d(Hn\nmn)] ≤E\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n#\n→0,\nmeaning d(Hn\nmn)\np→0.\nB.3\nProof of Theorem 3\nSuppose Yi,(1), Yi,(2), . . . are the serialized arm pulls from arm i. Let ¯Yi,(N) = 1\nN\nPN\nj=1 yi,(j) be the\nsequence of sample means. By Lemma 3, we have\n¯Yi,(N) −µ\nσ\n= o(N −1/2+ϵ) almost surely as N →∞.\n(3)\nfor some small ϵ, say ϵ = 0.01. Since ¯Yn,i = ¯Yi,(Nn,i), we have that ¯Yn,i −µi = Op(N −1/2+ϵ\nn,i\n).\nThus, ˆβn,i −β0,i = Op(N −1/2+ϵ\nn,i\n), meaning ˆβn,i is consistent for β0,i.\nWe prove the result in three steps by truncating both the posterior and normal distribution to the set\nCn = {β : ∀i, |βi−β0,i| ≤cn,i} for some sequence of random variables cn,i such that N 1/3\nn,i cn,i\np→0\nand N 1/2−ϵ\nn,i\ncn,i\np→∞as n →∞. We then show that\n∥π(β|Hn) −πCn(β|Hn)∥TV\np→0\n(4)\n∥πCn(β|Hn) −N Cn(β0, I−1\nn )∥TV\np→0\n(5)\n∥N Cn(β0, I−1\nn ) −N(β0, I−1\nn )∥TV\np→0.\n(6)\nWe first show (5). The likelihood function can be calculated as\nL(β; Hn) ∝\np\nY\ni=1\nexp(Nn,i( ¯Yn,iβi −b(βi))).\n22\nBy Taylor’s Theorem, for any β ∈Cn, we have\nL(β; Hn) ∝exp\n p\nX\ni=1\nNn,i[ ¯Yn,iβi −b(β0,i) −b′(β0,i)(βi −β0,i)\n−1\n2b′′(β0,i)(βi −β0,i)2 −1\n6b′′′(β∗\nn,β,i)(βi −β0,i)3]\n\u0013\nfor some β∗\nn,β ∈Cn. However, if b′′′ is bounded on a neighborhood of β0,i for each i and N 1/3\nn,i cn,i\np→\n0, we have that supβ∈Cn Nn,ib′′′(β∗\nn,β,i)(βi −β0,i)3\np→0. Thus, we can write\nL(β; Hn) ∝exp\n p\nX\ni=1\nNn,i[ ¯Yn,iβi −b(β0,i) −b′(β0,i)(βi −β0,i) −1\n2b′′(β0,i)(βi −β0,i)2] + op(1)\n!\n∝exp\n p\nX\ni=1\nNn,i[−1\n2b′′(β0,i)β2\ni + ( ¯Yn,i −b′(β0,i) + b′′(β0,i)β0,i)βi]\n!\n(1 + op(1))\n= exp\n p\nX\ni=1\nNn,i[−1\n2b′′(β0,i)β2\ni + b′′(β0,i)ˆβn,iβi]\n!\n(1 + op(1))\n∝exp\n \n−1\n2\np\nX\ni=1\nNn,ib′′(β0,i)(βi −ˆβn,i)2\n!\n(1 + op(1))\n(7)\n= exp(−1\n2(β −ˆβn)⊤In(β −ˆβn))(1 + op(1))\nLet the density of the truncated normal distribution N Cn(ˆβn,i, I−1\nn ) be expressed as\nAn1β∈Cn exp(−1\n2(β −ˆβn,i)⊤In(β −ˆβn,i)).\nwhere An is chosen such that the density is suitably normalized. Let dn(Hn) = ∥πCn(β|Hn) −\nN Cn(β0, I−1\nn )∥TV. By Lemma 2, we have\ndn(Hn) ≤\nZ  \nAn1β∈Cn\nπ(β)L(β; Hn)\nπ(ˆβn)L(ˆβn; Hn)\n−An1β∈Cn exp(−1\n2(β −ˆβn)⊤In(β −ˆβn))\n!\n+\ndβ\n≤\nZ\nAn1β∈Cn\n \nπ(β)\nπ(ˆβn)\nexp(−1\n2(β −ˆβn)⊤In(β −ˆβn))(1 + ¯op(1))\n−exp(−1\n2(β −ˆβn)⊤In(β −ˆβn))\n\u0013\n+\ndβ\nwhere we say a sequence of random variables Wn(β) indexed by β is ¯op(1) if supβ∈Cn Wn(β)\np→0.\nThus, we have\ndn(Hn) ≤\nZ   \nπ(β)\nπ(ˆβn)\n−1 + ¯op(1)\n!\nAn1β∈Cn exp(−1\n2(β −ˆβn)⊤In(β −ˆβn))\n!\n+\ndβ.\n= Eηn∼N Cn( ˆβn,i,I−1\nn )\n \nπ(ηn)\nπ(ˆβn)\n−1 + ¯op(1)\n!\n+\n≤\n\u0012supβ∈Cn π(β)\ninfβ∈Cn π(β) −1 + ¯op(1)\n\u0013\n+\n23\nThen, since cn,i\np→0 for each i and π is continuous, we know\nsupβ∈Cn π(β)\ninfβ∈Cn π(β)\np→1, meaning the above\nexpression indeed converges to 0 in probability. Next, we show (4). Note that\n∥π(β|Hn) −πCn(β|Hn)∥TV = Pπ(β ∈CC\nn |Hn)\n=\nR\nCC\nn π(β)L(β; Hn)dβ\nR\nRp π(β)L(β; Hn)dβ\n≤πmax\nπmin\nn\nR\nCC\nn L(β; Hn)dβ\nR\nCn L(β; Hn)dβ\nwhere πmax = supβ π(β) and πmin\nn\n= infβ∈Cn π(β). Note that if π is continuous and positive at β0,\nthen πmin\nn\nconverges to some positive constant. Then, since the ancillary function b of the exponential\nfamily is convex, the likelihood function is log-concave in β, meaning we have\nR\nCC\nn L(β; Hn)dβ\nR\nCn L(β; Hn)dβ ≤sup\nv∈∂Cn\nR ∞\n1\nL(β0 + (v −β0)t; Hn)tp−1dt\nR 1\n0 L(β0 + (v −β0)t; Hn)tp−1dt\n≤sup\nv∈∂Cn\nR ∞\n1\nexp(ℓn(β0) + (ℓn(v) −ℓn(β0))t)tp−1dt\nR 1\n0 exp(ℓn(β0) + (ℓn(v) −ℓn(β0))t)tp−1dt\n= sup\nv∈∂Cn\nR ∞\nℓn(β0)−ℓn(v) exp(−x)xp−1dx\nR ℓn(β0)−ℓn(v)\n0\nexp(−x)xp−1dx\n=\nR ∞\nan exp(−x)xp−1dx\nR an\n0\nexp(−x)xp−1dx\nwhere an = infv∈∂Cn ℓn(β0) −ℓn(v). Note that the function a 7→\nR ∞\na\nexp(−x)xp−1dx\nR a\n0 exp(−x)xp−1dx goes to zero as\na →∞. Thus, it suffices to show that an\np→∞. To do this, note that by expression 7, we have\nan =\ninf\nv∈∂Cn −1\n2\np\nX\ni=1\nNn,ib′′(β0,i)[(β0,i −ˆβn,i)2 −(vi −ˆβn,i)2] + ¯op(1).\nSince |vi −ˆβn,i| ≥∥vi −β0,i| −|β0,i −ˆβn,i∥= |cn,i −|β0,i −ˆβn,i∥, we have\nan ≥\ninf\nv∈∂Cn −1\n2\np\nX\ni=1\nNn,ib′′(β0,i)[|β0,i −ˆβn,i|2 −(cn,i −|β0,i −ˆβn,i|)2] + ¯op(1)\n=\ninf\nv∈∂Cn −1\n2\np\nX\ni=1\nNn,ib′′(β0,i)[|β0,i −ˆβn,i|2 −c2\nn,i + 2cn,i|β0,i −ˆβn,i| −|β0,i −ˆβn,i|2] + ¯op(1)\n=\ninf\nv∈∂Cn\n1\n2\np\nX\ni=1\nc2\nn,iNn,ib′′(β0,i)[1 −2|β0,i −ˆβn,i|\ncn,i\n] + ¯op(1)\nBy the definition of cn,i as well as Equation (3), we know c2\nn,iNn,i\np→∞and β0,i−ˆβn,i\ncn,i\np→0. Thus,\nwe indeed have an\np→∞. Lastly, we show (6). We have\n∥N(β0, I−1\nn ) −N Cn(β0, I−1\nn )∥TV = PX∼N (β0,I−1\nn )(∃i, |Xi −β0,i| > cn,i)\n= PX∼N (0,Ip×p)(∃i, |Xi| > cn,iN 1/2\nn,i b′′(β0,i)1/2).\nIf cn,iN 1/2\nn,i\np→∞, then the above expression indeed coverges to zero in probability.\n24\nB.4\nProof of Theorem 4\nWe only need to establish the consistency of ˆβn, after which the result follows by a similar argument\nto the proof of Theorem 2. Consistency can be shown from condition (ii) by Theorem 1 of [21].\nC\nTechnical Lemmas\nProof of Lemma 1:\nWe translate this directly to the bandit setting, after which this result follows\nfrom a similar version of Theorem 2.2 of [13]. For each i = 1, . . . , p, let µi = βi be the i-th\ncoordinate of β which represents the true mean of arm i. Let Ni,n = (Xn)ii be the number of times\narm i is pulled in the trajectory Hn\nmn. We serialize all arm pulls from each arm i—that is, suppose\nyn\ni,(1), yn\ni,(2), . . .\nind\n∼N(µi, σ2). Then, when drawing yn\nj |xn\nj ∼N(µxj, σ2), we look up the next\nunused sample in the serialized sequence yn\nxj,(1), . . . and use that as our observation.\nUnder this formulation of the data-generating process, the sample mean ˆβn,i is simply the mean\nof the first Ni,n samples from the serialized sequence yn\ni,(1), . . .. Note that the serialized sample\nmeans ¯yn\ni,t = 1\nt\nPt\nj=1 yn\ni,(j) satisfy ¯yn\ni,t\na.s.\n→µi as t →∞for each i, n by the strong law of large\nnumbers. Then, Proposition 1 implies that for each i, n, there exists some function ϵn\ni (·) satisfying\nlimB→∞ϵn\ni (B) = 0 such that for any B,\nP[N ≥B] ≥1 −1\nB =⇒P[|¯yn\ni,(N) −µi| ≤ϵn\ni (B)] ≥1 −ϵn\ni (B)\nCrucially, note that the functions ϵn\ni (·) can be chosen such that ϵn\ni (·) does not depend on n. This is\nbecause the distribution of serialized samples {yn\ni,(j)}i∈{1,...,p},j≥1 for each trajectory is identical,\nand in Proposition 1, the bound translation function ϵ is chosen solely based on the distribution of\nthe almost-surely convergent sequence. Finally, by condition (ii), we indeed have that Ni,n\np→∞\nfor each i, meaning the condition P[Ni,n ≥Bn] ≥1 −\n1\nBn is indeed satisfied for some sequence\nBn →∞- to see this, note that letting Bn = sup{B : P[Ni,n ≥B] ≥1 −1\nB }, we must have\nBn →∞as otherwise there would be some constant C where P[Ni,n ≤C] >\n1\nC for infinitely\nmany n which would contradict the statement that Ni,n diverges in probability. Thus, we have\nP[|¯yn\ni,(Ni,n) −µi| ≤ϵn\ni (Bn)] ≥1 −ϵn\ni (Bn) for each n meaning the sample mean ˆβn,i = ¯yn\ni,(Ni,n) is\nindeed consistent for µi.\nProposition 1. (Bound translation) Suppose Yn is a sequence of random variables such that Yn\na.s.\n→Y\nfor some real number Y . Then there exists a function ϵ(B) with ϵ(B) →0 as B →∞such that for\nany random variable N and positive integer B, we have\nP[N ≥B] ≥1 −1\nB =⇒P[|YN −Y | ≤ϵ(B)] ≥1 −ϵ(B).\nProof. Note that it is sufficient to show that for any fixed η, there is some function ϵη(B) such that\nlimB→∞ϵη(B) = 0 and\nP[N ≥B] ≥1 −1\nB =⇒P[|YN −Y | ≤η] ≥1 −ϵη(B).\nTo show this, we simply let ϵη(B) = 1\nB + P[∃n ≥B, |Yn −Y | > η]. If Yn converges almost surely\nto Y , then limB→∞ϵη(B) = 0. Also, we indeed have\nP[|YN −Y | ≤η] ≥P[N ≥B and ∀n ≥B, |Yn −Y | ≤η] ≥1 −ϵη(B).\nSince we can do this for any η, the statement of the result is indeed true.\nLemma 2. Let P(·) and Q(·) be continuous distributions and c ∈R be any constant. Then,\n∥P −Q∥TV = 1\n2\nR\n(P(x) −Q(x))+dx ≤\nR\n(cP(x) −Q(x))+dx.\n25\nProof. Let A = {x : P(x) > Q(x)}. Let a1 =\nR\nA P(x) −Q(x)dx and a2 =\nR\nAC Q(x) −P(x)dx.\nThen, note that\n∥P −Q∥TV = 1\n2\nZ\n(P(x) −Q(x))+dx = 1\n2a1 + 1\n2a2\nand\n0 =\nZ\nP(x) −Q(x)dx = a1 −a2\nSolving for a1 and a2, we get\na1 = a2 = ∥P −Q∥TV.\nTo show the desired result, if c ≥1, we have\n∥P −Q∥TV = a1 ≤\nZ\nA\ncP(x) −Q(x)dx ≤\nZ\n(cP(x) −Q(x))+dx.\nSimilarly, if c ≤1, then\n∥P −Q∥TV = a2 ≤\nZ\nAC Q(x) −cP(x)dx ≤\nZ\n(cP(x) −Q(x))+dx.\nLemma 3. Let X1, X2, . . . be i.i.d. samples from a distribution P with mean µ and finite variance\nσ2. Let ˆµn = 1\nn\nPn\ni=1 Xi. Then,\nn1/2−ϵ(ˆµn −µ)\na.s.\n→0.\nfor any ϵ > 0.\nProof. By the law of iterated logarithms, we have\nlim sup\nn→∞\n√n|ˆµn −µ|\np\n2σ2 log log n\n= 1\nwith probability 1. Then, we have\nlim sup\nn→∞n1/2−ϵ|ˆµn −µ| ≤\n \nlim sup\nn→∞\n√n|ˆµn −µ|\np\n2σ2 log log n\n!  \nlim sup\nn→∞\np\n2σ2 log log n\nnϵ\n!\n= 0\nwith probability 1.\nD\nAuxilliary results\nProposition 2. Let x1 be deterministic and take any real value. Let the adaptive sampling procedure\nsimply deterministically set xi = yi−1 for i > 1. Then, if β = 1, the MLE is asymptotically\nnon-Normal. However, condition (i) of Theorem 1 is satisfied by this sampling design.\nProof. The asymptotic non-Normality is established by Lai and Wei in [21, Example 3]. The second\npart of condition (i) is satisfied because the covariates are 1-dimensional and the first part is satisfied\nbecause Pn\ni=1 x2\ni\np→∞, which is a direct consequence of [21, Equation (4.9)].\n26\nWe now recall an assumption from [30]:\nAssumption 1 (Stability, [30, Assumption 1]). There exists a matrix K0 for which the maximum\nabsolute eigenvalue of A + BK0 is less than 1.\nProposition 3. Suppose we run the stepwise NCEC algorithm (Algorithm 1 in [30]) on an instance\nof LQR satisfying Assumption 1 using any configuration of the hyperparameters τ 2 > 0, β ∈[1/2, 1)\nand α > 0 permitted therein. Then, this satisfies condition (i) of Theorem 1.\nProof. First, observe that our Gram matrix ˜X⊤˜X is more simply expressed as Ik×k ⊗Gn where\n⊗denotes Kronecker product and Gn := Pn\ni=1\n\u0014\nxi\nui\n\u0015 \u0014\nxi\nui\n\u0015⊤\ndenotes the Gram matrix. Because the\nlargest and smallest eigenvalues of Ik×k ⊗Gn are the same as that of Gn it suffices to study the\nspectrum of the latter. Theorem 3 of [30] shows, in the stabilizable regime, that\nD−1\nn Gn\n\u0000D−1\nn\n\u0001⊤\np→Ik+d,\nwhere\nDn = nβ/2 logα/2(n)\n\u0014\nIk\n0\nK\nId\n\u0015 \u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015\n,\n(8)\nCn = n1−β log−α(n)\n∞\nX\np=0\n(A+BK)p((A+BK)p)⊤σ2 + τ 2\nβ\n∞\nX\nq=0\n(A+BK)qBB⊤((A+BK)q)⊤,\n(9)\nand K is a matrix that does not depend on n (see equation (3) of [30] for a precise definition).\nfrom which it follows that\n\r\rGn −DnD⊤\nn\n\r\r = op(1).\n(10)\nBelow, we show that λmax\n\u0000DnD⊤\nn\n\u0001\n≤O(n) and λmin\n\u0000DnD⊤\nn\n\u0001\n≥Ω(nβ logα(n)). Because Equa-\ntion (10) implies both that λmin(Gn) −λmin\n\u0000DnD⊤\nn\n\u0001\n= op(1) and λmin(Gn) −λmin\n\u0000DnD⊤\nn\n\u0001\n=\nop(1), we indeed have that condition (i) of Theorem 1 is met: log (λmax(Gn)) = op(λmin(Gn)) and\nλmin(Gn)\np→∞.\nUpper bound on λmax(DnD⊤\nn )\nBy submultiplicativity of the operator norm, we have, using\nequation (8), that the largest eigenvalue of DnD⊤\nn is at most\nnβ logα(n)\n\r\r\r\r\n\u0014\nIk\n0\nK\nId\n\u0015\r\r\r\r\n2 \r\r\r\r\n\u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015\r\r\r\r\n2\nThe middle term is of constant order and therefore it suffices to upper-bound the last term. Because\n\r\r\r\r\n\u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015\r\r\r\r ≤max\n\u0010\n∥C1/2\nn\n∥, ∥\np\nτ 2/βId∥\n\u0011\nand since ∥\np\nτ 2/βId∥is again of constant\norder it suffices to simply bound ∥C1/2\nn\n∥. Using equation (9) and triangle inequality, this is at most\nO(n(1−β)/2 log−α/2(n)). Consequently, the maximum eigenvalue of DnD⊤\nn is O (n).\nLower bound on λmin(DnD⊤\nn )\nTo lower bound the minimum eigenvalue of DnD⊤\nn , we apply the\nsame argument to the maximum eigenvalue of (D⊤\nn )−1D−1\nn . The largest eigenvalue of (D⊤\nn )−1D−1\nn\nis at most\nn−β log−α(n)\n\r\r\r\r\r\n\u0014\nIk\n0\nK\nId\n\u0015−1\r\r\r\r\r\n2 \r\r\r\r\r\n\u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015−1\r\r\r\r\r\n2\nThen,\n\r\r\r\r\r\n\u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015−1\r\r\r\r\r ≤max(||C−1\nn ||1/2,\np\nβ/τ 2) = max(λmin(Cn)−1/2,\np\nβ/τ 2).\nAgain using equation (9) and the triangle inequality, this is at most O(1), meaning the minimum\neigenvalue of DnD⊤\nn is at least Ω(nβ logα(n)).\n27\nProof of Corollary 1: Suppose by contradiction that there is some k where\nsup\nP ∈Pn\nP(||π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)||T V > c) > k\nfor infinitely many n. Then, for all such n, there is some sampling rule Λn whose corresponding\ndistribution trajectory P satisfies\nP(||π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)||T V > c) ≥k.\nHowever, this contradicts the statement of Theorem 2.\nE\nHeteroskedastic Bandits\nAlgorithm 2 Heteroskedastic Gaussian bandits\nInput Action set A = {1, . . . , p}, Arm sampling rule Λ : (A×R)∗→∆(A), Reward distributions\nBi = N(βi, σ2\ni ).\nOutput Sampling trajectory Hn\nH0 ←∅\nfor j = 1, . . . , n do\nSample aj ∼Λ(·|Hj−1)\nSample yj ∼N(βaj, σ2\naj)\nHj ←{(x1, y1), . . . , (xj, yj)}\nend for\nTheorem 5. Suppose we generate a length-mn trajectory Hn\nmn = ((an\n1, yn\n1 ), . . . , (an\nn, yn\nn)) for each\nn according to decision rules Λn in the case of Heteroskedastic Gaussian bandits. Assume the\nsampling procedure satisfies the following conditions:\n(i) mini Ni,n\np→∞where Ni,n = Pn\nj=1 I[an\nj = i].\n(ii) π(·) is continuous and bounded on Rp with positive density at β0.\nThen, the posterior distribution π(β|Hn\nmn) satisfies\n∥π(β|Hn\nmn) −N(ˆβn, diag(σ2\ni N −1\ni,n ))∥TV\np→0.\nProof. We rescale this problem to an instance of the homoskedastic bandit problem and apply\nTheorem 2. Let the covariate sampling rule be ˜Λn(Hj−1) = eΛn(Hj−1) and the rescaled parameters\nbe ˜βi = βi\nσi with the homoskedastic variance ˜σ = 1. Let ˜π(˜β) ∝π(σ ∗˜β) where ∗represents\nelement-wise multiplication where σ is the vector of (σi)p\ni=1. Then, by Theorem 2,\n∥˜π(˜β| ˜Hn\nmn) −N(˜ˆβn, ( ˜X⊤\nn ˜Xn)−1)∥TV\np→0.\nRescaling the posterior back to the original parameters gives the desired result.\nF\nParametric BvM\nThere are considerable challenges when extending the BvM result to parametric models satisfying\nonly weak regularity conditions such as differentiability in quadratic mean. Consider replacing the\nnormal distribution in Algorithm 1 with a general parametric model Pθ(·|xj) with θ = β⊤xj. We\nmay want to show that in this setting, the posterior distribution is asymptotically normal similar to\nthe BvM statement, i.e.\n28\n∥π(θ|Hn) −N(ˆθn, J−1\nn )∥TV\np→0\nwhere Jn = −Pn\nj=1 ¨ℓˆθn(yj|xj) is the empirical Fisher Information and ℓθ = log Pθ is the log-\nlikelihood. Suppose we follow the steps of the classical BvM proof. Letting πCn(θ|Hn) and\nN Cn(ˆθn, J−1\nn ) denote truncations of these distributions to the set Cn, the classical proof proceeds in\nthe following three steps—we show each of the following convergences:\n∥π(θ|Y ) −πCn(θ|Y )∥TV\np→0\n∥πCn(θ|Y ) −N Cn(ˆθn, J−1\nn )∥TV\np→0\n∥N Cn(ˆθn, J−1\nn ) −N(ˆθn, J−1\nn )∥TV\np→0.\nThe first step requires a method of truncating the posterior distribution, which is in general quite\ndifficult without strong assumptions on the behavior of the adaptive system similar to the results\nof [18] and [11]. With independent data, we could truncate the posterior due to Hoeffding bounds\non the likelihood, but these bounds do not apply in adaptively collected data. The second step was\nimplied by a second-order Taylor expansion of the log-likelihood in the classical proof, but in adaptive\nsettings, the Fisher information matrix Jn may grow at different rates in different directions, i.e.\nλmin(Jn) may grow at a different rate than λmax(Jn). This would make an argument based on a\nTaylor expansion more complicated and a valid proof seems to require further assumptions on the\ngrowth rate of Jn. Finally, the third step was implied in the i.i.d. setting by the local consistency of\nˆθn, i.e. J1/2\nn\n(ˆθn −θ0) = Op(1) but this condition may be violated in adaptive experiments. In fact,\nas Lai and Wei showed, we require some nontrivial assumptions on the empirical Fisher Information\nto guarentee consistency, even in the case of Gaussian regression.\nG\nReal-world example\nWe compute the BvM total variation distance on a real-world instance of Bernoulli Thompson\nsampling provided in [28]. The dataset consists of the interaction of a fashion recommendation\nalgorithm with users, where the algorithm is optimizing for the number of clicks on fashion items.\nAlthough the original dataset is modeled with a contextual bandit setting with a batch size of 3, we\nsimplify their environment to an 80-armed bandit problem by disregarding the context and flattening\nthe batch size. These simplifications were made solely for computational convenience. The dataset\nconsists of 12m impressions or time steps, during which we update the posterior distribution with a\nBeta(1, 1) prior. The BvM total variation distance throughout this rollout is shown below.\nFigure 4: BvM TV distance for the Zozo dataset with prior Beta(1, 1). TV estimates computed with\n104 samples and have standard error at most 0.004.\nAs seen in Figure 4, the convergence of the total variation distance is quite slow. This is perhaps\ndue to the sparse success rates in the Bernoulli bandit and the larger dimensionality of the parameter\n29\nspace. It should be noted that the original dataset contained missing data within batches, i.e. not all\nbatches had size 3.\nH\nPlots\nFigure 5: Frequentist coverage of the Bayesian credible interval for the contextual bandit and LQR,\nunder the same configurations as Section 6. Coverage estimates shown have standard error at most\n0.004.\nFigure 6: Frequentist coverage of the Bayesian credible interval for UCB on Gaussian bandits,\nBernoulli bandits and Poisson bandits, under the same configurations as Section 6. Coverage\nestimates shown have standard error at most 0.004.\n30\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Bernstein–von Mises for Adaptively Collected Data\nKevin Du\nDepartment of Statistics\nHarvard University\nkevindu@college.harvard.edu\nYash Nair\nDepartment of Statistics\nStanford University\nyashnair@stanford.edu\nLucas Janson\nDepartment of Statistics\nHarvard University\nljanson@fas.harvard.edu\nAbstract\nUncertainty quantification (UQ) for adaptively collected data, such as that coming\nfrom adaptive experiments, bandits, or reinforcement learning, is necessary for\ncritical elements of data collection such as ensuring safety and conducting after-\nstudy inference. The data’s adaptivity creates significant challenges for frequentist\nUQ, yet Bayesian UQ remains the same as if the data were independent and\nidentically distributed (i.i.d.), making it an appealing and commonly used approach.\nBayesian UQ requires the (correct) specification of a prior distribution while\nfrequentist UQ does not, but for i.i.d. data the celebrated Bernstein–von Mises\ntheorem shows that as the sample size grows, the prior ‘washes out’ and Bayesian\nUQ becomes frequentist-valid, implying that the choice of prior need not be a\nmajor impediment to Bayesian UQ as it makes no difference asymptotically. This\npaper for the first time extends the Bernstein–von Mises theorem to adaptively\ncollected data, proving asymptotic equivalence between Bayesian UQ and Wald-\ntype frequentist UQ in this challenging setting. Our result showing this asymptotic\nagreement does not require the standard stability condition required by works\nstudying validity of Wald-type frequentist UQ; in cases where stability is satisfied,\nour results combined with these prior studies of frequentist UQ imply frequentist\nvalidity of Bayesian UQ. Counterintuitively however, they also provide a negative\nresult that Bayesian UQ is not asymptotically frequentist valid when stability fails,\ndespite the fact that the prior washes out and Bayesian UQ asymptotically matches\nstandard Wald-type frequentist UQ. We empirically validate our theory (positive\nand negative) via a range of simulations.\n1\nIntroduction\nData in applications such as robotics [19], healthcare [32], clinical trials [5], online education [27],\nmobile health [2, 31], and online advertising [22, 10] is routinely being collected adaptively. This\nmeans that the data is collected sequentially, with decisions about the data collection itself being\nmade online based on all the data observed up to that time point. In particular, the online decisions\noften try to focus on actions or interventions that prior data indicates will produce large values of\nsome reward function, and this type of data collection includes adaptive experiments, multi-armed\nbandits, and reinforcement learning.\nTwo important elements of adaptive data collection are (1) assessing at the end of the data collection\nwhat has been learned (e.g., to assess confidence in a putatively optimal clinical treatment or inform\na future online advertising campaign) [33, 14] and (2) making sure to avoid certain bad outcomes\n39th Conference on Neural Information Processing Systems\n"
    },
    {
      "page_number": 2,
      "text": "during the data collection process (e.g., avoiding crashing a robot while it is reinforcement learning)\n[4]. Both of these elements, as well as others, rely critically on uncertainty quantification (UQ) of\nthe parameters of the data-generating environment, yet standard frequentist UQ, such as Wald-type\ninference based on the maximum likelihood estimator (MLE), is made far more complicated by the\nadaptivity of the data collection [21] which results in data which is not i.i.d., Markovian, or even\nstationary.\nOn the other hand, the Bayesian statistical paradigm translates seamlessly to the adaptive setting, so\nthat the vast wealth of Bayesian UQ methodology designed for i.i.d. data can be directly applied to\nadaptively collected data to provide strong Bayesian probabilistic guarantees. However, the validity\nof those guarantees is predicated on correctly specifying a prior distribution for the parameters, which\ncan be challenging to do or justify in practice. For i.i.d. data, the celebrated Bernstein–von Mises\n(BvM) theorem [29, Theorem 10.1] proves that for any reasonable choice of prior, Bayesian UQ\nasymptotically matches that of prior-free Wald-type frequentist UQ, providing strong Bayesian and\nfrequentist justification for Bayesian UQ in large samples without the analyst having to worry too\nmuch about the choice of prior. Although BvM has been extended beyond i.i.d. data in a number of\nways (see Section 2 for more details), it has not been extended to the setting of adaptively collected\ndata.\nContributions\nThis paper for the first time proves BvM results for adaptively collected data,\nshowing that Bayesian UQ asymptotically matches (asymptotically normal) Wald-type frequentist\nUQ under certain mild conditions. Our first result, in Section 3, applies to a very general class of\nadaptive linear Gaussian settings that include Gaussian multi-armed bandits, adaptive Gaussian linear\nbandits (a form of contextual bandit), and the linear quadratic regulator (a form of reinforcement\nlearning on a Markov decision process). We then show in Section 4 that in the special case of\nmulti-armed bandits, the conditions of our first result can be weakened, and the Gaussian assumption\ncan also be generalized to any exponential family. Finally, we show in Section 5 that the conditions\nof the first result can also be weakened for linear (contextual) bandits. Section 6 empirically validates\nour theoretical results. A surprising aspect of our BvM results is that they do not require the key\nstability condition used in frequentist validity results; however, it is known that Wald-type frequentist\ninference may fail to be asymptotically valid in cases when the stability condition does not hold.\nThe counterintuitive implication is that in some adaptive settings, Bayesian UQ asymptotically\nmatches frequentist UQ (and the prior becomes irrelevant), yet Bayesian UQ is not asymptotically\nfrequentist-valid.\nNotation and Terminology.\nThroughout this paper, we refer to a confidence set as asymptotically\nfrequentist-valid at level α if for any parameter value, the limit inferior of the frequentist coverage\nis at least 1 −α. Similarly, we refer to a credible set as asymptotically Bayesian-valid at level α if\nthe limit inferior of the Bayesian coverage is at least 1 −α. We will use the term frequentist (resp.\nBayesian) UQ generically to refer to any of the many forms of frequentist (resp. Bayesian) statistical\ninference such as hypothesis tests, confidence intervals, or prediction intervals (resp. such as Bayesian\nhypothesis tests, credible intervals, or posterior predictive intervals). By Wald-type frequentist UQ,\nwe refer to standard asymptotic hypothesis testing or confidence interval construction using the MLE\nand its asymptotic Normality—for instance, as demonstrated in Example 15.6 of [29]. Let λmin(A)\nand λmax(A) denote the minimum and maximum eigenvalues of the matrix A respectively.\n2\nBackground\nAs mentioned in the previous section, Bayesian statistical inference requires no adjustment for the\nadaptivity of the data. This both makes the Bayesian approach an appealing and commonly used\ntool for UQ in adaptively collected data and means there is no need for (and thus a lack of) prior\nwork extending it to the adaptive setting. Thus, prior work on UQ for adaptively collected data has\nfocused on frequentist UQ, with the primary approach being to make assumptions on both the data\nand the adaptive assignment algorithm by which it is collected. These assumptions either (1) enable\nthe use of martingale central limit theorems to establish asymptotic normality of common frequentist\nestimators like the MLE (see, e.g., [21, 33, 14, 6, 12, 23]) or (2) allow for conservative finite-sample\ninference via martingale-based concentration bounds [15, 16, 1]. These conditions are often highly\ntechnical and hard to check, and reduce to or are related to a stability condition introduced in [21];\nsee Section 3 for more details. Another line of work uses randomization testing for frequentist UQ\n2\n"
    },
    {
      "page_number": 3,
      "text": "for adaptively collected data, providing non-asymptotic and non-conservative guarantees but relying\non sampling procedures that can be computationally prohibitive to use [25].\nThe classical BvM theorem [29, Theorem 10.1] proves that under minimal conditions, for i.i.d. data\ndrawn from a parametric model, the posterior distribution is asymptotically normal centered at the\nMLE with variance equal to the inverse Fisher information, and hence agrees asymptotically with\n(and inherits the asymptotic frequentist validity of) Wald-style frequentist inference. Prior works\ngeneralizing BvM to non-i.i.d. data only apply in settings where the ratio of the maximum and\nminimum eigenvalues of the Fisher information matrix is bounded [18, 11, 8, 20, 7, 9]. This condition\nfails to hold in most adaptive settings where, over time, certain actions are learned to be better than\nothers and are asymptotically sampled infinitely more often than the worse actions, resulting in\nan unbounded ratio (e.g., regret-optimal multi-armed bandit algorithms sample suboptimal arms\nlogarithmically often, leading to an eigenvalue ratio that grows like n/ log(n) [3]).\n3\nBvM on Adaptive Linear Gaussian Data\nThis paper will primarily consider the adaptive data collection setting, laid out in Algorithm 1, that\nat time step j allows the data collector to choose via an arbitrary function Λ a covariate vector\nxj based on all data observed so far (Hj−1 = ((x1, y1), . . . , (xj−1, yj−1))), and then observe\nyj ∼N(x⊤\nj β, σ2). We assume the only unknown in this sampling procedure is β, and hence the task\nat hand is to perform UQ for β based on the full data trajectory Hn = (Xn, yn), where Xn is the\nn × p matrix with jth row given by x⊤\nj and yn is the n-vector with jth entry given by yj.\nAlgorithm 1 Adaptive Linear Gaussian Sampling Procedure\nInput Covariate sampling rule Λ : (Rp × R)∗→∆(Rp), coefficient vector β ∈Rp, variance\nσ2 ∈R+\nOutput Sampling trajectory Hn\nH0 ←∅\nfor j = 1, . . . , n do\nSample xj ∼Λ(·|Hj−1)\nSample yj ∼N(x⊤\nj β, σ2)\nHj ←((x1, y1), . . . , (xj, yj))\nend for\nHere, Λ denotes a placeholder that can represent any sampling algorithm including UCB, Thompson\nsampling, autoregressive models, etc. We now present our first result, which states that the posterior\ndistribution is asymptotically normal, centered at the MLE with variance equal to the inverse empirical\nFisher information.\nTheorem 1. Suppose π(β) is the prior distribution for β and π(β|Hn) is the posterior after observing\nthe trajectory Hn. Let ˆβn = (X⊤\nn Xn)−1X⊤\nn yn be the MLE for β and let β0 be the true value of β.\nAssume the sampling procedure in Algorithm 1 satisfies the following conditions:\n(i) λmin(X⊤\nn Xn)\np→∞and log λmax(X⊤\nn Xn) = op(λmin(X⊤\nn Xn)).\n(ii) π(·) is continuous with positive density at β0.\nThen, the posterior distribution π(β|Hn) satisfies\n∥π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV\np→0.\nTheorem 1’s proof, given in Appendix B.1, first upper bounds the key total variation (TV) distance\nin terms of the L1 distances of unnormalized densities, allowing us to ignore normalizing constants\nin our analysis. Our proof then uses the fact that the posterior distribution is not affected by the\nadaptivity of the algorithm, since terms in the likelihood involving the covariate sampling process Λ\ndo not depend on the parameter and can thus be absorbed into the normalizing constant. We also use\na common trick in BvM-style proofs [18, 29] of truncating the posterior to local ellipsoids defined as\n3\n"
    },
    {
      "page_number": 4,
      "text": "{β : ∥(X⊤\nn Xn)1/2(β −β0)∥≤Mn} for some local radius Mn. This truncation, however, differs\nfrom those used in previous BvM-style proofs, as here the empirical Fisher information X⊤\nn Xn/σ2\ndoes not necessarily have bounded condition number, implying that the resulting ellipsoids may be\nhighly anisotropic, stretching much wider in some directions than in others. This step of our proof\nrelies critically on the second part of condition (i) of Theorem 1 which allows us to show that the\ndensity of the representative normal distribution outside of local ellipsoids converges to zero. We\nfurthermore show that the posterior can be written as proportional to the prior multiplied by the\ndensity of the representative normal, meaning we can truncate the distribution to a local ellipsoid\neven if the prior is unbounded.\nOur proof requires condition (i) for a couple of reasons. If the maximum eigenvalue of the observation\nmatrix grows exponentially faster than the minimum eigenvalue, there are two potential pathological\nconsequences. First, Lai and Wei showed that condition (i) of Theorem 1 is nearly necessary for\nMLE consistency; in particular, if condition (i) of Theorem 1 is removed, the MLE is not necessarily\nconsistent [21, Example 1]. Second, even if ˆβn is consistent but condition (i) fails, it may no longer\nbe the case that the density of the distribution N(ˆβn, σ2(X⊤\nn Xn)−1) at some other point β′ ̸= β0\nconverges to 0. One can see this by writing the density as\nN(β′; ˆβn, σ2(X⊤\nn Xn)−1) =\ns\n|X⊤\nn Xn|\n(2πσ2)p exp(−1\n2σ2 (β′ −ˆβn)⊤X⊤\nn Xn(β′ −ˆβn)),\nwhere N(x; µ, σ2) denotes the density value at x of the N(µ, σ2) distribution. While the exponential\nterm in the above expression necessarily converges to zero if β′ ̸= β0, ˆβn is consistent, and\nλmin(X⊤\nn Xn)\np→∞, it is not necessarily true that the entire expression converges to zero or is\neven bounded. This is because the quadratic term inside the exponential may only be of order\nO(λmin(X⊤\nn Xn)) whereas the term |X⊤\nn Xn| grows at least as quickly as λmax(X⊤\nn Xn). Thus,\nalthough the measure of a neighborhood of β′ under the probability measure N(ˆβn, σ2(X⊤\nn Xn)−1)\nmust converge to zero, without condition (i), it is possible that the density of this distribution diverges\nat β′. We show in the proof of Theorem 1 that the likelihood function is proportional to the density\nof N(ˆβn, σ2(X⊤\nn Xn)−1), meaning that it might be possible that the density of both the prior and\nthe normalized likelihood function diverge at β′. This could potentially cause the posterior to have\nasymptotically non-negligible measure in a neighborhood of β′, which would violate the BvM\nstatement as the measure of the normal distribution N(ˆβn, σ2(X⊤\nn Xn)−1) converges to zero in a\nneighborhood of β′.\nNotably, Theorem 1 does not require the key stability condition, originally from [21] but used in\nmany works since then for frequentist UQ for adaptively collected data, which assumes the existence\nof a deterministic sequence Bn for which B−1\nn (X⊤\nn Xn)1/2\np→Ip×p. Without this condition, ˆβn may\nfail to be asymptotically normal [21, Example 3]. In Proposition 2 in Appendix D we show that their\nexample does, however, satisfy the conditions of our Theorem 1. This leads to the rather surprising\nresult that there are settings where Bayesian UQ is asymptotically equivalent to standard Wald-style\nfrequentist UQ, yet the latter (and therefore also the former) is asymptotically frequentist-invalid. We\nwill describe another such example in the next section, which requires the triangular array version of\nBvM we prove there.\nThe previous paragraph notes that Theorem 1 can indicate either asymptotic frequentist validity\nor invalidity of Bayesian UQ, depending on the situation. However, in terms of Bayesian validity,\nTheorem 1 (and indeed all the theorems in the paper) provide a strong positive result regarding\nmisspecified priors: they say that the prior distribution π is ‘washed out’ as n →∞. Thus, from a\nBayesian perspective, a credible interval asymptotically has the correct coverage even if the prior\nis misspecified, as long as both the correct prior and the misspecified prior used for the inference\nare continuous and bounded, with the misspecified prior’s support containing that of the correct\nprior. Note, however, that the rate at which the “wash out” effect occurs depends on the rate at\nwhich the posterior distribution converges in TV distance to a normal distribution, which may be\nlogarithmically slow for optimal bandit algorithms. Thus, it may not be accurate in finite samples to\ntreat a misspecified prior as having “washed out”.\n4\n"
    },
    {
      "page_number": 5,
      "text": "4\nBvM on Multi-armed Bandits\nA notable special case of Algorithm 1 is the multi-armed bandit setting, which corresponds to the case\nwhen Λ’s output is supported only on the basis vectors. The multi-armed bandit setting can model\nexperiments involving adaptively chosen treatment arms—typically by some optimization algorithm\nlike UCB [3] or Thompson sampling [17]—where we want to estimate the mean outcome of each\narm. For the bandit setting, we will use the notation Ni,n, ˆµ(n)\ni\nto denote the count and sample mean\nrespectively of the pulls of arm i in the first n steps.\nAs discussed in the prior section, our BvM proof requires the assumption that the maximum eigenvalue\nof the data matrix grows subexponentially with respect to the minimum eigenvalue. However, this\nassumption is violated by many popular bandit algorithms such as UCB, where suboptimal arms are\npulled only Op(log n) times. Thus, we require a modification of the above proof to specifically the\ncase of bandits to include these existing algorithms. We also show that we can generalize our result to\ntriangular arrays of data—that is, we now allow the adaptive decision rule to depend on the sequence\nlength n and superscript it by n to make this dependence explicit: Λn. However, we assume that the\nparameter β0, variance σ2, and prior π remain the same throughout the array. The triangular array\nformulation enables us to extend our results to sampling procedures which have policies depending\non the length of the overall experiment such as is the case in the batched bandit setting [33]. First, we\nshow a consistency result for the case of triangular array bandits.\nLemma 1. Suppose independent length-mn trajectories Hn\nmn = ((xn\n1, yn\n1 ), . . . , (xn\nmn, yn\nmn)) are\ndrawn via Algorithm 1 using sampling rules Λn for each n, where β and σ2 remain the same for all\ntrajectories. Let Xn =\n\u0000xn\n1\n· · ·\nxn\nmn\n\u0001⊤and yn =\n\u0000yn\n1\n· · ·\nyn\nmn\n\u0001⊤. Assume the triangular\narray version of Algorithm 1 satisfies the following conditions:\n(i) Each xn\nj is a basis vector, i.e. xn\nj ∈{e1, . . . , ep}.\n(ii) λmin(X⊤\nn Xn)\np→∞.\nThen, the MLE ˆβn = (X⊤\nn Xn)−1X⊤\nn yn is consistent, i.e. ˆβn\np→β0.\nThe proof of Lemma 1 appears in Appendix C. In addition to this consistency result, we also need a\ncondition that allows us to truncate the posterior distribution. Note that the theorem below assumes\nthat the prior density π is bounded which we did not need in the proof of Theorem 1; this condition\nguarantees that the posterior can be asymptotically approximated as proportional to the likelihood\nwhen the regularity condition on the maximum eigenvalue is not met. With these changes, we no\nlonger need the condition that log λmax(X⊤\nn Xn) = op(λmin(X⊤\nn Xn)), making Theorem 2 very\nbroadly applicable to standard bandit algorithms.\nTheorem 2. Assume the triangular array version of Algorithm 1 satisfies the following conditions:\n(i) Each xn\nj is a basis vector, i.e. xn\nj ∈{e1, . . . , ep}.\n(ii) λmin(X⊤\nn Xn)\np→∞.\n(iii) π(·) has continuous and bounded density on Rp which is positive at β0.\nThen, the posterior distribution π(β|Hn\nmn) satisfies\n∥π(β|Hn\nmn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV\np→0.\nRemark 1. This result assumes homoskedasticity of the bandits, but it also holds if each arm has a\ndifferent (but still known) variance σ2\n1, . . . , σ2\np > 0, as shown in Theorem 5 in the Appendix E.\nThe proof of Theorem 2 appears in Appendix B.2. Note that the rate of convergence of the TV distance\nin Theorem 2 depends on the growth rate of λmin(X⊤\nn Xn), which can be logarithmically slow for\noptimal bandit algorithms as seen in Section 6. Theorem 2 gives the following corollary (proven\nin Appendix D) in the setting of non-triangular-array bandits, showing that the BvM convergence\nstatement is uniform over sampling rules as long as λmin(X⊤\nn Xn) grows arbitrarily large among\nthese sampling rules.\n5\n"
    },
    {
      "page_number": 6,
      "text": "Corollary 1. For any sequences (rn, ϵn) for which rn →∞and ϵn →0, let Pn be the sequence\nof sets of distributions P of trajectories induced by sampling rules Λ such that for all n and for all\nP ∈Pn,\n(i) Each xn\nj is a basis vector, i.e. xn\nj ∈{e1, . . . , ep}.\n(ii) P(λmin(X⊤\nn Xn) > rn) > 1 −ϵn.\nThen, we have for any c > 0,\nlim sup\nn→∞\nsup\nP ∈Pn\nP(∥π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV > c) = 0.\nTheorem 2 implies that the interval [ˆµ(n)\ni\n± z1−α/2σN −1/2\ni,n\n] is an asymptotically valid Bayesian\ncredible interval. Note that this matches the Wald-type frequentist interval which one uses in i.i.d.\nsettings where the MLE is indeed normal. But [33] shows that for Thompson sampling in the two-arm\nbatched bandit setting, the distribution of the sample means is not asymptotically normal in the case\nwhen the true arm means are equal. Thus, Theorem 2 implies that in this setting the credible interval\nwill fail to be asymptotically frequentist-valid despite asymptotically matching the usual Wald-type\nfrequentist confidence interval.\nWe can generalize this result beyond Gaussian bandits to exponential family bandits, i.e., where we\nstill constrain xj in Algorithm 1 to be a basis vector, but now instead of yj being sampled from a\nGaussian with mean x⊤\nj β, it is sampled from a exponential family (with density exp(ηyj−b(η))h(yj))\nwith parameter η = x⊤\nj β. Exponential family models include many common distribution types such\nas Bernoulli, Poisson, Gamma, Beta, and Geometric distributions. For the remainder of our results in\nthis paper, we return to the non-triangular array setting where Λ is not allowed to depend on n.\nTheorem 3. Let β0 ∈Rp be the true parameter value and β0,i be the i-th coordinate of β0. Let Nn,i\nbe the number of times arm i was pulled and ¯Yn,i =\n1\nNn,i\nPn\nj=1 I[xj = i]yj. Let the local MLE\nbe ˆβn,i = β0,i +\n¯Yn,i−b′(β0,i)\nb′′(β0,i)\nand the empirical Fisher information be In = diag{Ni,nb′′(β0,i)}.\nSuppose the exponential family version of Algorithm 1 satisfies the following properties:\n(i) β0,1, . . . , β0,p are in the interior of the natural parameter space.\n(ii) mini Nn,i\np→∞.\n(iii) π(·) has continuous and bounded density on Rp which is positive at β0.\nThen\n∥π(β|Hn) −N(ˆβn, I−1\nn )∥TV\np→0.\nThe proof of Theorem 3 appears in Appendix B.3. The local MLE defined above is the maximizer of\nthe second order Taylor expansion of the log-likelihood around β0, and it is asymptotically equivalent\nto the true MLE by the asymptotic equivalence of the log-likelihood to its second-order Taylor\nexpansion in a local neighborhood of the true parameter value. Note that the above theorem requires\nthe true parameters to be in the interior of the natural parameter space, meaning for extremes of\ncommon models such as Poisson with rate near 0 or Bernoulli with probability near 1, the conclusion\nof the theorem may be a bad approximation for finite n.\nThe proof of this theorem is similar to that of Theorem 1, but here we require a Taylor expansion\nto express the likelihood as a function proportional to a Gaussian density. The bandit setting is\nparticularly nice for performing this expansion since the likelihood factors into the product of\nthe likelihoods for each individual arm. Thus, even if the eigenvalues of In grow at asymptotically\ndifferent rates, the likelihood is still well-approximated by the second-order expansion. This argument\nis harder when generalizing beyond bandits, as argued in Appendix F. Additionally, to argue that we\ncan truncate the posterior distribution to a local neighborhood, we use the convexity of b(·) to bound\nthe tails of the likelihood function. Our proof does not necessarily generalize to triangular arrays as it\nrelies on uniform bounds on a serialized sequence of samples, such as the one provided by the law of\niterated logarithms.\n6\n"
    },
    {
      "page_number": 7,
      "text": "5\nBvM on Gaussian Linear Bandits\nAnother special case for our BvM theorem is the case of Gaussian linear bandits, in which a\ncontext xj is observed for each arm pull aj, impacting the resulting reward distribution through a\nlinear transformation, i.e. yj ∼N(x⊤\nj θaj, σ2) where θ1, . . . , θm are parameter vectors. To model\ncontextual bandits with our adaptive linear Gaussian data process in Algorithm 1, we will let the\nparameter be β ∈Rmd where the (id −d + 1)–(id)th indices of β represent θi; in other words, we\nstack the parameter vectors θ1, . . . , θm vertically. Then, when we observe context x′ ∈Rd and action\ni ∈{1, . . . , m}, we sample the outcome from N(β⊤x, σ2) where the (id −d + 1)–(id)th index of\nx ∈Rmd represents x′. Then, we can state BvM in the contextual bandit setting as follows.\nTheorem 4. Suppose p = md and we decompose the covariate space as\nX :=\n\u0000Rd × {0d} × · · · × {0d}\n\u0001\n∪\n\u0000{0d} × Rd × · · · × {0d}\n\u0001\n∪· · · ∪\n\u0000{0d} × {0d} × · · · × Rd\u0001\nAssume the sampling procedure in Algorithm 1 satisfies the following conditions:\n(i) Each action xj falls in X.\n(ii) For all i = 1, . . . , m, let Ti : X →Rd be a projection onto the (id −d + 1)–(id)th coordinates.\nLetting In,i = Pn\nj=1 Ti(xj)Ti(xj)⊤∈Rd×d, for all i we have\nλmin(In,i)\np→∞and log λmax(In,i) = op(λmin(In,i)).\n(iii) π(·) is continuous with bounded density on Rp and positive density at β0.\nThen, the posterior distribution π(β|Hn) satisfies\n∥π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV\np→0.\nThe proof of Theorem 4 appears in Appendix B.4. Note that condition (ii) in Theorem 4 is different\nfrom condition (i) in Theorem 1 because it concerns the distribution of contexts conditioned on a\nparticular arm, rather than the distribution of arm pulls. Thus, it is possible for a contextual bandit\nalgorithm to satisfy condition (ii) when it pulls different arms at exponentially different rates. This\nresult does not immediately generalize to triangular arrays, since we rely on consistency which may\nnot hold in the triangular array setting for contextual bandits. However, the non-triangular array\nversion of Theorem 2 is a special case of Theorem 4.\n6\nNumerical Experiments\nWe discuss the empirical validity of the statement of Theorem 1 in the setting of multi-armed ban-\ndits, contextual bandits, and the linear quadratic regulator (LQR). As seen in the previous section,\nthe posterior distribution is asymptotically equivalent in TV distance to the normal distribution\nN(ˆβn, σ2(X⊤\nn Xn)−1), which we call the representative normal distribution. Although this equiva-\nlence holds asymptotically, the convergence rate to this representative normal distribution may be\nquite slow depending on the rate of growth of λmin(X⊤\nn Xn). We empirically perform posterior\ninference in three common adaptive settings and show that the posterior does empirically converge to\nthe representative normal.\nWe use Monte Carlo and the relation ∥P −Q∥TV = EX∼P\nh\nmax\n\u0010\n0, 1 −Q(X)\nP (X)\n\u0011i\nto approximate\nthe TV distance between the posterior distribution and the representative normal distribution. As seen\nin Figures 1, 2, and 3, the convergence rate of the TV distance depends on the configuration of the\ntrue parameters. Note that in the multi-armed bandit setting, the convergence seems fastest when\nthe arm means are equal and becomes slower as the margin increases. One explanation for this is\nthat the result of Theorem 2 requires the condition λmin(X⊤\nn Xn)\np→∞, suggesting that the rate at\nwhich this minimum eigenvalue grows determines the rate of convergence of the TV distance. More\nspecifically, we see that in our proof of Theorem 2, we use the expression E\n\u0014\u0010\nπ(κn)\nπ(β0) −1\n\u0011\n+\n\u0015\nas an\n7\n"
    },
    {
      "page_number": 8,
      "text": "Figure 1: (Left) Average TV distance measured in the BvM statement for UCB in two-arm Gaussian\nbandits over horizon T = 104 using 104 replicates under five different true parameter configurations\nlabelled by [µ1, µ2] where µ1, µ2 are the true means. (Right) Average TV distance measured in the\nBvM statement for lin-UCB on three-arm Gaussian linear contextual bandits with context distribution\nN(0, I2×2) under three different true parameter configurations. Standard Gaussian priors are used\nfor all arms. TV estimates shown have standard error at most 0.1 times the TV estimate.\nFigure 2: Average BvM TV distance for UCB on Bernoulli bandits and Poisson bandits, under the\nsame configurations as Figure 1. Beta(1, 1) priors are used for the Bernoulli bandit and Gamma(1, 1)\npriors for the Poisson bandit. The representative normal is centered at the true MLE, which is\nasymptotically equivalent to the local MLE used in Theorem 3. TV estimates shown have standard\nerror at most 0.1 times the TV estimate.\nupper bound on the TV distance where κn|Hn\nmn ∼N(ˆβn, σ2(X⊤\nn Xn)−1). Thus, we see that the\nrate at which λmin(X⊤\nn Xn) diverges affects the convergence rate of κn\np→β0, which in turn affects\nthe convergence π(κn)\nπ(β0) −1\np→0. This explains why the TV distance converges fastest for the zero\nmargin setting (even though that setting does not satisfy stability).\nWe simulate the algorithm Lin-UCB in the setting of contextual bandits under three different parameter\nconfigurations [22]. The “undominated” configuration represents a case where each arm is optimal\nfor some choice of context. In the “dominated” case, one arm is never optimal for any choice of\ncontext, and in the “duplicate” case, two arms share the same parameters. In Figure 1, we empirically\nsee that the convergence of the posterior to the representative normal is fastest for the undominated\nconfiguration, possibly because in this case, the optimal policy samples each arm with probability\nbounded away from zero. In the configuration where two arms are duplicate, there may be substantial\nbias in the estimation of the arm parameters [26], leading the “duplicate” case to have the slowest\nconvergence as seen in Figure 1.\nWe can also model the Linear Quadratic Regulator (LQR) [24], a common setting used in control\ntheory, using our adaptive linear Gaussian framework. In LQR, we control a state transition of the\n8\n"
    },
    {
      "page_number": 9,
      "text": "form xj+1 = Axj + Buj + ϵj where A ∈Rk×k, B ∈Rk×d, uj are adaptively chosen actions,\nϵj ∼N(0, σ2Ik) for j = 1, . . . , n, and we aim to estimate the transition matrices A, B. To\nrepresent this as an instance of Algorithm 1, we serialize the observed state vectors xj into the\nsequence of observed outcome variables—that is, we use a trajectory of length ˜n = nk where\n˜y(i−1)k+1, . . . , ˜yik represents xi. We let the parameter value β be (A\nB) in row major order. Then,\nthe ((i −1)(k + d) + 1)–(i(k + d)) indices of the covariate ˜x(j−1)k+i ∈Rk(k+d) represent\n\u0012\nxj\nuj\n\u0013\nand all other entries are zero.\nWe simulate the Noisy Certainty Equivalent Control (NCEC) algorithm on LQR under three different\nconfigurations [30]. The “determined” configuration represents a case where the action space has the\nsame dimensionality as the state space and the action transition matrix is full rank. The “stabilizable”\ncase is underdetermined with fewer action dimensions than state dimensions but where the optimal\npolicy allows the system to be stable. The “unstabilizable” case is underdetermined where no policy\nmakes the system stable. In all settings, we empirically see that the BvM TV distance decreases over\ntime, with the convergence rate being the fastest for the unstabilizable case as it has the highest growth\nrate of λmin(X⊤\nn Xn). Finally, as we show in Proposition 3 in Appendix D, the NCEC algorithm\nsatisfies condition (i) in Theorem 1 so long as a certain stability condition holds (see Assumption 1\nin the same appendix section for a precise definition); both our “determined” and “stabilizable”\nsimulation settings satisfy this condition.\nWe also simulate Thompson Sampling in the two-stage Gaussian batched bandit with two arms,\nwhich is the setting analyzed in Theorem 2 of [33]. We compute the TV distance between the\nposterior and the representative normal in the same way for different values of the margin. As shown\nin Figure 3, we again see that the average BvM distance is smallest when the margin is zero. We\nalso plot the empirical coverage of the 95% credible interval for the margin, which matches the\ncoverage level except near the zero-margin case, as suggested by [33]. This is an example of a fairly\nwell-behaved sampling process where our BvM result applies but Bayesian credible intervals are\nstill not asymptotically frequentist-valid. Note that this setting satisfies the stability condition if and\nonly if the margin is nonzero. This suggests that the asymptotic frequentist invalidity of Bayesian\ncredible intervals is a local phenomenon around the zero-margin case. Figure 3 supports this analysis\nand also reveals that in this local region, some parameter values lead to overcoverage and some to\nundercoverage from a frequentist perspective. Our BvM result may provide an explanation for this, as\nthe Bayesian coverage under a prior containing this local zero-margin region must be correct, meaning\nthe coverage probability aggregated over the erroneous local region should match the coverage level.\nAppendix G contains a demonstration of our main results on a simple real-world dataset. Coverage\nplots for the other simulations are shown in Appendix H.\n7\nDiscussion and Future Work\nThis paper has shown that, for a number of important classes of adaptively collected data, a Bernstein–\nvon Mises theorem applies, linking Bayesian UQ and Wald-type frequentist UQ. This ensures that\nunder extremely mild conditions, Bayesian UQ is asymptotically Bayesian-valid even when the\nprior is misspecified, and when the stability condition of [21] holds, it also ensures Bayesian UQ is\nasymptotically frequentist-valid.\nOne of the surprising takeaways from our work is that when the stability condition of [21] fails, BvM\nholds but Bayesian UQ is asymptotically frequentist invalid. We note that our work, however, only\nconsiders the case when a fixed, nonrandom prior is used for Bayesian UQ. This raises the question of\nwhether there is a data-dependent way to set the prior so that Bayesian UQ is always asymptotically\nfrequentist valid; we could think of such a method as empirical Bayesian UQ.\nAnother direction of inquiry would be to extend the BvM result to general parametric models beyond\njust the Gaussian and exponential family cases. However, there are several obstacles to showing the\nadaptive BvM result in more general parametric models which we discuss in Appendix F.\nNote that this paper does not suggest a method of asymptotically frequentist-valid Bayesian inference;\nin fact, we would like to warn practitioners against using either Wald-type frequentist UQ or Bayesian\nUQ as if it were asymptotically frequentist-valid.\n9\n"
    },
    {
      "page_number": 10,
      "text": "Figure 3: (Left) Average BvM TV distance and empirical coverage of the 95% credible interval\nfor the margin for Thompson Sampling in the two-batch two-arm Gaussian bandit setting with 104\nsamples per batch. Error bars are 95% confidence intervals over 2 × 105 replicates. Blacked dotted\nline is the correct coverage level. N(0, 1) priors are used. (Right) Average TV distance for Noisy\nCertainty Equivalent Control on LQR [30] under three different parameter configurations. Standard\nGaussian priors are used for all arms. TV estimates shown have standard error at most 0.2 times the\nTV estimate.\nDeclaration of LLM usage:\nChatGPT-4o was used to create code templates for a Python implementation of the lin-UCB and\nStepwise Noisy Certainty Equivalent Control algorithms. The authors revised the templates to ensure\ncorrect implementation and modified them to verify the BvM statement.\nBroader Impact:\nThe results presented here have the potential to guide the design of safer systems and more reliable\nhypothesis testing in adaptive experiments. However, it should be noted that the Bernstein–von Mises\ntheorem does not immediately imply frequentist-valid inference as discussed. Thus, Bayesian UQ\nshould not in general be treated as frequentist-valid. Instead, our result contributes to a more complete\nunderstanding of the differences between frequentist and Bayesian approaches in adaptively collected\ndata.\nReferences\n[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear\nstochastic bandits. Advances in neural information processing systems, 24, 2011.\n[2] Adrian Aguilera, Caroline A Figueroa, Rosa Hernandez-Ramos, Urmimala Sarkar, Anupama\nCemballi, Laura Gomez-Pathak, Jose Miramontes, Elad Yom-Tov, Bibhas Chakraborty, Xiaoxi\nYan, et al. mhealth app using machine learning to increase physical activity in diabetes and\ndepression: clinical trial protocol for the diamante study. BMJ open, 10(8):e034723, 2020.\n[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed\nbandit problem. Machine learning, 47:235–256, 2002.\n[4] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based\nreinforcement learning with stability guarantees. Advances in neural information processing\nsystems, 30, 2017.\n[5] Deepak L Bhatt and Cyrus Mehta. Adaptive designs for clinical trials. New England Journal of\nMedicine, 375(1):65–74, 2016.\n[6] Aurélien Bibaut, Maria Dimakopoulou, Nathan Kallus, Antoine Chambaz, and Mark van\nDer Laan. Post-contextual-bandit inference. Advances in neural information processing systems,\n34:28548–28559, 2021.\n10\n"
    },
    {
      "page_number": 11,
      "text": "[7] Natalia Bochkina. Bernstein–von mises theorem and misspecified models: A review. Founda-\ntions of modern statistics, pages 355–380, 2019.\n[8] Natalia A Bochkina and Peter J Green. The bernstein–von mises theorem and nonregular\nmodels. 2014.\n[9] Ismaël Castillo and Judith Rousseau. A bernstein–von mises theorem for smooth functionals in\nsemiparametric models. 2015.\n[10] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in\nneural information processing systems, 24, 2011.\n[11] Benjamin Connault. A weakly dependent bernstein–von mises theorem. Technical report,\nWorking Paper, 2014.\n[12] Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, and Matt Taddy. Accurate inference for\nadaptive linear models. In International Conference on Machine Learning, pages 1194–1203.\nPMLR, 2018.\n[13] Allan Gut. Stopped random walks. Springer, 2009.\n[14] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence\nintervals for policy evaluation in adaptive experiments. Proceedings of the national academy of\nsciences, 118(15):e2014602118, 2021.\n[15] Steven R Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon.\nTime-uniform,\nnonparametric, nonasymptotic confidence sequences. The Annals of Statistics, 49(2):1055–1080,\n2021.\n[16] Emilie Kaufmann and Wouter M Koolen. Mixture martingales revisited with applications to\nsequential tests and confidence intervals. Journal of Machine Learning Research, 22(246):1–44,\n2021.\n[17] Emilie Kaufmann, Nathaniel Korda, and Rémi Munos. Thompson sampling: An asymptotically\noptimal finite-time analysis. In International conference on algorithmic learning theory, pages\n199–213. Springer, 2012.\n[18] Bas JK Kleijn and Aad W Van der Vaart. The bernstein-von-mises theorem under misspecifica-\ntion. 2012.\n[19] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238–1274, 2013.\n[20] Geerten Koers, Botond Szabó, and Aad van der Vaart. Misspecified bernstein-von mises theorem\nfor hierarchical models. arXiv preprint arXiv:2308.07803, 2023.\n[21] Tze Leung Lai and Ching Zong Wei. Least squares estimates in stochastic regression models\nwith applications to identification and control of dynamic systems. The Annals of Statistics,\npages 154–166, 1982.\n[22] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to\npersonalized news article recommendation. In Proceedings of the 19th international conference\non World wide web, pages 661–670, 2010.\n[23] Alexander R Luedtke and Mark J Van Der Laan. Statistical inference for the mean outcome\nunder a possibly non-unique optimal treatment strategy. Annals of statistics, 44(2):713, 2016.\n[24] Volker Ludwig Mehrmann. The autonomous linear quadratic control problem: theory and\nnumerical solution. Springer, 1991.\n[25] Yash Nair and Lucas Janson. Randomization tests for adaptively collected data. arXiv preprint\narXiv:2301.05365, 2023.\n11\n"
    },
    {
      "page_number": 12,
      "text": "[26] Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou. Why adaptively collected data\nhave negative bias and how to correct for it. In International Conference on Artificial Intelligence\nand Statistics, pages 1261–1269. PMLR, 2018.\n[27] Anna Rafferty, Huiji Ying, Joseph Williams, et al. Statistical consequences of using multi-armed\nbandits to conduct adaptive educational experiments. Journal of Educational Data Mining,\n11(1):47–79, 2019.\n[28] Yuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. Large-scale open dataset,\npipeline, and benchmark for bandit algorithms. arXiv preprint arXiv:2008.07146, 2020.\n[29] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.\n[30] Feicheng Wang and Lucas Janson. Exact asymptotics for linear quadratic adaptive control.\nJournal of Machine Learning Research, 22(265):1–112, 2021.\n[31] Elad Yom-Tov, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, and Irit\nHochberg. Encouraging physical activity in patients with diabetes: intervention using a rein-\nforcement learning system. Journal of medical Internet research, 19(10):e338, 2017.\n[32] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare:\nA survey. ACM Computing Surveys (CSUR), 55(1):1–36, 2021.\n[33] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in\nneural information processing systems, 33:9818–9829, 2020.\n12\n"
    },
    {
      "page_number": 13,
      "text": "NeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: In the abstract and motivation, we claim that the BvM theorem generalizes to\nadaptive data without assuming a stability assumption on the data. This claim is shown in\nthe proof of Theorem 1 and intuition for this result is given in section 3 of the main text.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: In section 7 of the text, we discuss the phenomenon that Bayesian credible\nintervals may fail to be frequentist-valid and how our work does not explain how to charac-\nterize this discrepancy. We also discuss how our BvM result requires the data to be generated\nfrom an exponential family model which restricts the generality of the result.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\n13\n"
    },
    {
      "page_number": 14,
      "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: Our main results, Theorems 1, 2, 3, 4 are all proven in Appendix B, with the\nsupporting lemmas also proven in Appendix C.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: The configurations for experiments are given in the figure captions of Figures 1,\n2, 3 with the algorithms (lin-UCB and NCEC) cited. We also describe how the TV distance\nwas estimated in the text.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\n14\n"
    },
    {
      "page_number": 15,
      "text": "some way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: Access to the source code is provided in Appendix A.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Details for the experiment parameters are included in the captions of Figures 1,\n2, 3 with full details included in the source code in Appendix A.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: The caption of figures describes the standard error of estimates provided in the\nexperiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n15\n"
    },
    {
      "page_number": 16,
      "text": "• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We mention in Appendix A that the data in each figure require under 3 hours\nin CPU time to generate.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: Our work does not involve human subjects and our data do not contain any\nprivate or personal information. We hope that the statistical inference methods provided in\nthis paper guides more reliable experimentation methods in adaptive design.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We briefly discuss the broader impacts of this work at the end of the main text.\nGuidelines:\n16\n"
    },
    {
      "page_number": 17,
      "text": "• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This paper does not contain any results that can be misused.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: This paper does not use existing code or data. The existing algorithms used in\nthe paper are implemented independently.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n17\n"
    },
    {
      "page_number": 18,
      "text": "• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: This paper does not release new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: No crowdsourcing or research with human subjects was done in this paper.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: No crowdsourcing or research with human subjects was done in this paper.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n18\n"
    },
    {
      "page_number": 19,
      "text": "• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [Yes]\nJustification: The usage of LLMs for this project is described under the “Declaration of\nLLM usage” section at the end of the main text.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n19\n"
    },
    {
      "page_number": 20,
      "text": "A\nSource Code\nThe code for this project can be accessed at https://github.com/TheDukeVin/BvM/tree/main. Each\nfigure’s data takes under three hours of CPU time to generate.\nB\nMain Theorems\nB.1\nProof of Theorem 1\nWe first write an explicit expression for the posterior. Note that the likelihood can be written as, after\nremoving constant factors that don’t depend on β,\nL(β; Hn) =\n\n\nn\nY\nj=1\nΛ(xj|Hj−1)\n\n\n\n\nn\nY\nj=1\nP(yj|xj, β)\n\n\n∝\nn\nY\nj=1\nP(yj|xj, β) ∝\nn\nY\nj=1\nexp\n \n−(yj −x⊤\nj β)2\n2σ2\n!\n=\nn\nY\nj=1\nexp\n \n−(yj −ˆβ⊤\nn xj)2\n2σ2\n−(ˆβ⊤\nn xj −β⊤xj)2\n2σ2\n!\n∝exp(−1\n2σ2 (ˆβn −β)⊤X⊤\nn Xn(ˆβn −β)).\nThus, by Bayes’ rule, the posterior can be written as\nπ(β|Hn) ∝π(β) exp(−1\n2σ2 (ˆβn −β)⊤X⊤\nn Xn(ˆβn −β)).\nLet d(Hn) = ∥π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV. By assumption, π(β0) > 0, so using Lemma\n2, we have\nd(Hn) ≤\nZ \u0010 π(β)\nπ(β0)\n1\np\n(2πσ2)p|X⊤\nn Xn|−1 exp(−1\n2σ2 (β −ˆβn)⊤X⊤\nn Xn(β −ˆβn))\n−\n1\np\n(2πσ2)p|X⊤\nn Xn|−1 exp(−1\n2σ2 (β −ˆβn)⊤X⊤\nn Xn(β −ˆβn))\n\u0011\n+dβ\n=\nZ \u0012 π(β)\nπ(β0) −1\n\u0013\n+\n1\np\n(2πσ2)p|X⊤\nn Xn|−1 exp(−1\n2σ2 (β −ˆβn)⊤X⊤\nn Xn(β −ˆβn))dβ.\n= E\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n\f\f\fHn\n#\nwhere κn is a random variable marginally distributed as κn|Hn ∼N(ˆβn, σ2(X⊤\nn Xn)−1). Let cn\nbe a sequence of random variables satisfying\ncn\nlog λmax(X⊤\nn Xn)\np→∞and\ncn\nλmin(X⊤\nn Xn)\np→0, which\nmust exist by condition (i), i.e. we can let cn =\np\nλmin(X⊤\nn Xn) log λmax(X⊤\nn Xn). Then, to show\nd(Hn)\np→0, we use a common trick when showing BvM-style results [29, 11, 18]—that is, we will\npartition the expectation into parts inside and outside a local ellipsoid. We will show the following\ntwo statements.\n20\n"
    },
    {
      "page_number": 21,
      "text": "E\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 ≤cn]\n\f\f\fHn\n#\np→0\n(1)\nE\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\n#\np→0\n(2)\nTo show (1), note that we have\nE\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 ≤cn]\n\f\f\fHn\n#\n≤\n1\nπ(β0)E\n\u0014\n(π(κn) −π(β0))+ 1[∥(κn −ˆβn)∥2 ≤\ncn\nλmin(X⊤\nn Xn)]\n\f\f\fHn\n\u0015\n≤\n1\nπ(β0)\nsup\n∥κn−ˆβn∥≤\nc1/2\nn\nλmin(X⊤\nn Xn)1/2\n(π(κn) −π(β0))+\n≤\n1\nπ(β0)\nsup\n∥κn−β0∥≤\nc1/2\nn\nλmin(X⊤\nn Xn)1/2 +∥ˆβn−β0∥\n(π(κn) −π(β0))+\nLai and Wei showed that ˆβn is consistent if condition (i) is satisfied [21].\nThus, we have\nc1/2\nn\nλmin(X⊤\nn Xn)1/2 + ∥ˆβn −β0∥\np→0, meaning the above expression does indeed converge to zero\nin probability if π is continuous at β0.\nTo show (2), note that by the triangle inequality, we have\nE\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\n#\n≤\n1\nπ(β0)E\nh\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\ni\n+ E\nh\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\ni\nThe first term can then be bounded as\n1\nπ(β0)E\nh\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\ni\n=\n1\nπ(β0)\nZ\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]P(κn|Hn)dκn\n≤\n1\nπ(β0)\nZ\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n1\np\n(2πσ2)p|X⊤\nn Xn|−1 exp(−cn\n2σ2 )dκn\n≤(2πσ2)−p/2\nπ(β0)\nexp(−cn\n2σ2 )λmax(X⊤\nn Xn)p/2\nZ\nπ(κn)1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]dκn\n≤(2πσ2)−p/2\nπ(β0)\nexp(−cn\n2σ2 + p\n2 log λmax(X⊤\nn Xn))\nZ\nπ(κn)dκn\n= (2πσ2)−p/2\nπ(β0)\nexp(−cn\n2σ2 + p\n2 log λmax(X⊤\nn Xn))\n21\n"
    },
    {
      "page_number": 22,
      "text": "If\ncn\nlog λmax(X⊤\nn Xn)\np→∞, the above expression converges to zero in probability. To compute the\nsecond term, note that (X⊤\nn Xn)1/2(κn −ˆβn)|Hn is distributed as N(0, σ2Ip). Thus, if cn\np→∞,\nthen E\nh\n1[∥(X⊤\nn Xn)1/2(κn −ˆβn)∥2 > cn]\n\f\f\fHn\ni\np→0. We have shown that each term (1) and (2) in\nthe decomposition of the upper bound for d(Hn) converges to zero in probability, meaning d(Hn)\nconverges to zero in probability as well.\nB.2\nProof of Theorem 2\nAs in the proof of Theorem 1, the posterior distribution can be expressed as\nπ(β|Hn\nmn) ∝π(β) exp(−1\n2σ2 (ˆβn −β)⊤X⊤\nn Xn(ˆβn −β)).\nLetting d(Hn\nmn) = ∥π(β|Hn\nmn) −N(ˆβn, σ2(X⊤\nn Xn)−1)∥TV, we also have\nd(Hn\nmn) ≤E\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n\f\f\fHn\nmn\n#\nwhere κn|Hn\nmn ∼N(ˆβn, σ2(X⊤\nn Xn)−1).\nBy Lemma 1, we have that ˆβn is consistent.\nIf\nλmin(X⊤\nn Xn)\np→∞, then κn\np→β0 as we can write κn = β0 + (ˆβn −β0) + Zn where\nZn|Hn\nmn ∼N(0, σ2(X⊤\nn Xn)−1) and both the terms ˆβn −β0 and Zn converge to zero in prob-\nability. If π is continuous and bounded, by Vitali’s convergence theorem,\nE[d(Hn\nmn)] ≤E\n\"\u0012π(κn)\nπ(β0) −1\n\u0013\n+\n#\n→0,\nmeaning d(Hn\nmn)\np→0.\nB.3\nProof of Theorem 3\nSuppose Yi,(1), Yi,(2), . . . are the serialized arm pulls from arm i. Let ¯Yi,(N) = 1\nN\nPN\nj=1 yi,(j) be the\nsequence of sample means. By Lemma 3, we have\n¯Yi,(N) −µ\nσ\n= o(N −1/2+ϵ) almost surely as N →∞.\n(3)\nfor some small ϵ, say ϵ = 0.01. Since ¯Yn,i = ¯Yi,(Nn,i), we have that ¯Yn,i −µi = Op(N −1/2+ϵ\nn,i\n).\nThus, ˆβn,i −β0,i = Op(N −1/2+ϵ\nn,i\n), meaning ˆβn,i is consistent for β0,i.\nWe prove the result in three steps by truncating both the posterior and normal distribution to the set\nCn = {β : ∀i, |βi−β0,i| ≤cn,i} for some sequence of random variables cn,i such that N 1/3\nn,i cn,i\np→0\nand N 1/2−ϵ\nn,i\ncn,i\np→∞as n →∞. We then show that\n∥π(β|Hn) −πCn(β|Hn)∥TV\np→0\n(4)\n∥πCn(β|Hn) −N Cn(β0, I−1\nn )∥TV\np→0\n(5)\n∥N Cn(β0, I−1\nn ) −N(β0, I−1\nn )∥TV\np→0.\n(6)\nWe first show (5). The likelihood function can be calculated as\nL(β; Hn) ∝\np\nY\ni=1\nexp(Nn,i( ¯Yn,iβi −b(βi))).\n22\n"
    },
    {
      "page_number": 23,
      "text": "By Taylor’s Theorem, for any β ∈Cn, we have\nL(β; Hn) ∝exp\n p\nX\ni=1\nNn,i[ ¯Yn,iβi −b(β0,i) −b′(β0,i)(βi −β0,i)\n−1\n2b′′(β0,i)(βi −β0,i)2 −1\n6b′′′(β∗\nn,β,i)(βi −β0,i)3]\n\u0013\nfor some β∗\nn,β ∈Cn. However, if b′′′ is bounded on a neighborhood of β0,i for each i and N 1/3\nn,i cn,i\np→\n0, we have that supβ∈Cn Nn,ib′′′(β∗\nn,β,i)(βi −β0,i)3\np→0. Thus, we can write\nL(β; Hn) ∝exp\n p\nX\ni=1\nNn,i[ ¯Yn,iβi −b(β0,i) −b′(β0,i)(βi −β0,i) −1\n2b′′(β0,i)(βi −β0,i)2] + op(1)\n!\n∝exp\n p\nX\ni=1\nNn,i[−1\n2b′′(β0,i)β2\ni + ( ¯Yn,i −b′(β0,i) + b′′(β0,i)β0,i)βi]\n!\n(1 + op(1))\n= exp\n p\nX\ni=1\nNn,i[−1\n2b′′(β0,i)β2\ni + b′′(β0,i)ˆβn,iβi]\n!\n(1 + op(1))\n∝exp\n \n−1\n2\np\nX\ni=1\nNn,ib′′(β0,i)(βi −ˆβn,i)2\n!\n(1 + op(1))\n(7)\n= exp(−1\n2(β −ˆβn)⊤In(β −ˆβn))(1 + op(1))\nLet the density of the truncated normal distribution N Cn(ˆβn,i, I−1\nn ) be expressed as\nAn1β∈Cn exp(−1\n2(β −ˆβn,i)⊤In(β −ˆβn,i)).\nwhere An is chosen such that the density is suitably normalized. Let dn(Hn) = ∥πCn(β|Hn) −\nN Cn(β0, I−1\nn )∥TV. By Lemma 2, we have\ndn(Hn) ≤\nZ  \nAn1β∈Cn\nπ(β)L(β; Hn)\nπ(ˆβn)L(ˆβn; Hn)\n−An1β∈Cn exp(−1\n2(β −ˆβn)⊤In(β −ˆβn))\n!\n+\ndβ\n≤\nZ\nAn1β∈Cn\n \nπ(β)\nπ(ˆβn)\nexp(−1\n2(β −ˆβn)⊤In(β −ˆβn))(1 + ¯op(1))\n−exp(−1\n2(β −ˆβn)⊤In(β −ˆβn))\n\u0013\n+\ndβ\nwhere we say a sequence of random variables Wn(β) indexed by β is ¯op(1) if supβ∈Cn Wn(β)\np→0.\nThus, we have\ndn(Hn) ≤\nZ   \nπ(β)\nπ(ˆβn)\n−1 + ¯op(1)\n!\nAn1β∈Cn exp(−1\n2(β −ˆβn)⊤In(β −ˆβn))\n!\n+\ndβ.\n= Eηn∼N Cn( ˆβn,i,I−1\nn )\n \nπ(ηn)\nπ(ˆβn)\n−1 + ¯op(1)\n!\n+\n≤\n\u0012supβ∈Cn π(β)\ninfβ∈Cn π(β) −1 + ¯op(1)\n\u0013\n+\n23\n"
    },
    {
      "page_number": 24,
      "text": "Then, since cn,i\np→0 for each i and π is continuous, we know\nsupβ∈Cn π(β)\ninfβ∈Cn π(β)\np→1, meaning the above\nexpression indeed converges to 0 in probability. Next, we show (4). Note that\n∥π(β|Hn) −πCn(β|Hn)∥TV = Pπ(β ∈CC\nn |Hn)\n=\nR\nCC\nn π(β)L(β; Hn)dβ\nR\nRp π(β)L(β; Hn)dβ\n≤πmax\nπmin\nn\nR\nCC\nn L(β; Hn)dβ\nR\nCn L(β; Hn)dβ\nwhere πmax = supβ π(β) and πmin\nn\n= infβ∈Cn π(β). Note that if π is continuous and positive at β0,\nthen πmin\nn\nconverges to some positive constant. Then, since the ancillary function b of the exponential\nfamily is convex, the likelihood function is log-concave in β, meaning we have\nR\nCC\nn L(β; Hn)dβ\nR\nCn L(β; Hn)dβ ≤sup\nv∈∂Cn\nR ∞\n1\nL(β0 + (v −β0)t; Hn)tp−1dt\nR 1\n0 L(β0 + (v −β0)t; Hn)tp−1dt\n≤sup\nv∈∂Cn\nR ∞\n1\nexp(ℓn(β0) + (ℓn(v) −ℓn(β0))t)tp−1dt\nR 1\n0 exp(ℓn(β0) + (ℓn(v) −ℓn(β0))t)tp−1dt\n= sup\nv∈∂Cn\nR ∞\nℓn(β0)−ℓn(v) exp(−x)xp−1dx\nR ℓn(β0)−ℓn(v)\n0\nexp(−x)xp−1dx\n=\nR ∞\nan exp(−x)xp−1dx\nR an\n0\nexp(−x)xp−1dx\nwhere an = infv∈∂Cn ℓn(β0) −ℓn(v). Note that the function a 7→\nR ∞\na\nexp(−x)xp−1dx\nR a\n0 exp(−x)xp−1dx goes to zero as\na →∞. Thus, it suffices to show that an\np→∞. To do this, note that by expression 7, we have\nan =\ninf\nv∈∂Cn −1\n2\np\nX\ni=1\nNn,ib′′(β0,i)[(β0,i −ˆβn,i)2 −(vi −ˆβn,i)2] + ¯op(1).\nSince |vi −ˆβn,i| ≥∥vi −β0,i| −|β0,i −ˆβn,i∥= |cn,i −|β0,i −ˆβn,i∥, we have\nan ≥\ninf\nv∈∂Cn −1\n2\np\nX\ni=1\nNn,ib′′(β0,i)[|β0,i −ˆβn,i|2 −(cn,i −|β0,i −ˆβn,i|)2] + ¯op(1)\n=\ninf\nv∈∂Cn −1\n2\np\nX\ni=1\nNn,ib′′(β0,i)[|β0,i −ˆβn,i|2 −c2\nn,i + 2cn,i|β0,i −ˆβn,i| −|β0,i −ˆβn,i|2] + ¯op(1)\n=\ninf\nv∈∂Cn\n1\n2\np\nX\ni=1\nc2\nn,iNn,ib′′(β0,i)[1 −2|β0,i −ˆβn,i|\ncn,i\n] + ¯op(1)\nBy the definition of cn,i as well as Equation (3), we know c2\nn,iNn,i\np→∞and β0,i−ˆβn,i\ncn,i\np→0. Thus,\nwe indeed have an\np→∞. Lastly, we show (6). We have\n∥N(β0, I−1\nn ) −N Cn(β0, I−1\nn )∥TV = PX∼N (β0,I−1\nn )(∃i, |Xi −β0,i| > cn,i)\n= PX∼N (0,Ip×p)(∃i, |Xi| > cn,iN 1/2\nn,i b′′(β0,i)1/2).\nIf cn,iN 1/2\nn,i\np→∞, then the above expression indeed coverges to zero in probability.\n24\n"
    },
    {
      "page_number": 25,
      "text": "B.4\nProof of Theorem 4\nWe only need to establish the consistency of ˆβn, after which the result follows by a similar argument\nto the proof of Theorem 2. Consistency can be shown from condition (ii) by Theorem 1 of [21].\nC\nTechnical Lemmas\nProof of Lemma 1:\nWe translate this directly to the bandit setting, after which this result follows\nfrom a similar version of Theorem 2.2 of [13]. For each i = 1, . . . , p, let µi = βi be the i-th\ncoordinate of β which represents the true mean of arm i. Let Ni,n = (Xn)ii be the number of times\narm i is pulled in the trajectory Hn\nmn. We serialize all arm pulls from each arm i—that is, suppose\nyn\ni,(1), yn\ni,(2), . . .\nind\n∼N(µi, σ2). Then, when drawing yn\nj |xn\nj ∼N(µxj, σ2), we look up the next\nunused sample in the serialized sequence yn\nxj,(1), . . . and use that as our observation.\nUnder this formulation of the data-generating process, the sample mean ˆβn,i is simply the mean\nof the first Ni,n samples from the serialized sequence yn\ni,(1), . . .. Note that the serialized sample\nmeans ¯yn\ni,t = 1\nt\nPt\nj=1 yn\ni,(j) satisfy ¯yn\ni,t\na.s.\n→µi as t →∞for each i, n by the strong law of large\nnumbers. Then, Proposition 1 implies that for each i, n, there exists some function ϵn\ni (·) satisfying\nlimB→∞ϵn\ni (B) = 0 such that for any B,\nP[N ≥B] ≥1 −1\nB =⇒P[|¯yn\ni,(N) −µi| ≤ϵn\ni (B)] ≥1 −ϵn\ni (B)\nCrucially, note that the functions ϵn\ni (·) can be chosen such that ϵn\ni (·) does not depend on n. This is\nbecause the distribution of serialized samples {yn\ni,(j)}i∈{1,...,p},j≥1 for each trajectory is identical,\nand in Proposition 1, the bound translation function ϵ is chosen solely based on the distribution of\nthe almost-surely convergent sequence. Finally, by condition (ii), we indeed have that Ni,n\np→∞\nfor each i, meaning the condition P[Ni,n ≥Bn] ≥1 −\n1\nBn is indeed satisfied for some sequence\nBn →∞- to see this, note that letting Bn = sup{B : P[Ni,n ≥B] ≥1 −1\nB }, we must have\nBn →∞as otherwise there would be some constant C where P[Ni,n ≤C] >\n1\nC for infinitely\nmany n which would contradict the statement that Ni,n diverges in probability. Thus, we have\nP[|¯yn\ni,(Ni,n) −µi| ≤ϵn\ni (Bn)] ≥1 −ϵn\ni (Bn) for each n meaning the sample mean ˆβn,i = ¯yn\ni,(Ni,n) is\nindeed consistent for µi.\nProposition 1. (Bound translation) Suppose Yn is a sequence of random variables such that Yn\na.s.\n→Y\nfor some real number Y . Then there exists a function ϵ(B) with ϵ(B) →0 as B →∞such that for\nany random variable N and positive integer B, we have\nP[N ≥B] ≥1 −1\nB =⇒P[|YN −Y | ≤ϵ(B)] ≥1 −ϵ(B).\nProof. Note that it is sufficient to show that for any fixed η, there is some function ϵη(B) such that\nlimB→∞ϵη(B) = 0 and\nP[N ≥B] ≥1 −1\nB =⇒P[|YN −Y | ≤η] ≥1 −ϵη(B).\nTo show this, we simply let ϵη(B) = 1\nB + P[∃n ≥B, |Yn −Y | > η]. If Yn converges almost surely\nto Y , then limB→∞ϵη(B) = 0. Also, we indeed have\nP[|YN −Y | ≤η] ≥P[N ≥B and ∀n ≥B, |Yn −Y | ≤η] ≥1 −ϵη(B).\nSince we can do this for any η, the statement of the result is indeed true.\nLemma 2. Let P(·) and Q(·) be continuous distributions and c ∈R be any constant. Then,\n∥P −Q∥TV = 1\n2\nR\n(P(x) −Q(x))+dx ≤\nR\n(cP(x) −Q(x))+dx.\n25\n"
    },
    {
      "page_number": 26,
      "text": "Proof. Let A = {x : P(x) > Q(x)}. Let a1 =\nR\nA P(x) −Q(x)dx and a2 =\nR\nAC Q(x) −P(x)dx.\nThen, note that\n∥P −Q∥TV = 1\n2\nZ\n(P(x) −Q(x))+dx = 1\n2a1 + 1\n2a2\nand\n0 =\nZ\nP(x) −Q(x)dx = a1 −a2\nSolving for a1 and a2, we get\na1 = a2 = ∥P −Q∥TV.\nTo show the desired result, if c ≥1, we have\n∥P −Q∥TV = a1 ≤\nZ\nA\ncP(x) −Q(x)dx ≤\nZ\n(cP(x) −Q(x))+dx.\nSimilarly, if c ≤1, then\n∥P −Q∥TV = a2 ≤\nZ\nAC Q(x) −cP(x)dx ≤\nZ\n(cP(x) −Q(x))+dx.\nLemma 3. Let X1, X2, . . . be i.i.d. samples from a distribution P with mean µ and finite variance\nσ2. Let ˆµn = 1\nn\nPn\ni=1 Xi. Then,\nn1/2−ϵ(ˆµn −µ)\na.s.\n→0.\nfor any ϵ > 0.\nProof. By the law of iterated logarithms, we have\nlim sup\nn→∞\n√n|ˆµn −µ|\np\n2σ2 log log n\n= 1\nwith probability 1. Then, we have\nlim sup\nn→∞n1/2−ϵ|ˆµn −µ| ≤\n \nlim sup\nn→∞\n√n|ˆµn −µ|\np\n2σ2 log log n\n!  \nlim sup\nn→∞\np\n2σ2 log log n\nnϵ\n!\n= 0\nwith probability 1.\nD\nAuxilliary results\nProposition 2. Let x1 be deterministic and take any real value. Let the adaptive sampling procedure\nsimply deterministically set xi = yi−1 for i > 1. Then, if β = 1, the MLE is asymptotically\nnon-Normal. However, condition (i) of Theorem 1 is satisfied by this sampling design.\nProof. The asymptotic non-Normality is established by Lai and Wei in [21, Example 3]. The second\npart of condition (i) is satisfied because the covariates are 1-dimensional and the first part is satisfied\nbecause Pn\ni=1 x2\ni\np→∞, which is a direct consequence of [21, Equation (4.9)].\n26\n"
    },
    {
      "page_number": 27,
      "text": "We now recall an assumption from [30]:\nAssumption 1 (Stability, [30, Assumption 1]). There exists a matrix K0 for which the maximum\nabsolute eigenvalue of A + BK0 is less than 1.\nProposition 3. Suppose we run the stepwise NCEC algorithm (Algorithm 1 in [30]) on an instance\nof LQR satisfying Assumption 1 using any configuration of the hyperparameters τ 2 > 0, β ∈[1/2, 1)\nand α > 0 permitted therein. Then, this satisfies condition (i) of Theorem 1.\nProof. First, observe that our Gram matrix ˜X⊤˜X is more simply expressed as Ik×k ⊗Gn where\n⊗denotes Kronecker product and Gn := Pn\ni=1\n\u0014\nxi\nui\n\u0015 \u0014\nxi\nui\n\u0015⊤\ndenotes the Gram matrix. Because the\nlargest and smallest eigenvalues of Ik×k ⊗Gn are the same as that of Gn it suffices to study the\nspectrum of the latter. Theorem 3 of [30] shows, in the stabilizable regime, that\nD−1\nn Gn\n\u0000D−1\nn\n\u0001⊤\np→Ik+d,\nwhere\nDn = nβ/2 logα/2(n)\n\u0014\nIk\n0\nK\nId\n\u0015 \u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015\n,\n(8)\nCn = n1−β log−α(n)\n∞\nX\np=0\n(A+BK)p((A+BK)p)⊤σ2 + τ 2\nβ\n∞\nX\nq=0\n(A+BK)qBB⊤((A+BK)q)⊤,\n(9)\nand K is a matrix that does not depend on n (see equation (3) of [30] for a precise definition).\nfrom which it follows that\n\r\rGn −DnD⊤\nn\n\r\r = op(1).\n(10)\nBelow, we show that λmax\n\u0000DnD⊤\nn\n\u0001\n≤O(n) and λmin\n\u0000DnD⊤\nn\n\u0001\n≥Ω(nβ logα(n)). Because Equa-\ntion (10) implies both that λmin(Gn) −λmin\n\u0000DnD⊤\nn\n\u0001\n= op(1) and λmin(Gn) −λmin\n\u0000DnD⊤\nn\n\u0001\n=\nop(1), we indeed have that condition (i) of Theorem 1 is met: log (λmax(Gn)) = op(λmin(Gn)) and\nλmin(Gn)\np→∞.\nUpper bound on λmax(DnD⊤\nn )\nBy submultiplicativity of the operator norm, we have, using\nequation (8), that the largest eigenvalue of DnD⊤\nn is at most\nnβ logα(n)\n\r\r\r\r\n\u0014\nIk\n0\nK\nId\n\u0015\r\r\r\r\n2 \r\r\r\r\n\u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015\r\r\r\r\n2\nThe middle term is of constant order and therefore it suffices to upper-bound the last term. Because\n\r\r\r\r\n\u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015\r\r\r\r ≤max\n\u0010\n∥C1/2\nn\n∥, ∥\np\nτ 2/βId∥\n\u0011\nand since ∥\np\nτ 2/βId∥is again of constant\norder it suffices to simply bound ∥C1/2\nn\n∥. Using equation (9) and triangle inequality, this is at most\nO(n(1−β)/2 log−α/2(n)). Consequently, the maximum eigenvalue of DnD⊤\nn is O (n).\nLower bound on λmin(DnD⊤\nn )\nTo lower bound the minimum eigenvalue of DnD⊤\nn , we apply the\nsame argument to the maximum eigenvalue of (D⊤\nn )−1D−1\nn . The largest eigenvalue of (D⊤\nn )−1D−1\nn\nis at most\nn−β log−α(n)\n\r\r\r\r\r\n\u0014\nIk\n0\nK\nId\n\u0015−1\r\r\r\r\r\n2 \r\r\r\r\r\n\u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015−1\r\r\r\r\r\n2\nThen,\n\r\r\r\r\r\n\u0014\nC1/2\nn\n0\n0\np\nτ 2/βId\n\u0015−1\r\r\r\r\r ≤max(||C−1\nn ||1/2,\np\nβ/τ 2) = max(λmin(Cn)−1/2,\np\nβ/τ 2).\nAgain using equation (9) and the triangle inequality, this is at most O(1), meaning the minimum\neigenvalue of DnD⊤\nn is at least Ω(nβ logα(n)).\n27\n"
    },
    {
      "page_number": 28,
      "text": "Proof of Corollary 1: Suppose by contradiction that there is some k where\nsup\nP ∈Pn\nP(||π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)||T V > c) > k\nfor infinitely many n. Then, for all such n, there is some sampling rule Λn whose corresponding\ndistribution trajectory P satisfies\nP(||π(β|Hn) −N(ˆβn, σ2(X⊤\nn Xn)−1)||T V > c) ≥k.\nHowever, this contradicts the statement of Theorem 2.\nE\nHeteroskedastic Bandits\nAlgorithm 2 Heteroskedastic Gaussian bandits\nInput Action set A = {1, . . . , p}, Arm sampling rule Λ : (A×R)∗→∆(A), Reward distributions\nBi = N(βi, σ2\ni ).\nOutput Sampling trajectory Hn\nH0 ←∅\nfor j = 1, . . . , n do\nSample aj ∼Λ(·|Hj−1)\nSample yj ∼N(βaj, σ2\naj)\nHj ←{(x1, y1), . . . , (xj, yj)}\nend for\nTheorem 5. Suppose we generate a length-mn trajectory Hn\nmn = ((an\n1, yn\n1 ), . . . , (an\nn, yn\nn)) for each\nn according to decision rules Λn in the case of Heteroskedastic Gaussian bandits. Assume the\nsampling procedure satisfies the following conditions:\n(i) mini Ni,n\np→∞where Ni,n = Pn\nj=1 I[an\nj = i].\n(ii) π(·) is continuous and bounded on Rp with positive density at β0.\nThen, the posterior distribution π(β|Hn\nmn) satisfies\n∥π(β|Hn\nmn) −N(ˆβn, diag(σ2\ni N −1\ni,n ))∥TV\np→0.\nProof. We rescale this problem to an instance of the homoskedastic bandit problem and apply\nTheorem 2. Let the covariate sampling rule be ˜Λn(Hj−1) = eΛn(Hj−1) and the rescaled parameters\nbe ˜βi = βi\nσi with the homoskedastic variance ˜σ = 1. Let ˜π(˜β) ∝π(σ ∗˜β) where ∗represents\nelement-wise multiplication where σ is the vector of (σi)p\ni=1. Then, by Theorem 2,\n∥˜π(˜β| ˜Hn\nmn) −N(˜ˆβn, ( ˜X⊤\nn ˜Xn)−1)∥TV\np→0.\nRescaling the posterior back to the original parameters gives the desired result.\nF\nParametric BvM\nThere are considerable challenges when extending the BvM result to parametric models satisfying\nonly weak regularity conditions such as differentiability in quadratic mean. Consider replacing the\nnormal distribution in Algorithm 1 with a general parametric model Pθ(·|xj) with θ = β⊤xj. We\nmay want to show that in this setting, the posterior distribution is asymptotically normal similar to\nthe BvM statement, i.e.\n28\n"
    },
    {
      "page_number": 29,
      "text": "∥π(θ|Hn) −N(ˆθn, J−1\nn )∥TV\np→0\nwhere Jn = −Pn\nj=1 ¨ℓˆθn(yj|xj) is the empirical Fisher Information and ℓθ = log Pθ is the log-\nlikelihood. Suppose we follow the steps of the classical BvM proof. Letting πCn(θ|Hn) and\nN Cn(ˆθn, J−1\nn ) denote truncations of these distributions to the set Cn, the classical proof proceeds in\nthe following three steps—we show each of the following convergences:\n∥π(θ|Y ) −πCn(θ|Y )∥TV\np→0\n∥πCn(θ|Y ) −N Cn(ˆθn, J−1\nn )∥TV\np→0\n∥N Cn(ˆθn, J−1\nn ) −N(ˆθn, J−1\nn )∥TV\np→0.\nThe first step requires a method of truncating the posterior distribution, which is in general quite\ndifficult without strong assumptions on the behavior of the adaptive system similar to the results\nof [18] and [11]. With independent data, we could truncate the posterior due to Hoeffding bounds\non the likelihood, but these bounds do not apply in adaptively collected data. The second step was\nimplied by a second-order Taylor expansion of the log-likelihood in the classical proof, but in adaptive\nsettings, the Fisher information matrix Jn may grow at different rates in different directions, i.e.\nλmin(Jn) may grow at a different rate than λmax(Jn). This would make an argument based on a\nTaylor expansion more complicated and a valid proof seems to require further assumptions on the\ngrowth rate of Jn. Finally, the third step was implied in the i.i.d. setting by the local consistency of\nˆθn, i.e. J1/2\nn\n(ˆθn −θ0) = Op(1) but this condition may be violated in adaptive experiments. In fact,\nas Lai and Wei showed, we require some nontrivial assumptions on the empirical Fisher Information\nto guarentee consistency, even in the case of Gaussian regression.\nG\nReal-world example\nWe compute the BvM total variation distance on a real-world instance of Bernoulli Thompson\nsampling provided in [28]. The dataset consists of the interaction of a fashion recommendation\nalgorithm with users, where the algorithm is optimizing for the number of clicks on fashion items.\nAlthough the original dataset is modeled with a contextual bandit setting with a batch size of 3, we\nsimplify their environment to an 80-armed bandit problem by disregarding the context and flattening\nthe batch size. These simplifications were made solely for computational convenience. The dataset\nconsists of 12m impressions or time steps, during which we update the posterior distribution with a\nBeta(1, 1) prior. The BvM total variation distance throughout this rollout is shown below.\nFigure 4: BvM TV distance for the Zozo dataset with prior Beta(1, 1). TV estimates computed with\n104 samples and have standard error at most 0.004.\nAs seen in Figure 4, the convergence of the total variation distance is quite slow. This is perhaps\ndue to the sparse success rates in the Bernoulli bandit and the larger dimensionality of the parameter\n29\n"
    },
    {
      "page_number": 30,
      "text": "space. It should be noted that the original dataset contained missing data within batches, i.e. not all\nbatches had size 3.\nH\nPlots\nFigure 5: Frequentist coverage of the Bayesian credible interval for the contextual bandit and LQR,\nunder the same configurations as Section 6. Coverage estimates shown have standard error at most\n0.004.\nFigure 6: Frequentist coverage of the Bayesian credible interval for UCB on Gaussian bandits,\nBernoulli bandits and Poisson bandits, under the same configurations as Section 6. Coverage\nestimates shown have standard error at most 0.004.\n30\n"
    }
  ]
}