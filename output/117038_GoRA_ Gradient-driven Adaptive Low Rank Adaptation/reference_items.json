[
  {
    "ref_no": 1,
    "title": "Llama 2: Open foundation and fine-tuned chat models",
    "ids": {
      "arxiv": "2307.09288",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2307.09288",
    "raw_text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tun..."
  },
  {
    "ref_no": 2,
    "title": "The llama 3 herd of models",
    "ids": {
      "arxiv": "2407.21783",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2407.21783",
    "raw_text": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint ar..."
  },
  {
    "ref_no": 3,
    "title": "Code llama: Open foundation models for code",
    "ids": {
      "arxiv": "2308.12950",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2308.12950",
    "raw_text": "Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arX..."
  },
  {
    "ref_no": 4,
    "title": "",
    "ids": {
      "arxiv": "2409.12122",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2409.12122",
    "raw_text": "An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model vi..."
  },
  {
    "ref_no": 5,
    "title": "Mixed precision training",
    "ids": {
      "year": "2018"
    },
    "graph_id": "",
    "raw_text": "Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In In..."
  },
  {
    "ref_no": 6,
    "title": "A study of bfloat16 for deep learning training",
    "ids": {
      "arxiv": "1905.12322",
      "year": "1905"
    },
    "graph_id": "10.48550_arxiv.1905.12322",
    "raw_text": "Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat..."
  },
  {
    "ref_no": 7,
    "title": "Adam: A method for stochastic optimization",
    "ids": {
      "year": "2015"
    },
    "graph_id": "",
    "raw_text": "Diederik P Kingma. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015."
  },
  {
    "ref_no": 8,
    "title": "Lora: Low-rank adaptation of large language models",
    "ids": {
      "year": "2022"
    },
    "graph_id": "",
    "raw_text": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learnin..."
  },
  {
    "ref_no": 9,
    "title": "Lora learns less and forgets less",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets..."
  },
  {
    "ref_no": 10,
    "title": "A closer look at the limitations of instruction tuning",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, and Dinesh Manocha. A closer look at the limitations of instruction tuning. In Interna..."
  },
  {
    "ref_no": 11,
    "title": "A rank stabilization scaling factor for fine-tuning with lora",
    "ids": {
      "arxiv": "2312.03732",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2312.03732",
    "raw_text": "Damjan Kalajdzievski. A rank stabilization scaling factor for fine-tuning with lora. arXiv preprint arXiv:2312.03732, 2023."
  },
  {
    "ref_no": 12,
    "title": "Relora: High-rank training through low-rank updates",
    "ids": {
      "year": "2023"
    },
    "graph_id": "",
    "raw_text": "Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky. Relora: High-rank training through low-rank updates. In International Conference on Learning Representations, 2023."
  },
  {
    "ref_no": 13,
    "title": "Melora: mini-ensemble low-rank adapters for parameterefficient fine-tuning",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten Rijke, Zhumin Chen, and Jiahuan Pei. Melora: mini-ensemble low-rank adapters for parameterefficient fine-tuning. In Proceed..."
  },
  {
    "ref_no": 14,
    "title": "Adaptive budget allocation for parameter-efficient fine-tuning",
    "ids": {
      "year": "2023"
    },
    "graph_id": "",
    "raw_text": "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In International Conference on Learni..."
  },
  {
    "ref_no": 15,
    "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
    "ids": {
      "year": "2015"
    },
    "graph_id": "",
    "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference ..."
  },
  {
    "ref_no": 16,
    "title": "Pissa: Principal singular values and singular vectors adaptation of large language models",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. In Advances in Neural Information Processing Systems, volume 37, p..."
  },
  {
    "ref_no": 17,
    "title": "Milora: Harnessing minor singular components for parameter-efficient llm finetuning",
    "ids": {
      "year": "2025"
    },
    "graph_id": "",
    "raw_text": "Hanqing Wang, Yixia Li, Shuo Wang, Guanhua Chen, and Yun Chen. Milora: Harnessing minor singular components for parameter-efficient llm finetuning. In Proceedings of the 2025 Conference of the Nations..."
  },
  {
    "ref_no": 18,
    "title": "Lora-ga: Low-rank adaptation with gradient approximation",
    "ids": {},
    "graph_id": "",
    "raw_text": "Shaowen Wang, Linxi Yu, and Jian Li. Lora-ga: Low-rank adaptation with gradient approximation. In Advances in Neural Information Processing Systems, volume 37, pages 54905–54931,"
  },
  {
    "ref_no": 19,
    "title": "Flora: Low-rank adapters are secretly gradient compressors",
    "ids": {},
    "graph_id": "",
    "raw_text": "Yongchang Hao, Yanshuai Cao, and Lili Mou. Flora: Low-rank adapters are secretly gradient compressors. In International Conference on Machine Learning, pages 17554–17571. PMLR,"
  },
  {
    "ref_no": 20,
    "title": "Galore: Memory-efficient llm training by gradient low-rank projection",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. In International Conference on Machin..."
  },
  {
    "ref_no": 21,
    "title": "Periodiclora: Breaking the low-rank bottleneck in lora optimization",
    "ids": {
      "arxiv": "2402.16141",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2402.16141",
    "raw_text": "Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu, Xiaochen Wang, Peiyi Wang, Qingxiu Dong, Liang Chen, and Zhifang Sui. Periodiclora: Breaking the low-rank bottleneck in lora optimization. ..."
  },
  {
    "ref_no": 22,
    "title": "Structure-aware low-rank adaptation for parameter-efficient fine-tuning",
    "ids": {
      "year": "2023"
    },
    "graph_id": "",
    "raw_text": "Yahao Hu, Yifei Xie, Tianfeng Wang, Man Chen, and Zhisong Pan. Structure-aware low-rank adaptation for parameter-efficient fine-tuning. Mathematics, 11(20):4317, 2023."
  },
  {
    "ref_no": 23,
    "title": "Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning",
    "ids": {
      "arxiv": "2308.12043",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2308.12043",
    "raw_text": "Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, and Yiming Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. arXiv preprint arXiv:2308.1..."
  },
  {
    "ref_no": 24,
    "title": "Pytorch fsdp: experiences on scaling fully sharded data parallel",
    "ids": {
      "arxiv": "2304.11277",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2304.11277",
    "raw_text": "Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel..."
  },
  {
    "ref_no": 25,
    "title": "Zero: Memory optimizations toward training trillion parameter models",
    "ids": {
      "year": "2020"
    },
    "graph_id": "",
    "raw_text": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing..."
  },
  {
    "ref_no": 26,
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "ids": {
      "year": "2010"
    },
    "graph_id": "",
    "raw_text": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, 2010."
  },
  {
    "ref_no": 27,
    "title": "Olora: Orthonormal low-rank adaptation of large language models",
    "ids": {
      "arxiv": "2406.01775",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2406.01775",
    "raw_text": "Kerim Büyükakyüz. Olora: Orthonormal low-rank adaptation of large language models. arXiv preprint arXiv:2406.01775, 2024."
  },
  {
    "ref_no": 28,
    "title": "One initialization to rule them all: Fine-tuning via explained variance adaptation",
    "ids": {
      "arxiv": "2410.07170",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2410.07170",
    "raw_text": "Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, and Sepp Hochreiter. One initialization to rule them all: Fine-tuning via explained variance adaptation. arX..."
  },
  {
    "ref_no": 29,
    "title": "A note on lora",
    "ids": {
      "arxiv": "2404.05086",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2404.05086",
    "raw_text": "Vlad Fomenko, Han Yu, Jongho Lee, Stanley Hsieh, and Weizhu Chen. A note on lora. arXiv preprint arXiv:2404.05086, 2024."
  },
  {
    "ref_no": 30,
    "title": "Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning",
    "ids": {
      "arxiv": "2308.03303"
    },
    "graph_id": "10.48550_arxiv.2308.03303",
    "raw_text": "Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303,"
  },
  {
    "ref_no": 31,
    "title": "Platon: Pruning large transformer models with upper confidence bound of weight importance",
    "ids": {},
    "graph_id": "",
    "raw_text": "Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight importance. In Interna..."
  },
  {
    "ref_no": 32,
    "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
    "ids": {
      "year": "2020"
    },
    "graph_id": "",
    "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text trans..."
  },
  {
    "ref_no": 33,
    "title": "In International Conference on Learning Representations",
    "ids": {
      "year": "2019"
    },
    "graph_id": "",
    "raw_text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Confere..."
  },
  {
    "ref_no": 34,
    "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
    "ids": {
      "year": "2023"
    },
    "graph_id": "",
    "raw_text": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Adva..."
  },
  {
    "ref_no": 35,
    "title": "Training verifiers to solve math word problems",
    "ids": {
      "arxiv": "2110.14168",
      "year": "2021"
    },
    "graph_id": "10.48550_arxiv.2110.14168",
    "raw_text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word prob..."
  },
  {
    "ref_no": 36,
    "title": "Evaluating large language models trained on code",
    "ids": {
      "arxiv": "2107.03374",
      "year": "2021"
    },
    "graph_id": "10.48550_arxiv.2107.03374",
    "raw_text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models tra..."
  },
  {
    "ref_no": 37,
    "title": "Learning transferable visual models from natural language supervision",
    "ids": {
      "year": "2021"
    },
    "graph_id": "",
    "raw_text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natu..."
  },
  {
    "ref_no": 38,
    "title": "3d object representations for finegrained categorization",
    "ids": {
      "year": "2013"
    },
    "graph_id": "",
    "raw_text": "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pa..."
  },
  {
    "ref_no": 39,
    "title": "Describing textures in the wild",
    "ids": {
      "year": "2014"
    },
    "graph_id": "",
    "raw_text": "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, p..."
  },
  {
    "ref_no": 40,
    "title": "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
    "ids": {
      "year": "2019"
    },
    "graph_id": "",
    "raw_text": "Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in ..."
  },
  {
    "ref_no": 41,
    "title": "Detection of traffic signs in real-world images: The german traffic sign detection benchmark",
    "ids": {
      "year": "2013"
    },
    "graph_id": "",
    "raw_text": "Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The german traffic sign detection benchmark. In International Jo..."
  },
  {
    "ref_no": 42,
    "title": "Remote sensing image scene classification: Benchmark and state of the art",
    "ids": {
      "year": "2017"
    },
    "graph_id": "",
    "raw_text": "Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):1865–1883, 2017."
  },
  {
    "ref_no": 43,
    "title": "Sun database: Large-scale scene recognition from abbey to zoo",
    "ids": {
      "year": "2010"
    },
    "graph_id": "",
    "raw_text": "Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Visi..."
  },
  {
    "ref_no": 44,
    "title": "Reading digits in natural images with unsupervised feature learning",
    "ids": {
      "year": "2011"
    },
    "graph_id": "",
    "raw_text": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsu..."
  },
  {
    "ref_no": 45,
    "title": "Dora: Weight-decomposed low-rank adaptation",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In International Conference on Machin..."
  },
  {
    "ref_no": 46,
    "title": "Lora+ efficient low rank adaptation of large models",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+ efficient low rank adaptation of large models. In International Conference on Machine Learning, pages 17783–17806. PMLR, 2024."
  },
  {
    "ref_no": 47,
    "title": "Metamath: Bootstrap your own mathematical questions for large language models",
    "ids": {},
    "graph_id": "",
    "raw_text": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language ..."
  },
  {
    "ref_no": 48,
    "title": "Opencodeinterpreter: Integrating code generation with execution and refinement",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. In Findings of th..."
  },
  {
    "ref_no": 49,
    "title": "Wizardlm: Empowering large pre-trained language models to follow complex instructions",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions. ..."
  },
  {
    "ref_no": 50,
    "title": "Decoupled weight decay regularization",
    "ids": {
      "year": "2019"
    },
    "graph_id": "",
    "raw_text": "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019."
  },
  {
    "ref_no": 51,
    "title": "Gpt-4 technical report",
    "ids": {
      "arxiv": "2303.08774",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2303.08774",
    "raw_text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv prepr..."
  },
  {
    "ref_no": 52,
    "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
    "ids": {
      "arxiv": "2403.05530",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2403.05530",
    "raw_text": "Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across ..."
  },
  {
    "ref_no": 53,
    "title": "Lora-pro: Are low-rank adapters properly optimized? In International Conference on Learning Representations",
    "ids": {
      "year": "2025"
    },
    "graph_id": "",
    "raw_text": "Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, and Tieniu Tan. Lora-pro: Are low-rank adapters properly optimized? In International Conference on Learning Representations, 2025."
  },
  {
    "ref_no": 54,
    "title": "",
    "ids": {},
    "graph_id": "",
    "raw_text": "An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jin..."
  },
  {
    "ref_no": 55,
    "title": "Training deep nets with sublinear memory cost",
    "ids": {
      "arxiv": "1604.06174",
      "year": "2016"
    },
    "graph_id": "10.48550_arxiv.1604.06174",
    "raw_text": "Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016."
  },
  {
    "ref_no": 56,
    "title": "Liger kernel: Efficient triton kernels for llm training",
    "ids": {
      "arxiv": "2410.10989",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2410.10989",
    "raw_text": "Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for llm training. arXi..."
  },
  {
    "ref_no": 57,
    "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
    "ids": {
      "year": "2022"
    },
    "graph_id": "",
    "raw_text": "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, volum..."
  },
  {
    "ref_no": 58,
    "title": "Flashattention-2: Faster attention with better parallelism and work partitioning",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations, 2024."
  },
  {
    "ref_no": 59,
    "title": "Adalomo: Low-memory optimization with adaptive learning rate",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, and Xipeng Qiu. Adalomo: Low-memory optimization with adaptive learning rate. In Findings of the Association for Computational Linguistics: ACL 2024, pages 124..."
  }
]