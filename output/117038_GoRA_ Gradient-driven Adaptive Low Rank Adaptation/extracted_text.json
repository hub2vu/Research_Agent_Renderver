{
  "filename": "117038_GoRA_ Gradient-driven Adaptive Low Rank Adaptation.pdf",
  "total_pages": 29,
  "full_text": "GoRA: Gradient-driven Adaptive Low Rank\nAdaptation\nHaonan He1,2,3∗,\nPeng Ye3,4,5∗,\nYuchen Ren3,6,\nYuan Yuan2,\nLuyang Zhou7,\nShucun Ju7,\nLei Chen2†\n1University of Science and Technology of China\n2Institute of Intelligent Machines, HFIPS, Chinese Academy of Sciences\n3Shanghai Artificial Intelligence Laboratory\n4Fudan University\n5The Chinese University of Hong Kong\n6University of Sydney\n7Anhui Disaster Warning & Agrometeorological Information Center\nhehn@mail.ustc.edu.cn\nAbstract\nLow-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning large\nlanguage models (LLMs), with its effectiveness influenced by two key factors:\nrank selection and weight initialization. While numerous LoRA variants have\nbeen proposed to improve performance by addressing one of these aspects, they\noften compromise usability or computational efficiency. In this paper, we ana-\nlyze and identify the core limitations of existing approaches and propose a novel\nframework—GoRA (Gradient-driven Adaptive Low Rank Adaptation)—that si-\nmultaneously adapts both the rank and initialization strategy within a unified\nframework. GoRA leverages gradient information during training to dynamically\nassign optimal ranks and initialize low-rank adapter weights in an adaptive man-\nner. To our knowledge, GoRA is the first method that not only addresses the\nlimitations of prior approaches—which often focus on either rank selection or\ninitialization in isolation—but also unifies both aspects within a single framework,\nenabling more effective and efficient adaptation. Extensive experiments across\nvarious architectures and modalities show that GoRA consistently outperforms\nexisting LoRA-based methods while preserving the efficiency of vanilla LoRA.\nFor example, when fine-tuning Llama3.1-8B-Base for mathematical reasoning,\nGoRA achieves a 5.13-point improvement over standard LoRA and even outper-\nforms full fine-tuning by 2.05 points under high-rank settings. Code is available at:\nhttps://github.com/hhnqqq/MyTransformers.\n1\nIntroduction\nOpen-source pre-trained large language models (LLMs) such as the Llama series [1, 2] have demon-\nstrated exceptional capabilities. Through supervised fine-tuning, these models can be adapted\nto various downstream tasks such as code generation [3] and mathematical problem solving [4].\nHowever, when the model has a parameter size ϕ and uses FP16/BF16 mixed-precision training\nstrategy [5, 6] with the Adam optimizer [7], the parameters and gradients require 4ϕ bytes of mem-\nory, while the optimizer states require 12ϕ bytes. Thus, the minimum memory usage, excluding\nactivations, reaches 16ϕ bytes. Such high memory demands limit the training of large language\nmodels under constrained resources. To reduce memory usage, Low-Rank Adaptation (LoRA) [8]\ndecomposes the weight matrix W ∈Rm×n into W = W0 + ∆W = W0 + sAB, where s is a\nscaling factor, and A ∈Rm×r, B ∈Rr×n, r ≪min(m, n), as shown in Figure 1(a). LoRA only\nupdates the low-rank weights A and B, keeping the pre-trained weight W0 unchanged, thereby\nsignificantly reducing the memory footprint of optimizer states. Although LoRA performs well on\nsimple tasks, when applied to pre-trained large language models, its performance on more challenging\ntasks, such as mathematical reasoning and code generation, still lags behind full fine-tuning [9, 10].\n∗Equal contribution.\n†Corresponding author: chenlei@iim.ac.cn.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nFigure 1: Illustration of (a) LoRA; (b) LoRA variants utilizing adaptive rank masking strategies;\n(c) LoRA variants employing nonzero initialization strategies; and (d) GoRA, which introduces\nadaptively leveraging the weight W’s gradient to allocate the rank of the low-rank adapter and\ninitialize B. A0 and B0 denote the initialized value of A and B.\nOne of the critical factors in LoRA is its rank. Kalajdzievski [11] demonstrates that increasing\nthe rank of LoRA can significantly improve performance when paired with an appropriate scaling\nfactor. However, a direct increase in rank leads to a substantial rise in memory requirement overhead,\nthus imposing constraints on rank selection. To address this, several studies [12, 13] propose to\nensemble multiple low-rank subspaces, allowing for rank increases without proportionally increasing\nthe number of trainable parameters. Nevertheless, these approaches often come at the expense of\nusability due to their intrusion into architecture or training processes. Another promising line of\nresearch explores adaptively assigning ranks to pre-trained weights based on importance. For example,\nAdaLoRA [14] adaptively adjusts ranks by quantifying the importance of each rank during training\nand masking less significant ones, as illustrated in Figure 1(b). However, this masking mechanism\nnecessitates a larger parameter space (e.g., 1.5 times), increasing the number of trainable parameters\nand limiting the upper bound of rank. Consequently, as demonstrated in Section 5.1, adaptively\nallocating ranks without significantly increasing the training cost remains an open challenge.\nAnother vital factor in LoRA is its initialization strategy. In vanilla LoRA, A0 is initialized with\na normal distribution (In the PEFT library, A0 is initialized with a Kaiming distribution [15]) and\nB0 is initialized with zeros. This initialization method ensures that the weights W0 + A0B0\nremain unchanged at the beginning of training. Besides, zero initialization is not the only option:\nWhen A0B0 is nonzero, manipulating the pre-trained weight by subtracting A0B0 from W0 also\nensures stability. Existing nonzero initialization methods can be categorized into experience-driven\nand data-driven methods. In experience-driven methods, PiSSA [16] and MiLoRA [17] employ\ndecomposition techniques such as Singular Value Decomposition (SVD) to capture specific features\nof pre-trained weights. However, these methods are inherently task-agnostic, which limits their\ngeneralizability across diverse tasks. In contrast, data-driven methods incorporate task information.\nFor example, LoRA-GA [18] uses the singular features of gradients to initialize LoRA weights,\nminimizing the difference between LoRA and full fine-tuning. However, as illustrated in Figure 1(c)\nand Section 2.2, existing nonzero methods require manipulating the pre-trained weights, resulting\nin a training-inference gap. Thus, designing a nonzero initialization method without manipulating\npre-trained weights remains an open problem.\nGiven the challenges of adaptive rank allocation and nonzero initialization, we turn to gradients of\npre-trained weights, which are crucial for assessing the importance of pre-trained weights and deeply\nrelated to LoRA adapters’ optimization processes [19, 20]. As shown in Figure 1(d), we propose\nGoRA. Specifically, before training, we compute gradients of pre-trained weights on a subset of\ntraining samples, using these gradients to assess the importance of each pre-trained weight. Given a\nreference rank, we calculate a trainable parameter budget. Based on the normalized importance and the\ntrainable parameter budget, we allocate a new trainable parameter count and corresponding rank for\n2\neach low-rank adapter, achieving adaptive rank allocation without significantly increasing the trainable\nparameter count compared to LoRA and allowing for higher rank allocation upper bounds, as shown\nin Table 5. In GoRA’s initialization, we maintain LoRA’s initialization for A, while B is initialized\nusing −(A⊤\n0 A0)−1A⊤\n0 G, where G is the gradient of the weight W. This initialization ensures\nthat the computation result of the low-rank adapter −A0(A⊤\n0 A0)−1A⊤\n0 G ≈−G compresses the\ngradient optimally, setting a solid foundation for further optimization without manipulating the\npre-trained weight. Our key contributions are summarized as follows:\n1. We conduct an in-depth investigation into LoRA’s rank allocation and initialization strategy,\nuncovering the limitations of existing works. We propose GoRA, which achieves adaptive\nrank allocation and initialization without compromising usability and efficiency.\n2. We use the gradients of weights to assess their importance and allocate ranks. We then\ninitialize the low-rank weights using the pseudo-inverse of the compressed gradients, which\nenhances performance while ensuring training stability.\n3. We conduct extensive experiments, demonstrating that GoRA consistently outperforms\nlow-rank baselines and even rivals full fine-tuning in certain settings. For example, on the\nLlama3.1-8B-Base model fine-tuned for mathematical reasoning, GoRA achieves a 5.13-\npoint improvement over LoRA and even surpasses full fine-tuning high-rank configurations.\n2\nRelated Works\n2.1\nRank of LoRA\nThe choice of rank is crucial for LoRA, with higher ranks consistently yielding better outcomes [11].\nHowever, increasing the rank raises the number of trainable parameters and corresponding memory\nusage overhead, making it challenging to train with sufficient ranks on limited hardware resources.\nPrevious works [21, 12] attempt to continuously merge and reinitialize low-rank weights during\ntraining to stack the overall rank. However, these methods often require resetting the states of the\noptimizer and learning rate scheduler during reinitialization to ensure that updates take place in\ndistinct low-rank subspaces and ensure training stability, significantly increasing overall training\ncomplexity and making the training process unstable. MeLoRA [13] proposes aggregating multiple\nmini low-rank adapters diagonally to increase the overall rank. Nevertheless, this approach requires\nmodifying the structure of LoRA, limiting its usability.\nAt the same time, the significances of weights during training demonstrate heterogeneity, and\nan intuitive proposition is to assign larger ranks to relatively more important weights. Previous\nworks [14, 22] attempted to dynamically mask less important ranks during training to achieve adaptive\nrank adjusting. However, these methods require allocating larger matrices for low-rank adapters\nto reserve space for masked ranks, leading to an increase in the number of trainable parameters,\nwhich compromises their operational efficacy and establishes limitations on the upper threshold\nof rank. IncreLoRA [23] introduces an approach that begins with a single rank for each low-rank\nadapter and incrementally increases the rank during training. This method effectively addresses the\nchallenge of large initial matrices. Nevertheless, this approach demonstrates suboptimal compatibility\nwith distributed training architectures, notably FSDP[24] and ZeRO[25], which constitute essential\ninfrastructural components for the effective training of large-scale models.\n2.2\nInitialization of LoRA\nParameter initialization represents a fundamental paradigm in deep learning methodologies. Well-\nestablished initialization protocols, such as the strategy proposed by Xavier Glorot and Yoshua\nBengio [26], facilitate the convergent training trajectories of deep neural networks. Similarly, appro-\npriate initialization strategies constitute a critical determinant for LoRA. Beyond zero initialization\nused by vanilla LoRA, some studies have explored different initialization strategies: PiSSA [16]\nperforms SVD on pre-trained weights and uses the most important singular features to initialize\nlow-rank weights; MiLoRA [17], in contrast to PiSSA, uses the least important singular features to\ninitialize low-rank weights; similarly, OLoRA [27] uses QR decomposition of pre-trained weights\nto initialize low-rank weights; EVA [28] uses singular features of activations to initialize low-rank\nweights; and LoRA-GA [18] uses singular features of gradients to initialize low-rank weights. These\nmethods can improve LoRA’s performance to some extent.\n3\nNevertheless, owing to the inherent non-zero initialization characteristics of these methodologies,\nthey require subtracting the LoRA initialization results from the pre-trained weights to ensure\ncorrect forward and backward propagation during the initial phases of the training regimen, conse-\nquently creating a gap between training and inference. Recomputing the initialization result of these\nmethods during inference is not feasible in cases involving randomness [16] or requiring original\ntraining data [18, 28]. And the initialization process requires significant time for methods such as\nMiLoRA [17]. The most straightforward solution is to save not only the low-rank weights but also\nthe manipulated pre-trained weights, but this sacrifices one of LoRA’s significant advantages, namely,\nminimal checkpoint storage [29]. Another approach is to save the initialized LoRA weights and use\nblock matrix multiplication to eliminate the gap, but this reduces usability.\n3\nMethod\nIn this section, we will reinterpret LoRA adapters from the perspective of gradient compressors and\nintroduce GoRA’s gradient-driven adaptive rank allocation and initialization strategy.\n3.1\nView LoRA adapters as Gradient Compressors\nThe core idea of LoRA is to fine-tune a model by leveraging the intrinsic low-rank property of the\nupdate of a weight matrix W ∈Rm×n during training. Specifically, a pair of low-rank matrices\nA ∈Rm×r and B ∈Rr×n are initialized alongside the pre-trained weight W0. During training,\nW0 remains frozen, while the model is updated by training the low-rank matrices A and B, thereby\nreducing memory usage during training. For any training step t, the updated weight Wt is given\nby (1), where α is a tunable hyperparameter that ensures the scale of the LoRA computation depends\nonly on α and is independent of the rank r:\nWt = W0 + ∆Wt = W0 + α\nr AtBt.\n(1)\nSpecifically, given the training loss L, the gradient of the pre-trained weight W0 can be computed as\n∂L\n∂W0 . Using the chain rule, the gradients of A and B are\n∂L\n∂W0 B⊤\nt and A⊤\nt\n∂L\n∂W0 , respectively. (Note:\nin vanilla LoRA, B0 = 0.) Given a learning rate η, the updates to the weight are as shown in (2)-(3):\n∆Bt = −η α\nr\nT\nX\nt=1\nA⊤\nt−1\n∂Lt\n∂W0\n,\n∆At = −η α\nr\nT\nX\nt=1\n∂Lt\n∂W0\nB⊤\nt−1,\n(2)\n∆Wt = α\nr AtBt −α\nr A0B0 = α\nr (∆At∆Bt + A0∆Bt).\n(3)\nExperimental results from LoRA-FA [30] have shown that freezing the randomly initialized matrix\nA and only training the matrix B can achieve performance close to that of LoRA. When matrix A is\nfrozen (∆At = 0), the weight update is given by (4). One can observe that matrix B accumulates the\ngradients compressed by A⊤\n0 during training, and when multiplied by A0, the compressed gradients\nare up-projected. Thus, the training process of LoRA-FA can be viewed as a process of gradient\naccumulation and compression, with the compression matrix being the randomly initialized A.\n∆Wt = α\nr A0∆Bt = −η α\nr\nT\nX\nt=0\nA0A⊤\n0\n∂Lt\n∂W0\n.\n(4)\nThe update form of LoRA-FA provides significant inspiration. We hypothesize that vanilla LoRA has\nsimilar properties, i.e., LoRA adapters act as gradient compressors. Based on this hypothesis, we\ncan allocate larger ranks to weights whose gradients and weights themselves contain more low-rank\ninformation and initialize LoRA parameters using compressed gradients.\n3.2\nGoRA’s Adaptive Rank Allocation Strategy\nBased on the hypothesis that LoRA adapters function similarly to gradient compressors, our adaptive\nrank allocation strategy aims to: (1) allocate rank based on weight importance derived from pre-\ncomputed N-batch accumulated gradients G =\n1\nN\nPn\ni=1\n∂Li\n∂W0 before the formal training process;\n(2) complete rank allocation before training to avoid dynamic shape changes; (3) maintain a similar\nnumber of trainable parameters as LoRA (within 10%); and (4) preserve structural compatibility with\nLoRA for easy integration.\n4\nTo evaluate the importance of weights, we first consider the nuclear norm of the pre-computed\ngradient, which aggregates all singular values of a matrix and is often used to measure low-rank\nproperties [18]. However, as shown in Table 6, this metric does not effectively capture the importance\nof weights in practice. Instead, we adopt a sensitivity-based importance metric commonly used in\nmodel pruning [31]. Specifically, we define the importance of a weight matrix W as:\nI(W) = avg (|W ⊙G|) ,\n(5)\nwhere the operator ⊙denotes the Hadamard product and avg(·) computes the average value to\nyield a scalar importance score. After computing the importance scores for all target pre-trained\nweight matrices, we form an importance set {I(Wi)}N\ni=1. To facilitate adaptive rank allocation, we\nnormalize the importance set to compute an advantage ai for the i-th pre-trained weight Wi\n0:\nai =\nI(Wi\n0)\nΣN\ni=1I(Wi\n0).\n(6)\nWith the normalized advantages computed, we next determine the total trainable parameter budget\nb for the model. Given a reference rank rref, the budget for a single weight matrix Wi ∈Rm×n is\nestimated as:\nbi = (\n√\nm + n) × rref,\n(7)\nreflecting a smoothed parameter budget under vanilla LoRA. Summing over all budgets, the total\nbudget becomes b = ΣN\ni=1bi. Using this budget and the advantage ai, we allocate the adapter rank ri\nand its trainable parameter count pi for each weight matrix as:\nri =\n\u0014\npi\n√m + n\n\u0015\n=\n\u0014 b ∗ai\n√m + n\n\u0015\n,\ns.t.rmin ≤ri ≤rmax,\n(8)\nwhere the operator [·] denotes rounding to the nearest integer, and rmin, rmax are hyper-parameters\ndefining the allowable rank range. This formulation ensures that the total number of trainable\nparameters p = ΣN\ni=1pi approximately matches that of standard LoRA with rank rref closely.\nIn summary, before training begins, we use the N-batch accumulated gradients for all target weights.\nThese gradients are then used to estimate the importance of each weight matrix, based on which we\nperform adaptive rank allocation in GoRA, achieving all four objectives outlined earlier.\n3.3\nGoRA’s Adaptive Initialization Strategy\nOnce ranks are allocated for each layer, it is crucial to initialize the low-rank weights properly.\nThe compression form in (4) is suboptimal when A is randomly initialized and fixed; to achieve\nbetter alignment with the gradient dynamics, we can initialize B such that the computation of the\nlow-rank adapter at the start of training closely approximates the N-batch accumulated gradient G.\nThis optimal initialization can be derived using the Moore-Penrose inverse of A0 (this computation\nrequires negligible computational time as detailed in Appendix D.1):\nB0 = −(A⊤\n0 A0)−1A⊤\n0 G,\nA0B0 = −A0(A⊤\n0 A0)−1A⊤\n0 G.\n(9)\nAs shown in (9), initializing B as −(A⊤\n0 A0)−1A⊤\n0 G ensures that A0B0 provides the best low-rank\napproximation of G given a fixed A0, with proof provided in Appendix B.1.\nHowever, due to the properties of pseudo-inverse computation, the magnitude of A0B0 does not\nexactly match that of G. Assuming both G ∈Rm×n and A0 ∈Rm×r follow distributions with\nmean 0 and variance 1, the expected Frobenius norm of G, E[||G||F ], is √mn, while that of A0B0,\nE[||A0B0||F ], is √rn, as detailed in Appendix B.2.\nTo ensure that the initial computation of a low-rank adapter approximates a single step of stochastic\ngradient descent with a tunable step size γ, we introduce a scaling factor ξ for B0:\nα\nr A0(ξB0) ≈ξ α\nr\nr r\nmG ≈−γG.\n(10)\nThus, to make GoRA’s initialization equivalent to one step of gradient descent, ξ should be set to\nγ·√rm\nα\n. Inspired by rsLoRA [11], and to better utilize the larger ranks obtained through dynamic\nallocation, we modify the forward computation to Wt = W0 + ∆W = W0 +\nα\n√rAtBt, which\nadjusts ξ to γ·√m\nα\n. Setting γ to a relatively large value further improves performance and yields\noptimal results. The full algorithm is summarized in Algorithm 1 and Algorithm 2.\n5\nAlgorithm 1 Rank Allocation and Initialization of GoRA under Single Training Worker\nInput: Model f(·) with L layers, pre-trained parameters θ = {Wl\n0}L\nl=1, gradient accumulation\nsteps N, loss function F, scale factor γ, Trainable parameter budget b\nOutput: Initialized low-rank matrices {Al\n0}L\nl=1, {Bl\n0}L\nl=1\n1: for l = 1 to L do\n2:\nGlavg ←0\n▷Initialize gradients buffer in CPU memory.\n3: for i = 1 to N do\n▷Compute gradients without optimizer.\n4:\nRandomly sampled mini-batch {x, y}\n5:\nˆy ←f(x, θ), L ←F(y, ˆy)\n6:\nfor l = 1 to L do\n7:\nAccumulate on CPU: Glavg ←Glavg + 1\nN\n∂L\n∂Wl\n0\n8: for l = 1 to L do\n9:\nCompute importance I(Wl\n0) ←avg(|Wl\n0 ∗Glavg|)\n10: for l = 1 to L do\n11:\nCompute advantage al ←\nI(Wl\n0)\nPL\nl=1 I(Wl\n0)\n12: for l = 1 to L do\n13:\nm, n ←size(Wl\n0)\n14:\nrl ←clip(round(\nb·al\n√m+n), rmin, rmax)\n▷Clip by rmin and rmax to avoid extremum.\n15:\nAl\n0 ∼UKaiming(m × rl, a =\n√\n5)\n▷Initialize Al\n0 ∈Rm×rl with Kaiming uniform.\n16:\nBl\n0 ←−(Al⊤\n0 Al\n0)−1Al⊤\n0 Gl\navg\n▷Initialize Bl\n0 ∈Rrl×n.\n17:\nBl\n0 ←γ√m\nα\nBl\n0\n18: Return {Al\n0}L\nl=1, {Bl\n0}L\nl=1\n4\nExperiments\nWe conducted comprehensive experiments comparing GoRA with baseline methods on natural lan-\nguage understanding (Section 4.1), generation tasks (Section 4.2), and image classification tasks\n(Section 4.3). For understanding tasks, we trained T5-Base [32] on five tasks of GLUE [33] (MNLI,\nSST-2, CoLA, QNLI, MRPC) and reported accuracy on corresponding validation sets. For generation\ntasks, we fine-tuned Llama-3.1-8B-Base [2] and Llama-2-7B-Base [1] on chat, mathematics, and cod-\ning datasets, evaluating test performance on MTBench [34], GSM8k [35], and HumanEval [36]. For\nimage classification tasks, we fine-tuned CLIP-ViT-B/16 [37] on seven datasets including Stanford-\nCars [38], DTD [39], EuroSAT [40], GT-SRB [41], RESISC45 [42], SUN397 [43] and SVHN [44]\nand reported test accuracy. All experiments were conducted using single-epoch training across\nthree random seeds, with results reported as mean values and standard deviations. Unless specified\notherwise, we set the LoRA rank or GoRA’s reference rank rref to 8. The hyperparameters of GoRA\nare detailed in Appendix C.3.\n4.1\nExperimental Results on Natural Language Understanding Tasks\nTable 1: Performance of fine-tuning T5-Base on 5 sub-tasks of the GLUE benchmark. Bold and\nunderline indicate the highest and second-highest scores of low-rank methods with r = 8 or rref = 8.\nMethod\nMNLI\nSST-2\nCoLA\nQNLI\nMRPC\nAverage\nFull\n86.33±0.00\n94.75±0.21\n80.70±0.24\n93.19±0.22\n84.56±0.73\n87.91\nLoRA [8]\n85.30±0.04\n94.04±0.11\n69.35±0.05\n92.96±0.09\n68.38±0.01\n82.08\nConvergence Optimization Methods for LoRA\nrsLoRA [11]\n85.73±0.10\n94.19±0.23\n72.32±1.12\n93.12±0.09\n52.86±2.27\n79.64\nDoRA [45]\n85.67±0.09\n94.04±0.53\n72.04±0.94\n93.04±0.06\n68.08±0.51\n82.57\nLoRA+ [46]\n85.81±0.09\n93.85±0.24\n77.53±0.20\n93.14±0.03\n74.43±1.39\n84.95\nInitialization Optimization Methods for LoRA\nPiSSA [16]\n85.75±0.07\n94.07±0.06\n74.27±0.39\n93.15±0.14\n76.31±0.51\n84.71\nLoRA-GA [18]\n85.70±0.09\n94.11±0.18\n80.57±0.20\n93.18±0.06\n85.29±0.24\n87.77\nAdaptive Methods for LoRA\nAdaLoRA [14]\n85.45±0.11\n93.69±0.20\n69.16±0.24\n91.66±0.05\n68.14±0.28\n81.62\nGoRA\n85.91±0.02\n94.68±0.43\n79.86±0.35\n93.27±0.08\n86.10±0.20\n87.96\nSettings: We adopted baseline performances reported by LoRA-GA [18], maintaining their experi-\nmental parameters for fair comparison: Adam[7] optimizer (β1 = 0.9, β2 = 0.999, weight decay =\n6\n0), batch size 32, cosine decay learning rate with a warmup ratioo of 0.03. We trained all linear layers\nexcept the language head using a peak learning rate of 1e-4, a maximum sequence length of 128, and\nFP32 precision.\nResults: Table 1 compares GoRA against multiple baselines across five GLUE benchmark tasks.\nGoRA achieved superior performance on four datasets (MNLI, SST-2, QNLI, and MRPC), demon-\nstrating exceptional adaptability and generalization. While slightly underperforming LoRA-GA on\nCoLA by just 0.71 percentage points, GoRA’s average score (87.96) surpassed all baselines and even\nexceeded full fine-tuning (87.91). This confirms GoRA’s ability to maximize model potential while\nmaintaining parameter efficiency. Notably, GoRA showed particularly strong performance on MRPC\nand QNLI, highlighting its effectiveness in small-sample learning and sentence-pair tasks.\n4.2\nExperimental Results on Natural Language Generation Tasks\nSettings: We trained mathematical, coding, and dialogue capabilities using 100K MetamathQA [47],\n100K Code-FeedBack [48] (code-only labels), and 52K WizardLM [49] subsets, respectively.\nFor experiments on Llama-3.1-8B-base, training used AdamW [50] (β1 = 0.9, β2 = 0.999,\nweight decay = 5e −4) with batch size 64, cosine decay learning rate (warmup ratio=0.03, de-\ncay ratio=0.1), and BF16 mixed precision. For all methods, including GoRA, we trained attention\nmodules’ linear components with a peak learning rate of 5e-5 (5e-4 for AdaLoRA). Evaluation met-\nrics: mathematics—regex-extracted accuracy; coding—PASS@1; dialogue—average scores (0-10)\nfrom GPT-4o [51], Gemini-1.5-Pro [52], and Llama-3.1-70B-Instruct [2] using prompts from [34].\nFor experiments on Llama-2-7B-Base, we adopted baseline results from LoRA-GA [18], and we\nmaintained the same training and evaluation settings. Further details are provided in Appendix C.4.\nTable 2: Performance of fine-tuning Llama-3.1-8B-Base.\nMethod\nMTBench\nGSM8k\nHumanEval\nFull\n5.88±0.23\n73.69±0.28\n51.63±1.27\nLoRA [8]\n6.15±0.02\n67.78±1.25\n43.09±0.35\nrsLoRA [11]\n6.18±0.09\n68.36±0.74\n45.78±2.80\nDoRA [45]\n6.24±0.12\n69.17±1.00\n43.70±1.54\nLoRA+ [46]\n6.35±0.10\n71.29±0.93\n44.51±2.11\nOLoRA [27]\n6.13±0.04\n68.54±0.42\n43.29±2.44\nPiSSA [16]\n6.08±0.09\n68.56±1.03\n44.10±1.54\nLoRA-GA [18]\n5.99±0.06\n71.39±0.90\n43.29±0.61\nAdaLoRA [14]\n6.19±0.16\n70.63±0.77\n41.46±3.66\nGoRA\n6.34±0.04\n72.91±0.76\n48.98±2.14\nGoRArref=32\n6.21±0.10\n75.59±1.04\n51.22±1.83\nGoRArref=128\n5.82±0.31\n75.74±0.40\n52.03±1.41\nTable 3: Performance of fine-tuning Llama-2-7B-Base.\nMethod\nMTBench\nGSM8k\nHumanEval\nFull\n5.30 ± 0.11\n59.36 ± 0.85\n35.31 ± 2.13\nLoRA [8]\n5.61 ± 0.10\n42.08 ± 0.04\n14.76 ± 0.17\nrsLoRA [11]\n5.25 ± 0.03\n45.62 ± 0.10\n16.01 ± 0.79\nDoRA [45]\n5.97 ± 0.02\n53.07 ± 0.75\n19.75 ± 0.41\nLoRA+ [46]\n5.71 ± 0.08\n52.11 ± 0.62\n18.17 ± 0.52\nOLoRA [27]\n5.30 ± 0.04\n43.29 ± 0.83\n17.22 ± 0.12\nPiSSA [16]\n5.30 ± 0.02\n44.54 ± 0.27\n16.02 ± 0.17\nLoRA-GA [18]\n5.95 ± 0.16\n53.60 ± 0.30\n19.81 ± 1.46\nAdaLoRA [14]\n5.57 ± 0.05\n50.72 ± 1.39\n17.80 ± 0.44\nGoRA\n5.61 ± 0.12\n54.04 ± 0.22\n24.80 ± 1.04\nGoRArref=32\n5.75 ± 0.06\n56.18 ± 0.10\n26.83 ± 2.84\nGoRArref=128\n6.05 ± 0.04\n56.58 ± 0.12\n27.85 ± 0.58\nFigure 2: The training loss curves of full fine-\ntuning, LoRA, GoRA, and GoRAr0=128 on Llama-\n3.1-8B-Base. GoRA demonstrates lower start loss\nand faster convergence speed.\nResults: Table 2 and Table 3 show the per-\nformance of GoRA and baseline methods on\nfine-tuned Llama3.1-8B-Base and Llama2-7B-\nBase. Specifically, GoRA demonstrated excep-\ntional performance on the more challenging Hu-\nmanEval and GSM8k benchmarks, substantially\nsurpassing all baseline methods. For Llama3.1-\n8B-Base, on the GSM8k dataset, GoRA scored\n72.91, outperforming LoRA-GA’s 71.39 by\n1.52 points; on the HumanEval dataset, GoRA\nachieved 48.98, surpassing rsLoRA’s 45.78 by\n3.20 points.\nOn MTBench, GoRA slightly\nunderperforms in terms of overall effective-\nness, scoring 6.34, just 0.01 points lower than\nLoRA+’s 6.35. Notably, GoRA performed well\nacross different rank allocation settings. For ex-\nample, GoRArref=128 achieved 75.74 and 52.03\non the GSM8k and HumanEval, respectively,\nsurpassing full fine-tuning’s 73.69 and 51.63.\nEven the rref = 32 configuration of GoRA,\nwhile slightly underperforming rref = 128, still\noutperformed full fine-tuning on GSM8k. For\nLlama2-7B-Base, GoRA demonstrated similar\nsuperior results compared to baseline methods. Especially, GoRA outperformed LoRA-GA by 4.99\n7\nand 0.44 on HumanEval and GSM8k, respectively. These results validate the effectiveness of GoRA\nacross different LLMs and settings. The training loss curves of GoRA are depicted in Figure 2.\nTable 4: Performance of fine-tuning CLIP-VIP-B/16 on 7 image classification tasks.\nMethod\nCars\nDTD\nEuroSAT\nGTSRB\nRESISC45\nSUN397\nSVHN\nAverage\nZero-shot\n63.75\n44.39\n42.22\n35.22\n56.46\n62.56\n15.53\n45.73\nFull\n84.23±0.06\n77.44±0.19\n98.09±0.03\n94.31±0.28\n93.95±0.00\n75.35±0.10\n93.04±0.18\n88.06\nLoRA [8]\n72.81±0.13\n73.92±0.38\n96.93±0.07\n92.40±0.10\n90.03±0.14\n70.12±0.18\n88.02±0.07\n83.46\nDoRA [45]\n73.72±0.06\n73.72±0.33\n96.95±0.01\n92.38±0.08\n90.03±0.08\n70.20±0.19\n88.23±0.05\n83.48\nLoRA+ [46]\n72.87±0.18\n74.07±0.45\n97.01±0.02\n92.42±0.18\n89.96±0.11\n70.17±0.15\n88.08±0.05\n83.51\nLoRA-Pro [53]\n85.87±0.08\n78.64±0.85\n98.46±0.03\n95.66±0.05\n94.75±0.21\n76.42±0.14\n94.63±0.20\n89.20\nLoRA-GA [18]\n85.18±0.41\n77.50±0.12\n98.05±0.27\n95.28±0.10\n94.43±0.19\n75.44±0.06\n93.68±0.35\n88.51\nGoRA\n85.76±0.19\n78.17±0.32\n98.77±0.35\n96.66±0.36\n95.16±0.26\n76.46±0.08\n95.32±0.13\n89.47\n4.3\nExperimental Results on Image Classification Tasks\nSettings: We adopted baseline performances from LoRA-Pro [53], maintaining their experimental\nhyper-parameters for a fair comparison: Adam [7] optimizer (β1 = 0.9, β2 = 0.999, weight decay =\n0), batch size 64, cosine decay learning rate with 0.03 warmup ratio. We trained all linear layers in\nthe vision backend using a peak learning rate of 1e-4, and FP32 precision. The classifier is obtained\nusing prompts such as “a photo of a {class}.”\nResults: As shown in Table 4, GoRA outperforms baseline methods across all seven image classi-\nfication tasks. Specifically, GoRA outperforms full fine-tuning by a margin of 1.01; outperforms\nLoRA-GA by 0.96 and outperforms LoRA-Pro by 0.27. These results demonstrate that GoRA\nexhibits superior performance across different models and modalities.\n5\nDiscussions\nIn this section, we present a comprehensive set of ablation studies to evaluate the effectiveness\nof GoRA’s adaptive rank allocation and initialization strategy. We also examine the impact of\nGoRA’s hyperparameters and discuss their role in shaping performance. Additionally, we explore\nhyperparameter auto-tuning strategies to improve usability.\nFigure 3: (a) Result rank distribution of fine-tuning Llama-3.1-8B-Base on the MetaMathQA-100K\ndataset using GoRA;(b) Difference values between GoRA and LoRA in directional updates of pre-\ntrained weights after merging;(c) Difference values between GoRA and LoRA in magnitude updates\nof pre-trained weights after merging. Data points are presented for every two layers.\n5.1\nThe Effect of the Rank Allocation Strategy\nThe rank allocation strategy is a crucial component that influences the performance of GoRA. As\nhighlighted in Table 5, we conducted ablation studies to evaluate different rank allocation ranges. The\nresults demonstrate that a broader rank allocation range consistently leads to superior performance.\nFor instance, given γ = 5e −2, (rmin = 4, rmax = 32) achieved a score of 48.98 on HumanEval,\nsignificantly outperforming both the fixed rank allocation strategy (rmin = 8, rmax = 8) and the more\nconservative allocation strategy (rmin = 6, rmax = 15).\nFigure 3 illustrates the rank distribution of (rmin = 4, rmax = 32). Notably, most ranks are allocated\nto the wv layers, while the wq layers receive the fewest rank allocations. This observation aligns\n8\nwith findings reported in prior work [8]. Moreover, weights with higher ranks receive larger updates\nafter merging the low-rank matrices. These observations underscore the effectiveness of our rank\nallocation strategy.\nTable 5: Ablation study on hyperparameters. To maintain an approximately constant number of\ntrainable parameters, the rank allocation upper bound was reduced as the lower bound was increased.\nMethod\nrmin\nrmax\nγ\nGSM8k\nHumanEval\nAdaLoRA\n0\n12\n-\n70.63±0.77\n41.46±3.66\nLoRA\n8\n8\n0\n67.78±1.25\n43.09±0.35\nGoRA\n4\n32\n8e-2\n72.91±0.76\n46.54±1.54\nGoRA\n4\n32\n5e-2\n72.88±0.99\n48.98±2.14\nGoRA\n4\n32\n3e-2\n72.71±1.22\n45.93±1.27\nGoRA\n4\n32\n0\n72.45±1.14\n46.34±0.61\nGoRA\n0\n∞\n5e-2\n72.83±0.80\n46.13±3.36\nGoRA\n4\n32\n5e-2\n72.88±0.99\n48.98±2.14\nGoRA\n6\n15\n5e-2\n72.25±0.27\n45.85±3.18\nGoRA\n8\n8\n5e-2\n72.10±1.12\n44.75±3.97\n5.2\nThe Effect of the Initialization Strategy\nTable 5 also summarizes the results of ablation studies conducted with various scaling factors. Our\nexperiments revealed that the choice of scaling factor γ has a substantial impact on performance.\nNotably, GoRA achieves the best performance on HumanEval with γ = 5e −2, attaining a score of\n48.98. Meanwhile, GoRA with γ = 8e −2 slightly outperformed other configurations on the GSM8k,\nachieving a score of 72.91. Conversely, when γ = 0, GoRA exhibited the weakest performance\non GSM8k, scoring 72.45. A carefully selected scaling factor ensures that the initial low-rank\nadapter computation closely approximates a gradient descent step, establishing a robust foundation\nfor subsequent optimization. This is crucial for maintaining training stability.\n5.3\nThe Effect of Different Importance Metrics\nTable 6: Ablation studies on different importance\nmetrics, where || · ||∗represents the nuclear norm.\nMetric\nGSM8k\nHumanEval\navg(|W ⊙G|)\n72.88±0.99\n48.98±2.14\n||G||∗\n72.70±0.68\n43.09±0.93\n||W ⊙G||∗\n72.65±0.78\n45.12±3.17\nTable 6 compares several importance metrics:\nparameter sensitivity to loss (the metric used\nby GoRA), the nuclear norm of the gradient,\nand the nuclear norm of the parameter–gradient\nproduct. The results demonstrate that param-\neter sensitivity consistently outperforms the\nother metrics on both GSM8k and HumanEval.\nNotably, on HumanEval, parameter sensitivity\nachieves a score of 48.98, surpassing the nuclear norm of the gradient (43.09) and the nuclear norm\nof the parameter–gradient product (45.12).\n5.4\nHyperparameter Auto-tuning\nCompared to vanilla LoRA, GoRA introduces two additional hyperparameters: the number of gradient\naccumulation steps N and the initialization scaling factor γ. While hyperparameter tuning strategies\nfor LoRA have been well established in prior work, we aim to minimize the additional tuning burden\nintroduced by GoRA to ensure its practical usability. To this end, we design lightweight, automated\nstrategies that effectively eliminate the need for manual tuning of these new parameters:\n(1) Adaptive Gradient Accumulation: During gradient accumulation, we monitor the normalized\nlayer-wise importance scores at each accumulation step. Once the change between consecutive\nimportance score sets falls below a small pre-defined threshold (e.g., 0.01), indicating convergence\nof layer importance, we terminate accumulation early and proceed to the parameter update. This\nadaptive stopping criterion automatically determines an effective N on the fly, removing the need for\nmanual specification and often accelerating training.\n(2) Adaptive Scaling Factor: We hypothesize that the optimal scaling factor γ is the one that minimizes\nthe loss of the first training step. To find it, we start from a relatively large initial value (e.g., 1.0) and\niteratively reduce it by multiplying by a decay factor (e.g., 0.9) until a small lower bound (e.g., 5e-5)\n9\nis reached. For each candidate γ, we evaluate the loss on the first training batch without performing\nbackpropagation. The γ yielding the lowest loss is then used to initialize formal training.\nImportantly, the pre-defined constants used in these strategies—such as the convergence threshold\n(0.01), decay factor (0.9), and lower bound (5e-5) are empirically stable across tasks and model\nscales. In practice, they require little to no tuning, making GoRA highly usable out of the box while\npreserving its enhanced representational capacity.\nTable 7: Performance comparison of GoRA with\nadaptive hyperparameters.\nMethod\nGSM8k\nHumanEval\nOriginal\n72.91±0.76\n48.98±2.14\nAdaptive N\n72.96±0.19\n46.85±2.11\nAdaptive γ\n72.50±0.38\n50.00±3.23\nTo validate the effectiveness and practicality of\nour auto-tuning strategies, we conduct ablation\nstudies on the scaling factor γ and the gradient\naccumulation steps N. As shown in Table 7,\nour adaptive methods achieve performance com-\nparable to or better than manually tuned base-\nlines, while eliminating the need for hyperpa-\nrameter search. Notably, the auto-selected γ\nvalues (0.0549 for CodeFeedback and 0.0858 for MetaMathQA) closely align with those reported in\nTable 5 (5e-2 and 8e-2, respectively), confirming the reliability of our selection criterion.\n6\nComputational and Memory Analysis\nTable 8: Comparison of trainable parameter counts. For GoRA, we set rmin = 4 and rmax = 32\nModel\nDataset\nTarget Modules\nLoRA\nAdaLoRA\nGoRA\nT5-Base\nSST-2\nall-linear\n3.24M\n4.86M\n3.05M\nLlama3.1-8B-Base\nMetamathQA\nattention\n6.82M\n10.24M\n7.00M\nLlama2-7B-Base\nMetamathQA\nall-linear\n19.99M\n29.99M\n20.18M\nCLIP-ViT-B/16\nCars\nvision_model\n1.33M\n2.03M\n1.35M\nTable 9: Comparison of time and GPU memory costs during GoRA’s\ninitialization process and training process\nModel Size\nProcess\nTime Cost\nMemory Cost\n8B\nInitialization\n23.60s\n56,139MB\nTraining\n37min54.22s\n73,295MB\n32B\nInitialization\n2min44.65s\n64,821 MB\nTraining\n197min52.15s\n66,381 MB\nDuring training, GoRA’s\ncomputational and memory\noverhead is nearly iden-\ntical to LoRA’s, as both\nmethods maintain a simi-\nlar number of trainable pa-\nrameters given a reference\nrank. Table 8 compares the\ntrainable parameter counts\nof LoRA, AdaLoRA, and\nGoRA across various models, with all methods using a rank or average/reference rank of 8.\nAs outlined in Algorithm 1, GoRA requires a brief gradient pre-computing phase for rank allocation\nand initialization. However, this overhead is minimal in both computation and memory usage. First,\ngradient accumulation is performed over only a small number of micro-batches without any optimizer\nupdates. Second, gradients are computed layer-by-layer and immediately offloaded to CPU memory,\navoiding the storage of optimizer states. Moreover, Algorithm 2 integrates seamlessly with distributed\ndata Parallelism and can be readily extended to more complex parallel training setups. Specifically,\nrather than having each worker independently compute and transfer gradients to CPU memory,\nwhich would cause massive concurrent PCIe traffic and excessive CPU memory consumption, our\nmethod performs an immediate Reduce operation on per-layer gradients directly to rank 0 during\nthe backward pass. This ensures that only a single copy of the globally averaged gradient is ever\ntransferred to CPU memory, drastically reducing PCIe bandwidth usage and CPU memory footprint.\nTo empirically validate the efficiency of GoRA, we measure both training time and memory consump-\ntion when fine-tuning Llama-3.1-8B-Base and Qwen2.5-32B [54] on MetaMathQA using 8×A800\n80GB GPUs. For Llama-3.1-8B-Base, we use a per-GPU batch size of 8 (without activation check-\npoint); for the larger Qwen2.5-32B model, we reduce the per-GPU batch size to 1 (with activation\ncheckpoint [55]) due to memory constraints. We employ the adaptive N strategy described in Sec-\ntion 5.4, and all other experimental settings follow those of our main experiments. As shown in\nTable 9 (reported memories benefit from Liger kernel [56] and FlashAttention [57]), GoRA incurs\nnegligible additional time cost and no extra memory overhead compared to standard LoRA, even for\nmodels of vastly different scales, demonstrating its practicality for real-world deployment.\n10\n7\nAcknowledgement\nThis paper is supported by National Key Research and Development Program of China (Grants\nNo.2023YFD2000101), National Natural Science Foundation of China (Grants No.32471988,\n32271981), and the HFIPS Director’s Fund (Grant No.YZJJKX202401).\n11\nReferences\n[1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[2] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024.\n[3] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation\nmodels for code. arXiv preprint arXiv:2308.12950, 2023.\n[4] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng\nLiu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward\nmathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024.\n[5] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. In International Conference on Learning Representations, 2018.\n[6] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,\nSasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen,\net al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.\n[7] Diederik P Kingma. Adam: A method for stochastic optimization. In International Conference\non Learning Representations, 2015.\n[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.\nIn\nInternational Conference on Learning Representations, 2022.\n[9] Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard,\nConnor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns\nless and forgets less. Transactions on Machine Learning Research, 2024.\n[10] Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S, Deepali Aneja,\nZeyu Jin, Ramani Duraiswami, and Dinesh Manocha. A closer look at the limitations of\ninstruction tuning. In International Conference on Machine Learning, pages 15559–15589.\nPMLR, 2024.\n[11] Damjan Kalajdzievski. A rank stabilization scaling factor for fine-tuning with lora. arXiv\npreprint arXiv:2312.03732, 2023.\n[12] Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky.\nRelora:\nHigh-rank training through low-rank updates. In International Conference on Learning Repre-\nsentations, 2023.\n[13] Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten Rijke,\nZhumin Chen, and Jiahuan Pei. Melora: mini-ensemble low-rank adapters for parameter-\nefficient fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages 3052–3064, 2024.\n[14] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\nand Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In International\nConference on Learning Representations, 2023.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 1026–1034, 2015.\n[16] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular\nvectors adaptation of large language models. In Advances in Neural Information Processing\nSystems, volume 37, pages 121038–121072, 2024.\n[17] Hanqing Wang, Yixia Li, Shuo Wang, Guanhua Chen, and Yun Chen. Milora: Harnessing\nminor singular components for parameter-efficient llm finetuning. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Volume 1: Long Papers), pages 4823–4836, 2025.\n12\n[18] Shaowen Wang, Linxi Yu, and Jian Li. Lora-ga: Low-rank adaptation with gradient approxima-\ntion. In Advances in Neural Information Processing Systems, volume 37, pages 54905–54931,\n2024.\n[19] Yongchang Hao, Yanshuai Cao, and Lili Mou. Flora: Low-rank adapters are secretly gradient\ncompressors. In International Conference on Machine Learning, pages 17554–17571. PMLR,\n2024.\n[20] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong\nTian. Galore: Memory-efficient llm training by gradient low-rank projection. In International\nConference on Machine Learning, pages 61121–61143. PMLR, 2024.\n[21] Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu, Xiaochen Wang, Peiyi\nWang, Qingxiu Dong, Liang Chen, and Zhifang Sui. Periodiclora: Breaking the low-rank\nbottleneck in lora optimization. arXiv preprint arXiv:2402.16141, 2024.\n[22] Yahao Hu, Yifei Xie, Tianfeng Wang, Man Chen, and Zhisong Pan. Structure-aware low-rank\nadaptation for parameter-efficient fine-tuning. Mathematics, 11(20):4317, 2023.\n[23] Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, and Yiming Qian.\nIncrelora: Incremental parameter allocation method for parameter-efficient fine-tuning. arXiv\npreprint arXiv:2308.12043, 2023.\n[24] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,\nHamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully\nsharded data parallel. arXiv preprint arXiv:2304.11277, 2023.\n[25] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza-\ntions toward training trillion parameter models. In SC20: International Conference for High\nPerformance Computing, Networking, Storage and Analysis, pages 1–16, 2020.\n[26] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward\nneural networks. In International Conference on Artificial Intelligence and Statistics, 2010.\n[27] Kerim Büyükakyüz. Olora: Orthonormal low-rank adaptation of large language models. arXiv\npreprint arXiv:2406.01775, 2024.\n[28] Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth,\nand Sepp Hochreiter. One initialization to rule them all: Fine-tuning via explained variance\nadaptation. arXiv preprint arXiv:2410.07170, 2024.\n[29] Vlad Fomenko, Han Yu, Jongho Lee, Stanley Hsieh, and Weizhu Chen. A note on lora. arXiv\npreprint arXiv:2404.05086, 2024.\n[30] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient\nlow-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303,\n2023.\n[31] Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen,\nand Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight\nimportance. In International Conference on Machine Learning, pages 26809–26823. PMLR,\n2022.\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\n[33] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations, 2019.\n[34] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench\nand chatbot arena. In Advances in Neural Information Processing Systems, volume 36, pages\n46595–46623, 2023.\n[35] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n13\n[36] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748–8763. PMLR, 2021.\n[38] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-\ngrained categorization. In Proceedings of the IEEE International Conference on Computer\nVision Workshops, pages 554–561, 2013.\n[39] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.\nDescribing textures in the wild. In Proceedings of the IEEE conference on Computer Vision\nand Pattern Recognition, pages 3606–3613, 2014.\n[40] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel\ndataset and deep learning benchmark for land use and land cover classification. IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217–2226, 2019.\n[41] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel.\nDetection of traffic signs in real-world images: The german traffic sign detection benchmark. In\nInternational Joint Conference on Neural Networks, 2013.\n[42] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification:\nBenchmark and state of the art. Proceedings of the IEEE, 105(10):1865–1883, 2017.\n[43] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition, pages 3485–3492, 2010.\n[44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS Workshop on\nDeep Learning and Unsupervised Feature Learning, 2011.\n[45] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang,\nKwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In\nInternational Conference on Machine Learning, pages 32100–32121. PMLR, 2024.\n[46] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+ efficient low rank adaptation of large models.\nIn International Conference on Machine Learning, pages 17783–17806. PMLR, 2024.\n[47] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models. In International Conference on Learning Representations,\n2024.\n[48] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and\nXiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement.\nIn Findings of the Association for Computational Linguistics: ACL 2024, pages 12834–12859,\n2024.\n[49] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,\nQingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to\nfollow complex instructions. In International Conference on Learning Representations, 2024.\n[50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations, 2019.\n[51] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[52] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett\nTanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\n[53] Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, and Tieniu Tan. Lora-pro: Are low-rank\nadapters properly optimized? In International Conference on Learning Representations, 2025.\n14\n[54] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li,\nTianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\nYu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report,\n2025.\n[55] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174, 2016.\n[56] Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven\nShimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels\nfor llm training. arXiv preprint arXiv:2410.10989, 2024.\n[57] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast\nand memory-efficient exact attention with io-awareness. In Advances in Neural Information\nProcessing Systems, volume 35, pages 16344–16359, 2022.\n[58] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In\nInternational Conference on Learning Representations, 2024.\n[59] Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, and Xipeng Qiu. Adalomo: Low-memory optimiza-\ntion with adaptive learning rate. In Findings of the Association for Computational Linguistics:\nACL 2024, pages 12486–12502, 2024.\n15\nA\nNotations\nIn this section, we summarize the notations used in the paper in Table 10.\nTable 10: List of Notations used in the paper\nSymbol\nDescription\nW ∈Rm×n\nFull-rank weight matrix of a linear layer.\nW0 ∈Rm×n\nPre-trained weight matrix of a linear layer in a pre-trained model.\n∆W ∈Rm×n\nUpdate of the pre-trained weight matrix after fine-tuning.\n∆Wt ∈Rm×n\nUpdate of the pre-trained weight matrix at fine-tuning step t.\nWt ∈Rm×n\nWeight matrix of a linear layer at training step t (Wt = W0+∆Wt).\n∂Lt\n∂W0 ∈Rm×n\nGradient matrix of the pre-trained weight W0 at step t.\nG ∈Rm×n\nN-batch accumulated gradient matrix of the pre-trained weight W0.\nA ∈Rm×r, B ∈Rr×n\nTrainable low-rank matrices of a LoRA adapter.\nA0 ∈Rm×r, B0 ∈Rr×n\nInitial values of the low-rank matrices A and B.\nAt ∈Rm×r, Bt ∈Rr×n\nTrainable low-rank matrices of a LoRA adapter at step t.\nm\nInput dimension of the matrix W.\nn\nOutput dimension of the matrix W.\nr\nRank of the low-rank adapter, with r ≪min(m, n).\nrmax\nPre-defined maximum rank of a GoRA adapter.\nrmin\nPre-defined minimum rank of a GoRA adapter.\nrref\nReference rank of GoRA.\nα\nHyperparameter of LoRA and most of its variants.\nγ\nScaling hyperparameter of GoRA.\nξ\nScaling factor of GoRA, automatically determined by γ.\nUkaiming\nKaiming uniform distribution.\nη\nLearning rate used in the optimizer (e.g., AdamW).\n⊙\nHadamard (element-wise) product of two matrices.\n[·]\nRounding to the nearest integer.\n∥· ∥F\nFrobenius norm of a matrix.\n∥· ∥∗\nNuclear norm of a matrix.\navg(·)\nAverage operation (element-wise for a matrix).\nB\nProofs\nB.1\nProof of optimal approximation of G given A.\nLet G be an m × n matrix, and A be an m × r matrix where r ≪min(m, n). We aim to derive\nthe projection formula that minimizes the Frobenius norm of the error ∥G −ˆG∥F , where ˆG is the\noptimal approximation of G in the column space of A, denoted as Col(A).\nThe best approximation ˆG lies in Col(A), so we can express ˆG as:\nˆG = AB,\nwhere B is an r × n matrix of coefficients to be determined. Our goal is to find B such that the error\n∥G −ˆG∥F is minimized.\nThe error matrix is given by:\nE = G −ˆG = G −AB.\nTo minimize ∥E∥2\nF , we take the derivative of ∥E∥2\nF with respect to B and set it to zero. Expanding\n∥E∥2\nF , we have:\n∥E∥2\nF = Tr\n\u0000(G −AB)⊤(G −AB)\n\u0001\n,\nwhere Tr represents the trace of a matrix.\nExpanding this expression:\n∥E∥2\nF = Tr(G⊤G) −2Tr(B⊤A⊤G) + Tr(B⊤A⊤AB).\n16\nTaking the derivative with respect to B and setting it to zero:\n−2A⊤G + 2A⊤AB = 0.\nSimplifying:\nA⊤AB = A⊤G.\nAssuming A⊤A is invertible, we solve for B:\nB = (A⊤A)−1A⊤G.\nSubstituting B into ˆG = AB, we get:\nˆG = A(A⊤A)−1A⊤G.\nThus, the best approximation ˆG is:\nˆG = A(A⊤A)−1A⊤G.\nThe matrix ˆG = A(A⊤A)−1A⊤G is the projection of G onto the column space of A, and it\nminimizes the Frobenius norm of the error ∥G −ˆG∥F .\nB.2\nProof of Expectation of Frobenius Norm of AB.\nLet A be a random Gaussian matrix of size m×r, where each element of A is sampled independently\nfrom N(0, 1). Let G be a random Gaussian matrix of size m × n, where each element of G is also\nsampled independently from N(0, 1). Define:\nB = (A⊤A)−1A⊤G,\nand consider the product:\nAB = A(A⊤A)−1A⊤G.\nThe goal is to compute the expected Frobenius norm E[∥AB∥F ], where the Frobenius norm is defined\nas:\n∥AB∥F =\nsX\ni,j\n(AB)2\nij.\nFirst, observe that AB can be rewritten as:\nAB = A(A⊤A)−1A⊤G.\nLet P = A(A⊤A)−1A⊤. Note that P is a projection matrix onto the column space of A, and thus\nP satisfies:\nP2 = P,\nP⊤= P,\nand\nrank(P) = r.\nSubstituting P into the expression for AB, we have:\nAB = PG.\nThe Frobenius norm of AB is given by:\n∥AB∥2\nF = ∥PG∥2\nF = Tr((PG)(PG)⊤).\nSince (PG)⊤= G⊤P, this becomes:\n∥AB∥2\nF = Tr(PGG⊤P).\nThe matrix GG⊤is an m × m random Wishart matrix. When G is a standard Gaussian matrix of\nsize m × n, the expected value of GG⊤is:\nE[GG⊤] = n · Im,\n17\nwhere Im is the m × m identity matrix. Substituting this result into the expression for ∥AB∥2\nF , we\nget:\nE[∥AB∥2\nF ] = E[Tr(PGG⊤P)] = Tr(PE[GG⊤]P).\nUsing E[GG⊤] = n · Im, this simplifies to:\nE[∥AB∥2\nF ] = Tr(P(n · Im)P) = n · Tr(P2).\nSince P2 = P, we have:\nE[∥AB∥2\nF ] = n · Tr(P).\nThe trace of P is equal to its rank, which is the dimension of the column space of A. Since A is a\nm × r matrix, we have:\nTr(P) = r.\nThus:\nE[∥AB∥2\nF ] = n · r.\nTaking the square root, the expected Frobenius norm of AB is:\nE[∥AB∥F ] = √n · r.\nC\nImplementation Details\nC.1\nBaseline Methods\nWe compared GoRA with baseline methods to demonstrate the effectiveness of our approach:\na. Full: Trains all parameters in the target layers, resulting in the highest memory consumption.\nb. LoRA [8]: Introduces low-rank adapters into the target layers, significantly reducing the\nnumber of trainable parameters.\nc. Convergence Optimization Methods for LoRA\n- rsLoRA [11]: Modifies the scaling factor in LoRA from α\nr to\nα\n√r, enabling better perfor-\nmance with higher-rank adapters and stabilizing the training processes.\n- DoRA [45]: Decomposes the weight updates of pre-trained weights into magnitude and\ndirection components, and applies LoRA to update only the direction.\n- LoRA+ [46]: Addresses the imbalance between matrices A and B in LoRA by assigning a\nrelatively larger learning rate to matrix B than to matrix A.\nd. Initialization Optimization Methods for LoRA\n- OLoRA [27]: Initializes LoRA weights using the QR decomposition of the corresponding\npre-trained weights.\n- PiSSA [16]: Initializes LoRA weights based on the dominant singular vectors obtained\nfrom the SVD of pre-trained weights.\n- LoRA-GA [18]: Initializes LoRA weights using significant singular vectors derived from\nthe SVD of gradients of pre-trained weights.\ne. Adaptive Methods for LoRA\n- AdaLoRA [14]: Approximates the low-rank adapter structure using SVD, enabling dy-\nnamic rank allocation through singular value masking. It also introduces an orthogonal\nregularization term to the loss function for enhancing orthogonality among features in the\nlow-rank adapter.\nC.2\nImplementation Details for Baseline Methods\nSeveral baseline methods introduce tunable hyperparameters compared with vanilla LoRA [8]. To\nensure a fair comparison, we adopt the optimal settings reported in the original papers whenever\npossible. Specifically, for LoRA+ [46], we set the learning rate ratio of matrices A and B to 16. For\nLoRA-GA [18], we use the “stable” scaling method (the scaling hyperparameter γ of LoRA-GA is\nconfigured to 16) and manipulate the pre-trained weights during initialization. For AdaLoRA [14],\nthe initial rank is set to 12, the final rank to 8, with ti = 150 and tf = 900. For PiSSA [16], the\nnumber of iterations for fast SVD is set to 64.\n18\nC.3\nImplementation Details for GoRA\nFor all experiments with rref = 8, except for the model trained on MetaMathQA [47], we set the\nscaling factor γ to 5e −2. For the model trained on MetaMathQA, γ is set to 8e −2. For all\nexperiments with rref = 32, the scaling factor is set to 1e−2; and for rref = 128, we set the scaling\nfactor to 5e −3. This is because we observe that more gradient information is compressed by GoRA’s\ninitialization with a higher rank, even if γ can control the magnitude of the initialization results. To\naddress the imbalance in GoRA’s matrices A and B, we set the learning rate of matrix B to be 16\ntimes that of matrix A Throughout the experiments, the rmax was empirically defined as 4 × rref,\nthe rmin was set to rref/2 (as this setting can maintain a comparable parameter count compared to\nLoRA), and the gradient accumulation step for GoRA’s initialization was set to 64. In the ablation\nstudies, we adhered to the same hyperparameter settings as in the main experiments, unless otherwise\nspecified.\nFurthermore, Algorithm 1 presents a basic algorithm for non-parallel scenarios; however, parallel\ntraining strategies, particularly data parallelism, are widely employed for training large language\nmodels. We introduce the GoRA algorithm under distributed data parallelism in Algorithm 2, which\nis readily integrable into more complex parallel frameworks.\nAlgorithm 2 Rank Allocation and Initialization of GoRA under Distributed Data Parallelism\nInput: Number of layers L, gradient accumulation steps N, model parameters θ = {W l\n0}L\nl=1,\ncurrent data parallel worker ID w, total data parallel workers W\nOutput: Initialized low-rank matrices {Al\n0}L\nl=1, {Bl\n0}L\nl=1\n1: Initialize empty CPU buffers {Glavg}L\nl=1 only on worker 0\n2: for i = 1 to N do\n3:\nSample mini-batch and compute loss L\n4:\nfor l = 1 to L do\n5:\nCompute local gradient on GPU: Gl ←\n∂L\n∂Wl\n0\n6:\nif w = 0 then\n7:\nAccumulate on CPU: Glavg ←Glavg + 1\nN Gl\n8:\nRelease the GPU memory occupied by Gl: Gl ←None\n9: if w = 0 then\n10:\nfor l = 1 to L do\n11:\nCompute the importance I(Wl\n0) as detailed in Algorithm 1\n12:\nBroadcast the importance set {I(Wl\n0)}L\nl=1 to all other workers\n13: else\n14:\nReceive {I(Wl\n0)}L\nl=1 from worker 0\n15: for l = 1 to L do\n16:\nif w = 0 then\n17:\nLoad the gradient Gl\navg from CPU to GPU\n18:\nInitialize Al\n0, Bl\n0 as detailed in Algorithm 1\n19:\nClear the GPU memory occupied by Gl\navg\n20:\nBroadcast initialized Al\n0, Bl\n0 to all other workers\n21:\nelse\n22:\nReceive initialized Al\n0, Bl\n0 from worker 0\n23: Return {Al\n0}L\nl=1, {Bl\n0}L\nl=1\nC.4\nHyperparameters for Each Experiment\nThe hyperparameters used in each experiment are summarized in Table 11. For experiments on\nT5-Base and Llama2-7B-Base, we adopt the settings from LoRA-GA [18]; for CLIP-ViT-B/16, we\nfollow LoRA-Pro [53]. For experiments on Llama3.1-8B-Base, including both baseline methods and\nGoRA, we use one of the most commonly adopted hyperparameter configurations.\n19\nTable 11: Hyperparameters used in experiments\nModel\nLR\nLR Decay\nWarmup\nOptimizer\nBetas\nWeight Decay\nBatch Size\nT5-Base\n1e-4\n0\n0.3\nAdam\n0.9, 0.999\n0\n32\nLlama3.1-8B-Base\n5e-5\n0.1\n0.3\nAdamW\n0.9, 0.999\n5e-4\n64\nLlama2-7B-Base\n2e-5\n0\n0.3\nAdamW\n0.9, 0.999\n0\n32\nCLIP-ViT-B/16\n1e-4\n0\n0.3\nAdam\n0.9, 0.999\n0\n64\nC.5\nTraining Environments\nFor natural language understanding tasks reported in section 4.1, we conduct our experiments using\nthe Huggingface Transformers framework for model and trainer implementation on a single RTX\n4090 24GB. In contrast, for natural language generation tasks reported in Section 4.2 and Section 5,\nwe utilize the DeepSpeed ZeRO2 [25] data parallel framework and FlashAttention-2 [58] mechanism,\nleveraging the power of 8 RTX 4090 24 GB GPUs or 8 A800 80 GB GPUs. All codes of GoRA and\nbaseline methods are implemented in PyTorch.\nD\nAddtional Experiments\nD.1\nComputational Overhead of Pseudo-Inverse Initialization\nThe initialization strategy of GoRA involves computing a pseudo-inverse for each low-rank adapter\nmatrix. Although the theoretical cost of pseudo-inversion scales cubically with rank (O(r3)), our\nimplementation performs all operations on GPUs using optimized linear algebra routines, resulting in\nminimal practical overhead.\nWe benchmark the total initialization time for the Llama-3.1-8B-Base model across different ranks\nfollowing the settings of our main experiments. Results are summarized in Table 12. Even at rank\n128, where each pseudo-inverse requires approximately 2 million FLOPs, the entire initialization\ncompletes in under 4 seconds. This amounts to less than 0.1% of typical fine-tuning runtime,\nconfirming that the pseudo-inverse step does not constitute a computational bottleneck in practice.\nTable 12: Pseudo-inverse initialization time for Llama-3-8B-Base (GPU, end-to-end).\nRank\nInitialization Time\n8\n1.40s\n32\n1.56s\n128\n3.43s\nD.2\nCombining GoRA with QLoRA\nTo investigate whether GoRA can be effectively integrated with quantization techniques, we adopt\nthe quantization method from QLoRA, which quantizes pre-trained weights to NF4 precision for\nstorage and dequantizes them back to BF16 precision during computation. We evaluate this combined\napproach, which we term QGoRA, by fine-tuning the Llama-3.1-8B-Base model and assessing its\nperformance on math and code benchmarks, following the same protocol as in our main experiments.\nAs shown in Table 13, GoRA integrates seamlessly with QLoRA’s quantization framework and\nconsistently outperforms QLoRA across evaluated tasks.\nTable 13: Performance of QGoRA and QLoRA.\nMethod\nGSM8k\nHumanEval\nQLoRA\n65.10 ± 0.95\n43.49 ± 0.70\nQGoRA\n70.58 ± 1.33\n44.10 ± 3.68\nD.3\nGradient Reconstruction Accuracy of GoRA Initialization\nTo assess how well the GoRA initialization approximates the pre-computed gradients, we measure\nthe reconstruction error between the initialized low-rank adapters and the full gradients across all\nadapted layers in the Llama-3.1-8B-Base model. Results on two datasets are summarized in Table 14.\nThese results suggest that the initialization preserves a high proportion of gradient information\n(approximately 88–89% in relative terms).\n20\nTable 14: Gradient reconstruction error of GoRA initialization.\nDataset\nAbsolute Error\nRelative Error\nMetaMathQA\n0.0382\n0.1147\nCode-Feedback\n0.0322\n0.1152\nD.4\nMulti-Step Gradient Initialization\nWe explored an alternative initialization variant, GoRA-pro, which estimates layer importance\nusing multi-step stochastic gradients. In this approach, a lightweight pre-training phase performs\nn exploratory updates per layer using the AdaLomo [59] optimizer; gradients are accumulated on\nCPU and then discarded, and the original weights are restored before adapter initialization. This\nprocess uses approximately 23% less GPU memory than full fine-tuning (56.2 GB vs. 73.0 GB for\nLlama-3.1-8B-Base).\nWe evaluate GoRA-pro on GSM8K and HumanEval, comparing it to the standard GoRA (which\nuses one-step gradient accumulation). Results are shown in Table 15. The comparable performance\nsuggests that the multi-step gradient idea is viable and may offer further improvements with careful\ntuning of the exploration schedule, gradient normalization. We leave a systematic investigation of\nthis direction to future work.\nTable 15: Comparison of GoRA and GoRA-pro (Llama-3-8B-Base).\nMethod\nGSM8K\nHumanEval\nGoRA\n72.91 ± 0.76\n48.98 ± 2.14\nGoRA-pro\n72.30 ± 0.30\n47.36 ± 1.54\nE\nClarifications\nE.1\nClarification on the training-inference gap introduced by previous initialization methods\nThis is due to their reliance on manipulating pre-trained weights. Specifically:\nManipulation of Pre-Trained Weights: These methods are required to manipulate the value of\npre-trained weights during initialization, as A0B0 ̸= 0 and W0 + A0B0 ̸= W0. As a result, during\ninference, the term A0B0 must be recomputed to properly reconstruct the adapted weight matrix for\neffective model deployment, which is essential for correct model outputs.\nThe Inconvenience of Recalculating Initialization Results: During inference, it is often infeasible to\nrecompute the initialization results for methods that either rely on randomness, such as PiSSA[16], or\nrequire access to training data, such as LoRA-GA [18] and EVA[28]. Furthermore, for approaches like\nMiLoRA [17], the initialization process itself can be computationally expensive and time-consuming.\nIncompatibility Across Multiple Adapters: When multiple adapters are trained on different tasks\nusing previous data-driven non-zero initialization methods, the pre-trained weights are manipulated\ninconsistently. As the result of A0B0 depends on the task. This makes it challenging to serve multiple\nadapters simultaneously, limiting flexibility in multi-task scenarios.\nSaving manipulated pre-trained weights sacrifices one of the key advantages of LoRA: While\nit is possible to merge the low-rank adapter weights into the pre-trained weights after training,\nsaving the pre-trained weights post-merging sacrifices one of the key advantages of LoRA: minimal\nstorage requirements (e.g., 10MB compared to 14GB). Other potential approaches to eliminate the\ntraining-inference gap and their limitations are discussed in Section 2.2.\nE.2\nCompare GoRA’s initialization strategy with LoRA-GA\nBoth GoRA and LoRA-GA leverage gradients to initialize low-rank adapters, but there are several\nkey differences:\n1. Motivation:\n• LoRA-GA: Minimizes the difference in updates of weights between LoRA and full\nfine-tuning.\n21\n• GoRA: Views LoRA adapters, gradient compressors, and optimizes the compression\nform.\n2. Scaling factor:\n• LoRA-GA: Inspired by rsLoRA, its scaling aims to stabilize training.\n• GoRA: Scale the initialized product of adapters to any desired magnitude flexibly.\n3. Methodology:\n• LoRA-GA: Initialize the weights using SVD decomposed gradients.\n• GoRA: Initialize the weights of B using gradients compressed by pseudo-inverse of\nrandom initialized A.\nE.3\nFurther compare with previous related works\nBuilding upon previous works, GoRA makes several unique contributions:\n1. First data-driven initialization method without manipulating pre-trained weights.\n2. Efficient rank allocation strategy without additional trainable parameters and training com-\nplexity.\n3. Unified framework for gradient-driven rank allocation and initialization.\nF\nLimitations And Future Works\nIn this study, we demonstrate that GoRA outperforms baseline low-rank adaptation methods and\nachieves performance comparable to full fine-tuning. However, our evaluation has not yet extended\nto larger models and more extensive datasets. We hypothesize that for larger models, such as Llama-\n3.1-70B [2], GoRA could more effectively leverage the pre-trained knowledge inherent in these\nmodels. Additionally, while this research primarily focuses on language models and natural language\nprocessing tasks, there is potential to generalize GoRA to a broader range of model types and tasks,\nsuch as visual language models and visual question answering.\nAnother limitation of this study is that the initialization of matrix A is not restricted to random\ninitialization. Employing alternative methods, such as extracting distinguishing features from pre-\ntrained weights to initialize the matrix A, could potentially enhance performance, as it would combine\nthe benefits of both experience-driven and data-driven initialization approaches. Furthermore, it is\nworth noting that GoRA demonstrates theoretical compatibility with other LoRA variants, such as\nDoRA [45]. These promising avenues remain to be explored in future research endeavors.\n22\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: Yes, and we believe GoRA can accelerate the research on LoRA.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We have concluded our limitations in Appendix F.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\n23\nJustification: Yes, we have provided proofs in the Appendix B.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: Yes, we have provided experiment settings and our algorithm in the main paper\nand the Appendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n24\nAnswer: [Yes]\nJustification: Yes, we have provided the link to our official repository in the abstract.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: We have provided hyperparameter information in Section 4.1, Section 4.2 and\nSection 4.3. Additional information is provided in Appendix C.4.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer:\nJustification: We run each experiment with 3 different random seeds and report the mean\nand the standard deviation.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n25\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: Yes, we have provided related information in the Appendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: Yes.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: There is no societal impact of the work performed.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n26\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer:\nJustification: No risk of misuse in this work.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer:\nJustification: Yes.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n27\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer:\nJustification: No new assets introduced in the paper.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\n28\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: The core method development in this research does not involve LLMs as any\nimportant, original, or non-standard components.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n29\n",
  "pages": [
    {
      "page_number": 1,
      "text": "GoRA: Gradient-driven Adaptive Low Rank\nAdaptation\nHaonan He1,2,3∗,\nPeng Ye3,4,5∗,\nYuchen Ren3,6,\nYuan Yuan2,\nLuyang Zhou7,\nShucun Ju7,\nLei Chen2†\n1University of Science and Technology of China\n2Institute of Intelligent Machines, HFIPS, Chinese Academy of Sciences\n3Shanghai Artificial Intelligence Laboratory\n4Fudan University\n5The Chinese University of Hong Kong\n6University of Sydney\n7Anhui Disaster Warning & Agrometeorological Information Center\nhehn@mail.ustc.edu.cn\nAbstract\nLow-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning large\nlanguage models (LLMs), with its effectiveness influenced by two key factors:\nrank selection and weight initialization. While numerous LoRA variants have\nbeen proposed to improve performance by addressing one of these aspects, they\noften compromise usability or computational efficiency. In this paper, we ana-\nlyze and identify the core limitations of existing approaches and propose a novel\nframework—GoRA (Gradient-driven Adaptive Low Rank Adaptation)—that si-\nmultaneously adapts both the rank and initialization strategy within a unified\nframework. GoRA leverages gradient information during training to dynamically\nassign optimal ranks and initialize low-rank adapter weights in an adaptive man-\nner. To our knowledge, GoRA is the first method that not only addresses the\nlimitations of prior approaches—which often focus on either rank selection or\ninitialization in isolation—but also unifies both aspects within a single framework,\nenabling more effective and efficient adaptation. Extensive experiments across\nvarious architectures and modalities show that GoRA consistently outperforms\nexisting LoRA-based methods while preserving the efficiency of vanilla LoRA.\nFor example, when fine-tuning Llama3.1-8B-Base for mathematical reasoning,\nGoRA achieves a 5.13-point improvement over standard LoRA and even outper-\nforms full fine-tuning by 2.05 points under high-rank settings. Code is available at:\nhttps://github.com/hhnqqq/MyTransformers.\n1\nIntroduction\nOpen-source pre-trained large language models (LLMs) such as the Llama series [1, 2] have demon-\nstrated exceptional capabilities. Through supervised fine-tuning, these models can be adapted\nto various downstream tasks such as code generation [3] and mathematical problem solving [4].\nHowever, when the model has a parameter size ϕ and uses FP16/BF16 mixed-precision training\nstrategy [5, 6] with the Adam optimizer [7], the parameters and gradients require 4ϕ bytes of mem-\nory, while the optimizer states require 12ϕ bytes. Thus, the minimum memory usage, excluding\nactivations, reaches 16ϕ bytes. Such high memory demands limit the training of large language\nmodels under constrained resources. To reduce memory usage, Low-Rank Adaptation (LoRA) [8]\ndecomposes the weight matrix W ∈Rm×n into W = W0 + ∆W = W0 + sAB, where s is a\nscaling factor, and A ∈Rm×r, B ∈Rr×n, r ≪min(m, n), as shown in Figure 1(a). LoRA only\nupdates the low-rank weights A and B, keeping the pre-trained weight W0 unchanged, thereby\nsignificantly reducing the memory footprint of optimizer states. Although LoRA performs well on\nsimple tasks, when applied to pre-trained large language models, its performance on more challenging\ntasks, such as mathematical reasoning and code generation, still lags behind full fine-tuning [9, 10].\n∗Equal contribution.\n†Corresponding author: chenlei@iim.ac.cn.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n"
    },
    {
      "page_number": 2,
      "text": "Figure 1: Illustration of (a) LoRA; (b) LoRA variants utilizing adaptive rank masking strategies;\n(c) LoRA variants employing nonzero initialization strategies; and (d) GoRA, which introduces\nadaptively leveraging the weight W’s gradient to allocate the rank of the low-rank adapter and\ninitialize B. A0 and B0 denote the initialized value of A and B.\nOne of the critical factors in LoRA is its rank. Kalajdzievski [11] demonstrates that increasing\nthe rank of LoRA can significantly improve performance when paired with an appropriate scaling\nfactor. However, a direct increase in rank leads to a substantial rise in memory requirement overhead,\nthus imposing constraints on rank selection. To address this, several studies [12, 13] propose to\nensemble multiple low-rank subspaces, allowing for rank increases without proportionally increasing\nthe number of trainable parameters. Nevertheless, these approaches often come at the expense of\nusability due to their intrusion into architecture or training processes. Another promising line of\nresearch explores adaptively assigning ranks to pre-trained weights based on importance. For example,\nAdaLoRA [14] adaptively adjusts ranks by quantifying the importance of each rank during training\nand masking less significant ones, as illustrated in Figure 1(b). However, this masking mechanism\nnecessitates a larger parameter space (e.g., 1.5 times), increasing the number of trainable parameters\nand limiting the upper bound of rank. Consequently, as demonstrated in Section 5.1, adaptively\nallocating ranks without significantly increasing the training cost remains an open challenge.\nAnother vital factor in LoRA is its initialization strategy. In vanilla LoRA, A0 is initialized with\na normal distribution (In the PEFT library, A0 is initialized with a Kaiming distribution [15]) and\nB0 is initialized with zeros. This initialization method ensures that the weights W0 + A0B0\nremain unchanged at the beginning of training. Besides, zero initialization is not the only option:\nWhen A0B0 is nonzero, manipulating the pre-trained weight by subtracting A0B0 from W0 also\nensures stability. Existing nonzero initialization methods can be categorized into experience-driven\nand data-driven methods. In experience-driven methods, PiSSA [16] and MiLoRA [17] employ\ndecomposition techniques such as Singular Value Decomposition (SVD) to capture specific features\nof pre-trained weights. However, these methods are inherently task-agnostic, which limits their\ngeneralizability across diverse tasks. In contrast, data-driven methods incorporate task information.\nFor example, LoRA-GA [18] uses the singular features of gradients to initialize LoRA weights,\nminimizing the difference between LoRA and full fine-tuning. However, as illustrated in Figure 1(c)\nand Section 2.2, existing nonzero methods require manipulating the pre-trained weights, resulting\nin a training-inference gap. Thus, designing a nonzero initialization method without manipulating\npre-trained weights remains an open problem.\nGiven the challenges of adaptive rank allocation and nonzero initialization, we turn to gradients of\npre-trained weights, which are crucial for assessing the importance of pre-trained weights and deeply\nrelated to LoRA adapters’ optimization processes [19, 20]. As shown in Figure 1(d), we propose\nGoRA. Specifically, before training, we compute gradients of pre-trained weights on a subset of\ntraining samples, using these gradients to assess the importance of each pre-trained weight. Given a\nreference rank, we calculate a trainable parameter budget. Based on the normalized importance and the\ntrainable parameter budget, we allocate a new trainable parameter count and corresponding rank for\n2\n"
    },
    {
      "page_number": 3,
      "text": "each low-rank adapter, achieving adaptive rank allocation without significantly increasing the trainable\nparameter count compared to LoRA and allowing for higher rank allocation upper bounds, as shown\nin Table 5. In GoRA’s initialization, we maintain LoRA’s initialization for A, while B is initialized\nusing −(A⊤\n0 A0)−1A⊤\n0 G, where G is the gradient of the weight W. This initialization ensures\nthat the computation result of the low-rank adapter −A0(A⊤\n0 A0)−1A⊤\n0 G ≈−G compresses the\ngradient optimally, setting a solid foundation for further optimization without manipulating the\npre-trained weight. Our key contributions are summarized as follows:\n1. We conduct an in-depth investigation into LoRA’s rank allocation and initialization strategy,\nuncovering the limitations of existing works. We propose GoRA, which achieves adaptive\nrank allocation and initialization without compromising usability and efficiency.\n2. We use the gradients of weights to assess their importance and allocate ranks. We then\ninitialize the low-rank weights using the pseudo-inverse of the compressed gradients, which\nenhances performance while ensuring training stability.\n3. We conduct extensive experiments, demonstrating that GoRA consistently outperforms\nlow-rank baselines and even rivals full fine-tuning in certain settings. For example, on the\nLlama3.1-8B-Base model fine-tuned for mathematical reasoning, GoRA achieves a 5.13-\npoint improvement over LoRA and even surpasses full fine-tuning high-rank configurations.\n2\nRelated Works\n2.1\nRank of LoRA\nThe choice of rank is crucial for LoRA, with higher ranks consistently yielding better outcomes [11].\nHowever, increasing the rank raises the number of trainable parameters and corresponding memory\nusage overhead, making it challenging to train with sufficient ranks on limited hardware resources.\nPrevious works [21, 12] attempt to continuously merge and reinitialize low-rank weights during\ntraining to stack the overall rank. However, these methods often require resetting the states of the\noptimizer and learning rate scheduler during reinitialization to ensure that updates take place in\ndistinct low-rank subspaces and ensure training stability, significantly increasing overall training\ncomplexity and making the training process unstable. MeLoRA [13] proposes aggregating multiple\nmini low-rank adapters diagonally to increase the overall rank. Nevertheless, this approach requires\nmodifying the structure of LoRA, limiting its usability.\nAt the same time, the significances of weights during training demonstrate heterogeneity, and\nan intuitive proposition is to assign larger ranks to relatively more important weights. Previous\nworks [14, 22] attempted to dynamically mask less important ranks during training to achieve adaptive\nrank adjusting. However, these methods require allocating larger matrices for low-rank adapters\nto reserve space for masked ranks, leading to an increase in the number of trainable parameters,\nwhich compromises their operational efficacy and establishes limitations on the upper threshold\nof rank. IncreLoRA [23] introduces an approach that begins with a single rank for each low-rank\nadapter and incrementally increases the rank during training. This method effectively addresses the\nchallenge of large initial matrices. Nevertheless, this approach demonstrates suboptimal compatibility\nwith distributed training architectures, notably FSDP[24] and ZeRO[25], which constitute essential\ninfrastructural components for the effective training of large-scale models.\n2.2\nInitialization of LoRA\nParameter initialization represents a fundamental paradigm in deep learning methodologies. Well-\nestablished initialization protocols, such as the strategy proposed by Xavier Glorot and Yoshua\nBengio [26], facilitate the convergent training trajectories of deep neural networks. Similarly, appro-\npriate initialization strategies constitute a critical determinant for LoRA. Beyond zero initialization\nused by vanilla LoRA, some studies have explored different initialization strategies: PiSSA [16]\nperforms SVD on pre-trained weights and uses the most important singular features to initialize\nlow-rank weights; MiLoRA [17], in contrast to PiSSA, uses the least important singular features to\ninitialize low-rank weights; similarly, OLoRA [27] uses QR decomposition of pre-trained weights\nto initialize low-rank weights; EVA [28] uses singular features of activations to initialize low-rank\nweights; and LoRA-GA [18] uses singular features of gradients to initialize low-rank weights. These\nmethods can improve LoRA’s performance to some extent.\n3\n"
    },
    {
      "page_number": 4,
      "text": "Nevertheless, owing to the inherent non-zero initialization characteristics of these methodologies,\nthey require subtracting the LoRA initialization results from the pre-trained weights to ensure\ncorrect forward and backward propagation during the initial phases of the training regimen, conse-\nquently creating a gap between training and inference. Recomputing the initialization result of these\nmethods during inference is not feasible in cases involving randomness [16] or requiring original\ntraining data [18, 28]. And the initialization process requires significant time for methods such as\nMiLoRA [17]. The most straightforward solution is to save not only the low-rank weights but also\nthe manipulated pre-trained weights, but this sacrifices one of LoRA’s significant advantages, namely,\nminimal checkpoint storage [29]. Another approach is to save the initialized LoRA weights and use\nblock matrix multiplication to eliminate the gap, but this reduces usability.\n3\nMethod\nIn this section, we will reinterpret LoRA adapters from the perspective of gradient compressors and\nintroduce GoRA’s gradient-driven adaptive rank allocation and initialization strategy.\n3.1\nView LoRA adapters as Gradient Compressors\nThe core idea of LoRA is to fine-tune a model by leveraging the intrinsic low-rank property of the\nupdate of a weight matrix W ∈Rm×n during training. Specifically, a pair of low-rank matrices\nA ∈Rm×r and B ∈Rr×n are initialized alongside the pre-trained weight W0. During training,\nW0 remains frozen, while the model is updated by training the low-rank matrices A and B, thereby\nreducing memory usage during training. For any training step t, the updated weight Wt is given\nby (1), where α is a tunable hyperparameter that ensures the scale of the LoRA computation depends\nonly on α and is independent of the rank r:\nWt = W0 + ∆Wt = W0 + α\nr AtBt.\n(1)\nSpecifically, given the training loss L, the gradient of the pre-trained weight W0 can be computed as\n∂L\n∂W0 . Using the chain rule, the gradients of A and B are\n∂L\n∂W0 B⊤\nt and A⊤\nt\n∂L\n∂W0 , respectively. (Note:\nin vanilla LoRA, B0 = 0.) Given a learning rate η, the updates to the weight are as shown in (2)-(3):\n∆Bt = −η α\nr\nT\nX\nt=1\nA⊤\nt−1\n∂Lt\n∂W0\n,\n∆At = −η α\nr\nT\nX\nt=1\n∂Lt\n∂W0\nB⊤\nt−1,\n(2)\n∆Wt = α\nr AtBt −α\nr A0B0 = α\nr (∆At∆Bt + A0∆Bt).\n(3)\nExperimental results from LoRA-FA [30] have shown that freezing the randomly initialized matrix\nA and only training the matrix B can achieve performance close to that of LoRA. When matrix A is\nfrozen (∆At = 0), the weight update is given by (4). One can observe that matrix B accumulates the\ngradients compressed by A⊤\n0 during training, and when multiplied by A0, the compressed gradients\nare up-projected. Thus, the training process of LoRA-FA can be viewed as a process of gradient\naccumulation and compression, with the compression matrix being the randomly initialized A.\n∆Wt = α\nr A0∆Bt = −η α\nr\nT\nX\nt=0\nA0A⊤\n0\n∂Lt\n∂W0\n.\n(4)\nThe update form of LoRA-FA provides significant inspiration. We hypothesize that vanilla LoRA has\nsimilar properties, i.e., LoRA adapters act as gradient compressors. Based on this hypothesis, we\ncan allocate larger ranks to weights whose gradients and weights themselves contain more low-rank\ninformation and initialize LoRA parameters using compressed gradients.\n3.2\nGoRA’s Adaptive Rank Allocation Strategy\nBased on the hypothesis that LoRA adapters function similarly to gradient compressors, our adaptive\nrank allocation strategy aims to: (1) allocate rank based on weight importance derived from pre-\ncomputed N-batch accumulated gradients G =\n1\nN\nPn\ni=1\n∂Li\n∂W0 before the formal training process;\n(2) complete rank allocation before training to avoid dynamic shape changes; (3) maintain a similar\nnumber of trainable parameters as LoRA (within 10%); and (4) preserve structural compatibility with\nLoRA for easy integration.\n4\n"
    },
    {
      "page_number": 5,
      "text": "To evaluate the importance of weights, we first consider the nuclear norm of the pre-computed\ngradient, which aggregates all singular values of a matrix and is often used to measure low-rank\nproperties [18]. However, as shown in Table 6, this metric does not effectively capture the importance\nof weights in practice. Instead, we adopt a sensitivity-based importance metric commonly used in\nmodel pruning [31]. Specifically, we define the importance of a weight matrix W as:\nI(W) = avg (|W ⊙G|) ,\n(5)\nwhere the operator ⊙denotes the Hadamard product and avg(·) computes the average value to\nyield a scalar importance score. After computing the importance scores for all target pre-trained\nweight matrices, we form an importance set {I(Wi)}N\ni=1. To facilitate adaptive rank allocation, we\nnormalize the importance set to compute an advantage ai for the i-th pre-trained weight Wi\n0:\nai =\nI(Wi\n0)\nΣN\ni=1I(Wi\n0).\n(6)\nWith the normalized advantages computed, we next determine the total trainable parameter budget\nb for the model. Given a reference rank rref, the budget for a single weight matrix Wi ∈Rm×n is\nestimated as:\nbi = (\n√\nm + n) × rref,\n(7)\nreflecting a smoothed parameter budget under vanilla LoRA. Summing over all budgets, the total\nbudget becomes b = ΣN\ni=1bi. Using this budget and the advantage ai, we allocate the adapter rank ri\nand its trainable parameter count pi for each weight matrix as:\nri =\n\u0014\npi\n√m + n\n\u0015\n=\n\u0014 b ∗ai\n√m + n\n\u0015\n,\ns.t.rmin ≤ri ≤rmax,\n(8)\nwhere the operator [·] denotes rounding to the nearest integer, and rmin, rmax are hyper-parameters\ndefining the allowable rank range. This formulation ensures that the total number of trainable\nparameters p = ΣN\ni=1pi approximately matches that of standard LoRA with rank rref closely.\nIn summary, before training begins, we use the N-batch accumulated gradients for all target weights.\nThese gradients are then used to estimate the importance of each weight matrix, based on which we\nperform adaptive rank allocation in GoRA, achieving all four objectives outlined earlier.\n3.3\nGoRA’s Adaptive Initialization Strategy\nOnce ranks are allocated for each layer, it is crucial to initialize the low-rank weights properly.\nThe compression form in (4) is suboptimal when A is randomly initialized and fixed; to achieve\nbetter alignment with the gradient dynamics, we can initialize B such that the computation of the\nlow-rank adapter at the start of training closely approximates the N-batch accumulated gradient G.\nThis optimal initialization can be derived using the Moore-Penrose inverse of A0 (this computation\nrequires negligible computational time as detailed in Appendix D.1):\nB0 = −(A⊤\n0 A0)−1A⊤\n0 G,\nA0B0 = −A0(A⊤\n0 A0)−1A⊤\n0 G.\n(9)\nAs shown in (9), initializing B as −(A⊤\n0 A0)−1A⊤\n0 G ensures that A0B0 provides the best low-rank\napproximation of G given a fixed A0, with proof provided in Appendix B.1.\nHowever, due to the properties of pseudo-inverse computation, the magnitude of A0B0 does not\nexactly match that of G. Assuming both G ∈Rm×n and A0 ∈Rm×r follow distributions with\nmean 0 and variance 1, the expected Frobenius norm of G, E[||G||F ], is √mn, while that of A0B0,\nE[||A0B0||F ], is √rn, as detailed in Appendix B.2.\nTo ensure that the initial computation of a low-rank adapter approximates a single step of stochastic\ngradient descent with a tunable step size γ, we introduce a scaling factor ξ for B0:\nα\nr A0(ξB0) ≈ξ α\nr\nr r\nmG ≈−γG.\n(10)\nThus, to make GoRA’s initialization equivalent to one step of gradient descent, ξ should be set to\nγ·√rm\nα\n. Inspired by rsLoRA [11], and to better utilize the larger ranks obtained through dynamic\nallocation, we modify the forward computation to Wt = W0 + ∆W = W0 +\nα\n√rAtBt, which\nadjusts ξ to γ·√m\nα\n. Setting γ to a relatively large value further improves performance and yields\noptimal results. The full algorithm is summarized in Algorithm 1 and Algorithm 2.\n5\n"
    },
    {
      "page_number": 6,
      "text": "Algorithm 1 Rank Allocation and Initialization of GoRA under Single Training Worker\nInput: Model f(·) with L layers, pre-trained parameters θ = {Wl\n0}L\nl=1, gradient accumulation\nsteps N, loss function F, scale factor γ, Trainable parameter budget b\nOutput: Initialized low-rank matrices {Al\n0}L\nl=1, {Bl\n0}L\nl=1\n1: for l = 1 to L do\n2:\nGlavg ←0\n▷Initialize gradients buffer in CPU memory.\n3: for i = 1 to N do\n▷Compute gradients without optimizer.\n4:\nRandomly sampled mini-batch {x, y}\n5:\nˆy ←f(x, θ), L ←F(y, ˆy)\n6:\nfor l = 1 to L do\n7:\nAccumulate on CPU: Glavg ←Glavg + 1\nN\n∂L\n∂Wl\n0\n8: for l = 1 to L do\n9:\nCompute importance I(Wl\n0) ←avg(|Wl\n0 ∗Glavg|)\n10: for l = 1 to L do\n11:\nCompute advantage al ←\nI(Wl\n0)\nPL\nl=1 I(Wl\n0)\n12: for l = 1 to L do\n13:\nm, n ←size(Wl\n0)\n14:\nrl ←clip(round(\nb·al\n√m+n), rmin, rmax)\n▷Clip by rmin and rmax to avoid extremum.\n15:\nAl\n0 ∼UKaiming(m × rl, a =\n√\n5)\n▷Initialize Al\n0 ∈Rm×rl with Kaiming uniform.\n16:\nBl\n0 ←−(Al⊤\n0 Al\n0)−1Al⊤\n0 Gl\navg\n▷Initialize Bl\n0 ∈Rrl×n.\n17:\nBl\n0 ←γ√m\nα\nBl\n0\n18: Return {Al\n0}L\nl=1, {Bl\n0}L\nl=1\n4\nExperiments\nWe conducted comprehensive experiments comparing GoRA with baseline methods on natural lan-\nguage understanding (Section 4.1), generation tasks (Section 4.2), and image classification tasks\n(Section 4.3). For understanding tasks, we trained T5-Base [32] on five tasks of GLUE [33] (MNLI,\nSST-2, CoLA, QNLI, MRPC) and reported accuracy on corresponding validation sets. For generation\ntasks, we fine-tuned Llama-3.1-8B-Base [2] and Llama-2-7B-Base [1] on chat, mathematics, and cod-\ning datasets, evaluating test performance on MTBench [34], GSM8k [35], and HumanEval [36]. For\nimage classification tasks, we fine-tuned CLIP-ViT-B/16 [37] on seven datasets including Stanford-\nCars [38], DTD [39], EuroSAT [40], GT-SRB [41], RESISC45 [42], SUN397 [43] and SVHN [44]\nand reported test accuracy. All experiments were conducted using single-epoch training across\nthree random seeds, with results reported as mean values and standard deviations. Unless specified\notherwise, we set the LoRA rank or GoRA’s reference rank rref to 8. The hyperparameters of GoRA\nare detailed in Appendix C.3.\n4.1\nExperimental Results on Natural Language Understanding Tasks\nTable 1: Performance of fine-tuning T5-Base on 5 sub-tasks of the GLUE benchmark. Bold and\nunderline indicate the highest and second-highest scores of low-rank methods with r = 8 or rref = 8.\nMethod\nMNLI\nSST-2\nCoLA\nQNLI\nMRPC\nAverage\nFull\n86.33±0.00\n94.75±0.21\n80.70±0.24\n93.19±0.22\n84.56±0.73\n87.91\nLoRA [8]\n85.30±0.04\n94.04±0.11\n69.35±0.05\n92.96±0.09\n68.38±0.01\n82.08\nConvergence Optimization Methods for LoRA\nrsLoRA [11]\n85.73±0.10\n94.19±0.23\n72.32±1.12\n93.12±0.09\n52.86±2.27\n79.64\nDoRA [45]\n85.67±0.09\n94.04±0.53\n72.04±0.94\n93.04±0.06\n68.08±0.51\n82.57\nLoRA+ [46]\n85.81±0.09\n93.85±0.24\n77.53±0.20\n93.14±0.03\n74.43±1.39\n84.95\nInitialization Optimization Methods for LoRA\nPiSSA [16]\n85.75±0.07\n94.07±0.06\n74.27±0.39\n93.15±0.14\n76.31±0.51\n84.71\nLoRA-GA [18]\n85.70±0.09\n94.11±0.18\n80.57±0.20\n93.18±0.06\n85.29±0.24\n87.77\nAdaptive Methods for LoRA\nAdaLoRA [14]\n85.45±0.11\n93.69±0.20\n69.16±0.24\n91.66±0.05\n68.14±0.28\n81.62\nGoRA\n85.91±0.02\n94.68±0.43\n79.86±0.35\n93.27±0.08\n86.10±0.20\n87.96\nSettings: We adopted baseline performances reported by LoRA-GA [18], maintaining their experi-\nmental parameters for fair comparison: Adam[7] optimizer (β1 = 0.9, β2 = 0.999, weight decay =\n6\n"
    },
    {
      "page_number": 7,
      "text": "0), batch size 32, cosine decay learning rate with a warmup ratioo of 0.03. We trained all linear layers\nexcept the language head using a peak learning rate of 1e-4, a maximum sequence length of 128, and\nFP32 precision.\nResults: Table 1 compares GoRA against multiple baselines across five GLUE benchmark tasks.\nGoRA achieved superior performance on four datasets (MNLI, SST-2, QNLI, and MRPC), demon-\nstrating exceptional adaptability and generalization. While slightly underperforming LoRA-GA on\nCoLA by just 0.71 percentage points, GoRA’s average score (87.96) surpassed all baselines and even\nexceeded full fine-tuning (87.91). This confirms GoRA’s ability to maximize model potential while\nmaintaining parameter efficiency. Notably, GoRA showed particularly strong performance on MRPC\nand QNLI, highlighting its effectiveness in small-sample learning and sentence-pair tasks.\n4.2\nExperimental Results on Natural Language Generation Tasks\nSettings: We trained mathematical, coding, and dialogue capabilities using 100K MetamathQA [47],\n100K Code-FeedBack [48] (code-only labels), and 52K WizardLM [49] subsets, respectively.\nFor experiments on Llama-3.1-8B-base, training used AdamW [50] (β1 = 0.9, β2 = 0.999,\nweight decay = 5e −4) with batch size 64, cosine decay learning rate (warmup ratio=0.03, de-\ncay ratio=0.1), and BF16 mixed precision. For all methods, including GoRA, we trained attention\nmodules’ linear components with a peak learning rate of 5e-5 (5e-4 for AdaLoRA). Evaluation met-\nrics: mathematics—regex-extracted accuracy; coding—PASS@1; dialogue—average scores (0-10)\nfrom GPT-4o [51], Gemini-1.5-Pro [52], and Llama-3.1-70B-Instruct [2] using prompts from [34].\nFor experiments on Llama-2-7B-Base, we adopted baseline results from LoRA-GA [18], and we\nmaintained the same training and evaluation settings. Further details are provided in Appendix C.4.\nTable 2: Performance of fine-tuning Llama-3.1-8B-Base.\nMethod\nMTBench\nGSM8k\nHumanEval\nFull\n5.88±0.23\n73.69±0.28\n51.63±1.27\nLoRA [8]\n6.15±0.02\n67.78±1.25\n43.09±0.35\nrsLoRA [11]\n6.18±0.09\n68.36±0.74\n45.78±2.80\nDoRA [45]\n6.24±0.12\n69.17±1.00\n43.70±1.54\nLoRA+ [46]\n6.35±0.10\n71.29±0.93\n44.51±2.11\nOLoRA [27]\n6.13±0.04\n68.54±0.42\n43.29±2.44\nPiSSA [16]\n6.08±0.09\n68.56±1.03\n44.10±1.54\nLoRA-GA [18]\n5.99±0.06\n71.39±0.90\n43.29±0.61\nAdaLoRA [14]\n6.19±0.16\n70.63±0.77\n41.46±3.66\nGoRA\n6.34±0.04\n72.91±0.76\n48.98±2.14\nGoRArref=32\n6.21±0.10\n75.59±1.04\n51.22±1.83\nGoRArref=128\n5.82±0.31\n75.74±0.40\n52.03±1.41\nTable 3: Performance of fine-tuning Llama-2-7B-Base.\nMethod\nMTBench\nGSM8k\nHumanEval\nFull\n5.30 ± 0.11\n59.36 ± 0.85\n35.31 ± 2.13\nLoRA [8]\n5.61 ± 0.10\n42.08 ± 0.04\n14.76 ± 0.17\nrsLoRA [11]\n5.25 ± 0.03\n45.62 ± 0.10\n16.01 ± 0.79\nDoRA [45]\n5.97 ± 0.02\n53.07 ± 0.75\n19.75 ± 0.41\nLoRA+ [46]\n5.71 ± 0.08\n52.11 ± 0.62\n18.17 ± 0.52\nOLoRA [27]\n5.30 ± 0.04\n43.29 ± 0.83\n17.22 ± 0.12\nPiSSA [16]\n5.30 ± 0.02\n44.54 ± 0.27\n16.02 ± 0.17\nLoRA-GA [18]\n5.95 ± 0.16\n53.60 ± 0.30\n19.81 ± 1.46\nAdaLoRA [14]\n5.57 ± 0.05\n50.72 ± 1.39\n17.80 ± 0.44\nGoRA\n5.61 ± 0.12\n54.04 ± 0.22\n24.80 ± 1.04\nGoRArref=32\n5.75 ± 0.06\n56.18 ± 0.10\n26.83 ± 2.84\nGoRArref=128\n6.05 ± 0.04\n56.58 ± 0.12\n27.85 ± 0.58\nFigure 2: The training loss curves of full fine-\ntuning, LoRA, GoRA, and GoRAr0=128 on Llama-\n3.1-8B-Base. GoRA demonstrates lower start loss\nand faster convergence speed.\nResults: Table 2 and Table 3 show the per-\nformance of GoRA and baseline methods on\nfine-tuned Llama3.1-8B-Base and Llama2-7B-\nBase. Specifically, GoRA demonstrated excep-\ntional performance on the more challenging Hu-\nmanEval and GSM8k benchmarks, substantially\nsurpassing all baseline methods. For Llama3.1-\n8B-Base, on the GSM8k dataset, GoRA scored\n72.91, outperforming LoRA-GA’s 71.39 by\n1.52 points; on the HumanEval dataset, GoRA\nachieved 48.98, surpassing rsLoRA’s 45.78 by\n3.20 points.\nOn MTBench, GoRA slightly\nunderperforms in terms of overall effective-\nness, scoring 6.34, just 0.01 points lower than\nLoRA+’s 6.35. Notably, GoRA performed well\nacross different rank allocation settings. For ex-\nample, GoRArref=128 achieved 75.74 and 52.03\non the GSM8k and HumanEval, respectively,\nsurpassing full fine-tuning’s 73.69 and 51.63.\nEven the rref = 32 configuration of GoRA,\nwhile slightly underperforming rref = 128, still\noutperformed full fine-tuning on GSM8k. For\nLlama2-7B-Base, GoRA demonstrated similar\nsuperior results compared to baseline methods. Especially, GoRA outperformed LoRA-GA by 4.99\n7\n"
    },
    {
      "page_number": 8,
      "text": "and 0.44 on HumanEval and GSM8k, respectively. These results validate the effectiveness of GoRA\nacross different LLMs and settings. The training loss curves of GoRA are depicted in Figure 2.\nTable 4: Performance of fine-tuning CLIP-VIP-B/16 on 7 image classification tasks.\nMethod\nCars\nDTD\nEuroSAT\nGTSRB\nRESISC45\nSUN397\nSVHN\nAverage\nZero-shot\n63.75\n44.39\n42.22\n35.22\n56.46\n62.56\n15.53\n45.73\nFull\n84.23±0.06\n77.44±0.19\n98.09±0.03\n94.31±0.28\n93.95±0.00\n75.35±0.10\n93.04±0.18\n88.06\nLoRA [8]\n72.81±0.13\n73.92±0.38\n96.93±0.07\n92.40±0.10\n90.03±0.14\n70.12±0.18\n88.02±0.07\n83.46\nDoRA [45]\n73.72±0.06\n73.72±0.33\n96.95±0.01\n92.38±0.08\n90.03±0.08\n70.20±0.19\n88.23±0.05\n83.48\nLoRA+ [46]\n72.87±0.18\n74.07±0.45\n97.01±0.02\n92.42±0.18\n89.96±0.11\n70.17±0.15\n88.08±0.05\n83.51\nLoRA-Pro [53]\n85.87±0.08\n78.64±0.85\n98.46±0.03\n95.66±0.05\n94.75±0.21\n76.42±0.14\n94.63±0.20\n89.20\nLoRA-GA [18]\n85.18±0.41\n77.50±0.12\n98.05±0.27\n95.28±0.10\n94.43±0.19\n75.44±0.06\n93.68±0.35\n88.51\nGoRA\n85.76±0.19\n78.17±0.32\n98.77±0.35\n96.66±0.36\n95.16±0.26\n76.46±0.08\n95.32±0.13\n89.47\n4.3\nExperimental Results on Image Classification Tasks\nSettings: We adopted baseline performances from LoRA-Pro [53], maintaining their experimental\nhyper-parameters for a fair comparison: Adam [7] optimizer (β1 = 0.9, β2 = 0.999, weight decay =\n0), batch size 64, cosine decay learning rate with 0.03 warmup ratio. We trained all linear layers in\nthe vision backend using a peak learning rate of 1e-4, and FP32 precision. The classifier is obtained\nusing prompts such as “a photo of a {class}.”\nResults: As shown in Table 4, GoRA outperforms baseline methods across all seven image classi-\nfication tasks. Specifically, GoRA outperforms full fine-tuning by a margin of 1.01; outperforms\nLoRA-GA by 0.96 and outperforms LoRA-Pro by 0.27. These results demonstrate that GoRA\nexhibits superior performance across different models and modalities.\n5\nDiscussions\nIn this section, we present a comprehensive set of ablation studies to evaluate the effectiveness\nof GoRA’s adaptive rank allocation and initialization strategy. We also examine the impact of\nGoRA’s hyperparameters and discuss their role in shaping performance. Additionally, we explore\nhyperparameter auto-tuning strategies to improve usability.\nFigure 3: (a) Result rank distribution of fine-tuning Llama-3.1-8B-Base on the MetaMathQA-100K\ndataset using GoRA;(b) Difference values between GoRA and LoRA in directional updates of pre-\ntrained weights after merging;(c) Difference values between GoRA and LoRA in magnitude updates\nof pre-trained weights after merging. Data points are presented for every two layers.\n5.1\nThe Effect of the Rank Allocation Strategy\nThe rank allocation strategy is a crucial component that influences the performance of GoRA. As\nhighlighted in Table 5, we conducted ablation studies to evaluate different rank allocation ranges. The\nresults demonstrate that a broader rank allocation range consistently leads to superior performance.\nFor instance, given γ = 5e −2, (rmin = 4, rmax = 32) achieved a score of 48.98 on HumanEval,\nsignificantly outperforming both the fixed rank allocation strategy (rmin = 8, rmax = 8) and the more\nconservative allocation strategy (rmin = 6, rmax = 15).\nFigure 3 illustrates the rank distribution of (rmin = 4, rmax = 32). Notably, most ranks are allocated\nto the wv layers, while the wq layers receive the fewest rank allocations. This observation aligns\n8\n"
    },
    {
      "page_number": 9,
      "text": "with findings reported in prior work [8]. Moreover, weights with higher ranks receive larger updates\nafter merging the low-rank matrices. These observations underscore the effectiveness of our rank\nallocation strategy.\nTable 5: Ablation study on hyperparameters. To maintain an approximately constant number of\ntrainable parameters, the rank allocation upper bound was reduced as the lower bound was increased.\nMethod\nrmin\nrmax\nγ\nGSM8k\nHumanEval\nAdaLoRA\n0\n12\n-\n70.63±0.77\n41.46±3.66\nLoRA\n8\n8\n0\n67.78±1.25\n43.09±0.35\nGoRA\n4\n32\n8e-2\n72.91±0.76\n46.54±1.54\nGoRA\n4\n32\n5e-2\n72.88±0.99\n48.98±2.14\nGoRA\n4\n32\n3e-2\n72.71±1.22\n45.93±1.27\nGoRA\n4\n32\n0\n72.45±1.14\n46.34±0.61\nGoRA\n0\n∞\n5e-2\n72.83±0.80\n46.13±3.36\nGoRA\n4\n32\n5e-2\n72.88±0.99\n48.98±2.14\nGoRA\n6\n15\n5e-2\n72.25±0.27\n45.85±3.18\nGoRA\n8\n8\n5e-2\n72.10±1.12\n44.75±3.97\n5.2\nThe Effect of the Initialization Strategy\nTable 5 also summarizes the results of ablation studies conducted with various scaling factors. Our\nexperiments revealed that the choice of scaling factor γ has a substantial impact on performance.\nNotably, GoRA achieves the best performance on HumanEval with γ = 5e −2, attaining a score of\n48.98. Meanwhile, GoRA with γ = 8e −2 slightly outperformed other configurations on the GSM8k,\nachieving a score of 72.91. Conversely, when γ = 0, GoRA exhibited the weakest performance\non GSM8k, scoring 72.45. A carefully selected scaling factor ensures that the initial low-rank\nadapter computation closely approximates a gradient descent step, establishing a robust foundation\nfor subsequent optimization. This is crucial for maintaining training stability.\n5.3\nThe Effect of Different Importance Metrics\nTable 6: Ablation studies on different importance\nmetrics, where || · ||∗represents the nuclear norm.\nMetric\nGSM8k\nHumanEval\navg(|W ⊙G|)\n72.88±0.99\n48.98±2.14\n||G||∗\n72.70±0.68\n43.09±0.93\n||W ⊙G||∗\n72.65±0.78\n45.12±3.17\nTable 6 compares several importance metrics:\nparameter sensitivity to loss (the metric used\nby GoRA), the nuclear norm of the gradient,\nand the nuclear norm of the parameter–gradient\nproduct. The results demonstrate that param-\neter sensitivity consistently outperforms the\nother metrics on both GSM8k and HumanEval.\nNotably, on HumanEval, parameter sensitivity\nachieves a score of 48.98, surpassing the nuclear norm of the gradient (43.09) and the nuclear norm\nof the parameter–gradient product (45.12).\n5.4\nHyperparameter Auto-tuning\nCompared to vanilla LoRA, GoRA introduces two additional hyperparameters: the number of gradient\naccumulation steps N and the initialization scaling factor γ. While hyperparameter tuning strategies\nfor LoRA have been well established in prior work, we aim to minimize the additional tuning burden\nintroduced by GoRA to ensure its practical usability. To this end, we design lightweight, automated\nstrategies that effectively eliminate the need for manual tuning of these new parameters:\n(1) Adaptive Gradient Accumulation: During gradient accumulation, we monitor the normalized\nlayer-wise importance scores at each accumulation step. Once the change between consecutive\nimportance score sets falls below a small pre-defined threshold (e.g., 0.01), indicating convergence\nof layer importance, we terminate accumulation early and proceed to the parameter update. This\nadaptive stopping criterion automatically determines an effective N on the fly, removing the need for\nmanual specification and often accelerating training.\n(2) Adaptive Scaling Factor: We hypothesize that the optimal scaling factor γ is the one that minimizes\nthe loss of the first training step. To find it, we start from a relatively large initial value (e.g., 1.0) and\niteratively reduce it by multiplying by a decay factor (e.g., 0.9) until a small lower bound (e.g., 5e-5)\n9\n"
    },
    {
      "page_number": 10,
      "text": "is reached. For each candidate γ, we evaluate the loss on the first training batch without performing\nbackpropagation. The γ yielding the lowest loss is then used to initialize formal training.\nImportantly, the pre-defined constants used in these strategies—such as the convergence threshold\n(0.01), decay factor (0.9), and lower bound (5e-5) are empirically stable across tasks and model\nscales. In practice, they require little to no tuning, making GoRA highly usable out of the box while\npreserving its enhanced representational capacity.\nTable 7: Performance comparison of GoRA with\nadaptive hyperparameters.\nMethod\nGSM8k\nHumanEval\nOriginal\n72.91±0.76\n48.98±2.14\nAdaptive N\n72.96±0.19\n46.85±2.11\nAdaptive γ\n72.50±0.38\n50.00±3.23\nTo validate the effectiveness and practicality of\nour auto-tuning strategies, we conduct ablation\nstudies on the scaling factor γ and the gradient\naccumulation steps N. As shown in Table 7,\nour adaptive methods achieve performance com-\nparable to or better than manually tuned base-\nlines, while eliminating the need for hyperpa-\nrameter search. Notably, the auto-selected γ\nvalues (0.0549 for CodeFeedback and 0.0858 for MetaMathQA) closely align with those reported in\nTable 5 (5e-2 and 8e-2, respectively), confirming the reliability of our selection criterion.\n6\nComputational and Memory Analysis\nTable 8: Comparison of trainable parameter counts. For GoRA, we set rmin = 4 and rmax = 32\nModel\nDataset\nTarget Modules\nLoRA\nAdaLoRA\nGoRA\nT5-Base\nSST-2\nall-linear\n3.24M\n4.86M\n3.05M\nLlama3.1-8B-Base\nMetamathQA\nattention\n6.82M\n10.24M\n7.00M\nLlama2-7B-Base\nMetamathQA\nall-linear\n19.99M\n29.99M\n20.18M\nCLIP-ViT-B/16\nCars\nvision_model\n1.33M\n2.03M\n1.35M\nTable 9: Comparison of time and GPU memory costs during GoRA’s\ninitialization process and training process\nModel Size\nProcess\nTime Cost\nMemory Cost\n8B\nInitialization\n23.60s\n56,139MB\nTraining\n37min54.22s\n73,295MB\n32B\nInitialization\n2min44.65s\n64,821 MB\nTraining\n197min52.15s\n66,381 MB\nDuring training, GoRA’s\ncomputational and memory\noverhead is nearly iden-\ntical to LoRA’s, as both\nmethods maintain a simi-\nlar number of trainable pa-\nrameters given a reference\nrank. Table 8 compares the\ntrainable parameter counts\nof LoRA, AdaLoRA, and\nGoRA across various models, with all methods using a rank or average/reference rank of 8.\nAs outlined in Algorithm 1, GoRA requires a brief gradient pre-computing phase for rank allocation\nand initialization. However, this overhead is minimal in both computation and memory usage. First,\ngradient accumulation is performed over only a small number of micro-batches without any optimizer\nupdates. Second, gradients are computed layer-by-layer and immediately offloaded to CPU memory,\navoiding the storage of optimizer states. Moreover, Algorithm 2 integrates seamlessly with distributed\ndata Parallelism and can be readily extended to more complex parallel training setups. Specifically,\nrather than having each worker independently compute and transfer gradients to CPU memory,\nwhich would cause massive concurrent PCIe traffic and excessive CPU memory consumption, our\nmethod performs an immediate Reduce operation on per-layer gradients directly to rank 0 during\nthe backward pass. This ensures that only a single copy of the globally averaged gradient is ever\ntransferred to CPU memory, drastically reducing PCIe bandwidth usage and CPU memory footprint.\nTo empirically validate the efficiency of GoRA, we measure both training time and memory consump-\ntion when fine-tuning Llama-3.1-8B-Base and Qwen2.5-32B [54] on MetaMathQA using 8×A800\n80GB GPUs. For Llama-3.1-8B-Base, we use a per-GPU batch size of 8 (without activation check-\npoint); for the larger Qwen2.5-32B model, we reduce the per-GPU batch size to 1 (with activation\ncheckpoint [55]) due to memory constraints. We employ the adaptive N strategy described in Sec-\ntion 5.4, and all other experimental settings follow those of our main experiments. As shown in\nTable 9 (reported memories benefit from Liger kernel [56] and FlashAttention [57]), GoRA incurs\nnegligible additional time cost and no extra memory overhead compared to standard LoRA, even for\nmodels of vastly different scales, demonstrating its practicality for real-world deployment.\n10\n"
    },
    {
      "page_number": 11,
      "text": "7\nAcknowledgement\nThis paper is supported by National Key Research and Development Program of China (Grants\nNo.2023YFD2000101), National Natural Science Foundation of China (Grants No.32471988,\n32271981), and the HFIPS Director’s Fund (Grant No.YZJJKX202401).\n11\n"
    },
    {
      "page_number": 12,
      "text": "References\n[1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[2] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024.\n[3] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,\nYossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation\nmodels for code. arXiv preprint arXiv:2308.12950, 2023.\n[4] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng\nLiu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward\nmathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024.\n[5] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. In International Conference on Learning Representations, 2018.\n[6] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,\nSasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen,\net al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.\n[7] Diederik P Kingma. Adam: A method for stochastic optimization. In International Conference\non Learning Representations, 2015.\n[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.\nIn\nInternational Conference on Learning Representations, 2022.\n[9] Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard,\nConnor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns\nless and forgets less. Transactions on Machine Learning Research, 2024.\n[10] Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S, Deepali Aneja,\nZeyu Jin, Ramani Duraiswami, and Dinesh Manocha. A closer look at the limitations of\ninstruction tuning. In International Conference on Machine Learning, pages 15559–15589.\nPMLR, 2024.\n[11] Damjan Kalajdzievski. A rank stabilization scaling factor for fine-tuning with lora. arXiv\npreprint arXiv:2312.03732, 2023.\n[12] Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky.\nRelora:\nHigh-rank training through low-rank updates. In International Conference on Learning Repre-\nsentations, 2023.\n[13] Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten Rijke,\nZhumin Chen, and Jiahuan Pei. Melora: mini-ensemble low-rank adapters for parameter-\nefficient fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages 3052–3064, 2024.\n[14] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\nand Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In International\nConference on Learning Representations, 2023.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 1026–1034, 2015.\n[16] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular\nvectors adaptation of large language models. In Advances in Neural Information Processing\nSystems, volume 37, pages 121038–121072, 2024.\n[17] Hanqing Wang, Yixia Li, Shuo Wang, Guanhua Chen, and Yun Chen. Milora: Harnessing\nminor singular components for parameter-efficient llm finetuning. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Volume 1: Long Papers), pages 4823–4836, 2025.\n12\n"
    },
    {
      "page_number": 13,
      "text": "[18] Shaowen Wang, Linxi Yu, and Jian Li. Lora-ga: Low-rank adaptation with gradient approxima-\ntion. In Advances in Neural Information Processing Systems, volume 37, pages 54905–54931,\n2024.\n[19] Yongchang Hao, Yanshuai Cao, and Lili Mou. Flora: Low-rank adapters are secretly gradient\ncompressors. In International Conference on Machine Learning, pages 17554–17571. PMLR,\n2024.\n[20] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong\nTian. Galore: Memory-efficient llm training by gradient low-rank projection. In International\nConference on Machine Learning, pages 61121–61143. PMLR, 2024.\n[21] Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu, Xiaochen Wang, Peiyi\nWang, Qingxiu Dong, Liang Chen, and Zhifang Sui. Periodiclora: Breaking the low-rank\nbottleneck in lora optimization. arXiv preprint arXiv:2402.16141, 2024.\n[22] Yahao Hu, Yifei Xie, Tianfeng Wang, Man Chen, and Zhisong Pan. Structure-aware low-rank\nadaptation for parameter-efficient fine-tuning. Mathematics, 11(20):4317, 2023.\n[23] Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, and Yiming Qian.\nIncrelora: Incremental parameter allocation method for parameter-efficient fine-tuning. arXiv\npreprint arXiv:2308.12043, 2023.\n[24] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,\nHamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully\nsharded data parallel. arXiv preprint arXiv:2304.11277, 2023.\n[25] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza-\ntions toward training trillion parameter models. In SC20: International Conference for High\nPerformance Computing, Networking, Storage and Analysis, pages 1–16, 2020.\n[26] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward\nneural networks. In International Conference on Artificial Intelligence and Statistics, 2010.\n[27] Kerim Büyükakyüz. Olora: Orthonormal low-rank adaptation of large language models. arXiv\npreprint arXiv:2406.01775, 2024.\n[28] Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth,\nand Sepp Hochreiter. One initialization to rule them all: Fine-tuning via explained variance\nadaptation. arXiv preprint arXiv:2410.07170, 2024.\n[29] Vlad Fomenko, Han Yu, Jongho Lee, Stanley Hsieh, and Weizhu Chen. A note on lora. arXiv\npreprint arXiv:2404.05086, 2024.\n[30] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient\nlow-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303,\n2023.\n[31] Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen,\nand Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight\nimportance. In International Conference on Machine Learning, pages 26809–26823. PMLR,\n2022.\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\n[33] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations, 2019.\n[34] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench\nand chatbot arena. In Advances in Neural Information Processing Systems, volume 36, pages\n46595–46623, 2023.\n[35] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n13\n"
    },
    {
      "page_number": 14,
      "text": "[36] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748–8763. PMLR, 2021.\n[38] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-\ngrained categorization. In Proceedings of the IEEE International Conference on Computer\nVision Workshops, pages 554–561, 2013.\n[39] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.\nDescribing textures in the wild. In Proceedings of the IEEE conference on Computer Vision\nand Pattern Recognition, pages 3606–3613, 2014.\n[40] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel\ndataset and deep learning benchmark for land use and land cover classification. IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217–2226, 2019.\n[41] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel.\nDetection of traffic signs in real-world images: The german traffic sign detection benchmark. In\nInternational Joint Conference on Neural Networks, 2013.\n[42] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification:\nBenchmark and state of the art. Proceedings of the IEEE, 105(10):1865–1883, 2017.\n[43] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition, pages 3485–3492, 2010.\n[44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.\nReading digits in natural images with unsupervised feature learning. In NIPS Workshop on\nDeep Learning and Unsupervised Feature Learning, 2011.\n[45] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang,\nKwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In\nInternational Conference on Machine Learning, pages 32100–32121. PMLR, 2024.\n[46] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+ efficient low rank adaptation of large models.\nIn International Conference on Machine Learning, pages 17783–17806. PMLR, 2024.\n[47] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models. In International Conference on Learning Representations,\n2024.\n[48] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and\nXiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement.\nIn Findings of the Association for Computational Linguistics: ACL 2024, pages 12834–12859,\n2024.\n[49] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,\nQingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to\nfollow complex instructions. In International Conference on Learning Representations, 2024.\n[50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations, 2019.\n[51] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[52] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett\nTanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\n[53] Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, and Tieniu Tan. Lora-pro: Are low-rank\nadapters properly optimized? In International Conference on Learning Representations, 2025.\n14\n"
    },
    {
      "page_number": 15,
      "text": "[54] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li,\nTianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\nYu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report,\n2025.\n[55] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174, 2016.\n[56] Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven\nShimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels\nfor llm training. arXiv preprint arXiv:2410.10989, 2024.\n[57] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast\nand memory-efficient exact attention with io-awareness. In Advances in Neural Information\nProcessing Systems, volume 35, pages 16344–16359, 2022.\n[58] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In\nInternational Conference on Learning Representations, 2024.\n[59] Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, and Xipeng Qiu. Adalomo: Low-memory optimiza-\ntion with adaptive learning rate. In Findings of the Association for Computational Linguistics:\nACL 2024, pages 12486–12502, 2024.\n15\n"
    },
    {
      "page_number": 16,
      "text": "A\nNotations\nIn this section, we summarize the notations used in the paper in Table 10.\nTable 10: List of Notations used in the paper\nSymbol\nDescription\nW ∈Rm×n\nFull-rank weight matrix of a linear layer.\nW0 ∈Rm×n\nPre-trained weight matrix of a linear layer in a pre-trained model.\n∆W ∈Rm×n\nUpdate of the pre-trained weight matrix after fine-tuning.\n∆Wt ∈Rm×n\nUpdate of the pre-trained weight matrix at fine-tuning step t.\nWt ∈Rm×n\nWeight matrix of a linear layer at training step t (Wt = W0+∆Wt).\n∂Lt\n∂W0 ∈Rm×n\nGradient matrix of the pre-trained weight W0 at step t.\nG ∈Rm×n\nN-batch accumulated gradient matrix of the pre-trained weight W0.\nA ∈Rm×r, B ∈Rr×n\nTrainable low-rank matrices of a LoRA adapter.\nA0 ∈Rm×r, B0 ∈Rr×n\nInitial values of the low-rank matrices A and B.\nAt ∈Rm×r, Bt ∈Rr×n\nTrainable low-rank matrices of a LoRA adapter at step t.\nm\nInput dimension of the matrix W.\nn\nOutput dimension of the matrix W.\nr\nRank of the low-rank adapter, with r ≪min(m, n).\nrmax\nPre-defined maximum rank of a GoRA adapter.\nrmin\nPre-defined minimum rank of a GoRA adapter.\nrref\nReference rank of GoRA.\nα\nHyperparameter of LoRA and most of its variants.\nγ\nScaling hyperparameter of GoRA.\nξ\nScaling factor of GoRA, automatically determined by γ.\nUkaiming\nKaiming uniform distribution.\nη\nLearning rate used in the optimizer (e.g., AdamW).\n⊙\nHadamard (element-wise) product of two matrices.\n[·]\nRounding to the nearest integer.\n∥· ∥F\nFrobenius norm of a matrix.\n∥· ∥∗\nNuclear norm of a matrix.\navg(·)\nAverage operation (element-wise for a matrix).\nB\nProofs\nB.1\nProof of optimal approximation of G given A.\nLet G be an m × n matrix, and A be an m × r matrix where r ≪min(m, n). We aim to derive\nthe projection formula that minimizes the Frobenius norm of the error ∥G −ˆG∥F , where ˆG is the\noptimal approximation of G in the column space of A, denoted as Col(A).\nThe best approximation ˆG lies in Col(A), so we can express ˆG as:\nˆG = AB,\nwhere B is an r × n matrix of coefficients to be determined. Our goal is to find B such that the error\n∥G −ˆG∥F is minimized.\nThe error matrix is given by:\nE = G −ˆG = G −AB.\nTo minimize ∥E∥2\nF , we take the derivative of ∥E∥2\nF with respect to B and set it to zero. Expanding\n∥E∥2\nF , we have:\n∥E∥2\nF = Tr\n\u0000(G −AB)⊤(G −AB)\n\u0001\n,\nwhere Tr represents the trace of a matrix.\nExpanding this expression:\n∥E∥2\nF = Tr(G⊤G) −2Tr(B⊤A⊤G) + Tr(B⊤A⊤AB).\n16\n"
    },
    {
      "page_number": 17,
      "text": "Taking the derivative with respect to B and setting it to zero:\n−2A⊤G + 2A⊤AB = 0.\nSimplifying:\nA⊤AB = A⊤G.\nAssuming A⊤A is invertible, we solve for B:\nB = (A⊤A)−1A⊤G.\nSubstituting B into ˆG = AB, we get:\nˆG = A(A⊤A)−1A⊤G.\nThus, the best approximation ˆG is:\nˆG = A(A⊤A)−1A⊤G.\nThe matrix ˆG = A(A⊤A)−1A⊤G is the projection of G onto the column space of A, and it\nminimizes the Frobenius norm of the error ∥G −ˆG∥F .\nB.2\nProof of Expectation of Frobenius Norm of AB.\nLet A be a random Gaussian matrix of size m×r, where each element of A is sampled independently\nfrom N(0, 1). Let G be a random Gaussian matrix of size m × n, where each element of G is also\nsampled independently from N(0, 1). Define:\nB = (A⊤A)−1A⊤G,\nand consider the product:\nAB = A(A⊤A)−1A⊤G.\nThe goal is to compute the expected Frobenius norm E[∥AB∥F ], where the Frobenius norm is defined\nas:\n∥AB∥F =\nsX\ni,j\n(AB)2\nij.\nFirst, observe that AB can be rewritten as:\nAB = A(A⊤A)−1A⊤G.\nLet P = A(A⊤A)−1A⊤. Note that P is a projection matrix onto the column space of A, and thus\nP satisfies:\nP2 = P,\nP⊤= P,\nand\nrank(P) = r.\nSubstituting P into the expression for AB, we have:\nAB = PG.\nThe Frobenius norm of AB is given by:\n∥AB∥2\nF = ∥PG∥2\nF = Tr((PG)(PG)⊤).\nSince (PG)⊤= G⊤P, this becomes:\n∥AB∥2\nF = Tr(PGG⊤P).\nThe matrix GG⊤is an m × m random Wishart matrix. When G is a standard Gaussian matrix of\nsize m × n, the expected value of GG⊤is:\nE[GG⊤] = n · Im,\n17\n"
    },
    {
      "page_number": 18,
      "text": "where Im is the m × m identity matrix. Substituting this result into the expression for ∥AB∥2\nF , we\nget:\nE[∥AB∥2\nF ] = E[Tr(PGG⊤P)] = Tr(PE[GG⊤]P).\nUsing E[GG⊤] = n · Im, this simplifies to:\nE[∥AB∥2\nF ] = Tr(P(n · Im)P) = n · Tr(P2).\nSince P2 = P, we have:\nE[∥AB∥2\nF ] = n · Tr(P).\nThe trace of P is equal to its rank, which is the dimension of the column space of A. Since A is a\nm × r matrix, we have:\nTr(P) = r.\nThus:\nE[∥AB∥2\nF ] = n · r.\nTaking the square root, the expected Frobenius norm of AB is:\nE[∥AB∥F ] = √n · r.\nC\nImplementation Details\nC.1\nBaseline Methods\nWe compared GoRA with baseline methods to demonstrate the effectiveness of our approach:\na. Full: Trains all parameters in the target layers, resulting in the highest memory consumption.\nb. LoRA [8]: Introduces low-rank adapters into the target layers, significantly reducing the\nnumber of trainable parameters.\nc. Convergence Optimization Methods for LoRA\n- rsLoRA [11]: Modifies the scaling factor in LoRA from α\nr to\nα\n√r, enabling better perfor-\nmance with higher-rank adapters and stabilizing the training processes.\n- DoRA [45]: Decomposes the weight updates of pre-trained weights into magnitude and\ndirection components, and applies LoRA to update only the direction.\n- LoRA+ [46]: Addresses the imbalance between matrices A and B in LoRA by assigning a\nrelatively larger learning rate to matrix B than to matrix A.\nd. Initialization Optimization Methods for LoRA\n- OLoRA [27]: Initializes LoRA weights using the QR decomposition of the corresponding\npre-trained weights.\n- PiSSA [16]: Initializes LoRA weights based on the dominant singular vectors obtained\nfrom the SVD of pre-trained weights.\n- LoRA-GA [18]: Initializes LoRA weights using significant singular vectors derived from\nthe SVD of gradients of pre-trained weights.\ne. Adaptive Methods for LoRA\n- AdaLoRA [14]: Approximates the low-rank adapter structure using SVD, enabling dy-\nnamic rank allocation through singular value masking. It also introduces an orthogonal\nregularization term to the loss function for enhancing orthogonality among features in the\nlow-rank adapter.\nC.2\nImplementation Details for Baseline Methods\nSeveral baseline methods introduce tunable hyperparameters compared with vanilla LoRA [8]. To\nensure a fair comparison, we adopt the optimal settings reported in the original papers whenever\npossible. Specifically, for LoRA+ [46], we set the learning rate ratio of matrices A and B to 16. For\nLoRA-GA [18], we use the “stable” scaling method (the scaling hyperparameter γ of LoRA-GA is\nconfigured to 16) and manipulate the pre-trained weights during initialization. For AdaLoRA [14],\nthe initial rank is set to 12, the final rank to 8, with ti = 150 and tf = 900. For PiSSA [16], the\nnumber of iterations for fast SVD is set to 64.\n18\n"
    },
    {
      "page_number": 19,
      "text": "C.3\nImplementation Details for GoRA\nFor all experiments with rref = 8, except for the model trained on MetaMathQA [47], we set the\nscaling factor γ to 5e −2. For the model trained on MetaMathQA, γ is set to 8e −2. For all\nexperiments with rref = 32, the scaling factor is set to 1e−2; and for rref = 128, we set the scaling\nfactor to 5e −3. This is because we observe that more gradient information is compressed by GoRA’s\ninitialization with a higher rank, even if γ can control the magnitude of the initialization results. To\naddress the imbalance in GoRA’s matrices A and B, we set the learning rate of matrix B to be 16\ntimes that of matrix A Throughout the experiments, the rmax was empirically defined as 4 × rref,\nthe rmin was set to rref/2 (as this setting can maintain a comparable parameter count compared to\nLoRA), and the gradient accumulation step for GoRA’s initialization was set to 64. In the ablation\nstudies, we adhered to the same hyperparameter settings as in the main experiments, unless otherwise\nspecified.\nFurthermore, Algorithm 1 presents a basic algorithm for non-parallel scenarios; however, parallel\ntraining strategies, particularly data parallelism, are widely employed for training large language\nmodels. We introduce the GoRA algorithm under distributed data parallelism in Algorithm 2, which\nis readily integrable into more complex parallel frameworks.\nAlgorithm 2 Rank Allocation and Initialization of GoRA under Distributed Data Parallelism\nInput: Number of layers L, gradient accumulation steps N, model parameters θ = {W l\n0}L\nl=1,\ncurrent data parallel worker ID w, total data parallel workers W\nOutput: Initialized low-rank matrices {Al\n0}L\nl=1, {Bl\n0}L\nl=1\n1: Initialize empty CPU buffers {Glavg}L\nl=1 only on worker 0\n2: for i = 1 to N do\n3:\nSample mini-batch and compute loss L\n4:\nfor l = 1 to L do\n5:\nCompute local gradient on GPU: Gl ←\n∂L\n∂Wl\n0\n6:\nif w = 0 then\n7:\nAccumulate on CPU: Glavg ←Glavg + 1\nN Gl\n8:\nRelease the GPU memory occupied by Gl: Gl ←None\n9: if w = 0 then\n10:\nfor l = 1 to L do\n11:\nCompute the importance I(Wl\n0) as detailed in Algorithm 1\n12:\nBroadcast the importance set {I(Wl\n0)}L\nl=1 to all other workers\n13: else\n14:\nReceive {I(Wl\n0)}L\nl=1 from worker 0\n15: for l = 1 to L do\n16:\nif w = 0 then\n17:\nLoad the gradient Gl\navg from CPU to GPU\n18:\nInitialize Al\n0, Bl\n0 as detailed in Algorithm 1\n19:\nClear the GPU memory occupied by Gl\navg\n20:\nBroadcast initialized Al\n0, Bl\n0 to all other workers\n21:\nelse\n22:\nReceive initialized Al\n0, Bl\n0 from worker 0\n23: Return {Al\n0}L\nl=1, {Bl\n0}L\nl=1\nC.4\nHyperparameters for Each Experiment\nThe hyperparameters used in each experiment are summarized in Table 11. For experiments on\nT5-Base and Llama2-7B-Base, we adopt the settings from LoRA-GA [18]; for CLIP-ViT-B/16, we\nfollow LoRA-Pro [53]. For experiments on Llama3.1-8B-Base, including both baseline methods and\nGoRA, we use one of the most commonly adopted hyperparameter configurations.\n19\n"
    },
    {
      "page_number": 20,
      "text": "Table 11: Hyperparameters used in experiments\nModel\nLR\nLR Decay\nWarmup\nOptimizer\nBetas\nWeight Decay\nBatch Size\nT5-Base\n1e-4\n0\n0.3\nAdam\n0.9, 0.999\n0\n32\nLlama3.1-8B-Base\n5e-5\n0.1\n0.3\nAdamW\n0.9, 0.999\n5e-4\n64\nLlama2-7B-Base\n2e-5\n0\n0.3\nAdamW\n0.9, 0.999\n0\n32\nCLIP-ViT-B/16\n1e-4\n0\n0.3\nAdam\n0.9, 0.999\n0\n64\nC.5\nTraining Environments\nFor natural language understanding tasks reported in section 4.1, we conduct our experiments using\nthe Huggingface Transformers framework for model and trainer implementation on a single RTX\n4090 24GB. In contrast, for natural language generation tasks reported in Section 4.2 and Section 5,\nwe utilize the DeepSpeed ZeRO2 [25] data parallel framework and FlashAttention-2 [58] mechanism,\nleveraging the power of 8 RTX 4090 24 GB GPUs or 8 A800 80 GB GPUs. All codes of GoRA and\nbaseline methods are implemented in PyTorch.\nD\nAddtional Experiments\nD.1\nComputational Overhead of Pseudo-Inverse Initialization\nThe initialization strategy of GoRA involves computing a pseudo-inverse for each low-rank adapter\nmatrix. Although the theoretical cost of pseudo-inversion scales cubically with rank (O(r3)), our\nimplementation performs all operations on GPUs using optimized linear algebra routines, resulting in\nminimal practical overhead.\nWe benchmark the total initialization time for the Llama-3.1-8B-Base model across different ranks\nfollowing the settings of our main experiments. Results are summarized in Table 12. Even at rank\n128, where each pseudo-inverse requires approximately 2 million FLOPs, the entire initialization\ncompletes in under 4 seconds. This amounts to less than 0.1% of typical fine-tuning runtime,\nconfirming that the pseudo-inverse step does not constitute a computational bottleneck in practice.\nTable 12: Pseudo-inverse initialization time for Llama-3-8B-Base (GPU, end-to-end).\nRank\nInitialization Time\n8\n1.40s\n32\n1.56s\n128\n3.43s\nD.2\nCombining GoRA with QLoRA\nTo investigate whether GoRA can be effectively integrated with quantization techniques, we adopt\nthe quantization method from QLoRA, which quantizes pre-trained weights to NF4 precision for\nstorage and dequantizes them back to BF16 precision during computation. We evaluate this combined\napproach, which we term QGoRA, by fine-tuning the Llama-3.1-8B-Base model and assessing its\nperformance on math and code benchmarks, following the same protocol as in our main experiments.\nAs shown in Table 13, GoRA integrates seamlessly with QLoRA’s quantization framework and\nconsistently outperforms QLoRA across evaluated tasks.\nTable 13: Performance of QGoRA and QLoRA.\nMethod\nGSM8k\nHumanEval\nQLoRA\n65.10 ± 0.95\n43.49 ± 0.70\nQGoRA\n70.58 ± 1.33\n44.10 ± 3.68\nD.3\nGradient Reconstruction Accuracy of GoRA Initialization\nTo assess how well the GoRA initialization approximates the pre-computed gradients, we measure\nthe reconstruction error between the initialized low-rank adapters and the full gradients across all\nadapted layers in the Llama-3.1-8B-Base model. Results on two datasets are summarized in Table 14.\nThese results suggest that the initialization preserves a high proportion of gradient information\n(approximately 88–89% in relative terms).\n20\n"
    },
    {
      "page_number": 21,
      "text": "Table 14: Gradient reconstruction error of GoRA initialization.\nDataset\nAbsolute Error\nRelative Error\nMetaMathQA\n0.0382\n0.1147\nCode-Feedback\n0.0322\n0.1152\nD.4\nMulti-Step Gradient Initialization\nWe explored an alternative initialization variant, GoRA-pro, which estimates layer importance\nusing multi-step stochastic gradients. In this approach, a lightweight pre-training phase performs\nn exploratory updates per layer using the AdaLomo [59] optimizer; gradients are accumulated on\nCPU and then discarded, and the original weights are restored before adapter initialization. This\nprocess uses approximately 23% less GPU memory than full fine-tuning (56.2 GB vs. 73.0 GB for\nLlama-3.1-8B-Base).\nWe evaluate GoRA-pro on GSM8K and HumanEval, comparing it to the standard GoRA (which\nuses one-step gradient accumulation). Results are shown in Table 15. The comparable performance\nsuggests that the multi-step gradient idea is viable and may offer further improvements with careful\ntuning of the exploration schedule, gradient normalization. We leave a systematic investigation of\nthis direction to future work.\nTable 15: Comparison of GoRA and GoRA-pro (Llama-3-8B-Base).\nMethod\nGSM8K\nHumanEval\nGoRA\n72.91 ± 0.76\n48.98 ± 2.14\nGoRA-pro\n72.30 ± 0.30\n47.36 ± 1.54\nE\nClarifications\nE.1\nClarification on the training-inference gap introduced by previous initialization methods\nThis is due to their reliance on manipulating pre-trained weights. Specifically:\nManipulation of Pre-Trained Weights: These methods are required to manipulate the value of\npre-trained weights during initialization, as A0B0 ̸= 0 and W0 + A0B0 ̸= W0. As a result, during\ninference, the term A0B0 must be recomputed to properly reconstruct the adapted weight matrix for\neffective model deployment, which is essential for correct model outputs.\nThe Inconvenience of Recalculating Initialization Results: During inference, it is often infeasible to\nrecompute the initialization results for methods that either rely on randomness, such as PiSSA[16], or\nrequire access to training data, such as LoRA-GA [18] and EVA[28]. Furthermore, for approaches like\nMiLoRA [17], the initialization process itself can be computationally expensive and time-consuming.\nIncompatibility Across Multiple Adapters: When multiple adapters are trained on different tasks\nusing previous data-driven non-zero initialization methods, the pre-trained weights are manipulated\ninconsistently. As the result of A0B0 depends on the task. This makes it challenging to serve multiple\nadapters simultaneously, limiting flexibility in multi-task scenarios.\nSaving manipulated pre-trained weights sacrifices one of the key advantages of LoRA: While\nit is possible to merge the low-rank adapter weights into the pre-trained weights after training,\nsaving the pre-trained weights post-merging sacrifices one of the key advantages of LoRA: minimal\nstorage requirements (e.g., 10MB compared to 14GB). Other potential approaches to eliminate the\ntraining-inference gap and their limitations are discussed in Section 2.2.\nE.2\nCompare GoRA’s initialization strategy with LoRA-GA\nBoth GoRA and LoRA-GA leverage gradients to initialize low-rank adapters, but there are several\nkey differences:\n1. Motivation:\n• LoRA-GA: Minimizes the difference in updates of weights between LoRA and full\nfine-tuning.\n21\n"
    },
    {
      "page_number": 22,
      "text": "• GoRA: Views LoRA adapters, gradient compressors, and optimizes the compression\nform.\n2. Scaling factor:\n• LoRA-GA: Inspired by rsLoRA, its scaling aims to stabilize training.\n• GoRA: Scale the initialized product of adapters to any desired magnitude flexibly.\n3. Methodology:\n• LoRA-GA: Initialize the weights using SVD decomposed gradients.\n• GoRA: Initialize the weights of B using gradients compressed by pseudo-inverse of\nrandom initialized A.\nE.3\nFurther compare with previous related works\nBuilding upon previous works, GoRA makes several unique contributions:\n1. First data-driven initialization method without manipulating pre-trained weights.\n2. Efficient rank allocation strategy without additional trainable parameters and training com-\nplexity.\n3. Unified framework for gradient-driven rank allocation and initialization.\nF\nLimitations And Future Works\nIn this study, we demonstrate that GoRA outperforms baseline low-rank adaptation methods and\nachieves performance comparable to full fine-tuning. However, our evaluation has not yet extended\nto larger models and more extensive datasets. We hypothesize that for larger models, such as Llama-\n3.1-70B [2], GoRA could more effectively leverage the pre-trained knowledge inherent in these\nmodels. Additionally, while this research primarily focuses on language models and natural language\nprocessing tasks, there is potential to generalize GoRA to a broader range of model types and tasks,\nsuch as visual language models and visual question answering.\nAnother limitation of this study is that the initialization of matrix A is not restricted to random\ninitialization. Employing alternative methods, such as extracting distinguishing features from pre-\ntrained weights to initialize the matrix A, could potentially enhance performance, as it would combine\nthe benefits of both experience-driven and data-driven initialization approaches. Furthermore, it is\nworth noting that GoRA demonstrates theoretical compatibility with other LoRA variants, such as\nDoRA [45]. These promising avenues remain to be explored in future research endeavors.\n22\n"
    },
    {
      "page_number": 23,
      "text": "NeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: Yes, and we believe GoRA can accelerate the research on LoRA.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We have concluded our limitations in Appendix F.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\n23\n"
    },
    {
      "page_number": 24,
      "text": "Justification: Yes, we have provided proofs in the Appendix B.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: Yes, we have provided experiment settings and our algorithm in the main paper\nand the Appendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n24\n"
    },
    {
      "page_number": 25,
      "text": "Answer: [Yes]\nJustification: Yes, we have provided the link to our official repository in the abstract.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: We have provided hyperparameter information in Section 4.1, Section 4.2 and\nSection 4.3. Additional information is provided in Appendix C.4.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer:\nJustification: We run each experiment with 3 different random seeds and report the mean\nand the standard deviation.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n25\n"
    },
    {
      "page_number": 26,
      "text": "• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: Yes, we have provided related information in the Appendix.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: Yes.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: There is no societal impact of the work performed.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n26\n"
    },
    {
      "page_number": 27,
      "text": "• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer:\nJustification: No risk of misuse in this work.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer:\nJustification: Yes.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n27\n"
    },
    {
      "page_number": 28,
      "text": "• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer:\nJustification: No new assets introduced in the paper.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\n28\n"
    },
    {
      "page_number": 29,
      "text": "Question: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: The core method development in this research does not involve LLMs as any\nimportant, original, or non-standard components.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n29\n"
    }
  ]
}