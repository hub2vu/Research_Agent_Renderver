{
  "filename": "eghAocvqBk_Diffusion Bridge Implicit Models.pdf",
  "total_pages": 28,
  "full_text": "Published as a conference paper at ICLR 2025\nDIFFUSION BRIDGE IMPLICIT MODELS\nKaiwen Zheng12‚àó,\nGuande He1‚àó,\nJianfei Chen1,\nFan Bao2,\nJun Zhu‚Ä†123\n1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center\n1Tsinghua-Bosch Joint ML Center, Tsinghua University, Beijing, China\n2Shengshu Technology, Beijing\n3Pazhou Lab (Huangpu), Guangzhou, China\nzkwthu@gmail.com; guande.he17@outlook.com;\nfan.bao@shengshu.ai; {jianfeic, dcszj}@tsinghua.edu.cn\nABSTRACT\nDenoising diffusion bridge models (DDBMs) are a powerful variant of diffu-\nsion models for interpolating between two arbitrary paired distributions given\nas endpoints. Despite their promising performance in tasks like image transla-\ntion, DDBMs require a computationally intensive sampling process that involves\nthe simulation of a (stochastic) differential equation through hundreds of network\nevaluations. In this work, we take the first step in fast sampling of DDBMs with-\nout extra training, motivated by the well-established recipes in diffusion mod-\nels. We generalize DDBMs via a class of non-Markovian diffusion bridges de-\nfined on the discretized timesteps concerning sampling, which share the same\nmarginal distributions and training objectives, give rise to generative processes\nranging from stochastic to deterministic, and result in diffusion bridge implicit\nmodels (DBIMs). DBIMs are not only up to 25√ó faster than the vanilla sam-\npler of DDBMs but also induce a novel, simple, and insightful form of ordinary\ndifferential equation (ODE) which inspires high-order numerical solvers. More-\nover, DBIMs maintain the generation diversity in a distinguished way, by using a\nbooting noise in the initial sampling step, which enables faithful encoding, recon-\nstruction, and semantic interpolation in image translation tasks. Code is available\nat https://github.com/thu-ml/DiffusionBridge.\n1\nINTRODUCTION\nDiffusion models (Song et al., 2021c; Sohl-Dickstein et al., 2015; Ho et al., 2020) represent a family\nof powerful generative models, with high-quality generation ability, stable training, and scalability\nto high dimensions. They have consistently obtained state-of-the-art performance in various do-\nmains, including image synthesis (Dhariwal & Nichol, 2021; Karras et al., 2022), speech and video\ngeneration (Chen et al., 2021a; Ho et al., 2022), controllable image manipulation (Nichol et al.,\n2022; Ramesh et al., 2022; Rombach et al., 2022; Meng et al., 2022), density estimation (Song\net al., 2021b; Kingma et al., 2021; Lu et al., 2022a; Zheng et al., 2023b) and inverse problem solv-\ning (Chung et al., 2022; Kawar et al., 2022). They also act as fundamental components of modern\ntext-to-image (Rombach et al., 2022) and text-to-video (Gupta et al., 2023; Bao et al., 2024) synthe-\nsis systems, ushering in the era of AI-generated content.\nHowever, diffusion models are not well-suited for solving tasks like image translation or restoration,\nwhere the transport between two arbitrary probability distributions is to be modeled given paired\nendpoints. Diffusion models are rooted in a stochastic process that gradually transforms between\ndata and noise, and the prior distribution is typically restricted to the ‚Äúnon-informative‚Äù random\nGaussian noises. Adapting diffusion models to scenarios where a more informative prior natu-\nrally exists, such as image translation/restoration, involves modifying the generation pipeline (Meng\net al., 2022; Su et al., 2022) or adding extra guidance terms during sampling (Chung et al., 2022;\nKawar et al., 2022). On the one hand, these approaches are task-agnostic at training and adaptable\nto multiple tasks at inference time. On the other hand, despite recent advances in accelerated inverse\nproblem solving (Liu et al., 2023a; Pandey et al., 2024), they inevitably deliver either sub-par perfor-\nmance or slow and resource-intensive inference compared to training-based ones. Tailored diffusion\n‚àóEqual contribution;\n‚Ä†The corresponding author.\n1\nPublished as a conference paper at ICLR 2025\n(a) Condition\n(b)\nDDBM\nNFE=100 (FID 6.46)\n(c)\nDBIM (Œ∑ = 0) (Ours)\nNFE=10 (FID 4.51)\n(c)\nDBIM (3rd-order) (Ours)\nNFE=10 (FID 4.34)\nFigure 1: Inpainting results on the ImageNet 256√ó256 dataset (Deng et al., 2009) by DDBM (Zhou et al.,\n2023) with 100 number of function evaluations (NFE), and DBIM (ours) with only 10 NFE.\nmodel variants become essential in task-specific scenarios where paired training data are available\nand fast inference is critical.\nRecently, denoising diffusion bridge models (DDBMs) (Zhou et al., 2023) have emerged as a scal-\nable and promising approach to solving the distribution translation tasks. By considering the reverse-\ntime processes of a diffusion bridge, which represent diffusion processes conditioned on given end-\npoints, DDBMs offer a general framework for distribution translation. While excelling in image\ntranslation tasks with exceptional quality and fidelity, sampling from DDBMs requires simulating a\n(stochastic) differential equation corresponding to the reverse-time process. Even with the introduc-\ntion of their hybrid sampler, achieving high-fidelity results for high-resolution images still demands\nover 100 steps. Compared to the efficient samplers for diffusion models (Song et al., 2021a; Zhang\n& Chen, 2022; Lu et al., 2022b), which require around 10 steps to generate reasonable samples,\nDDBMs are falling behind, urging the development of efficient variants.\nThis work represents the first pioneering effort toward accelerated sampling of DDBMs. As sug-\ngested by well-established recipes in diffusion models, training-free accelerations of diffusion sam-\npling primarily focus on reducing stochasticity (e.g., the prominent denoising diffusion implicit\nmodels, DDIMs) and utilizing higher-order information (e.g., high-order solvers). We present diffu-\nsion bridge implicit models (DBIMs) as an approach that explores both aspects within the diffusion\nbridge framework. Firstly, we investigate the continuous-time forward process of DDBMs on dis-\ncretized timesteps and generalize them to a series of non-Markovian diffusion bridges controlled by\na variance parameter, while maintaining identical marginal distributions and training objectives as\nDDBMs. Secondly, the induced reverse generative processes correspond to sampling procedures of\nvarying levels of stochasticity, including deterministic ones. Consequently, DBIMs can be viewed\nas a bridge counterpart and extension of DDIMs. Furthermore, in the continuous time limit, DBIMs\ncan induce a novel form of ordinary differential equation (ODE), which is linked to the probability\nflow ODE (PF-ODE) in DDBMs while being simpler and significantly more efficient. The induced\nODE also facilitates novel high-order numerical diffusion bridge solvers for faster convergence.\nWe demonstrate the superiority of DBIMs by applying them in image translation and restoration\ntasks, where they offer up to 25√ó faster sampling compared to DDBMs and achieve state-of-the-art\nperformance on challenging high-resolution datasets. Unlike conventional diffusion sampling, the\ninitial step in DBIMs is forced to be stochastic with a booting noise to avoid singularity issues arising\nfrom the fixed starting point on a bridge. By viewing the booting noise as the latent variable, DBIMs\nmaintain the generation diversity of typical generative models while enabling faithful encoding,\nreconstruction, and semantically meaningful interpolation in the data space.\n2\nBACKGROUND\n2.1\nDIFFUSION MODELS\nGiven a d-dimensional data distribution q0(x0), diffusion models (Song et al., 2021c; Sohl-Dickstein\net al., 2015; Ho et al., 2020) build a diffusion process by defining a forward stochastic differential\nequation (SDE) starting from x0 ‚àºq0:\ndxt = f(t)xtdt + g(t)dwt\n(1)\n2\nPublished as a conference paper at ICLR 2025\nwhere t ‚àà[0, T] for some finite horizon T, f, g : [0, T] ‚ÜíR is the scalar-valued drift and diffusion\nterm, and wt ‚ààRd is a standard Wiener process. As a linear SDE, the forward process owns an\nanalytic Gaussian transition kernel\nqt|0(xt|x0) = N(Œ±tx0, œÉ2\nt I)\n(2)\nby ItÀÜo‚Äôs formula (ItÀÜo, 1951), where Œ±t, œÉt are called noise schedules satisfying f(t)\n=\nd log Œ±t\ndt\n,\ng2(t) =\ndœÉ2\nt\ndt ‚àí2 d log Œ±t\ndt\nœÉ2\nt (Kingma et al., 2021). The forward SDE is accompanied\nby a series of marginal distributions {qt}T\nt=0 of {xt}T\nt=0, and f, g are properly designed so that the\nterminal distribution is approximately a pure Gaussian, i.e., qT (xT ) ‚âàN(0, œÉ2\nT I).\nTo sample from the data distribution q0(x0), we can solve the reverse SDE or probability flow\nODE (Song et al., 2021c) from t = T to t = 0:\ndxt = [f(t)xt ‚àíg2(t)‚àáxt log qt(xt)]dt + g(t)d ¬Øwt,\n(3)\ndxt =\n\u0014\nf(t)xt ‚àí1\n2g2(t)‚àáxt log qt(xt)\n\u0015\ndt.\n(4)\nThey share the same marginal distributions {qt}T\nt=0 with the forward SDE, where\n¬Øwt\nis the reverse-time Wiener process,\nand the only unknown term ‚àáxt log qt(xt) is the\nscore function of the marginal density qt.\nBy denoising score matching (DSM) (Vin-\ncent,\n2011),\na\nscore\nprediction\nnetwork\nsŒ∏(xt, t)\ncan\nbe\nparameterized\nto\nminimize\nEtEx0‚àºq0(x0)Ext‚àºqt|0(xt|x0)\n\u0002\nw(t)‚à•sŒ∏(xt, t) ‚àí‚àáxt log qt|0(xt|x0)‚à•2\n2\n\u0003\n, where qt|0 is the analytic\nforward transition kernel and w(t) is a positive weighting function. sŒ∏ can be plugged into the re-\nverse SDE and the probability flow ODE to obtain the parameterized diffusion SDE and diffusion\nODE. There are various dedicated solvers for diffusion SDE or ODE (Song et al., 2021a; Zhang &\nChen, 2022; Lu et al., 2022b; Gonzalez et al., 2023).\n2.2\nDENOISING DIFFUSION BRIDGE MODELS\nDenoising diffusion bridge models (DDBMs) (Zhou et al., 2023) consider driving the diffusion\nprocess in Eqn. (1) to arrive at a particular point y ‚ààRd almost surely via Doob‚Äôs h-transform (Doob\n& Doob, 1984):\ndxt = f(t)xtdt + g2(t)‚àáxt log q(xT = y|xt) + g(t)dwt,\nx0 ‚àºq0 = pdata, xT = y.\n(5)\nThe endpoint y is not restricted to Gaussian noise as in diffusion models, but instead chosen as\ninformative priors (such as the degraded image in image restoration tasks). Given a starting point\nx0, the process in Eqn. (5) also owns an analytic forward transition kernel\nq(xt|x0, xT ) = N(atxT +btx0, c2\ntI),\nat = Œ±t\nŒ±T\nSNRT\nSNRt\n, bt = Œ±t(1‚àíSNRT\nSNRt\n), c2\nt = œÉ2\nt (1‚àíSNRT\nSNRt\n)\n(6)\nwhich forms a diffusion bridge, and SNRt = Œ±2\nt/œÉ2\nt is the signal-to-noise ratio at time t. DDBMs\nshow that the forward process Eqn. (5) is associated with a reverse SDE and a probability flow ODE\nstarting from xT = y:\ndxt =\n\u0002\nf(t)xt ‚àíg2(t)\n\u0000‚àáxt log q(xt|xT = y) ‚àí‚àáxt log qT |t(xT = y|xt)\n\u0001\u0003\ndt + g(t)d ¬Øwt, (7)\ndxt =\n\u0014\nf(t)xt ‚àíg2(t)\n\u00121\n2‚àáxt log q(xt|xT = y) ‚àí‚àáxt log qT |t(xT = y|xt)\n\u0013\u0015\ndt.\n(8)\nThey share the same marginal distributions {q(xt|xT = y)}T\nt=0 with the forward process, where\n¬Øwt is the reverse-time Wiener process, qT |t is analytically known similar to Eqn. (2), and the only\nunknown term ‚àáxt log q(xt|xT = y) is the bridge score function. Denoising bridge score matching\n(DBSM) is proposed to learn the unknown score term q(xt|xT = y) with a parameterized network\nsŒ∏(xt, t, y), by minimizing\nLw(Œ∏) = EtE(x0,y)‚àºpdata(x0,y)Ext‚àºq(xt|x0,xT =y)\n\u0002\nw(t)‚à•sŒ∏(xt, t, y) ‚àí‚àáxt log q(xt|x0, xT = y)‚à•2\n2\n\u0003\n(9)\nwhere q(xt|x0, xT = y) is the forward transition kernel in Eqn. (6) and w(t) is a positive weighting\nfunction. To sample from diffusion bridges with Eqn. (7) and Eqn. (8), DDBMs propose a high-order\nhybrid sampler that alternately simulates the ODE and SDE steps to enhance the sample quality,\ninspired by the Heun sampler in diffusion models (Karras et al., 2022). However, it is not dedicated\nto diffusion bridges and lacks theoretical insights in developing efficient diffusion samplers.\n3\nPublished as a conference paper at ICLR 2025\n3\nGENERATIVE MODEL THROUGH NON-MARKOVIAN DIFFUSION BRIDGES\nWe start by examining the forward process of the diffusion bridge (Eqn. (5)) on a set of discretized\ntimesteps 0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN‚àí1 < tN = T that will be used for reverse sampling. Since\nthe bridge score ‚àáxt log q(xt|xT ) only depends on the marginal distribution q(xt|xT ), we can\nconstruct alternative probabilistic models that induce new sampling procedures while reusing the\nlearned bridge score sŒ∏(xt, t, xT ), as long as they agree on the N marginals {q(xtn|xT )}N‚àí1\nn=0 .\n3.1\nNON-MARKOVIAN DIFFUSION BRIDGES AS FORWARD PROCESS\nWe consider a family of probability distributions q(œÅ)(xt0:N‚àí1|xT ), controlled by a variance param-\neter œÅ ‚ààRN‚àí1:\nq(œÅ)(xt0:N‚àí1|xT ) = q0(xt0)\nN‚àí1\nY\nn=1\nq(œÅ)(xtn|x0, xtn+1, xT )\n(10)\nwhere q0 is the data distribution at time 0 and for 1 ‚â§n ‚â§N ‚àí1\nq(œÅ)(xtn|x0, xtn+1, xT ) = N(atnxT + btnx0 +\nq\nc2\ntn ‚àíœÅ2n\nxtn+1 ‚àíatn+1xT ‚àíbtn+1x0\nctn+1\n, œÅ2\nnI)\n(11)\nwhere œÅn is the n-th element of œÅ satisfying œÅN‚àí1 = ctN‚àí1, and at, bt, ct are terms related to the\nnoise schedule, as defined in the original diffusion bridge (Eqn. (6)). Intuitively, this decreases the\nvariance (noise level) of the bridge while incorporating additional noise components from the last\nstep. Under this construction, we can prove that q(œÅ) maintains consistency in marginal distributions\nwith the original forward process q governed by Eqn. (5).\nProposition 3.1 (Marginal Preservation, proof in Appendix B.1). For 0 ‚â§n ‚â§N ‚àí1, we have\nq(œÅ)(xtn|xT ) = q(xtn|xT ).\nThe definition of q(œÅ) in Eqn. (10) represents the inference process, since it is factorized as\nthe distribution of xtn given xtn+1 at the previous timestep.\nConversely, the forward process\nq(œÅ)(xtn+1|x0, xtn, xT ) can be induced by Bayes‚Äô rule (Appendix C.1). As xtn+1 in q(œÅ) can si-\nmultaneously depend on xtn and x0, we refer to it as non-Markovian diffusion bridges, in contrast\nto Markovian ones (such as Brownian bridges, and the diffusion bridge defined by the forward SDE\nin Eqn. (5)) which should satisfy q(xtn+1|x0, xtn, xT ) = q(xtn+1|xtn, xT ).\n3.2\nREVERSE GENERATIVE PROCESS AND EQUIVALENT TRAINING OBJECTIVE\nEqn. (10) can be naturally transformed into a parameterized and learnable generative model, by\nreplacing the unknown x0 in Eqn. (10) with a data predictor xŒ∏(xt, t, xT ). Intuitively, xt on the\ndiffusion bridge is a weighted mixture of xT , x0 and some random Gaussian noise according to\nEqn. (6), where the weightings at, bt, ct are determined by the timestep t. The network xŒ∏ is trained\nto recover the clean data x0 given xt, xT and t.\nSpecifically, we define the generative process starting from xT as\npŒ∏(xtn|xtn+1, xT ) =\n\u001aN(xŒ∏(xt1, t1, xT ), œÅ2\n0I),\nn = 0\nq(œÅ)(xtn|xŒ∏(xtn+1, tn+1, xT ), xtn+1, xT ),\n1 ‚â§n ‚â§N ‚àí1\n(12)\nand the joint distribution as pŒ∏(xt0:N‚àí1|xT ) = QN‚àí1\nn=0 pŒ∏(xtn|xtn+1, xT ). To optimize the network\nparameter Œ∏, we can adopt the common variational inference objective as in DDPMs (Ho et al.,\n2020), except that the distributions are conditioned on xT :\nJ (œÅ)(Œ∏) = Eq(xT )Eq(œÅ)(xt0:N‚àí1|xT )\nh\nlog q(œÅ)(xt1:N‚àí1|x0, xT ) ‚àílog pŒ∏(xt0:N‚àí1|xT )\ni\n(13)\nIt seems that the DDBM objective Lw in Eqn. (9) is distinct from J (œÅ): respectively, they are defined\non continuous and discrete timesteps; they originate from score matching and variational inference;\nthey have different parameterizations of score and data prediction1. However, we show they are\nequivalent by focusing on the discretized timesteps and transforming the parameterization.\n1The diffusion bridge models are usually parameterized differently from score prediction, but can be con-\nverted to score prediction. See Appendix F.1 for details.\n4\nPublished as a conference paper at ICLR 2025\nTable 1: Comparison between different diffusion models and diffusion bridge models.\nDiffusion Models\nDiffusion Bridge Models\nDDPM\n(Ho et al., 2020)\nScoreSDE\n(Song et al., 2021c)\nDDIM\n(Song et al., 2021a)\nI2SB\n(Liu et al., 2023b)\nDDBM\n(Zhou et al., 2023)\nDBIM (Ours)\nNoise Schedule\nVP\nAny\nAny\nVE\nAny\nAny\nTimesteps\nDiscrete\nContinuous\nDiscrete\nDiscrete\nContinuous\nDiscrete\nForward Distribution\nq(xn|x0)\nq(xt|x0)\nq(xn|xn‚àí1, x0)\nq(xn|x0, xN)\nq(xt|x0, xT )\nq(xtn+1|x0, xtn, xT )\nInference Process\npŒ∏(xn‚àí1|xn)\nSDE/ODE\npŒ∏(xn‚àí1|xn)\npŒ∏(xn‚àí1|xn)\nSDE/ODE\npŒ∏(xtn|xtn+1, xT )\nNon-Markovian\n‚úó\n‚úó\n‚úì\n‚úó\n‚úó\n‚úì\nProposition 3.2 (Training Equivalence, proof in Appendix B.2). For œÅ > 0, there exists certain\nweights Œ≥ so that J (œÅ)(Œ∏) = LŒ≥(Œ∏)+C on the discretized timesteps {tn}N\nn=1, where C is a constant\nirrelevant to Œ∏. Besides, the bridge score predictor sŒ∏ in LŒ≥(Œ∏) has the following relationship with\nthe data predictor xŒ∏ in J (œÅ)(Œ∏):\nsŒ∏(xt, t, xT ) = ‚àíxt ‚àíatxT ‚àíbtxŒ∏(xt, t, xT )\nc2\nt\n(14)\nThough the weighting Œ≥ may not precisely match the actual weighting w for training sŒ∏, this dis-\ncrepancy doesn‚Äôt affect our utilization of sŒ∏ (Appendix C.2). Hence, it is reasonable to reuse the\nnetwork trained by L while leveraging various œÅ for improved sampling efficiency.\n4\nSAMPLING WITH GENERALIZED DIFFUSION BRIDGES\nNow that we have confirmed the rationality and built the theoretical foundations for applying the\ngeneralized diffusion bridge pŒ∏ to pretrained DDBMs, a range of inference processes is now at our\ndisposal, controlled by the variance parameter œÅ. This positions us to explore the resultant sampling\nprocedures and the effects of œÅ in pursuit of better and more efficient generation.\n4.1\nDIFFUSION BRIDGE IMPLICIT MODELS\nSuppose we sample in reverse time on the discretized timesteps 0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN‚àí1 <\ntN = T. The number N and the schedule of sampling steps can be made independently of the\noriginal timesteps on which the bridge model is trained, whether discrete (Liu et al., 2023b) or\ncontinuous (Zhou et al., 2023). According to the generative process of pŒ∏ in Eqn. (12), the updating\nrule from tn+1 to tn is described by\nxtn = atnxT + btn ÀÜx0 +\nq\nc2\ntn ‚àíœÅ2n\nxtn+1 ‚àíatn+1xT ‚àíbtn+1 ÀÜx0\nctn+1\n|\n{z\n}\npredicted noise ÀÜœµ\n+œÅnœµ,\nœµ ‚àºN(0, I)\n(15)\nwhere ÀÜx0 = xŒ∏(xtn+1, tn+1, xT ) denotes the predicted clean data at time 0.\nIntuition of the Sampling Procedure\nIntuitively, the form of Eqn. (15) resembles the forward\ntransition kernel of the diffusion bridge in Eqn. (6) (which can be rewritten as xt = atxT + btx0 +\nctœµ, œµ ‚àºN(0, I)). In comparison, x0 is substituted with the predicted ÀÜx0, and a portion of the\nstandard Gaussian noise œµ now stems from the predicted noise ÀÜœµ. The predicted noise ÀÜœµ is derived\nfrom xtn+1 at the previous timestep and can be expressed by the predicted clean data ÀÜx0.\nEffects of the Variance Parameter\nWe investigate the effects of the variance parameter œÅ\nfrom the theoretical perspective by considering two extreme cases. Firstly, we note that when\nœÅn = œÉtn\nq\n1 ‚àí\nSNRtn+1\nSNRtn\nfor each 0 ‚â§n ‚â§N ‚àí1, the xT term in Eqn. (15) is canceled out.\nIn this scenario, the forward process in Eqn. (4.1) becomes a Markovian bridge (see details in\nAppendix C.1). Besides, the inference process will get rid of xT and simplify to pŒ∏(xtn|xtn+1),\nakin to the sampling mechanism in DDPMs (Ho et al., 2020). Secondly, when œÅn = 0 for each\n0 ‚â§n ‚â§N ‚àí1, the inference process will be free from random noise and composed of deterministic\niterative updates, characteristic of an implicit probabilistic model (Mohamed & Lakshminarayanan,\n5\nPublished as a conference paper at ICLR 2025\nbooting noise\ncondition\nùë°= ùëá\nùë°= ùëá‚àíùúñ\nùë°= 0\ndeterministic\nFigure 2: Illustration of the DBIM‚Äôs deterministic sampling procedure when œÅ = 0.\n2016). Consequently, we name the resulting model diffusion bridge implicit models (DBIMs), draw-\ning parallels with denoising diffusion implicit models (DDIMs) (Song et al., 2021a). DBIMs serve\nas the bridge counterpart and extension of DDIMs, as illustrated in Table 1.\nWhen we choose œÅ that lies between these two boundary cases, we can obtain non-Markovian dif-\nfusion bridges with intermediate and non-zero stochastic levels. Such bridges may potentially yield\nsuperior sample quality. We present detailed ablations in Section 6.1.\nThe Singularity at the Initial Step for Deterministic Sampling\nOne important aspect to note\nregarding DBIMs is that its initial step exhibits singularity when œÅ = 0, a property essentially\ndistinct from DDIMs in diffusion models. Specifically, in the initial step we have tn+1 = T, and\nctn+1 in the denominator in Eqn. (15) equals 0. This phenomenon can be understood intuitively:\ngiven a fixed starting point xT , the variable xt for t < T is typically still stochastically distributed\n(the marginal pŒ∏(xt|xT ) is not a Dirac distribution). For instance, in inpainting tasks, there should\nbe various plausible complete images corresponding to a fixed masked image. However, a fully\ndeterministic sampling procedure disrupts such stochasticity.\nTo be theoretically robust, we employ the other boundary choice œÅn = œÉtn\nq\n1 ‚àí\nSNRtn+1\nSNRtn\nin the\ninitial step2, which is aligned with our previous restriction that œÅN‚àí1 = ctN‚àí1. This will introduce\nan additional standard Gaussian noise œµ which we term as the booting noise. It accounts for the\nstochasticity of the final sample x0 under a given fixed xT and can be viewed as the latent variable.\nWe illustrate the complete DBIM pipeline in Figure 2.\n4.2\nCONNECTION TO PROBABILITY FLOW ODE\nIt is intuitive to perceive that the deterministic sampling can be related to solving an ODE. By setting\nœÅ = 0, tn+1 = t and tn+1 ‚àítn = ‚àÜt in Eqn. (15), the DBIM updating rule can be reorganized\nas xt‚àí‚àÜt\nct‚àí‚àÜt =\nxt\nct +\n\u0010\nat‚àí‚àÜt\nct‚àí‚àÜt ‚àíat\nct\n\u0011\nxT +\n\u0010\nbt‚àí‚àÜt\nct‚àí‚àÜt ‚àíbt\nct\n\u0011\nxŒ∏(xt, t, xT ). As at, bt, ct are continuous\nfunctions of time t defined in Eqn. (6), the ratios at\nct and bt\nct also remain continuous functions of\nt. Therefore, DBIM (œÅ = 0) can be treated as an Euler discretization of the following ordinary\ndifferential equation (ODE):\nd\n\u0012xt\nct\n\u0013\n= xT d\n\u0012at\nct\n\u0013\n+ xŒ∏(xt, t, xT )d\n\u0012bt\nct\n\u0013\n(16)\nThough it does not resemble a conventional ODE involving dt, the two infinitesimal terms d\n\u0010\nat\nct\n\u0011\nand d\n\u0010\nbt\nct\n\u0011\ncan be expressed with dt by the chain rule of derivatives. The ODE form also suggests\nthat with a sufficient number of discretization steps, we can reverse the sampling process and obtain\nencodings of the observed data, which can be useful for interpolation or other downstream tasks.\nIn DDBMs, the PF-ODE (Eqn. (8)) involving dxt and dt is proposed and used for deterministic\nsampling. We reveal in the following proposition that our ODE in Eqn. (16) can exactly yield the\nPF-ODE without relying on the advanced Kolmogorov forward (or Fokker-Planck) equation.\nProposition 4.1 (Equivalence to Probability Flow ODE, proof in Appendix B.3). Suppose\nsŒ∏(xt, t, xT ) is learned as the ground-truth bridge score ‚àáxt log q(xt|xT ), and xŒ∏ is related to sŒ∏\nthrough Eqn. (14), then Eqn. (16) can be converted to the PF-ODE (Eqn. (8)) proposed in DDBMs.\n2With this choice, at the initial step n = N ‚àí1, we have œÅn = œÉtn\nq\n1 ‚àí\nSNRtT\nSNRtn = ctn ‚áí\nq\nc2\ntn ‚àíœÅ2n = 0,\nso ctn+1 in the denominator in Eqn. (15) will be canceled out.\n6\nPublished as a conference paper at ICLR 2025\nCondition\nGround-truth\nDDBM (NFE=20)\nDDBM (NFE=100)\nDBIM (NFE=20)\nDBIM (NFE=100)\nFigure 3: Image translation results on the DIODE-Outdoor dataset with DDBM and DBIM.\nThough the conversion from our ODE to the PF-ODE is straightforward, the reverse conversion can\nbe non-trivial and require complex tools such as exponential integrators (Calvo & Palencia, 2006;\nHochbruck et al., 2009) (Appendix C.4). We highlight our differences from the PF-ODE in DDBMs:\n(1) Our ODE has a novel form with exceptional neatness. (2) Despite their theoretical equivalence,\nour ODE describes the evolution of xt\nct rather than xt, and its discretization is performed with respect\nto d\n\u0010\nat\nct\n\u0011\nand d\n\u0010\nbt\nct\n\u0011\ninstead of dt. (3) Empirically, DBIMs (œÅ = 0) prove significantly more\nefficient than the Euler discretization of the PF-ODE, thereby accelerating DDBMs by a substantial\nmargin. (4) In contrast to the fully deterministic ODE, DBIMs are capable of various stochastic\nlevels to achieve the best generation quality under the same sampling steps.\n4.3\nEXTENSION TO HIGH-ORDER METHODS\nThe simplicity and efficiency of our ODE (Eqn. (16)) also inspire novel high-order numerical solvers\ntailored for DDBMs, potentially bringing faster convergence than the first-order Euler discretization.\nSpecifically, using the time change-of-variable Œªt = log\n\u0010\nbt\nct\n\u0011\n= 1\n2 (SNRt ‚àíSNRT ), the solution\nof Eqn. (16) from time t to time s < t can be represented as\nxs = cs\nct\nxt +\n\u0012\nas ‚àícs\nct\nat\n\u0013\nxT + cs\nZ Œªs\nŒªt\neŒªxŒ∏(xtŒª, tŒª, xT )dŒª\n(17)\nwhere tŒª is the inverse function of Œªt. The intractable integral can be approximated by Taylor ex-\npansion of xŒ∏ and finite difference estimations of high-order derivatives, following well-established\nnumerical methods (Hochbruck & Ostermann, 2005) and their extensive application in diffusion\nmodels (Zhang & Chen, 2022; Lu et al., 2022b; Gonzalez et al., 2023). We present the derivations\nof our high-order solvers in Appendix D, and the detailed algorithm in Appendix E.\n5\nRELATED WORK\nWe present detailed related work in Appendix A, including diffusion models, diffusion bridge mod-\nels, and fast sampling techniques. We additionally discuss some special cases of DBIM and their\nconnection to flow matching, DDIM and posterior sampling in Appendix C.3.\n6\nEXPERIMENTS\nIn this section, we show that DBIMs surpass the original sampling procedure of DDBMs by a large\nmargin, in terms of both sample quality and sample efficiency. We also showcase DBIM‚Äôs capabil-\nities in latent-space encoding, reconstruction, and interpolation using deterministic sampling. All\ncomparisons between DBIMs and DDBMs are conducted using identically trained models. For\nDDBMs, we employ their proposed hybrid sampler for sampling. For DBIMs, we control the vari-\nance parameter œÅ by interpolating between its boundary selections:\nœÅn = Œ∑œÉtn\ns\n1 ‚àíSNRtn+1\nSNRtn\n,\nŒ∑ ‚àà[0, 1]\n(18)\nwhere Œ∑ = 0 and Œ∑ = 1 correspond to deterministic sampling and Markovian stochastic sampling.\nWe conduct experiments including (1) image-to-image translation tasks on Edges‚ÜíHandbags (Isola\net al., 2017) (64 √ó 64) and DIODE-Outdoor (Vasiljevic et al., 2019) (256 √ó 256) (2) image restora-\ntion task of inpainting on ImageNet (Deng et al., 2009) (256 √ó 256) with 128 √ó 128 center mask.\n7\nPublished as a conference paper at ICLR 2025\nTable 2: Quantitative results in the image translation task. ‚Ä†Baseline results are taken directly from\nDDBMs, where they did not report the exact NFE. Gray-colored rows denote methods that do not\nrequire paired training but only a prior diffusion model trained on the target domain.\nEdges‚ÜíHandbags (64 √ó 64)\nDIODE-Outdoor (256 √ó 256)\nNFE\nFID ‚Üì\nIS ‚Üë\nLPIPS ‚Üì\nMSE ‚Üì\nFID ‚Üì\nIS ‚Üë\nLPIPS ‚Üì\nMSE ‚Üì\nDDIB (Su et al., 2022)\n‚â•40‚Ä†\n186.84\n2.04\n0.869\n1.05\n242.3\n4.22\n0.798\n0.794\nSDEdit (Meng et al., 2022)\n‚â•40\n26.5\n3.58\n0.271\n0.510\n31.14\n5.70\n0.714\n0.534\nPix2Pix (Isola et al., 2017)\n1\n74.8\n3.24\n0.356\n0.209\n82.4\n4.22\n0.556\n0.133\nI2SB (Liu et al., 2023b)\n‚â•40\n7.43\n3.40\n0.244\n0.191\n9.34\n5.77\n0.373\n0.145\nDDBM (Zhou et al., 2023)\n118\n1.83\n3.73\n0.142\n0.040\n4.43\n6.21\n0.244\n0.084\nDDBM (Zhou et al., 2023)\n200\n0.88\n3.69\n0.110\n0.006\n3.34\n5.95\n0.215\n0.020\nDBIM (Ours)\n20\n1.74\n3.63\n0.095\n0.005\n4.99\n6.10\n0.201\n0.017\nDBIM (Ours)\n100\n0.89\n3.62\n0.100\n0.006\n2.57\n6.06\n0.198\n0.018\nTable 3: Quantative results in the image\nrestoration task.\nInpainting\nImageNet (256 √ó 256)\nCenter (128 √ó 128)\nNFE\nFID ‚Üì\nCA ‚Üë\nDDRM (Kawar et al., 2022)\n20\n24.4\n62.1\nŒ†GDM (Song et al., 2023a)\n100\n7.3\n72.6\nDDNM (Wang et al., 2023)\n100\n15.1\n55.9\nPalette (Saharia et al., 2022)\n1000\n6.1\n63.0\nI2SB (Liu et al., 2023b)\n10\n5.24\n66.1\nI2SB (Liu et al., 2023b)\n20\n4.98\n65.9\nI2SB (Liu et al., 2023b)\n1000\n4.9\n66.1\nDDBM (Zhou et al., 2023)\n500\n4.27\n71.8\nDBIM (Ours)\n10\n4.48\n71.3\nDBIM (Ours)\n20\n4.07\n72.3\nDBIM (Ours)\n100\n3.88\n72.7\nTable 4: Ablation of the variance parameter controlled\nby Œ∑ for image restoration, measured by FID.\nSampler\nNFE\n5\n10\n20\n50\n100\n200\n500\nInpainting, ImageNet (256 √ó 256), Center (128 √ó 128)\nŒ∑\n0.0\n6.08\n4.51\n4.11\n3.95\n3.91\n3.91\n3.91\n0.3\n6.12\n4.48\n4.09\n3.95\n3.92\n3.90\n3.88\n0.5\n6.25\n4.52\n4.07\n3.92\n3.90\n3.84\n3.86\n0.8\n6.81\n4.79\n4.16\n3.91\n3.88\n3.84\n3.81\n1.0\n8.62\n5.61\n4.51\n4.05\n3.91\n3.80\n3.80\nDDBM\n275.25\n57.18\n29.65\n10.63\n6.46\n4.95\n4.27\nWe report the Fr¬¥echet inception distance (FID) (Heusel et al., 2017) for all experiments, and addi-\ntionally measure Inception Scores (IS) (Barratt & Sharma, 2018), Learned Perceptual Image Patch\nSimilarity (LPIPS) (Zhang et al., 2018), Mean Square Error (MSE) (for image-to-image translation)\nand Classifier Accuracy (CA) (for image inpainting), following previous works (Liu et al., 2023b;\nZhou et al., 2023). The metrics are computed using the complete training set for Edges‚ÜíHandbags\nand DIODE-Outdoor, and 10k images from validation set for ImageNet. We provide the inference\ntime comparison in Appendix G.1. Additional experiment details are provided in Appendix F.\n6.1\nSAMPLE QUALITY AND EFFICIENCY\nWe present the quantitative results of DBIMs in Table 2 and Table 3, compared with baselines\nincluding GAN-based, diffusion-based and bridge-based methods3. We set the number of function\nevaluations (NFEs) of DBIM to 20 and 100 to demonstrate both efficiency at small NFEs and quality\nat large NFEs. We select Œ∑ from the set [0.0, 0.3, 0.5, 0.8, 1.0] for DBIM and report the best results.\nIn image translation tasks, DDBM achieves the best sample quality (measured by FID) among the\nbaselines, but requires NFE > 100. In contrast, DBIM with only NFE = 20 already surpasses\nall baselines, performing better than or on par with DDBM at NFE = 118. When increasing the\nNFE to 100, DBIM further improves the sample quality and outperforms DDBM with NFE = 200\non DIODE-Outdoor. In the more challenging image inpainting task on ImageNet 256 √ó 256, the\nsuperiority of DBIM is highlighted even further. In particular, DBIM with NFE = 20 outperforms\nall baselines, including DDBM with NFE = 500, achieving a 25√ó speed-up. With NFE = 100,\nDBIM continues to improve sample quality, reaching a FID lower than 4 for the first time.\nThe comparison of visual quality is illustrated in Figure 1 and Figure 3, where DBIM produces\nsmoother outputs with significantly fewer noisy artifacts compared to DDBM‚Äôs hybrid sampler.\nAdditional samples are provided in Appendix H.\nAblation of the Variance Parameter We investigate the impact of the variance parameter œÅ (con-\ntrolled by Œ∑) to identify how the level of stochasticity affects sample quality across various NFEs, as\nshown in Table 4 and Table 5. For image translation tasks, we consistently observe that employing\n3It is worth noting that, the released checkpoints of I2SB are actually flow matching/interpolant models\ninstead of bridge models, as they (1) start with noisy conditions instead of clean conditions and (2) perform a\nstraight interpolation between the condition and the sample without adding extra intermediate noise.\n8\nPublished as a conference paper at ICLR 2025\nTable 5: Ablation of the variance parameter controlled by Œ∑ for image translation, measured by FID.\nSampler\nNFE\n5\n10\n20\n50\n100\n200\n500\n5\n10\n20\n50\n100\n200\n500\nImage Translation, Edges‚ÜíHandbags (64 √ó 64)\nImage Translation, DIODE-Outdoor (256 √ó 256)\nŒ∑\n0.0\n3.62\n2.49\n1.76\n1.17\n0.91\n0.75\n0.65\n14.25\n7.96\n4.97\n3.18\n2.56\n2.26\n2.10\n0.3\n3.64\n2.53\n1.81\n1.21\n0.94\n0.76\n0.65\n14.48\n8.25\n5.22\n3.37\n2.68\n2.33\n2.12\n0.5\n3.69\n2.61\n1.91\n1.30\n1.00\n0.81\n0.67\n14.93\n8.75\n5.68\n3.71\n2.92\n2.47\n2.17\n0.8\n3.87\n2.91\n2.25\n1.58\n1.23\n0.96\n0.76\n16.41\n10.30\n6.98\n4.63\n3.58\n2.90\n2.41\n1.0\n4.21\n3.38\n2.72\n1.96\n1.50\n1.15\n0.85\n19.17\n12.59\n8.85\n5.98\n4.55\n3.59\n2.82\nDDBM\n317.22\n137.15\n46.74\n7.79\n2.40\n0.88\n0.53\n328.33\n151.93\n41.03\n15.19\n6.54\n3.34\n2.26\na deterministic sampler with Œ∑ = 0 yields superior performance compared to stochastic samplers\nwith Œ∑ > 0. We attribute it to the characteristics of the datasets, where the target image is highly\ncorrelated with and dependent on the condition, resulting in a generative model that lacks diver-\nsity. In this case, a straightforward mapping without the involvement of stochasticity is preferred.\nConversely, for image inpainting on the more diverse dataset ImageNet 256√ó256, the parameter Œ∑\nexhibits significance across different NFEs. When NFE ‚â§20, Œ∑ = 0 is near the optimal choice, with\nFID steadily increasing as Œ∑ ascends. However, when NFE ‚â•50, a relatively large level of stochas-\nticity at Œ∑ = 0.8 or even Œ∑ = 1 yields optimal FID. Notably, the FID of Œ∑ = 0 converges to 3.91\nat NFE = 100, with no further improvement at larger NFEs, indicating convergence to the ground-\ntruth sample by the corresponding PF-ODE. This observation aligns with diffusion models, where\ndeterministic sampling facilitates rapid convergence, while introducing stochasticity in sampling\nenhances diversity, ultimately culminating in the highest sample quality when NFE is substantial.\nTable 6: The effects of high-order methods, measured by FID.\nSampler\nNFE\n5\n10\n20\n50\n100\n5\n10\n20\n50\n100\n5\n10\n20\n50\n100\nImage Translation\nInpainting\nEdges‚ÜíHandbags (64 √ó 64)\nDIODE-Outdoor (256 √ó 256)\nImageNet (256 √ó 256)\nDBIM (Œ∑ = 0)\n3.62\n2.49\n1.76\n1.17\n0.91\n14.25\n7.96\n4.97\n3.18\n2.56\n6.08\n4.51\n4.11\n3.95\n3.91\nDBIM (2nd-order)\n3.44\n2.16\n1.48\n0.99\n0.79\n13.54\n7.18\n4.34\n2.87\n2.41\n5.53\n4.33\n4.07\n3.94\n3.91\nDBIM (3rd-order)\n3.40\n2.12\n1.45\n0.97\n0.79\n13.41\n7.01\n4.20\n2.84\n2.40\n5.50\n4.34\n4.07\n3.93\n3.91\nHigh-Order Methods\nWe further demonstrate the effects of high-order methods by comparing\nthem to deterministic DBIM, the first-order case. As shown in Table 6, high-order methods consis-\ntently improve FID scores in image translation tasks, as well as in inpainting tasks when NFE‚â§50,\nresulting in enhanced generation quality in the low NFE regime. Besides, the 3rd-order variant\nperforms slightly better than the 2nd-order variant. However, in contrast to the numerical solvers\nin diffusion models, the benefits of high-order extensions are relatively minor in diffusion bridges\nand less pronounced than the improvement when adjusting Œ∑ from 1 to 0. Nevertheless, high-order\nDBIMs are significantly more efficient than DDBM‚Äôs PF-ODE-based high-order solvers.\nAs illustrated in Figure 1, our high-order sampler produces images of similar semantic content to\nthe first-order case, using the same booting noise. In contrast, the visual quality is improved with\nfiner textures, resulting in better FID. This indicates that the high-order gradient information from\npast network outputs benefits the generation quality by adding high-frequency visual details.\nGeneration Diversity\nWe quantitatively measure the generation diversity by the diversity score,\ncalculated as the pixel-level variance of multiple generations, following CMDE (Batzolis et al.,\n2021) and BBDM (Li et al., 2023). As detailed in Appendix G.2, increasing NFE or decreasing Œ∑\ncan both increase the diversity score, confirming the effect of the booting noise.\n6.2\nRECONSTRUCTION AND INTERPOLATION\nAs discussed in Section 4.2, the deterministic nature of DBIMs at Œ∑ = 0 and its connection to neural\nODEs enable faithful encoding and reconstruction by treating the booting noise as the latent variable.\nFurthermore, employing spherical linear interpolation in the latent space and subsequently decoding\n9\nPublished as a conference paper at ICLR 2025\n20 steps\nCondition\n20 steps\n100 steps\n100 steps\n(a) Encoding/Reconstruction\n(b) Semantic Interpolation\nFigure 4: Illustration of generation diversity with deterministic DBIMs.\nback to the image space allows for semantic image interpolation in image translation and image\nrestoration tasks. These capabilities cannot be achieved by DBIMs with Œ∑ > 0, or by DDBM‚Äôs hy-\nbrid sampler which incorporates stochastic steps. We showcase the encoding and decoding results in\nFigure 4a, indicating that accurate reconstruction is achievable with a sufficient number of sampling\nsteps. We also illustrate the interpolation process in Figure 4b.\n7\nCONCLUSION\nIn this work, we introduce diffusion bridge implicit models (DBIMs) for accelerated sampling of\nDDBMs without extra training. In contrast to DDBM‚Äôs continuous-time generation processes, we\nconcentrate on discretized sampling steps and propose a series of generalized diffusion bridge mod-\nels including non-Markovian variants. The induced sampling procedures serve as bridge counter-\nparts and extensions of DDIMs and are further extended to develop high-order numerical solvers,\nfilling the missing perspectives in the context of diffusion bridges. Experiments on high-resolution\ndatasets and challenging inpainting tasks demonstrate DBIM‚Äôs superiority in both the sample quality\nand sample efficiency, achieving state-of-the-art FID scores with 100 steps and providing up to 25√ó\nacceleration of DDBM‚Äôs sampling procedure.\nFigure 5: DBIM case\n(Œ∑ = 0, NFE=500).\nLimitations and Failure Cases Despite the notable speed-up for diffusion\nbridge models, DBIMs still lag behind GAN-based methods in one-step gen-\neration. The generation quality is unsatisfactory when NFE is small, and\nblurry regions still exist even using high-order methods (Figure 1). This is\nnot fast enough for real-time applications. Besides, as a training-free infer-\nence algorithm, DBIM cannot surpass the capability and quality upper bound\nof the pretrained diffusion bridge model. In difficult and delicate inpainting\nscenarios, such as human faces and hands, DBIM fails to fix the artifacts, even\nunder large NFEs.\nACKNOWLEDGMENTS\nThis work was supported by the NSFC Projects (Nos. 62350080, 62106120, 92270001), the Na-\ntional Key Research and Development Program of China (No. 2021ZD0110502), Tsinghua Institute\nfor Guo Qiang, and the High Performance Computing Center, Tsinghua University. J.Z was also\nsupported by the XPlorer Prize.\nREFERENCES\nMichael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying\nframework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\nFan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao,\nShilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-\nvideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.\nShane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,\n2018.\nGeorgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch¬®onlieb, and Christian Etmann. Conditional\nimage generation with score-based diffusion models. arXiv preprint arXiv:2111.13606, 2021.\n10\nPublished as a conference paper at ICLR 2025\nChristopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, vol-\nume 4. Springer, 2006.\nMari Paz Calvo and C¬¥esar Palencia. A class of explicit multistep exponential integrators for semi-\nlinear problems. Numerische Mathematik, 102:367‚Äì381, 2006.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wave-\ngrad: Estimating gradients for waveform generation. In International Conference on Learning\nRepresentations, 2021a.\nTianrong Chen, Guan-Horng Liu, and Evangelos A Theodorou.\nLikelihood training of schr\\‚Äù\nodinger bridge using forward-backward sdes theory. arXiv preprint arXiv:2110.11291, 2021b.\nZehua Chen, Guande He, Kaiwen Zheng, Xu Tan, and Jun Zhu. Schrodinger bridges beat diffusion\nmodels on text-to-speech synthesis. arXiv preprint arXiv:2312.03491, 2023.\nZixiang Chen, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, and Quanquan Gu. Fast\nsampling via discrete non-markov diffusion models with predetermined transition time. In The\nThirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\nHyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul\nYe. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022.\nValentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr¬®odinger\nbridge with applications to score-based generative modeling. Advances in Neural Information\nProcessing Systems, 34:17695‚Äì17709, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248‚Äì255. IEEE, 2009.\nWei Deng, Weijian Luo, Yixin Tan, Marin BiloÀás, Yu Chen, Yuriy Nevmyvaka, and Ricky TQ Chen.\nVariational schr\\‚Äù odinger diffusion models. arXiv preprint arXiv:2405.04795, 2024.\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis.\nIn Advances in Neural Information Processing Systems, volume 34, pp. 8780‚Äì8794, 2021.\nJoseph L Doob and JI Doob. Classical potential theory and its probabilistic counterpart, volume\n262. Springer, 1984.\nMartin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, and Nader Masmoudi.\nSeeds: Exponential sde solvers for fast high-quality sampling from diffusion models.\narXiv\npreprint arXiv:2305.14267, 2023.\nAgrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang,\nand Jos¬¥e Lezama.\nPhotorealistic video generation with diffusion models.\narXiv preprint\narXiv:2312.06662, 2023.\nGuande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, and Jun Zhu. Consistency diffusion bridge\nmodels. arXiv preprint arXiv:2410.22637, 2024.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGANs trained by a two time-scale update rule converge to a local Nash equilibrium.\nIn Is-\nabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.\nVishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems,\nvolume 30, pp. 6626‚Äì6637, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances\nin Neural Information Processing Systems, volume 33, pp. 6840‚Äì6851, 2020.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n11\nPublished as a conference paper at ICLR 2025\nMarlis Hochbruck and Alexander Ostermann. Explicit exponential Runge-Kutta methods for semi-\nlinear parabolic problems. SIAM Journal on Numerical Analysis, 43(3):1069‚Äì1090, 2005.\nMarlis Hochbruck, Alexander Ostermann, and Julia Schweitzer. Exponential rosenbrock-type meth-\nods. SIAM Journal on Numerical Analysis, 47(1):786‚Äì803, 2009.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditional adversarial networks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1125‚Äì1134, 2017.\nKiyosi ItÀÜo. On a formula concerning stochastic differentials. Nagoya Mathematical Journal, 3:\n55‚Äì65, 1951.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\nbased generative models. In Advances in Neural Information Processing Systems, 2022.\nBahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration\nmodels. In Advances in Neural Information Processing Systems, 2022.\nDongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka,\nYutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning prob-\nability flow ode trajectory of diffusion. In The Twelfth International Conference on Learning\nRepresentations, 2023.\nDiederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In\nAdvances in Neural Information Processing Systems, 2021.\nBo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Bbdm: Image-to-image translation with brownian\nbridge diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern Recognition, pp. 1952‚Äì1961, 2023.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching\nfor generative modeling. arXiv preprint arXiv:2210.02747, 2022.\nGongye Liu, Haoze Sun, Jiayi Li, Fei Yin, and Yujiu Yang. Accelerating diffusion models for inverse\nproblems through shortcut sampling. arXiv preprint arXiv:2305.16965, 2023a.\nGuan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima\nAnandkumar. I2sb: Image-to-image schr¬®odinger bridge. In International Conference on Ma-\nchine Learning, pp. 22042‚Äì22062. PMLR, 2023b.\nCheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood\ntraining for score-based diffusion odes by high order denoising score matching. In International\nConference on Machine Learning, pp. 14429‚Äì14460. PMLR, 2022a.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural\nInformation Processing Systems, 2022b.\nChenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit:\nImage synthesis and editing with stochastic differential equations. In International Conference\non Learning Representations, 2022.\nShakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv\npreprint arXiv:1610.03483, 2016.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and\nediting with text-guided diffusion models. In International Conference on Machine Learning, pp.\n16784‚Äì16804. PMLR, 2022.\nKushagra Pandey, Maja Rudolph, and Stephan Mandt. Efficient integrators for diffusion generative\nmodels. arXiv preprint arXiv:2310.07894, 2023.\n12\nPublished as a conference paper at ICLR 2025\nKushagra Pandey, Ruihan Yang, and Stephan Mandt. Fast samplers for inverse problems in iterative\nrefinement models. arXiv preprint arXiv:2405.17673, 2024.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¬®orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 10684‚Äì10695, 2022.\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David\nFleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH\n2022 Conference Proceedings, pp. 1‚Äì10, 2022.\nAxel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion dis-\ntillation. arXiv preprint arXiv:2311.17042, 2023.\nYuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr¬®odinger\nbridge matching. Advances in Neural Information Processing Systems, 36, 2024.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learn-\ning, pp. 2256‚Äì2265. PMLR, 2015.\nVignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause,\nand Charlotte Bunne. Aligned diffusion schr¬®odinger bridges. In Uncertainty in Artificial Intelli-\ngence, pp. 1985‚Äì1995. PMLR, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-\ntional Conference on Learning Representations, 2021a.\nJiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion\nmodels for inverse problems. In International Conference on Learning Representations, 2023a.\nURL https://openreview.net/forum?id=9_gsMA8MRKQ.\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-\nbased diffusion models. In Advances in Neural Information Processing Systems, volume 34, pp.\n1415‚Äì1428, 2021b.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In Interna-\ntional Conference on Learning Representations, 2021c.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International\nConference on Machine Learning, pp. 32211‚Äì32252. PMLR, 2023b.\nXuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for\nimage-to-image translation. arXiv preprint arXiv:2203.08382, 2022.\nIgor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z Dai, Andrea F\nDaniele, Mohammadreza Mostajabi, Steven Basart, Matthew R Walter, et al. Diode: A dense\nindoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019.\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural compu-\ntation, 23(7):1661‚Äì1674, 2011.\nYinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion\nnull-space model. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=mRieQgMtNTQ.\nYuang Wang, Siyeop Yoon, Pengfei Jin, Matthew Tivnan, Zhennong Chen, Rui Hu, Li Zhang,\nZhiqiang Chen, Quanzheng Li, and Dufan Wu. Implicit image-to-image schrodinger bridge for\nimage restoration. arXiv preprint arXiv:2403.06069, 2024a.\n13\nPublished as a conference paper at ICLR 2025\nYufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu,\nYu Qiao, Alex C Kot, and Bihan Wen. Sinsr: diffusion-based image super-resolution in a single\nstep. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 25796‚Äì25805, 2024b.\nJintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen.\nSageatten-\ntion2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. arXiv\npreprint arXiv:2411.10958, 2024.\nJintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit\nattention for plug-and-play inference acceleration. In International Conference on Learning Rep-\nresentations (ICLR), 2025a.\nJintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei\nChen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint\narXiv:2502.18137, 2025b.\nQinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator.\nIn The Eleventh International Conference on Learning Representations, 2022.\nQinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit\nmodels. arXiv preprint arXiv:2206.05564, 2022.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 586‚Äì595, 2018.\nKaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode\nsolver with empirical model statistics. In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023a.\nKaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood\nestimation for diffusion odes. In International Conference on Machine Learning, pp. 42363‚Äì\n42389. PMLR, 2023b.\nKaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked\ndiffusion models are secretly time-agnostic masked models and exploit inaccurate categorical\nsampling. arXiv preprint arXiv:2409.02908, 2024.\nLinqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models.\narXiv preprint arXiv:2309.16948, 2023.\n14\nPublished as a conference paper at ICLR 2025\nA\nRELATED WORK\nFast Sampling of Diffusion Models\nFast sampling of diffusion models can be classified into\ntraining-free and training-based methods. A prevalent training-free fast sampler is the denoising\ndiffusion implicit models (DDIMs) (Song et al., 2021a) that employ alternative non-Markovian gen-\neration processes in place of DDPMs, a discrete-time diffusion model. ScoreSDE (Song et al.,\n2021c) further links discrete-time DDPMs to continuous-time score-based models, unrevealing the\ngeneration process to be ordinary and stochastic differential equations (ODEs and SDEs). DDIM\ncan be generalized to develop integrators for broader diffusion models (Zhang et al., 2022; Pandey\net al., 2023). The concept of implicit sampling, in a broad sense, can also be extended to discrete\ndiffusion models (Chen et al., 2024; Zheng et al., 2024), although there are fundamental differences\nin their underlying mechanisms. Subsequent training-free samplers concentrate on developing ded-\nicated numerical solvers to the diffusion ODE or SDE, particularly Heun‚Äôs methods (Karras et al.,\n2022) and exponential integrators (Zhang & Chen, 2022; Lu et al., 2022b; Zheng et al., 2023a; Gon-\nzalez et al., 2023). These methods typically require around 10 steps for high-quality generation. In\ncontrast, training-based methods, particularly adversarial distillation (Sauer et al., 2023) and consis-\ntency distillation (Song et al., 2023b; Kim et al., 2023), become notable for their ability to achieve\nhigh-quality generation with just one or two steps. Our work serves as a thorough exploration of\ntraining-free fast sampling of DDBMs. Exploring bridge distillation methods, such as consistency\nbridge distillation (He et al., 2024), would be promising future research avenues to decrease the\ninference cost further. Infrastructure improvements, such as quantized or sparse attention (Zhang\net al., 2025a;b; 2024), can also be used to accelerate the inference of diffusion bridge models.\nDiffusion Bridges\nDiffusion bridges (De Bortoli et al., 2021; Chen et al., 2021b; Liu et al., 2023b;\nSomnath et al., 2023; Zhou et al., 2023; Chen et al., 2023; Shi et al., 2024; Deng et al., 2024) are a\npromising generative variant of diffusion models for modeling the transport between two arbitrary\ndistributions. One line of work is the diffusion Schrodinger bridge models (De Bortoli et al., 2021;\nChen et al., 2021b; Shi et al., 2024; Deng et al., 2024), which solves an entropy-regularized opti-\nmal transport problem between two probability distributions. However, their reliance on expensive\niterative procedures has limited their application scope, particularly for high-dimensional data. Sub-\nsequent works have endeavored to enhance the tractability of the Schrodinger bridge problem by\nmaking assumptions such as paired data (Liu et al., 2023b; Somnath et al., 2023; Chen et al., 2023).\nOn the other hand, DDBMs (Zhou et al., 2023) construct diffusion bridges via Doob‚Äôs h-transform,\noffering a reverse-time perspective of a diffusion process conditioned on given endpoints. This ap-\nproach aligns the design spaces and training algorithms of DDBMs closely with those of score-based\ngenerative models, leading to state-of-the-art performance in image translation tasks. However, the\nsampling procedure of DDBMs still relies on inefficient simulations of differential equations, lack-\ning theoretical insights to develop efficient samplers. BBDM (Li et al., 2023) and I3SB (Wang\net al., 2024a) extend the concept of DDIM to the contexts of Brownian bridge and I2SB (Liu et al.,\n2023b), respectively. SinSR (Wang et al., 2024b) is also motivated by DDIM, while the application\nis concentrated on the mean-reverting diffusion process, which ends in a Gaussian instead of a delta\ndistribution. In contrast to them, our work provides the first systematic exploration of implicit sam-\npling within the broader DDBM framework, offering theoretical insights and connections while also\nproposing novel high-order diffusion bridge solvers.\nB\nPROOFS\nB.1\nPROOF OF PROPOSITION 3.1\nProof. Since q(œÅ) in Eqn. (10) is factorized as q(œÅ)(xt0:N‚àí1|xT ) = q0(x0)q(œÅ)(xt1:N‚àí1|x0, xT )\nwhere q(œÅ)(xt1:N‚àí1|x0, xT ) = QN‚àí1\nn=1 q(œÅ)(xtn|x0, xtn+1, xT ), we have q(œÅ)(x0|xT ) = q0(x0) =\nq(x0|xT ), which proves the case for n = 0. For 1 ‚â§n ‚â§N ‚àí1, we have\nq(œÅ)(xtn|xT ) =\nZ\nq(œÅ)(xtn|x0, xT )q(œÅ)(x0|xT )dx0\n(19)\nand\nq(xtn|xT ) =\nZ\nq(xtn|x0, xT )q(x0|xT )dx0\n(20)\n15\nPublished as a conference paper at ICLR 2025\nSince q(œÅ)(x0|xT ) = q(x0|xT ), we only need to prove q(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ).\nFirstly, when n = N ‚àí1, we have tn+1 = T. Note that œÅ is restricted by œÅN‚àí1 = ctN‚àí1, and\nEqn. (11) becomes\nq(œÅ)(xtN‚àí1|x0, xT ) = N(atN‚àí1xT + btN‚àí1x0, c2\ntN‚àí1I)\n(21)\nwhich is exactly the same as the forward transition kernel of q in Eqn. (6).\nTherefore,\nq(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ) holds for n = N ‚àí1.\nSecondly, suppose q(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ) holds for n = k, we aim to prove that it\nholds for n = k ‚àí1. Specifically, q(œÅ)(xtk‚àí1|x0, xT ) can be expressed as\nq(œÅ)(xtk‚àí1|x0, xT ) =\nZ\nq(œÅ)(xtk‚àí1|x0, xtk, xT )q(œÅ)(xtk|x0, xT )dxtk\n=\nZ\nq(œÅ)(xtk‚àí1|x0, xtk, xT )q(xtk|x0, xT )dxtk\n=\nZ\nN(xtk‚àí1; ¬µk‚àí1|k, œÅ2\nk‚àí1I)N(xtk; atkxT + btkx0, c2\ntkI)dxtk\n(22)\nwhere\n¬µk‚àí1|k = atk‚àí1xT + btk‚àí1x0 +\nq\nc2\ntk‚àí1 ‚àíœÅ2\nk‚àí1\nxtk ‚àíatkxT ‚àíbtkx0\nctk\n(23)\nFrom (Bishop & Nasrabadi, 2006) (2.115), q(œÅ)(xtk‚àí1|x0, xT ) is a Gaussian, denoted as\nN(¬µk‚àí1, Œ£k‚àí1), where\n¬µk‚àí1 = atk‚àí1xT + btk‚àí1x0 +\nq\nc2\ntk‚àí1 ‚àíœÅ2\nk‚àí1\natkxT + btkx0 ‚àíatkxT ‚àíbtkx0\nctk\n= atk‚àí1xT + btk‚àí1x0\n(24)\nand\nŒ£k‚àí1 = œÅ2\nk‚àí1I +\nq\nc2\ntk‚àí1 ‚àíœÅ2\nk‚àí1\nctk\nc2\ntk\nq\nc2\ntk‚àí1 ‚àíœÅ2\nk‚àí1\nctk\nI\n= c2\ntk‚àí1I\n(25)\nTherefore, q(œÅ)(xtk‚àí1|x0, xT ) = q(xtk‚àí1|x0, xT ) = N(atk‚àí1xT + btk‚àí1x0, c2\ntk‚àí1I). By math-\nematical deduction, q(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ) holds for every 1 ‚â§n ‚â§N ‚àí1, which\ncompletes the proof.\nB.2\nPROOF OF PROPOSITION 3.2\nProof. Substituting Eqn. (10) and the joint distribution into Eqn. (13), we have\nJ (œÅ)(Œ∏)\n=Eq(xT )Eq(œÅ)(xt0:N‚àí1|xT )\nh\nlog q(œÅ)(xt1:N‚àí1|x0, xT ) ‚àílog pŒ∏(xt0:N‚àí1|xT )\ni\n=Eq(xT )Eq(œÅ)(xt0:N‚àí1|xT )\n\"N‚àí1\nX\nn=1\nlog q(œÅ)(xtn|x0, xtn+1, xT ) ‚àí\nN‚àí1\nX\nn=0\nlog pŒ∏(xtn|xtn+1, xT )\n#\n=\nN‚àí1\nX\nn=1\nEq(xT )Eq(œÅ)(x0,xtn+1|xT )\nh\nDKL(q(œÅ)(xtn|x0, xtn+1, xT ) ‚à•pŒ∏(xtn|xtn+1, xT ))\ni\n‚àíEq(xT )Eq(œÅ)(x0,xt1|xT ) [log pŒ∏(x0|xt1, xT )]\n(26)\nwhere\nDKL(q(œÅ)(xtn|x0, xtn+1, xT ) ‚à•pŒ∏(xtn|xtn+1, xT ))\n=DKL(q(œÅ)(xtn|x0, xtn+1, xT ) ‚à•q(œÅ)(xtn|xŒ∏(xtn+1, tn+1, xT ), xtn+1, xT ))\n=d2\nn‚à•xŒ∏(xtn+1, tn+1, xT ) ‚àíx0‚à•2\n2\n2œÅ2n\n(27)\n16\nPublished as a conference paper at ICLR 2025\nwhere we have denoted dn := btn ‚àí\nq\nc2\ntn ‚àíœÅ2n\nbtn+1\nctn+1 . Besides, we have\nlog pŒ∏(x0|xt1, xT ) = log q(œÅ)(x0|xŒ∏(xt1, t1, xT ), xt1, xT )\n= log N(xŒ∏(xt1, t1, xT ), œÅ2\n0I)\n= ‚àí‚à•xŒ∏(xt1, t1, xT ) ‚àíx0‚à•2\n2\n2œÅ2\n0\n+ C\n(28)\nwhere C is irrelevant to Œ∏. According to Eqn. (6), the conditional score is\n‚àáxt log q(xt|x0, xT ) = ‚àíxt ‚àíatxT ‚àíbtx0\nc2\nt\n(29)\nTherefore,\n‚à•xŒ∏(xtn, tn, xT ) ‚àíx0‚à•2\n2\n=c4\ntn\nb2\ntn\n\r\r\r\r‚àíxtn ‚àíatnxT ‚àíbtnxŒ∏(xtn, tn, xT )\nc2\ntn\n‚àí\n\u0012\n‚àíxtn ‚àíatnxT ‚àíbtnx0\nc2\ntn\n\u0013\r\r\r\r\n2\n2\n=c4\ntn\nb2\ntn\n‚à•sŒ∏(xtn, tn, xT ) ‚àí‚àáxtn log q(xtn|x0, xT )‚à•2\n2\n(30)\nwhere sŒ∏ is related to xŒ∏ by\nsŒ∏(xt, t, xT ) = ‚àíxt ‚àíatxT ‚àíbtxŒ∏(xt, t, xT )\nc2\nt\n(31)\nDefine d0 = 1, the loss J (œÅ)(Œ∏) is further simplified to\nJ (œÅ)(Œ∏) ‚àíC\n=\nN‚àí1\nX\nn=0\nEq(xT )q(œÅ)(x0,xtn+1|xT )\n\u0014d2\nn‚à•xŒ∏(xtn+1, tn+1, xT ) ‚àíx0‚à•2\n2\n2œÅ2n\n\u0015\n=\nN\nX\nn=1\nd2\nn‚àí1\n2œÅ2\nn‚àí1\nEq(xT )q(x0|xT )q(xtn|x0,xT )\n\u0002\n‚à•xŒ∏(xtn, tn, xT ) ‚àíx0‚à•2\n2\n\u0003\n=\nN\nX\nn=1\nd2\nn‚àí1c4\ntn\n2œÅ2\nn‚àí1b2\ntn\nEq(xT )q(x0|xT )q(xtn|x0,xT )\n\u0002\n‚à•sŒ∏(xtn, tn, xT ) ‚àí‚àáxtn log q(xtn|x0, xT )‚à•2\n2\n\u0003\n(32)\nCompared to the training objective of DDBMs in Eqn. (9), J (œÅ)(Œ∏) is totally equivalent up to a\nconstant, by concentrating on the discretized timesteps {tn}N\nn=1, choosing q(xT )q(x0|xT ) as the\npaired data distribution and using the weighting function Œ≥ that satisfies Œ≥(tn) =\nd2\nn‚àí1c4\ntn\n2œÅ2\nn‚àí1b2\ntn .\nB.3\nPROOF OF PROPOSITION 4.1\nProof. We first represent the PF-ODE (Eqn. (8))\ndxt =\n\u0014\nf(t)xt ‚àíg2(t)\n\u00121\n2‚àáxt log q(xt|xT ) ‚àí‚àáxt log qT |t(xT |xt)\n\u0013\u0015\ndt\n(33)\nwith the data predictor xŒ∏(xt, t, xT ). We replace the bridge score ‚àáxt log q(xt|xT ) with the net-\nwork sŒ∏(xt, t, xT ), which is related to xŒ∏(xt, t, xT ) by Eqn. (14). Besides, ‚àáxt log qT |t(xT |xt)\n17\nPublished as a conference paper at ICLR 2025\ncan be analytically computed as\n‚àáxt log qT |t(xT |xt) = ‚àáxt log q(xt|x0, xT )q(xT |x0)\nq(xt|x0)\n= ‚àáxt log q(xt|x0, xT ) ‚àí‚àáxt log q(xt|x0)\n= ‚àíxt ‚àíatxT ‚àíbtx0\nc2\nt\n+ xt ‚àíŒ±tx0\nœÉ2\nt\n= ‚àí\nSNRT\nSNRt (xt ‚àíŒ±t\nŒ±T xT )\nœÉ2\nt (1 ‚àíSNRT\nSNRt )\n= ‚àí\nat( Œ±T\nŒ±t xt ‚àíxT )\nc2\nt\n(34)\nSubstituting Eqn. (14) and Eqn. (34) into Eqn. (33), the PF-ODE is transformed to\ndxt =\n\"\nf(t)xt ‚àíg2(t)\n \n‚àíxt ‚àíatxT ‚àíbtxŒ∏(xt, t, xT )\n2c2\nt\n+\nat( Œ±T\nŒ±t xt ‚àíxT )\nc2\nt\n!#\ndt\n=\n\u0014\u0012\nf(t) + g2(t)\n1 ‚àí2at\nŒ±T\nŒ±t\n2c2\nt\n\u0013\nxt + g2(t) at\n2c2\nt\nxT ‚àíg2(t) bt\n2c2\nt\nxŒ∏(xt, t, xT )\n\u0015\ndt\n=\n\u0014\u0012\nf(t) + g2(t)\nœÉ2\nt\n‚àíg2(t)\n2c2\nt\n\u0013\nxt + g2(t) at\n2c2\nt\nxT ‚àíg2(t) bt\n2c2\nt\nxŒ∏(xt, t, xT )\n\u0015\ndt\n(35)\nOn the other hand, the ODE corresponding to DBIMs (Eqn. (16)) can be expanded as\ndxt\nct\n‚àíc‚Ä≤\nt\nc2\nt\nxtdt =\n\"\u0012at\nct\n\u0013‚Ä≤\nxT +\n\u0012bt\nct\n\u0013‚Ä≤\nxŒ∏(xt, t, xT )\n#\ndt\n(36)\nwhere we have denoted (¬∑)‚Ä≤ := d(¬∑)\ndt . Further simplification gives\ndxt =\n\u0014c‚Ä≤\nt\nct\nxt +\n\u0012\na‚Ä≤\nt ‚àíat\nc‚Ä≤\nt\nct\n\u0013\nxT +\n\u0012\nb‚Ä≤\nt ‚àíbt\nc‚Ä≤\nt\nct\n\u0013\nxŒ∏(xt, t, xT )\n\u0015\ndt\n(37)\nThe coefficients at, bt, ct are determined by the noise schedule Œ±t, œÉt in diffusion models. Comput-\ning their derivatives will produce terms involving f(t), g(t), which are used to define the forward\nSDE. As revealed in diffusion models, f(t), g(t) are related to Œ±t, œÉt by f(t) = d log Œ±t\ndt\n,\ng2(t) =\ndœÉ2\nt\ndt ‚àí2 d log Œ±t\ndt\nœÉ2\nt . We can derive the reverse relation of Œ±t, œÉt and f(t), g(t):\nŒ±t = e\nR t\n0 f(œÑ)dœÑ,\nœÉ2\nt = Œ±2\nt\nZ t\n0\ng2(œÑ)\nŒ±2œÑ\ndœÑ\n(38)\nwhich can facilitate subsequent calculation. We first compute the derivative of a common term in\nat, bt, ct:\n\u0012\n1\nSNRt\n\u0013‚Ä≤\n=\n\u0012œÉ2\nt\nŒ±2\nt\n\u0013‚Ä≤\n= g2(t)\nŒ±2\nt\n(39)\nFor ct, since c2\nt = œÉ2\nt (1 ‚àíSNRT\nSNRt ), we have\nc‚Ä≤\nt\nct\n= (log ct)‚Ä≤ = 1\n2(log c2\nt)‚Ä≤ = 1\n2(log œÉ2\nt + log(1 ‚àíSNRT\nSNRt\n))‚Ä≤\n(40)\nwhere\n(log œÉ2\nt )‚Ä≤ = (log œÉ2\nt\nŒ±2\nt\n)‚Ä≤ + (log Œ±2\nt)‚Ä≤ = g2(t)\nŒ±2\nt\nŒ±2\nt\nœÉ2\nt\n+ 2f(t) = g2(t)\nœÉ2\nt\n+ 2f(t)\n(41)\nand\n(log(1 ‚àíSNRT\nSNRt\n))‚Ä≤ = ‚àíSNRT\n1 ‚àíSNRT\nSNRt\n\u0012\n1\nSNRt\n\u0013‚Ä≤\n= ‚àíSNRT\nc2\nt\nœÉ2\nt\ng2(t)\nŒ±2\nt\n= ‚àíg2(t)\nc2\nt\nSNRT\nSNRt\n(42)\n18\nPublished as a conference paper at ICLR 2025\nSubstituting Eqn. (41) and Eqn. (42) into Eqn. (40), and using the relation SNRT\nSNRt = 1 ‚àíc2\nt\nœÉ2\nt , we have\nc‚Ä≤\nt\nct\n= f(t) + g2(t)\n2œÉ2\nt\n‚àíg2(t)\n2c2\nt\nSNRT\nSNRt\n= f(t) + g2(t)\nœÉ2\nt\n‚àíg2(t)\n2c2\nt\n(43)\nFor at, since at = Œ±t\nŒ±T\nSNRT\nSNRt , we have\na‚Ä≤\nt\nat\n= (log at)‚Ä≤ = (log Œ±t)‚Ä≤ + (log SNRT\nSNRt\n)‚Ä≤ = f(t) + SNRt\ng2(t)\nŒ±2\nt\n= f(t) + g2(t)\nœÉ2\nt\n(44)\nFor bt, since bt = Œ±t(1 ‚àíSNRT\nSNRt ), we have\nb‚Ä≤\nt\nbt\n= (log bt)‚Ä≤ = (log Œ±t)‚Ä≤+(log(1‚àíSNRT\nSNRt\n))‚Ä≤ = f(t)‚àíg2(t)\nc2\nt\nSNRT\nSNRt\n= f(t)+g2(t)\nœÉ2\nt\n‚àíg2(t)\nc2\nt\n(45)\nTherefore,\na‚Ä≤\nt ‚àíat\nc‚Ä≤\nt\nct\n= at(a‚Ä≤\nt\nat\n‚àíc‚Ä≤\nt\nct\n) = g2(t)\n2c2\nt\nat\n(46)\nand\nb‚Ä≤\nt ‚àíbt\nc‚Ä≤\nt\nct\n= bt(b‚Ä≤\nt\nbt\n‚àíc‚Ä≤\nt\nct\n) = ‚àíg2(t)\n2c2\nt\nbt\n(47)\nSubstituting Eqn. (43), Eqn. (46) and Eqn. (47) into the ODE of DBIMs in Eqn. (37), we obtain\nexactly the PF-ODE in Eqn. (35).\nC\nMORE THEORETICAL DISCUSSIONS\nC.1\nMARKOV PROPERTY OF THE GENERALIZED DIFFUSION BRIDGES\nWe aim to analyze the Markov property of the forward process corresponding to our generalized\ndiffusion bridge in Section 3.1. The forward process of q(œÅ) can be induced by Bayes‚Äô rule as\nq(œÅ)(xtn+1|x0, xtn, xT ) = q(œÅ)(xtn|x0, xtn+1, xT )q(œÅ)(xtn+1|x0, xT )\nq(œÅ)(xtn|x0, xT )\n(48)\nwhere q(œÅ)(xt|x0, xT ) = q(xt|x0, xT ) is the marginal distribution of the diffusion bridge in\nEqn. (6), and q(œÅ)(xtn|x0, xtn+1, xT ) is defined in Eqn. (11) as\nq(œÅ)(xtn|x0, xtn+1, xT ) = N(atnxT + btnx0 +\nq\nc2\ntn ‚àíœÅ2n\nxtn+1 ‚àíatn+1xT ‚àíbtn+1x0\nctn+1\n, œÅ2\nnI).\n(49)\nDue to the marginal preservation property (Proposition 3.1), we have q(œÅ)(xtn+1|x0, xT ) =\nq(xtn+1|x0, xT ) and q(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ), where q(xt|x0, xT ) = N(atxT +\nbtx0, c2\ntI) is the forward transition kernel in Eqn. (6). To identify whether q(œÅ)(xtn+1|x0, xtn, xT )\nis Markovian, we only need to examine the dependence of xtn+1 on x0. To this end, we proceed to\nderive conditions under which ‚àáxtn+1 log q(œÅ)(xtn+1|x0, xtn, xT ) involves terms concerning x0.\nSpecifically, ‚àáxtn+1 log q(œÅ)(xtn+1|x0, xtn, xT ) can be calculated as\n‚àáxtn+1 log q(œÅ)(xtn+1|x0, xtn, xT )\n=‚àáxtn+1 log q(œÅ)(xtn|x0, xtn+1, xT ) + ‚àáxtn+1 log q(œÅ)(xtn+1|x0, xT )\n= ‚àí\nq\nc2\ntn ‚àíœÅ2n(atnxT + btnx0 +\nq\nc2\ntn ‚àíœÅ2n\nxtn+1‚àíatn+1xT ‚àíbtn+1x0\nctn+1\n‚àíxtn)\nctn+1œÅ2n\n‚àíxtn+1 ‚àíatn+1xT ‚àíbtn+1x0\nc2\ntn+1\n=\nbtn+1c2\ntn ‚àíbtnctn+1\nq\nc2\ntn ‚àíœÅ2n\nc2\ntn+1œÅ2n\nx0 + C(xtn, xtn+1, xT )\n(50)\n19\nPublished as a conference paper at ICLR 2025\nwhere C(xtn, xtn+1, xT ) are terms irrelevant to x0. Therefore,\nq(œÅ)(xtn+1|x0, xtn, xT ) is Markovian ‚áê‚áí\nbtn+1c2\ntn ‚àíbtnctn+1\nq\nc2\ntn ‚àíœÅ2n\nc2\ntn+1œÅ2n\n= 0\n‚áê‚áíœÅn = œÉtn\ns\n1 ‚àíSNRtn+1\nSNRtn\n(51)\nwhich is exactly a boundary choice of the variance parameter œÅ. Under the other boundary choice\nœÅn = 0 and intermediate ones satisfying 0 < œÅn < œÉtn\nq\n1 ‚àí\nSNRtn+1\nSNRtn , the forward process\nq(œÅ)(xtn+1|x0, xtn, xT ) is non-Markovian.\nC.2\nTHE INSIGNIFICANCE OF LOSS WEIGHTING IN TRAINING\nThe insignificance of the weighting mismatch in Proposition 3.2 can be interpreted from two aspects.\nOn the one hand, L consists of independent terms concerning individual timesteps (as long as the\nnetwork‚Äôs parameters are not shared across different t), ensuring that the global minimum remains\nthe same as minimizing the loss at each timestep, regardless of the weighting. On the other hand, L\nunder different weightings are mutually bounded by mint wt\nmaxt Œ≥t LŒ≥(Œ∏) ‚â§Lw(Œ∏) ‚â§maxt wt\nmint Œ≥t LŒ≥(Œ∏). Be-\nsides, it is widely acknowledged that in diffusion models, the weighting corresponding to variational\ninference may yield superior likelihood but suboptimal sample quality (Ho et al., 2020; Song et al.,\n2021c), which is not preferred in practice.\nC.3\nSPECIAL CASES AND RELATIONSHIP WITH PRIOR WORKS\nConnection to Flow Matching\nWhen the noise schedule Œ±t = 1, T = 1 and œÉt = ‚àöŒ≤t, the\nforward process becomes q(xt|x0, x1) = N(txT + (1 ‚àít)x0, Œ≤t(1 ‚àít)) which is a Brownian\nbridge. When Œ≤ ‚Üí0, there will be no intermediate noise and the forward process is similar to flow\nmatching (Lipman et al., 2022; Albergo et al., 2023). In this limit, the DBIM (Œ∑ = 1) updating\nrule from time t to time s < t will become xs = sxT + (1 ‚àís)xŒ∏(xt, t, xT ) =\ns\nt xt + (1 ‚àí\ns\nt )xŒ∏(xt, t) = xt ‚àí(t ‚àís)vŒ∏(xt, t). Here we define vŒ∏(xt, t) :=\nxt‚àíxŒ∏(xt,t)\nt\nas the velocity\nfunction of the probability flow (i.e., the drift of the ODE) in flow matching methods. Therefore, in\nthe flow matching case, DBIM is a simple Euler step of the flow.\nConnection to DDIM\nIn the regions where t is small and SNRT\nSNRt is close to 0, we have at ‚âà0, bt ‚âà\nŒ±t, ct ‚âàœÉt. Therefore, the forward process of DDBM in this case is approximately q(xt|x0, xT ) =\nN(Œ±tx0, œÉ2\nt I), which is the forward process of the corresponding diffusion model. Moreover, in\nthis case, the DBIM (Œ∑ = 0) step is approximately\nxs ‚âàœÉs\nœÉt\nxt + œÉs\n\u0012Œ±s\nœÉs\n‚àíŒ±t\nœÉt\n\u0013\nxŒ∏(xt, t, xT )\n(52)\nwhich is exactly DDIM (Song et al., 2021a), except that the data prediction network xŒ∏ is dependent\non xT . This indicates that when t is small so that the component of xT in xt is negligible, DBIM\nrecovers DDIM.\nConnection to Posterior Sampling\nThe previous work I2SB (Liu et al., 2023b) also employs\ndiffusion bridges with discrete timesteps, though their noise schedule is restricted to the variance\nexploding (VE) type with f(t) = 0 in the forward process. For generation, they adopt a sim-\nilar approach to DDPM (Ho et al., 2020) by iterative sampling from the posterior distribution\npŒ∏(xn‚àí1|xn), which is a parameterized and shortened diffusion bridge between the endpoints\nÀÜx0 = xŒ∏(xn, tn, xN) and xn.\nSince the posterior distribution is not conditioned on xT (ex-\ncept through the parameterized network), the corresponding forward diffusion bridge is Markovian.\nThus, the posterior sampling in I2SB is a special case of DBIM by setting Œ∑ = 0 and f(t) = 0.\nC.4\nPERSPECTIVE OF EXPONENTIAL INTEGRATORS\nExponential integrators (Calvo & Palencia, 2006; Hochbruck et al., 2009) are widely adopted in\nrecent works concerning fast sampling of diffusion models (Zhang & Chen, 2022; Zheng et al.,\n20\nPublished as a conference paper at ICLR 2025\n2023a; Gonzalez et al., 2023). Suppose we have an ODE\ndxt = [a(t)xt + b(t)FŒ∏(xt, t)]dt\n(53)\nwhere FŒ∏ is the parameterized prediction function that we want to approximate with Taylor ex-\npansion. The usual way of representing its analytic solution xt at time t with respect to an initial\ncondition xs at time s is\nxt = xs +\nZ t\ns\n[a(œÑ)xœÑ + b(œÑ)FŒ∏(xœÑ, œÑ)]dœÑ\n(54)\nBy approximating the involved integrals in Eqn. (54), we can obtain direct discretizations of\nEqn. (53) such as Euler‚Äôs method. The key insight of exponential integrators is that, it is often bet-\nter to utilize the ‚Äúsemi-linear‚Äù structure of Eqn. (53) and analytically cancel the linear term a(t)xt.\nThis way, we can obtain solutions that only involve integrals of FŒ∏ and result in lower discretization\nerrors. Specifically, by the ‚Äúvariation-of-constants‚Äù formula, the exact solution of Eqn. (53) can be\nalternatively given by\nxt = e\nR t\ns a(œÑ)dœÑxs +\nZ t\ns\ne\nR t\nœÑ a(r)drb(œÑ)FŒ∏(xœÑ, œÑ)dœÑ\n(55)\nWe can apply this transformation to the PF-ODE in DDBMs. By collecting the linear terms w.r.t.\nxt, Eqn. (8) can be rewritten as (already derived in Appendix B.3)\ndxt =\n\u0014\u0012\nf(t) + g2(t)\nœÉ2\nt\n‚àíg2(t)\n2c2\nt\n\u0013\nxt + g2(t) at\n2c2\nt\nxT ‚àíg2(t) bt\n2c2\nt\nxŒ∏(xt, t, xT )\n\u0015\ndt\n(56)\nBy corresponding it to Eqn. (53), we have\na(t) = f(t) + g2(t)\nœÉ2\nt\n‚àíg2(t)\n2c2\nt\n,\nb1(t) = g2(t) at\n2c2\nt\n,\nb2(t) = ‚àíg2(t) bt\n2c2\nt\n(57)\nFrom Eqn. (43), Eqn. (46) and Eqn. (47), we know\na(t) = d log ct\ndt\n,\nb1(t) = at\nd log(at/ct)\ndt\n,\nb2(t) = bt\nd log(bt/ct)\ndt\n(58)\nNote that these relations are known in advance when converting from our ODE to the PF-ODE.\nOtherwise, finding them in this inverse conversion will be challenging. The integrals in Eqn. (55)\ncan then be calculated as\nR t\ns a(œÑ)dœÑ = log ct ‚àílog cs. Thus\ne\nR t\ns a(œÑ)dœÑ = ct\ncs\n,\ne\nR t\nœÑ a(r)dr = ct\ncœÑ\n(59)\nTherefore, the exact solution in Eqn. (55) becomes\nxt = ct\ncs\nxs + ct\nZ t\ns\naœÑ\ncœÑ\nxT d log\n\u0012aœÑ\ncœÑ\n\u0013\n+ ct\nZ t\ns\nbœÑ\ncœÑ\nxŒ∏(xœÑ, œÑ, xT )d log\n\u0012bœÑ\ncœÑ\n\u0013\n= ct\ncs\nxs +\n\u0012\nat ‚àíct\ncs\nas\n\u0013\nxT + ct\nZ t\ns\nbœÑ\ncœÑ\nxŒ∏(xœÑ, œÑ, xT )d log\n\u0012bœÑ\ncœÑ\n\u0013\n(60)\nwhich is the same as Eqn. (17) after exchanging s and t and changing the time variable in the integral\nto Œªt = log\n\u0010\nbt\nct\n\u0011\n.\nLastly, we emphasize the advantage of DBIM over employing exponential integrators. First, deriving\nour ODE via exponential integrators requires the PF-ODE as preliminary. However, the PF-ODE\nalone cannot handle the singularity at the start point and presents theoretical challenges. Moreover,\nthe conversion process from the PF-ODE to our ODE is intricate, while DBIM retains the overall\nsimplicity. Additionally, DBIM supports varying levels of stochasticity during sampling, unlike the\ndeterministic nature of ODE-based methods. This stochasticity can mitigate sampling errors via the\nLangevin mechanism (Song et al., 2021c), potentially enhancing the generation quality.\n21\nPublished as a conference paper at ICLR 2025\nD\nDERIVATION OF OUR HIGH-ORDER NUMERICAL SOLVERS\nHigh-order solvers of Eqn. (17) can be developed by approximating xŒ∏ in the integral with Tay-\nlor expansion.\nSpecifically, as a function of Œª, we have xŒ∏(xtŒª, tŒª, xT ) ‚âàxŒ∏(xt, t, xT ) +\nPn\nk=1\n(Œª‚àíŒªt)k\nk!\nx(k)\nŒ∏ (xt, t, xT ), where x(k)\nŒ∏ (xt, t, xT ) =\ndkxŒ∏(xtŒª,tŒª,xT )\ndŒªk\n\f\f\f\nŒª=Œªt is the k-th order\nderivative w.r.t. Œª, which can be estimated with finite difference of past network outputs.\n2nd-Order Case\nWith the Taylor expansion xŒ∏(xtŒª, tŒª, xT )\n‚âà\nxŒ∏(xt, t, xT ) + (Œª ‚àí\nŒªt)x(1)\nŒ∏ (xt, t, xT ), we have\nZ Œªs\nŒªt\neŒªxŒ∏(xtŒª, tŒª, xT )dŒª ‚âà\n Z Œªs\nŒªt\neŒªdŒª\n!\nxŒ∏(xt, t, xT ) +\n Z Œªs\nŒªt\n(Œª ‚àíŒªt)eŒªdŒª\n!\nx(1)\nŒ∏ (xt, t, xT )\n‚âàeŒªs h\n(1 ‚àíe‚àíh)ÀÜxt + (h ‚àí1 + e‚àíh)ÀÜx(1)\nt\ni\n(61)\nwhere we use ÀÜxt to denote the network output at time t, and h = Œªs ‚àíŒªt > 0. Suppose we have\nused a previous timestep u (s < t < u), the first-order derivative can be estimated by\nÀÜx(1)\nt\n‚âàÀÜxt ‚àíÀÜxu\nh1\n,\nh1 = Œªt ‚àíŒªu\n(62)\n3rd-Order Case\nWith the Taylor expansion xŒ∏(xtŒª, tŒª, xT )\n‚âà\nxŒ∏(xt, t, xT ) + (Œª ‚àí\nŒªt)x(1)\nŒ∏ (xt, t, xT ) + (Œª‚àíŒªt)2\n2\nx(2)\nŒ∏ (xt, t, xT ), we have\nZ Œªs\nŒªt\neŒªxŒ∏(xtŒª, tŒª, xT )dŒª\n‚âà\n Z Œªs\nŒªt\neŒªdŒª\n!\nxŒ∏(xt, t, xT ) +\n Z Œªs\nŒªt\n(Œª ‚àíŒªt)eŒªdŒª\n!\nx(1)\nŒ∏ (xt, t, xT )\n+\n Z Œªs\nŒªt\n(Œª ‚àíŒªt)2\n2\neŒªdŒª\n!\nx(2)\nŒ∏ (xt, t, xT )\n‚âàeŒªs\n\u0014\n(1 ‚àíe‚àíh)ÀÜxt + (h ‚àí1 + e‚àíh)ÀÜx(1)\nt\n+\n\u0012h2\n2 ‚àíh + 1 ‚àíe‚àíh\n\u0013\nÀÜx(2)\nt\n\u0015\n(63)\nSimilarly, suppose we have two previous timesteps u1, u2 (s < t < u1 < u2), and denote h1 :=\nŒªt ‚àíŒªu1, h2 := Œªu1 ‚àíŒªu2, the first-order and second-order derivatives can be estimated by\nÀÜx(1)\nt\n‚âà\nÀÜxt‚àíÀÜxu1\nh1\n(2h1 + h2) ‚àí\nÀÜxu1‚àíÀÜxu2\nh2\nh1\nh1 + h2\n,\nÀÜx(2)\nt\n‚âà2\nÀÜxt‚àíÀÜxu1\nh1\n‚àí\nÀÜxu1‚àíÀÜxu2\nh2\nh1 + h2\n(64)\nThe high-order samplers for DDBMs also theoretically guarantee the order of convergence, similar\nto those for diffusion models (Zhang & Chen, 2022; Lu et al., 2022b; Zheng et al., 2023a). We omit\nthe proofs here as they deviate from our main contributions.\n22\nPublished as a conference paper at ICLR 2025\nE\nALGORITHM\nAlgorithm 1 DBIM (high-order)\nRequire: condition xT , timesteps 0 ‚â§t0 < t1 < ¬∑ ¬∑ ¬∑ < tN‚àí1 < tN = T, data prediction model\nxŒ∏, booting noise œµ ‚àºN(0, I), noise schedule at, bt, ct, Œªt = log(bt/ct), order o (2 or 3).\n1: ÀÜxT ‚ÜêxŒ∏(xT , T, xT )\n2: xtN‚àí1 ‚ÜêatxT + bt ÀÜxT + ctœµ\n3: for i ‚ÜêN ‚àí1 to 1 do\n4:\ns, t ‚Üêti‚àí1, ti; h ‚ÜêŒªs ‚àíŒªt\n5:\nÀÜxt ‚ÜêxŒ∏(xt, t, xT )\n6:\nif o = 2 or i = N ‚àí1 then\n7:\nu ‚Üêti+1; h1 ‚ÜêŒªt ‚àíŒªu\n8:\nEstimate ÀÜx(1)\nt\nwith Eqn. (62)\n9:\nÀÜI ‚ÜêeŒªs\nh\n(1 ‚àíe‚àíh)ÀÜxt + (h ‚àí1 + e‚àíh)ÀÜx(1)\nt\ni\n10:\nelse\n11:\nu1, u2 ‚Üêti+1, ti+2; h1 ‚ÜêŒªt ‚àíŒªu1; h2 ‚ÜêŒªu1 ‚àíŒªu2\n12:\nEstimate ÀÜx(1)\nt , ÀÜx(2)\nt\nwith Eqn. (64)\n13:\nÀÜI ‚ÜêeŒªs\nh\n(1 ‚àíe‚àíh)ÀÜxt + (h ‚àí1 + e‚àíh)ÀÜx(1)\nt\n+\n\u0010\nh2\n2 ‚àíh + 1 ‚àíe‚àíh\u0011\nÀÜx(2)\nt\ni\n14:\nend if\n15:\nxs ‚Üêcs\nct xt +\n\u0010\nas ‚àícs\nct at\n\u0011\nxT + cs ÀÜI\n16: end for\n17: return xt0\nF\nEXPERIMENT DETAILS\nF.1\nMODEL DETAILS\nDDBMs and DBIMs are assessed using the same trained diffusion bridge models. For image trans-\nlation tasks, we directly adopt the pretrained checkpoints provided by DDBMs. The data pre-\ndiction model xŒ∏(xt, t, xT ) mentioned in the main text is parameterized by the network FŒ∏ as\nxŒ∏(xt, t, xT ) = cskip(t)xt + cout(t)FŒ∏(cin(t)xt, cnoise(t), xT ), where\ncin(t) =\n1\np\na2\ntœÉ2\nT + b2\ntœÉ2\n0 + 2atbtœÉ0T + ct\n,\ncout(t) =\nq\na2\nt(œÉ2\nT œÉ2\n0 ‚àíœÉ2\n0T ) + œÉ2\n0ctcin(t)\ncskip(t) = (btœÉ2\n0 + atœÉ0T )c2\nin(t),\ncnoise(t) = 1\n4 log t\n(65)\nand\nœÉ2\n0 = Var[x0], œÉ2\nT = Var[xT ], œÉ0T = Cov[x0, xT ]\n(66)\nFor the image inpainting task on ImageNet 256√ó256 with 128√ó128 center mask, DDBMs do not\nprovide available checkpoints. Therefore, we train a new model from scratch using the noise sched-\nule of I2SB (Liu et al., 2023b). The network is initialized from the pretrained class-conditional\ndiffusion model on ImageNet 256√ó256 provided by (Dhariwal & Nichol, 2021), while addition-\nally conditioned on xT .\nThe data prediction model in this case is parameterized by the net-\nwork FŒ∏ as xŒ∏(xt, t, xT ) = xt ‚àíœÉtFŒ∏(xt, t, xT ) and trained by minimizing the loss L(Œ∏) =\nEt,x0,xT\nh\n1\nœÉ2\nt ‚à•xŒ∏(xt, t, xT ) ‚àíx0‚à•2\n2\ni\n. We train the model on 8 NVIDIA A800 GPU cards with a\nbatch size of 256 for 400k iterations, which takes around 19 days.\nF.2\nSAMPLING DETAILS\nWe elaborate on the sampling configurations of different approaches, including the choice of\ntimesteps {ti}N\ni=0 and details of the samplers. In this work, we adopt tmin = 0.0001 and tmax = 1\nfollowing (Zhou et al., 2023). For the DDBM baseline, we use the hybrid, high-order Heun sampler\n23\nPublished as a conference paper at ICLR 2025\nproposed in their work with an Euler step ratio of 0.33, which is the best performing configuration for\nthe image-to-image translation task. We use the same timesteps distributed according to EDM (Kar-\nras et al., 2022)‚Äôs scheduling (t1/œÅ\nmax + i\nN (t1/œÅ\nmin ‚àít1/œÅ\nmax))œÅ, consistent with the official implementation\nof DDBM. For DBIM, since the initial sampling step is distinctly forced to be stochastic, we specif-\nically set it to transition from tmax to tmax ‚àí0.0001, and employ a simple uniformly distributed\ntimestep scheme in [tmin, tmax ‚àí0.0001) for the remaining timesteps, across all settings. For inter-\npolation experiments, to enhance diversity, we increase the step size of the first step from 0.0001 to\n0.01.\nF.3\nLICENSE\nTable 7: The used datasets, codes and their licenses.\nName\nURL\nCitation\nLicense\nEdges‚ÜíHandbags\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\nIsola et al. (2017)\nBSD\nDIODE-Outdoor\nhttps://diode-dataset.org/\nVasiljevic et al. (2019)\nMIT\nImageNet\nhttps://www.image-net.org\nDeng et al. (2009)\n\\\nGuided-Diffusion\nhttps://github.com/openai/guided-diffusion\nDhariwal & Nichol (2021)\nMIT\nI2SB\nhttps://github.com/NVlabs/I2SB\nLiu et al. (2023b)\nCC-BY-NC-SA-4.0\nDDBM\nhttps://github.com/alexzhou907/DDBM\nZhou et al. (2023)\n\\\nWe list the used datasets, codes and their licenses in Table 7.\nG\nADDITIONAL RESULTS\nG.1\nRUNTIME COMPARISON\nTable 8 shows the inference time of DBIM and previous methods on a single NVIDIA A100 under\ndifferent settings. We use torch.cuda.Event and torch.cuda.synchronize to accu-\nrately compute the runtime. We evaluate the runtime on 8 batches (dropping the first batch since\nit contains extra initializations) and report the mean and std. We can see that the runtime is pro-\nportional to NFE. This is because the main computation costs are the serial evaluations of the large\nneural network, and the calculation of other coefficients requires neglectable costs. Therefore, the\nspeedup for the NFE is approximately the actual speedup of the inference time.\nTable 8: Runtime of different methods to generate a single batch (second / batch, ¬±std) on a single\nNVIDIA A100, varying the number of function evaluations (NFE).\nMethod\nNFE\n5\n10\n15\n20\nCenter 128 √ó 128 Inpainting, ImageNet 256 √ó 256 (batch size = 16)\nI2SB (Liu et al., 2023b)\n2.8128 ¬± 0.0111\n5.6049 ¬± 0.0152\n8.3919 ¬± 0.0166\n11.1494 ¬± 0.0259\nDDBM (Zhou et al., 2023)\n2.8711 ¬± 0.0318\n5.7283 ¬± 0.0572\n8.3787 ¬± 0.1667\n11.0678 ¬± 0.3061\nDBIM (Œ∑ = 0)\n2.8755 ¬± 0.0706\n5.7810 ¬± 0.1494\n8.5890 ¬± 0.2730\n11.1613 ¬± 0.3372\nDBIM (2nd-order)\n2.8859 ¬± 0.0675\n5.7884 ¬± 0.1734\n8.6284 ¬± 0.1907\n11.5898 ¬± 0.2260\nDBIM (3rd-order)\n2.9234 ¬± 0.0361\n5.8109 ¬± 0.2982\n8.6449 ¬± 0.2118\n11.3710 ¬± 0.3237\nG.2\nDIVERSITY SCORE\nWe measure the diversity score (Batzolis et al., 2021; Li et al., 2023) on the ImageNet center inpaint-\ning task. We calculate the standard deviation of 5 generated samples (numerical range 0 ‚àº255)\ngiven each observation (condition) xT , averaged over all pixels and 1000 conditions.\nAs shown in Table 9, the diversity score keeps increasing with larger NFE. DBIM (Œ∑ = 0) consis-\ntently surpasses the flow matching baseline I2SB, and DDBM‚Äôs hybrid sampler which introduces\ndiversity through SDE steps. Surprisingly, we find that the DBIM Œ∑ = 0 case exhibits a larger di-\nversity score than the Œ∑ = 1 case. This demonstrates that the booting noise can introduce enough\n24\nPublished as a conference paper at ICLR 2025\nTable 9:\nDiversity scores on the ImageNet center inpainting task, varying Œ∑ and the NFE. We\nexclude statistics for DDBM (NFE‚â§10) as they correspond to severely degraded nonsense samples.\nMethod\nNFE\n5\n10\n20\n50\n100\n200\n500\nI2SB (Liu et al., 2023b)\n3.27\n4.45\n5.21\n5.75\n5.95\n6.04\n6.15\nDDBM (Zhou et al., 2023)\n-\n-\n2.96\n4.03\n4.69\n5.29\n5.83\nDBIM\nŒ∑ = 0\n3.74\n4.56\n5.20\n5.80\n6.10\n6.29\n6.42\nŒ∑ = 1\n2.62\n3.40\n4.18\n5.01\n5.45\n5.81\n6.16\nstochasticity to ensure diverse generation. Moreover, the Œ∑ = 0 case tends to generate sharper\nimages, which may favor the diversity score measured by pixel-level variance.\nH\nADDITIONAL SAMPLES\n25\nPublished as a conference paper at ICLR 2025\n(a) Condition\n(b) Ground-truth\n(c) DDBM (NFE=20), FID 46.74\n(d) DBIM (NFE=20 , Œ∑ = 0.0), FID 1.76\n(e) DDBM (NFE=100), FID 2.40\n(f) DBIM (NFE=100 , Œ∑ = 0.0), FID 0.91\n(g) DDBM (NFE=200), FID 0.88\n(h) DBIM (NFE=200 , Œ∑ = 0.0), FID 0.75\nFigure 6: Edges‚ÜíHandbags samples on the translation task.\n26\nPublished as a conference paper at ICLR 2025\n(a) Condition\n(b) Ground-truth\n(c) DDBM (NFE=20), FID 41.03\n(d) DDBM (NFE=500), FID 2.26\n(e) DBIM (NFE=20 , Œ∑ = 0.0), FID 4.97\n(f) DBIM (NFE=200 , Œ∑ = 0.0), FID 2.26\nFigure 7: DIODE-Outdoor samples on the translation task.\n27\nPublished as a conference paper at ICLR 2025\n(a) Condition\n(b) Ground-truth\n(c) DDBM (NFE=10), FID 57.18\n(d) DDBM (NFE=500), FID 4.27\n(e) DBIM (NFE=10 , Œ∑ = 0.0), FID 4.51\n(f) DBIM (NFE=100 , Œ∑ = 0.0), FID 3.91\n(g) DBIM (NFE=100 , Œ∑ = 0.8), FID 3.88\n(h) DBIM (NFE=500 , Œ∑ = 1.0), FID 3.80\nFigure 8: ImageNet 256 √ó 256 samples on the inpainting task with center 128 √ó 128 mask.\n28\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Published as a conference paper at ICLR 2025\nDIFFUSION BRIDGE IMPLICIT MODELS\nKaiwen Zheng12‚àó,\nGuande He1‚àó,\nJianfei Chen1,\nFan Bao2,\nJun Zhu‚Ä†123\n1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center\n1Tsinghua-Bosch Joint ML Center, Tsinghua University, Beijing, China\n2Shengshu Technology, Beijing\n3Pazhou Lab (Huangpu), Guangzhou, China\nzkwthu@gmail.com; guande.he17@outlook.com;\nfan.bao@shengshu.ai; {jianfeic, dcszj}@tsinghua.edu.cn\nABSTRACT\nDenoising diffusion bridge models (DDBMs) are a powerful variant of diffu-\nsion models for interpolating between two arbitrary paired distributions given\nas endpoints. Despite their promising performance in tasks like image transla-\ntion, DDBMs require a computationally intensive sampling process that involves\nthe simulation of a (stochastic) differential equation through hundreds of network\nevaluations. In this work, we take the first step in fast sampling of DDBMs with-\nout extra training, motivated by the well-established recipes in diffusion mod-\nels. We generalize DDBMs via a class of non-Markovian diffusion bridges de-\nfined on the discretized timesteps concerning sampling, which share the same\nmarginal distributions and training objectives, give rise to generative processes\nranging from stochastic to deterministic, and result in diffusion bridge implicit\nmodels (DBIMs). DBIMs are not only up to 25√ó faster than the vanilla sam-\npler of DDBMs but also induce a novel, simple, and insightful form of ordinary\ndifferential equation (ODE) which inspires high-order numerical solvers. More-\nover, DBIMs maintain the generation diversity in a distinguished way, by using a\nbooting noise in the initial sampling step, which enables faithful encoding, recon-\nstruction, and semantic interpolation in image translation tasks. Code is available\nat https://github.com/thu-ml/DiffusionBridge.\n1\nINTRODUCTION\nDiffusion models (Song et al., 2021c; Sohl-Dickstein et al., 2015; Ho et al., 2020) represent a family\nof powerful generative models, with high-quality generation ability, stable training, and scalability\nto high dimensions. They have consistently obtained state-of-the-art performance in various do-\nmains, including image synthesis (Dhariwal & Nichol, 2021; Karras et al., 2022), speech and video\ngeneration (Chen et al., 2021a; Ho et al., 2022), controllable image manipulation (Nichol et al.,\n2022; Ramesh et al., 2022; Rombach et al., 2022; Meng et al., 2022), density estimation (Song\net al., 2021b; Kingma et al., 2021; Lu et al., 2022a; Zheng et al., 2023b) and inverse problem solv-\ning (Chung et al., 2022; Kawar et al., 2022). They also act as fundamental components of modern\ntext-to-image (Rombach et al., 2022) and text-to-video (Gupta et al., 2023; Bao et al., 2024) synthe-\nsis systems, ushering in the era of AI-generated content.\nHowever, diffusion models are not well-suited for solving tasks like image translation or restoration,\nwhere the transport between two arbitrary probability distributions is to be modeled given paired\nendpoints. Diffusion models are rooted in a stochastic process that gradually transforms between\ndata and noise, and the prior distribution is typically restricted to the ‚Äúnon-informative‚Äù random\nGaussian noises. Adapting diffusion models to scenarios where a more informative prior natu-\nrally exists, such as image translation/restoration, involves modifying the generation pipeline (Meng\net al., 2022; Su et al., 2022) or adding extra guidance terms during sampling (Chung et al., 2022;\nKawar et al., 2022). On the one hand, these approaches are task-agnostic at training and adaptable\nto multiple tasks at inference time. On the other hand, despite recent advances in accelerated inverse\nproblem solving (Liu et al., 2023a; Pandey et al., 2024), they inevitably deliver either sub-par perfor-\nmance or slow and resource-intensive inference compared to training-based ones. Tailored diffusion\n‚àóEqual contribution;\n‚Ä†The corresponding author.\n1\n"
    },
    {
      "page_number": 2,
      "text": "Published as a conference paper at ICLR 2025\n(a) Condition\n(b)\nDDBM\nNFE=100 (FID 6.46)\n(c)\nDBIM (Œ∑ = 0) (Ours)\nNFE=10 (FID 4.51)\n(c)\nDBIM (3rd-order) (Ours)\nNFE=10 (FID 4.34)\nFigure 1: Inpainting results on the ImageNet 256√ó256 dataset (Deng et al., 2009) by DDBM (Zhou et al.,\n2023) with 100 number of function evaluations (NFE), and DBIM (ours) with only 10 NFE.\nmodel variants become essential in task-specific scenarios where paired training data are available\nand fast inference is critical.\nRecently, denoising diffusion bridge models (DDBMs) (Zhou et al., 2023) have emerged as a scal-\nable and promising approach to solving the distribution translation tasks. By considering the reverse-\ntime processes of a diffusion bridge, which represent diffusion processes conditioned on given end-\npoints, DDBMs offer a general framework for distribution translation. While excelling in image\ntranslation tasks with exceptional quality and fidelity, sampling from DDBMs requires simulating a\n(stochastic) differential equation corresponding to the reverse-time process. Even with the introduc-\ntion of their hybrid sampler, achieving high-fidelity results for high-resolution images still demands\nover 100 steps. Compared to the efficient samplers for diffusion models (Song et al., 2021a; Zhang\n& Chen, 2022; Lu et al., 2022b), which require around 10 steps to generate reasonable samples,\nDDBMs are falling behind, urging the development of efficient variants.\nThis work represents the first pioneering effort toward accelerated sampling of DDBMs. As sug-\ngested by well-established recipes in diffusion models, training-free accelerations of diffusion sam-\npling primarily focus on reducing stochasticity (e.g., the prominent denoising diffusion implicit\nmodels, DDIMs) and utilizing higher-order information (e.g., high-order solvers). We present diffu-\nsion bridge implicit models (DBIMs) as an approach that explores both aspects within the diffusion\nbridge framework. Firstly, we investigate the continuous-time forward process of DDBMs on dis-\ncretized timesteps and generalize them to a series of non-Markovian diffusion bridges controlled by\na variance parameter, while maintaining identical marginal distributions and training objectives as\nDDBMs. Secondly, the induced reverse generative processes correspond to sampling procedures of\nvarying levels of stochasticity, including deterministic ones. Consequently, DBIMs can be viewed\nas a bridge counterpart and extension of DDIMs. Furthermore, in the continuous time limit, DBIMs\ncan induce a novel form of ordinary differential equation (ODE), which is linked to the probability\nflow ODE (PF-ODE) in DDBMs while being simpler and significantly more efficient. The induced\nODE also facilitates novel high-order numerical diffusion bridge solvers for faster convergence.\nWe demonstrate the superiority of DBIMs by applying them in image translation and restoration\ntasks, where they offer up to 25√ó faster sampling compared to DDBMs and achieve state-of-the-art\nperformance on challenging high-resolution datasets. Unlike conventional diffusion sampling, the\ninitial step in DBIMs is forced to be stochastic with a booting noise to avoid singularity issues arising\nfrom the fixed starting point on a bridge. By viewing the booting noise as the latent variable, DBIMs\nmaintain the generation diversity of typical generative models while enabling faithful encoding,\nreconstruction, and semantically meaningful interpolation in the data space.\n2\nBACKGROUND\n2.1\nDIFFUSION MODELS\nGiven a d-dimensional data distribution q0(x0), diffusion models (Song et al., 2021c; Sohl-Dickstein\net al., 2015; Ho et al., 2020) build a diffusion process by defining a forward stochastic differential\nequation (SDE) starting from x0 ‚àºq0:\ndxt = f(t)xtdt + g(t)dwt\n(1)\n2\n"
    },
    {
      "page_number": 3,
      "text": "Published as a conference paper at ICLR 2025\nwhere t ‚àà[0, T] for some finite horizon T, f, g : [0, T] ‚ÜíR is the scalar-valued drift and diffusion\nterm, and wt ‚ààRd is a standard Wiener process. As a linear SDE, the forward process owns an\nanalytic Gaussian transition kernel\nqt|0(xt|x0) = N(Œ±tx0, œÉ2\nt I)\n(2)\nby ItÀÜo‚Äôs formula (ItÀÜo, 1951), where Œ±t, œÉt are called noise schedules satisfying f(t)\n=\nd log Œ±t\ndt\n,\ng2(t) =\ndœÉ2\nt\ndt ‚àí2 d log Œ±t\ndt\nœÉ2\nt (Kingma et al., 2021). The forward SDE is accompanied\nby a series of marginal distributions {qt}T\nt=0 of {xt}T\nt=0, and f, g are properly designed so that the\nterminal distribution is approximately a pure Gaussian, i.e., qT (xT ) ‚âàN(0, œÉ2\nT I).\nTo sample from the data distribution q0(x0), we can solve the reverse SDE or probability flow\nODE (Song et al., 2021c) from t = T to t = 0:\ndxt = [f(t)xt ‚àíg2(t)‚àáxt log qt(xt)]dt + g(t)d ¬Øwt,\n(3)\ndxt =\n\u0014\nf(t)xt ‚àí1\n2g2(t)‚àáxt log qt(xt)\n\u0015\ndt.\n(4)\nThey share the same marginal distributions {qt}T\nt=0 with the forward SDE, where\n¬Øwt\nis the reverse-time Wiener process,\nand the only unknown term ‚àáxt log qt(xt) is the\nscore function of the marginal density qt.\nBy denoising score matching (DSM) (Vin-\ncent,\n2011),\na\nscore\nprediction\nnetwork\nsŒ∏(xt, t)\ncan\nbe\nparameterized\nto\nminimize\nEtEx0‚àºq0(x0)Ext‚àºqt|0(xt|x0)\n\u0002\nw(t)‚à•sŒ∏(xt, t) ‚àí‚àáxt log qt|0(xt|x0)‚à•2\n2\n\u0003\n, where qt|0 is the analytic\nforward transition kernel and w(t) is a positive weighting function. sŒ∏ can be plugged into the re-\nverse SDE and the probability flow ODE to obtain the parameterized diffusion SDE and diffusion\nODE. There are various dedicated solvers for diffusion SDE or ODE (Song et al., 2021a; Zhang &\nChen, 2022; Lu et al., 2022b; Gonzalez et al., 2023).\n2.2\nDENOISING DIFFUSION BRIDGE MODELS\nDenoising diffusion bridge models (DDBMs) (Zhou et al., 2023) consider driving the diffusion\nprocess in Eqn. (1) to arrive at a particular point y ‚ààRd almost surely via Doob‚Äôs h-transform (Doob\n& Doob, 1984):\ndxt = f(t)xtdt + g2(t)‚àáxt log q(xT = y|xt) + g(t)dwt,\nx0 ‚àºq0 = pdata, xT = y.\n(5)\nThe endpoint y is not restricted to Gaussian noise as in diffusion models, but instead chosen as\ninformative priors (such as the degraded image in image restoration tasks). Given a starting point\nx0, the process in Eqn. (5) also owns an analytic forward transition kernel\nq(xt|x0, xT ) = N(atxT +btx0, c2\ntI),\nat = Œ±t\nŒ±T\nSNRT\nSNRt\n, bt = Œ±t(1‚àíSNRT\nSNRt\n), c2\nt = œÉ2\nt (1‚àíSNRT\nSNRt\n)\n(6)\nwhich forms a diffusion bridge, and SNRt = Œ±2\nt/œÉ2\nt is the signal-to-noise ratio at time t. DDBMs\nshow that the forward process Eqn. (5) is associated with a reverse SDE and a probability flow ODE\nstarting from xT = y:\ndxt =\n\u0002\nf(t)xt ‚àíg2(t)\n\u0000‚àáxt log q(xt|xT = y) ‚àí‚àáxt log qT |t(xT = y|xt)\n\u0001\u0003\ndt + g(t)d ¬Øwt, (7)\ndxt =\n\u0014\nf(t)xt ‚àíg2(t)\n\u00121\n2‚àáxt log q(xt|xT = y) ‚àí‚àáxt log qT |t(xT = y|xt)\n\u0013\u0015\ndt.\n(8)\nThey share the same marginal distributions {q(xt|xT = y)}T\nt=0 with the forward process, where\n¬Øwt is the reverse-time Wiener process, qT |t is analytically known similar to Eqn. (2), and the only\nunknown term ‚àáxt log q(xt|xT = y) is the bridge score function. Denoising bridge score matching\n(DBSM) is proposed to learn the unknown score term q(xt|xT = y) with a parameterized network\nsŒ∏(xt, t, y), by minimizing\nLw(Œ∏) = EtE(x0,y)‚àºpdata(x0,y)Ext‚àºq(xt|x0,xT =y)\n\u0002\nw(t)‚à•sŒ∏(xt, t, y) ‚àí‚àáxt log q(xt|x0, xT = y)‚à•2\n2\n\u0003\n(9)\nwhere q(xt|x0, xT = y) is the forward transition kernel in Eqn. (6) and w(t) is a positive weighting\nfunction. To sample from diffusion bridges with Eqn. (7) and Eqn. (8), DDBMs propose a high-order\nhybrid sampler that alternately simulates the ODE and SDE steps to enhance the sample quality,\ninspired by the Heun sampler in diffusion models (Karras et al., 2022). However, it is not dedicated\nto diffusion bridges and lacks theoretical insights in developing efficient diffusion samplers.\n3\n"
    },
    {
      "page_number": 4,
      "text": "Published as a conference paper at ICLR 2025\n3\nGENERATIVE MODEL THROUGH NON-MARKOVIAN DIFFUSION BRIDGES\nWe start by examining the forward process of the diffusion bridge (Eqn. (5)) on a set of discretized\ntimesteps 0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN‚àí1 < tN = T that will be used for reverse sampling. Since\nthe bridge score ‚àáxt log q(xt|xT ) only depends on the marginal distribution q(xt|xT ), we can\nconstruct alternative probabilistic models that induce new sampling procedures while reusing the\nlearned bridge score sŒ∏(xt, t, xT ), as long as they agree on the N marginals {q(xtn|xT )}N‚àí1\nn=0 .\n3.1\nNON-MARKOVIAN DIFFUSION BRIDGES AS FORWARD PROCESS\nWe consider a family of probability distributions q(œÅ)(xt0:N‚àí1|xT ), controlled by a variance param-\neter œÅ ‚ààRN‚àí1:\nq(œÅ)(xt0:N‚àí1|xT ) = q0(xt0)\nN‚àí1\nY\nn=1\nq(œÅ)(xtn|x0, xtn+1, xT )\n(10)\nwhere q0 is the data distribution at time 0 and for 1 ‚â§n ‚â§N ‚àí1\nq(œÅ)(xtn|x0, xtn+1, xT ) = N(atnxT + btnx0 +\nq\nc2\ntn ‚àíœÅ2n\nxtn+1 ‚àíatn+1xT ‚àíbtn+1x0\nctn+1\n, œÅ2\nnI)\n(11)\nwhere œÅn is the n-th element of œÅ satisfying œÅN‚àí1 = ctN‚àí1, and at, bt, ct are terms related to the\nnoise schedule, as defined in the original diffusion bridge (Eqn. (6)). Intuitively, this decreases the\nvariance (noise level) of the bridge while incorporating additional noise components from the last\nstep. Under this construction, we can prove that q(œÅ) maintains consistency in marginal distributions\nwith the original forward process q governed by Eqn. (5).\nProposition 3.1 (Marginal Preservation, proof in Appendix B.1). For 0 ‚â§n ‚â§N ‚àí1, we have\nq(œÅ)(xtn|xT ) = q(xtn|xT ).\nThe definition of q(œÅ) in Eqn. (10) represents the inference process, since it is factorized as\nthe distribution of xtn given xtn+1 at the previous timestep.\nConversely, the forward process\nq(œÅ)(xtn+1|x0, xtn, xT ) can be induced by Bayes‚Äô rule (Appendix C.1). As xtn+1 in q(œÅ) can si-\nmultaneously depend on xtn and x0, we refer to it as non-Markovian diffusion bridges, in contrast\nto Markovian ones (such as Brownian bridges, and the diffusion bridge defined by the forward SDE\nin Eqn. (5)) which should satisfy q(xtn+1|x0, xtn, xT ) = q(xtn+1|xtn, xT ).\n3.2\nREVERSE GENERATIVE PROCESS AND EQUIVALENT TRAINING OBJECTIVE\nEqn. (10) can be naturally transformed into a parameterized and learnable generative model, by\nreplacing the unknown x0 in Eqn. (10) with a data predictor xŒ∏(xt, t, xT ). Intuitively, xt on the\ndiffusion bridge is a weighted mixture of xT , x0 and some random Gaussian noise according to\nEqn. (6), where the weightings at, bt, ct are determined by the timestep t. The network xŒ∏ is trained\nto recover the clean data x0 given xt, xT and t.\nSpecifically, we define the generative process starting from xT as\npŒ∏(xtn|xtn+1, xT ) =\n\u001aN(xŒ∏(xt1, t1, xT ), œÅ2\n0I),\nn = 0\nq(œÅ)(xtn|xŒ∏(xtn+1, tn+1, xT ), xtn+1, xT ),\n1 ‚â§n ‚â§N ‚àí1\n(12)\nand the joint distribution as pŒ∏(xt0:N‚àí1|xT ) = QN‚àí1\nn=0 pŒ∏(xtn|xtn+1, xT ). To optimize the network\nparameter Œ∏, we can adopt the common variational inference objective as in DDPMs (Ho et al.,\n2020), except that the distributions are conditioned on xT :\nJ (œÅ)(Œ∏) = Eq(xT )Eq(œÅ)(xt0:N‚àí1|xT )\nh\nlog q(œÅ)(xt1:N‚àí1|x0, xT ) ‚àílog pŒ∏(xt0:N‚àí1|xT )\ni\n(13)\nIt seems that the DDBM objective Lw in Eqn. (9) is distinct from J (œÅ): respectively, they are defined\non continuous and discrete timesteps; they originate from score matching and variational inference;\nthey have different parameterizations of score and data prediction1. However, we show they are\nequivalent by focusing on the discretized timesteps and transforming the parameterization.\n1The diffusion bridge models are usually parameterized differently from score prediction, but can be con-\nverted to score prediction. See Appendix F.1 for details.\n4\n"
    },
    {
      "page_number": 5,
      "text": "Published as a conference paper at ICLR 2025\nTable 1: Comparison between different diffusion models and diffusion bridge models.\nDiffusion Models\nDiffusion Bridge Models\nDDPM\n(Ho et al., 2020)\nScoreSDE\n(Song et al., 2021c)\nDDIM\n(Song et al., 2021a)\nI2SB\n(Liu et al., 2023b)\nDDBM\n(Zhou et al., 2023)\nDBIM (Ours)\nNoise Schedule\nVP\nAny\nAny\nVE\nAny\nAny\nTimesteps\nDiscrete\nContinuous\nDiscrete\nDiscrete\nContinuous\nDiscrete\nForward Distribution\nq(xn|x0)\nq(xt|x0)\nq(xn|xn‚àí1, x0)\nq(xn|x0, xN)\nq(xt|x0, xT )\nq(xtn+1|x0, xtn, xT )\nInference Process\npŒ∏(xn‚àí1|xn)\nSDE/ODE\npŒ∏(xn‚àí1|xn)\npŒ∏(xn‚àí1|xn)\nSDE/ODE\npŒ∏(xtn|xtn+1, xT )\nNon-Markovian\n‚úó\n‚úó\n‚úì\n‚úó\n‚úó\n‚úì\nProposition 3.2 (Training Equivalence, proof in Appendix B.2). For œÅ > 0, there exists certain\nweights Œ≥ so that J (œÅ)(Œ∏) = LŒ≥(Œ∏)+C on the discretized timesteps {tn}N\nn=1, where C is a constant\nirrelevant to Œ∏. Besides, the bridge score predictor sŒ∏ in LŒ≥(Œ∏) has the following relationship with\nthe data predictor xŒ∏ in J (œÅ)(Œ∏):\nsŒ∏(xt, t, xT ) = ‚àíxt ‚àíatxT ‚àíbtxŒ∏(xt, t, xT )\nc2\nt\n(14)\nThough the weighting Œ≥ may not precisely match the actual weighting w for training sŒ∏, this dis-\ncrepancy doesn‚Äôt affect our utilization of sŒ∏ (Appendix C.2). Hence, it is reasonable to reuse the\nnetwork trained by L while leveraging various œÅ for improved sampling efficiency.\n4\nSAMPLING WITH GENERALIZED DIFFUSION BRIDGES\nNow that we have confirmed the rationality and built the theoretical foundations for applying the\ngeneralized diffusion bridge pŒ∏ to pretrained DDBMs, a range of inference processes is now at our\ndisposal, controlled by the variance parameter œÅ. This positions us to explore the resultant sampling\nprocedures and the effects of œÅ in pursuit of better and more efficient generation.\n4.1\nDIFFUSION BRIDGE IMPLICIT MODELS\nSuppose we sample in reverse time on the discretized timesteps 0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tN‚àí1 <\ntN = T. The number N and the schedule of sampling steps can be made independently of the\noriginal timesteps on which the bridge model is trained, whether discrete (Liu et al., 2023b) or\ncontinuous (Zhou et al., 2023). According to the generative process of pŒ∏ in Eqn. (12), the updating\nrule from tn+1 to tn is described by\nxtn = atnxT + btn ÀÜx0 +\nq\nc2\ntn ‚àíœÅ2n\nxtn+1 ‚àíatn+1xT ‚àíbtn+1 ÀÜx0\nctn+1\n|\n{z\n}\npredicted noise ÀÜœµ\n+œÅnœµ,\nœµ ‚àºN(0, I)\n(15)\nwhere ÀÜx0 = xŒ∏(xtn+1, tn+1, xT ) denotes the predicted clean data at time 0.\nIntuition of the Sampling Procedure\nIntuitively, the form of Eqn. (15) resembles the forward\ntransition kernel of the diffusion bridge in Eqn. (6) (which can be rewritten as xt = atxT + btx0 +\nctœµ, œµ ‚àºN(0, I)). In comparison, x0 is substituted with the predicted ÀÜx0, and a portion of the\nstandard Gaussian noise œµ now stems from the predicted noise ÀÜœµ. The predicted noise ÀÜœµ is derived\nfrom xtn+1 at the previous timestep and can be expressed by the predicted clean data ÀÜx0.\nEffects of the Variance Parameter\nWe investigate the effects of the variance parameter œÅ\nfrom the theoretical perspective by considering two extreme cases. Firstly, we note that when\nœÅn = œÉtn\nq\n1 ‚àí\nSNRtn+1\nSNRtn\nfor each 0 ‚â§n ‚â§N ‚àí1, the xT term in Eqn. (15) is canceled out.\nIn this scenario, the forward process in Eqn. (4.1) becomes a Markovian bridge (see details in\nAppendix C.1). Besides, the inference process will get rid of xT and simplify to pŒ∏(xtn|xtn+1),\nakin to the sampling mechanism in DDPMs (Ho et al., 2020). Secondly, when œÅn = 0 for each\n0 ‚â§n ‚â§N ‚àí1, the inference process will be free from random noise and composed of deterministic\niterative updates, characteristic of an implicit probabilistic model (Mohamed & Lakshminarayanan,\n5\n"
    },
    {
      "page_number": 6,
      "text": "Published as a conference paper at ICLR 2025\nbooting noise\ncondition\nùë°= ùëá\nùë°= ùëá‚àíùúñ\nùë°= 0\ndeterministic\nFigure 2: Illustration of the DBIM‚Äôs deterministic sampling procedure when œÅ = 0.\n2016). Consequently, we name the resulting model diffusion bridge implicit models (DBIMs), draw-\ning parallels with denoising diffusion implicit models (DDIMs) (Song et al., 2021a). DBIMs serve\nas the bridge counterpart and extension of DDIMs, as illustrated in Table 1.\nWhen we choose œÅ that lies between these two boundary cases, we can obtain non-Markovian dif-\nfusion bridges with intermediate and non-zero stochastic levels. Such bridges may potentially yield\nsuperior sample quality. We present detailed ablations in Section 6.1.\nThe Singularity at the Initial Step for Deterministic Sampling\nOne important aspect to note\nregarding DBIMs is that its initial step exhibits singularity when œÅ = 0, a property essentially\ndistinct from DDIMs in diffusion models. Specifically, in the initial step we have tn+1 = T, and\nctn+1 in the denominator in Eqn. (15) equals 0. This phenomenon can be understood intuitively:\ngiven a fixed starting point xT , the variable xt for t < T is typically still stochastically distributed\n(the marginal pŒ∏(xt|xT ) is not a Dirac distribution). For instance, in inpainting tasks, there should\nbe various plausible complete images corresponding to a fixed masked image. However, a fully\ndeterministic sampling procedure disrupts such stochasticity.\nTo be theoretically robust, we employ the other boundary choice œÅn = œÉtn\nq\n1 ‚àí\nSNRtn+1\nSNRtn\nin the\ninitial step2, which is aligned with our previous restriction that œÅN‚àí1 = ctN‚àí1. This will introduce\nan additional standard Gaussian noise œµ which we term as the booting noise. It accounts for the\nstochasticity of the final sample x0 under a given fixed xT and can be viewed as the latent variable.\nWe illustrate the complete DBIM pipeline in Figure 2.\n4.2\nCONNECTION TO PROBABILITY FLOW ODE\nIt is intuitive to perceive that the deterministic sampling can be related to solving an ODE. By setting\nœÅ = 0, tn+1 = t and tn+1 ‚àítn = ‚àÜt in Eqn. (15), the DBIM updating rule can be reorganized\nas xt‚àí‚àÜt\nct‚àí‚àÜt =\nxt\nct +\n\u0010\nat‚àí‚àÜt\nct‚àí‚àÜt ‚àíat\nct\n\u0011\nxT +\n\u0010\nbt‚àí‚àÜt\nct‚àí‚àÜt ‚àíbt\nct\n\u0011\nxŒ∏(xt, t, xT ). As at, bt, ct are continuous\nfunctions of time t defined in Eqn. (6), the ratios at\nct and bt\nct also remain continuous functions of\nt. Therefore, DBIM (œÅ = 0) can be treated as an Euler discretization of the following ordinary\ndifferential equation (ODE):\nd\n\u0012xt\nct\n\u0013\n= xT d\n\u0012at\nct\n\u0013\n+ xŒ∏(xt, t, xT )d\n\u0012bt\nct\n\u0013\n(16)\nThough it does not resemble a conventional ODE involving dt, the two infinitesimal terms d\n\u0010\nat\nct\n\u0011\nand d\n\u0010\nbt\nct\n\u0011\ncan be expressed with dt by the chain rule of derivatives. The ODE form also suggests\nthat with a sufficient number of discretization steps, we can reverse the sampling process and obtain\nencodings of the observed data, which can be useful for interpolation or other downstream tasks.\nIn DDBMs, the PF-ODE (Eqn. (8)) involving dxt and dt is proposed and used for deterministic\nsampling. We reveal in the following proposition that our ODE in Eqn. (16) can exactly yield the\nPF-ODE without relying on the advanced Kolmogorov forward (or Fokker-Planck) equation.\nProposition 4.1 (Equivalence to Probability Flow ODE, proof in Appendix B.3). Suppose\nsŒ∏(xt, t, xT ) is learned as the ground-truth bridge score ‚àáxt log q(xt|xT ), and xŒ∏ is related to sŒ∏\nthrough Eqn. (14), then Eqn. (16) can be converted to the PF-ODE (Eqn. (8)) proposed in DDBMs.\n2With this choice, at the initial step n = N ‚àí1, we have œÅn = œÉtn\nq\n1 ‚àí\nSNRtT\nSNRtn = ctn ‚áí\nq\nc2\ntn ‚àíœÅ2n = 0,\nso ctn+1 in the denominator in Eqn. (15) will be canceled out.\n6\n"
    },
    {
      "page_number": 7,
      "text": "Published as a conference paper at ICLR 2025\nCondition\nGround-truth\nDDBM (NFE=20)\nDDBM (NFE=100)\nDBIM (NFE=20)\nDBIM (NFE=100)\nFigure 3: Image translation results on the DIODE-Outdoor dataset with DDBM and DBIM.\nThough the conversion from our ODE to the PF-ODE is straightforward, the reverse conversion can\nbe non-trivial and require complex tools such as exponential integrators (Calvo & Palencia, 2006;\nHochbruck et al., 2009) (Appendix C.4). We highlight our differences from the PF-ODE in DDBMs:\n(1) Our ODE has a novel form with exceptional neatness. (2) Despite their theoretical equivalence,\nour ODE describes the evolution of xt\nct rather than xt, and its discretization is performed with respect\nto d\n\u0010\nat\nct\n\u0011\nand d\n\u0010\nbt\nct\n\u0011\ninstead of dt. (3) Empirically, DBIMs (œÅ = 0) prove significantly more\nefficient than the Euler discretization of the PF-ODE, thereby accelerating DDBMs by a substantial\nmargin. (4) In contrast to the fully deterministic ODE, DBIMs are capable of various stochastic\nlevels to achieve the best generation quality under the same sampling steps.\n4.3\nEXTENSION TO HIGH-ORDER METHODS\nThe simplicity and efficiency of our ODE (Eqn. (16)) also inspire novel high-order numerical solvers\ntailored for DDBMs, potentially bringing faster convergence than the first-order Euler discretization.\nSpecifically, using the time change-of-variable Œªt = log\n\u0010\nbt\nct\n\u0011\n= 1\n2 (SNRt ‚àíSNRT ), the solution\nof Eqn. (16) from time t to time s < t can be represented as\nxs = cs\nct\nxt +\n\u0012\nas ‚àícs\nct\nat\n\u0013\nxT + cs\nZ Œªs\nŒªt\neŒªxŒ∏(xtŒª, tŒª, xT )dŒª\n(17)\nwhere tŒª is the inverse function of Œªt. The intractable integral can be approximated by Taylor ex-\npansion of xŒ∏ and finite difference estimations of high-order derivatives, following well-established\nnumerical methods (Hochbruck & Ostermann, 2005) and their extensive application in diffusion\nmodels (Zhang & Chen, 2022; Lu et al., 2022b; Gonzalez et al., 2023). We present the derivations\nof our high-order solvers in Appendix D, and the detailed algorithm in Appendix E.\n5\nRELATED WORK\nWe present detailed related work in Appendix A, including diffusion models, diffusion bridge mod-\nels, and fast sampling techniques. We additionally discuss some special cases of DBIM and their\nconnection to flow matching, DDIM and posterior sampling in Appendix C.3.\n6\nEXPERIMENTS\nIn this section, we show that DBIMs surpass the original sampling procedure of DDBMs by a large\nmargin, in terms of both sample quality and sample efficiency. We also showcase DBIM‚Äôs capabil-\nities in latent-space encoding, reconstruction, and interpolation using deterministic sampling. All\ncomparisons between DBIMs and DDBMs are conducted using identically trained models. For\nDDBMs, we employ their proposed hybrid sampler for sampling. For DBIMs, we control the vari-\nance parameter œÅ by interpolating between its boundary selections:\nœÅn = Œ∑œÉtn\ns\n1 ‚àíSNRtn+1\nSNRtn\n,\nŒ∑ ‚àà[0, 1]\n(18)\nwhere Œ∑ = 0 and Œ∑ = 1 correspond to deterministic sampling and Markovian stochastic sampling.\nWe conduct experiments including (1) image-to-image translation tasks on Edges‚ÜíHandbags (Isola\net al., 2017) (64 √ó 64) and DIODE-Outdoor (Vasiljevic et al., 2019) (256 √ó 256) (2) image restora-\ntion task of inpainting on ImageNet (Deng et al., 2009) (256 √ó 256) with 128 √ó 128 center mask.\n7\n"
    },
    {
      "page_number": 8,
      "text": "Published as a conference paper at ICLR 2025\nTable 2: Quantitative results in the image translation task. ‚Ä†Baseline results are taken directly from\nDDBMs, where they did not report the exact NFE. Gray-colored rows denote methods that do not\nrequire paired training but only a prior diffusion model trained on the target domain.\nEdges‚ÜíHandbags (64 √ó 64)\nDIODE-Outdoor (256 √ó 256)\nNFE\nFID ‚Üì\nIS ‚Üë\nLPIPS ‚Üì\nMSE ‚Üì\nFID ‚Üì\nIS ‚Üë\nLPIPS ‚Üì\nMSE ‚Üì\nDDIB (Su et al., 2022)\n‚â•40‚Ä†\n186.84\n2.04\n0.869\n1.05\n242.3\n4.22\n0.798\n0.794\nSDEdit (Meng et al., 2022)\n‚â•40\n26.5\n3.58\n0.271\n0.510\n31.14\n5.70\n0.714\n0.534\nPix2Pix (Isola et al., 2017)\n1\n74.8\n3.24\n0.356\n0.209\n82.4\n4.22\n0.556\n0.133\nI2SB (Liu et al., 2023b)\n‚â•40\n7.43\n3.40\n0.244\n0.191\n9.34\n5.77\n0.373\n0.145\nDDBM (Zhou et al., 2023)\n118\n1.83\n3.73\n0.142\n0.040\n4.43\n6.21\n0.244\n0.084\nDDBM (Zhou et al., 2023)\n200\n0.88\n3.69\n0.110\n0.006\n3.34\n5.95\n0.215\n0.020\nDBIM (Ours)\n20\n1.74\n3.63\n0.095\n0.005\n4.99\n6.10\n0.201\n0.017\nDBIM (Ours)\n100\n0.89\n3.62\n0.100\n0.006\n2.57\n6.06\n0.198\n0.018\nTable 3: Quantative results in the image\nrestoration task.\nInpainting\nImageNet (256 √ó 256)\nCenter (128 √ó 128)\nNFE\nFID ‚Üì\nCA ‚Üë\nDDRM (Kawar et al., 2022)\n20\n24.4\n62.1\nŒ†GDM (Song et al., 2023a)\n100\n7.3\n72.6\nDDNM (Wang et al., 2023)\n100\n15.1\n55.9\nPalette (Saharia et al., 2022)\n1000\n6.1\n63.0\nI2SB (Liu et al., 2023b)\n10\n5.24\n66.1\nI2SB (Liu et al., 2023b)\n20\n4.98\n65.9\nI2SB (Liu et al., 2023b)\n1000\n4.9\n66.1\nDDBM (Zhou et al., 2023)\n500\n4.27\n71.8\nDBIM (Ours)\n10\n4.48\n71.3\nDBIM (Ours)\n20\n4.07\n72.3\nDBIM (Ours)\n100\n3.88\n72.7\nTable 4: Ablation of the variance parameter controlled\nby Œ∑ for image restoration, measured by FID.\nSampler\nNFE\n5\n10\n20\n50\n100\n200\n500\nInpainting, ImageNet (256 √ó 256), Center (128 √ó 128)\nŒ∑\n0.0\n6.08\n4.51\n4.11\n3.95\n3.91\n3.91\n3.91\n0.3\n6.12\n4.48\n4.09\n3.95\n3.92\n3.90\n3.88\n0.5\n6.25\n4.52\n4.07\n3.92\n3.90\n3.84\n3.86\n0.8\n6.81\n4.79\n4.16\n3.91\n3.88\n3.84\n3.81\n1.0\n8.62\n5.61\n4.51\n4.05\n3.91\n3.80\n3.80\nDDBM\n275.25\n57.18\n29.65\n10.63\n6.46\n4.95\n4.27\nWe report the Fr¬¥echet inception distance (FID) (Heusel et al., 2017) for all experiments, and addi-\ntionally measure Inception Scores (IS) (Barratt & Sharma, 2018), Learned Perceptual Image Patch\nSimilarity (LPIPS) (Zhang et al., 2018), Mean Square Error (MSE) (for image-to-image translation)\nand Classifier Accuracy (CA) (for image inpainting), following previous works (Liu et al., 2023b;\nZhou et al., 2023). The metrics are computed using the complete training set for Edges‚ÜíHandbags\nand DIODE-Outdoor, and 10k images from validation set for ImageNet. We provide the inference\ntime comparison in Appendix G.1. Additional experiment details are provided in Appendix F.\n6.1\nSAMPLE QUALITY AND EFFICIENCY\nWe present the quantitative results of DBIMs in Table 2 and Table 3, compared with baselines\nincluding GAN-based, diffusion-based and bridge-based methods3. We set the number of function\nevaluations (NFEs) of DBIM to 20 and 100 to demonstrate both efficiency at small NFEs and quality\nat large NFEs. We select Œ∑ from the set [0.0, 0.3, 0.5, 0.8, 1.0] for DBIM and report the best results.\nIn image translation tasks, DDBM achieves the best sample quality (measured by FID) among the\nbaselines, but requires NFE > 100. In contrast, DBIM with only NFE = 20 already surpasses\nall baselines, performing better than or on par with DDBM at NFE = 118. When increasing the\nNFE to 100, DBIM further improves the sample quality and outperforms DDBM with NFE = 200\non DIODE-Outdoor. In the more challenging image inpainting task on ImageNet 256 √ó 256, the\nsuperiority of DBIM is highlighted even further. In particular, DBIM with NFE = 20 outperforms\nall baselines, including DDBM with NFE = 500, achieving a 25√ó speed-up. With NFE = 100,\nDBIM continues to improve sample quality, reaching a FID lower than 4 for the first time.\nThe comparison of visual quality is illustrated in Figure 1 and Figure 3, where DBIM produces\nsmoother outputs with significantly fewer noisy artifacts compared to DDBM‚Äôs hybrid sampler.\nAdditional samples are provided in Appendix H.\nAblation of the Variance Parameter We investigate the impact of the variance parameter œÅ (con-\ntrolled by Œ∑) to identify how the level of stochasticity affects sample quality across various NFEs, as\nshown in Table 4 and Table 5. For image translation tasks, we consistently observe that employing\n3It is worth noting that, the released checkpoints of I2SB are actually flow matching/interpolant models\ninstead of bridge models, as they (1) start with noisy conditions instead of clean conditions and (2) perform a\nstraight interpolation between the condition and the sample without adding extra intermediate noise.\n8\n"
    },
    {
      "page_number": 9,
      "text": "Published as a conference paper at ICLR 2025\nTable 5: Ablation of the variance parameter controlled by Œ∑ for image translation, measured by FID.\nSampler\nNFE\n5\n10\n20\n50\n100\n200\n500\n5\n10\n20\n50\n100\n200\n500\nImage Translation, Edges‚ÜíHandbags (64 √ó 64)\nImage Translation, DIODE-Outdoor (256 √ó 256)\nŒ∑\n0.0\n3.62\n2.49\n1.76\n1.17\n0.91\n0.75\n0.65\n14.25\n7.96\n4.97\n3.18\n2.56\n2.26\n2.10\n0.3\n3.64\n2.53\n1.81\n1.21\n0.94\n0.76\n0.65\n14.48\n8.25\n5.22\n3.37\n2.68\n2.33\n2.12\n0.5\n3.69\n2.61\n1.91\n1.30\n1.00\n0.81\n0.67\n14.93\n8.75\n5.68\n3.71\n2.92\n2.47\n2.17\n0.8\n3.87\n2.91\n2.25\n1.58\n1.23\n0.96\n0.76\n16.41\n10.30\n6.98\n4.63\n3.58\n2.90\n2.41\n1.0\n4.21\n3.38\n2.72\n1.96\n1.50\n1.15\n0.85\n19.17\n12.59\n8.85\n5.98\n4.55\n3.59\n2.82\nDDBM\n317.22\n137.15\n46.74\n7.79\n2.40\n0.88\n0.53\n328.33\n151.93\n41.03\n15.19\n6.54\n3.34\n2.26\na deterministic sampler with Œ∑ = 0 yields superior performance compared to stochastic samplers\nwith Œ∑ > 0. We attribute it to the characteristics of the datasets, where the target image is highly\ncorrelated with and dependent on the condition, resulting in a generative model that lacks diver-\nsity. In this case, a straightforward mapping without the involvement of stochasticity is preferred.\nConversely, for image inpainting on the more diverse dataset ImageNet 256√ó256, the parameter Œ∑\nexhibits significance across different NFEs. When NFE ‚â§20, Œ∑ = 0 is near the optimal choice, with\nFID steadily increasing as Œ∑ ascends. However, when NFE ‚â•50, a relatively large level of stochas-\nticity at Œ∑ = 0.8 or even Œ∑ = 1 yields optimal FID. Notably, the FID of Œ∑ = 0 converges to 3.91\nat NFE = 100, with no further improvement at larger NFEs, indicating convergence to the ground-\ntruth sample by the corresponding PF-ODE. This observation aligns with diffusion models, where\ndeterministic sampling facilitates rapid convergence, while introducing stochasticity in sampling\nenhances diversity, ultimately culminating in the highest sample quality when NFE is substantial.\nTable 6: The effects of high-order methods, measured by FID.\nSampler\nNFE\n5\n10\n20\n50\n100\n5\n10\n20\n50\n100\n5\n10\n20\n50\n100\nImage Translation\nInpainting\nEdges‚ÜíHandbags (64 √ó 64)\nDIODE-Outdoor (256 √ó 256)\nImageNet (256 √ó 256)\nDBIM (Œ∑ = 0)\n3.62\n2.49\n1.76\n1.17\n0.91\n14.25\n7.96\n4.97\n3.18\n2.56\n6.08\n4.51\n4.11\n3.95\n3.91\nDBIM (2nd-order)\n3.44\n2.16\n1.48\n0.99\n0.79\n13.54\n7.18\n4.34\n2.87\n2.41\n5.53\n4.33\n4.07\n3.94\n3.91\nDBIM (3rd-order)\n3.40\n2.12\n1.45\n0.97\n0.79\n13.41\n7.01\n4.20\n2.84\n2.40\n5.50\n4.34\n4.07\n3.93\n3.91\nHigh-Order Methods\nWe further demonstrate the effects of high-order methods by comparing\nthem to deterministic DBIM, the first-order case. As shown in Table 6, high-order methods consis-\ntently improve FID scores in image translation tasks, as well as in inpainting tasks when NFE‚â§50,\nresulting in enhanced generation quality in the low NFE regime. Besides, the 3rd-order variant\nperforms slightly better than the 2nd-order variant. However, in contrast to the numerical solvers\nin diffusion models, the benefits of high-order extensions are relatively minor in diffusion bridges\nand less pronounced than the improvement when adjusting Œ∑ from 1 to 0. Nevertheless, high-order\nDBIMs are significantly more efficient than DDBM‚Äôs PF-ODE-based high-order solvers.\nAs illustrated in Figure 1, our high-order sampler produces images of similar semantic content to\nthe first-order case, using the same booting noise. In contrast, the visual quality is improved with\nfiner textures, resulting in better FID. This indicates that the high-order gradient information from\npast network outputs benefits the generation quality by adding high-frequency visual details.\nGeneration Diversity\nWe quantitatively measure the generation diversity by the diversity score,\ncalculated as the pixel-level variance of multiple generations, following CMDE (Batzolis et al.,\n2021) and BBDM (Li et al., 2023). As detailed in Appendix G.2, increasing NFE or decreasing Œ∑\ncan both increase the diversity score, confirming the effect of the booting noise.\n6.2\nRECONSTRUCTION AND INTERPOLATION\nAs discussed in Section 4.2, the deterministic nature of DBIMs at Œ∑ = 0 and its connection to neural\nODEs enable faithful encoding and reconstruction by treating the booting noise as the latent variable.\nFurthermore, employing spherical linear interpolation in the latent space and subsequently decoding\n9\n"
    },
    {
      "page_number": 10,
      "text": "Published as a conference paper at ICLR 2025\n20 steps\nCondition\n20 steps\n100 steps\n100 steps\n(a) Encoding/Reconstruction\n(b) Semantic Interpolation\nFigure 4: Illustration of generation diversity with deterministic DBIMs.\nback to the image space allows for semantic image interpolation in image translation and image\nrestoration tasks. These capabilities cannot be achieved by DBIMs with Œ∑ > 0, or by DDBM‚Äôs hy-\nbrid sampler which incorporates stochastic steps. We showcase the encoding and decoding results in\nFigure 4a, indicating that accurate reconstruction is achievable with a sufficient number of sampling\nsteps. We also illustrate the interpolation process in Figure 4b.\n7\nCONCLUSION\nIn this work, we introduce diffusion bridge implicit models (DBIMs) for accelerated sampling of\nDDBMs without extra training. In contrast to DDBM‚Äôs continuous-time generation processes, we\nconcentrate on discretized sampling steps and propose a series of generalized diffusion bridge mod-\nels including non-Markovian variants. The induced sampling procedures serve as bridge counter-\nparts and extensions of DDIMs and are further extended to develop high-order numerical solvers,\nfilling the missing perspectives in the context of diffusion bridges. Experiments on high-resolution\ndatasets and challenging inpainting tasks demonstrate DBIM‚Äôs superiority in both the sample quality\nand sample efficiency, achieving state-of-the-art FID scores with 100 steps and providing up to 25√ó\nacceleration of DDBM‚Äôs sampling procedure.\nFigure 5: DBIM case\n(Œ∑ = 0, NFE=500).\nLimitations and Failure Cases Despite the notable speed-up for diffusion\nbridge models, DBIMs still lag behind GAN-based methods in one-step gen-\neration. The generation quality is unsatisfactory when NFE is small, and\nblurry regions still exist even using high-order methods (Figure 1). This is\nnot fast enough for real-time applications. Besides, as a training-free infer-\nence algorithm, DBIM cannot surpass the capability and quality upper bound\nof the pretrained diffusion bridge model. In difficult and delicate inpainting\nscenarios, such as human faces and hands, DBIM fails to fix the artifacts, even\nunder large NFEs.\nACKNOWLEDGMENTS\nThis work was supported by the NSFC Projects (Nos. 62350080, 62106120, 92270001), the Na-\ntional Key Research and Development Program of China (No. 2021ZD0110502), Tsinghua Institute\nfor Guo Qiang, and the High Performance Computing Center, Tsinghua University. J.Z was also\nsupported by the XPlorer Prize.\nREFERENCES\nMichael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying\nframework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\nFan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao,\nShilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-\nvideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.\nShane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,\n2018.\nGeorgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch¬®onlieb, and Christian Etmann. Conditional\nimage generation with score-based diffusion models. arXiv preprint arXiv:2111.13606, 2021.\n10\n"
    },
    {
      "page_number": 11,
      "text": "Published as a conference paper at ICLR 2025\nChristopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, vol-\nume 4. Springer, 2006.\nMari Paz Calvo and C¬¥esar Palencia. A class of explicit multistep exponential integrators for semi-\nlinear problems. Numerische Mathematik, 102:367‚Äì381, 2006.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wave-\ngrad: Estimating gradients for waveform generation. In International Conference on Learning\nRepresentations, 2021a.\nTianrong Chen, Guan-Horng Liu, and Evangelos A Theodorou.\nLikelihood training of schr\\‚Äù\nodinger bridge using forward-backward sdes theory. arXiv preprint arXiv:2110.11291, 2021b.\nZehua Chen, Guande He, Kaiwen Zheng, Xu Tan, and Jun Zhu. Schrodinger bridges beat diffusion\nmodels on text-to-speech synthesis. arXiv preprint arXiv:2312.03491, 2023.\nZixiang Chen, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, and Quanquan Gu. Fast\nsampling via discrete non-markov diffusion models with predetermined transition time. In The\nThirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\nHyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul\nYe. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022.\nValentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr¬®odinger\nbridge with applications to score-based generative modeling. Advances in Neural Information\nProcessing Systems, 34:17695‚Äì17709, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248‚Äì255. IEEE, 2009.\nWei Deng, Weijian Luo, Yixin Tan, Marin BiloÀás, Yu Chen, Yuriy Nevmyvaka, and Ricky TQ Chen.\nVariational schr\\‚Äù odinger diffusion models. arXiv preprint arXiv:2405.04795, 2024.\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis.\nIn Advances in Neural Information Processing Systems, volume 34, pp. 8780‚Äì8794, 2021.\nJoseph L Doob and JI Doob. Classical potential theory and its probabilistic counterpart, volume\n262. Springer, 1984.\nMartin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, and Nader Masmoudi.\nSeeds: Exponential sde solvers for fast high-quality sampling from diffusion models.\narXiv\npreprint arXiv:2305.14267, 2023.\nAgrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang,\nand Jos¬¥e Lezama.\nPhotorealistic video generation with diffusion models.\narXiv preprint\narXiv:2312.06662, 2023.\nGuande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, and Jun Zhu. Consistency diffusion bridge\nmodels. arXiv preprint arXiv:2410.22637, 2024.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGANs trained by a two time-scale update rule converge to a local Nash equilibrium.\nIn Is-\nabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.\nVishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems,\nvolume 30, pp. 6626‚Äì6637, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances\nin Neural Information Processing Systems, volume 33, pp. 6840‚Äì6851, 2020.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n11\n"
    },
    {
      "page_number": 12,
      "text": "Published as a conference paper at ICLR 2025\nMarlis Hochbruck and Alexander Ostermann. Explicit exponential Runge-Kutta methods for semi-\nlinear parabolic problems. SIAM Journal on Numerical Analysis, 43(3):1069‚Äì1090, 2005.\nMarlis Hochbruck, Alexander Ostermann, and Julia Schweitzer. Exponential rosenbrock-type meth-\nods. SIAM Journal on Numerical Analysis, 47(1):786‚Äì803, 2009.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditional adversarial networks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1125‚Äì1134, 2017.\nKiyosi ItÀÜo. On a formula concerning stochastic differentials. Nagoya Mathematical Journal, 3:\n55‚Äì65, 1951.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\nbased generative models. In Advances in Neural Information Processing Systems, 2022.\nBahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration\nmodels. In Advances in Neural Information Processing Systems, 2022.\nDongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka,\nYutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning prob-\nability flow ode trajectory of diffusion. In The Twelfth International Conference on Learning\nRepresentations, 2023.\nDiederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In\nAdvances in Neural Information Processing Systems, 2021.\nBo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Bbdm: Image-to-image translation with brownian\nbridge diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern Recognition, pp. 1952‚Äì1961, 2023.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching\nfor generative modeling. arXiv preprint arXiv:2210.02747, 2022.\nGongye Liu, Haoze Sun, Jiayi Li, Fei Yin, and Yujiu Yang. Accelerating diffusion models for inverse\nproblems through shortcut sampling. arXiv preprint arXiv:2305.16965, 2023a.\nGuan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima\nAnandkumar. I2sb: Image-to-image schr¬®odinger bridge. In International Conference on Ma-\nchine Learning, pp. 22042‚Äì22062. PMLR, 2023b.\nCheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood\ntraining for score-based diffusion odes by high order denoising score matching. In International\nConference on Machine Learning, pp. 14429‚Äì14460. PMLR, 2022a.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural\nInformation Processing Systems, 2022b.\nChenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit:\nImage synthesis and editing with stochastic differential equations. In International Conference\non Learning Representations, 2022.\nShakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv\npreprint arXiv:1610.03483, 2016.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and\nediting with text-guided diffusion models. In International Conference on Machine Learning, pp.\n16784‚Äì16804. PMLR, 2022.\nKushagra Pandey, Maja Rudolph, and Stephan Mandt. Efficient integrators for diffusion generative\nmodels. arXiv preprint arXiv:2310.07894, 2023.\n12\n"
    },
    {
      "page_number": 13,
      "text": "Published as a conference paper at ICLR 2025\nKushagra Pandey, Ruihan Yang, and Stephan Mandt. Fast samplers for inverse problems in iterative\nrefinement models. arXiv preprint arXiv:2405.17673, 2024.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¬®orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 10684‚Äì10695, 2022.\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David\nFleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH\n2022 Conference Proceedings, pp. 1‚Äì10, 2022.\nAxel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion dis-\ntillation. arXiv preprint arXiv:2311.17042, 2023.\nYuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr¬®odinger\nbridge matching. Advances in Neural Information Processing Systems, 36, 2024.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learn-\ning, pp. 2256‚Äì2265. PMLR, 2015.\nVignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause,\nand Charlotte Bunne. Aligned diffusion schr¬®odinger bridges. In Uncertainty in Artificial Intelli-\ngence, pp. 1985‚Äì1995. PMLR, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-\ntional Conference on Learning Representations, 2021a.\nJiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion\nmodels for inverse problems. In International Conference on Learning Representations, 2023a.\nURL https://openreview.net/forum?id=9_gsMA8MRKQ.\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-\nbased diffusion models. In Advances in Neural Information Processing Systems, volume 34, pp.\n1415‚Äì1428, 2021b.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In Interna-\ntional Conference on Learning Representations, 2021c.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International\nConference on Machine Learning, pp. 32211‚Äì32252. PMLR, 2023b.\nXuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for\nimage-to-image translation. arXiv preprint arXiv:2203.08382, 2022.\nIgor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z Dai, Andrea F\nDaniele, Mohammadreza Mostajabi, Steven Basart, Matthew R Walter, et al. Diode: A dense\nindoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019.\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural compu-\ntation, 23(7):1661‚Äì1674, 2011.\nYinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion\nnull-space model. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=mRieQgMtNTQ.\nYuang Wang, Siyeop Yoon, Pengfei Jin, Matthew Tivnan, Zhennong Chen, Rui Hu, Li Zhang,\nZhiqiang Chen, Quanzheng Li, and Dufan Wu. Implicit image-to-image schrodinger bridge for\nimage restoration. arXiv preprint arXiv:2403.06069, 2024a.\n13\n"
    },
    {
      "page_number": 14,
      "text": "Published as a conference paper at ICLR 2025\nYufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu,\nYu Qiao, Alex C Kot, and Bihan Wen. Sinsr: diffusion-based image super-resolution in a single\nstep. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 25796‚Äì25805, 2024b.\nJintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen.\nSageatten-\ntion2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. arXiv\npreprint arXiv:2411.10958, 2024.\nJintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit\nattention for plug-and-play inference acceleration. In International Conference on Learning Rep-\nresentations (ICLR), 2025a.\nJintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei\nChen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint\narXiv:2502.18137, 2025b.\nQinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator.\nIn The Eleventh International Conference on Learning Representations, 2022.\nQinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit\nmodels. arXiv preprint arXiv:2206.05564, 2022.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 586‚Äì595, 2018.\nKaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode\nsolver with empirical model statistics. In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023a.\nKaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood\nestimation for diffusion odes. In International Conference on Machine Learning, pp. 42363‚Äì\n42389. PMLR, 2023b.\nKaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked\ndiffusion models are secretly time-agnostic masked models and exploit inaccurate categorical\nsampling. arXiv preprint arXiv:2409.02908, 2024.\nLinqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models.\narXiv preprint arXiv:2309.16948, 2023.\n14\n"
    },
    {
      "page_number": 15,
      "text": "Published as a conference paper at ICLR 2025\nA\nRELATED WORK\nFast Sampling of Diffusion Models\nFast sampling of diffusion models can be classified into\ntraining-free and training-based methods. A prevalent training-free fast sampler is the denoising\ndiffusion implicit models (DDIMs) (Song et al., 2021a) that employ alternative non-Markovian gen-\neration processes in place of DDPMs, a discrete-time diffusion model. ScoreSDE (Song et al.,\n2021c) further links discrete-time DDPMs to continuous-time score-based models, unrevealing the\ngeneration process to be ordinary and stochastic differential equations (ODEs and SDEs). DDIM\ncan be generalized to develop integrators for broader diffusion models (Zhang et al., 2022; Pandey\net al., 2023). The concept of implicit sampling, in a broad sense, can also be extended to discrete\ndiffusion models (Chen et al., 2024; Zheng et al., 2024), although there are fundamental differences\nin their underlying mechanisms. Subsequent training-free samplers concentrate on developing ded-\nicated numerical solvers to the diffusion ODE or SDE, particularly Heun‚Äôs methods (Karras et al.,\n2022) and exponential integrators (Zhang & Chen, 2022; Lu et al., 2022b; Zheng et al., 2023a; Gon-\nzalez et al., 2023). These methods typically require around 10 steps for high-quality generation. In\ncontrast, training-based methods, particularly adversarial distillation (Sauer et al., 2023) and consis-\ntency distillation (Song et al., 2023b; Kim et al., 2023), become notable for their ability to achieve\nhigh-quality generation with just one or two steps. Our work serves as a thorough exploration of\ntraining-free fast sampling of DDBMs. Exploring bridge distillation methods, such as consistency\nbridge distillation (He et al., 2024), would be promising future research avenues to decrease the\ninference cost further. Infrastructure improvements, such as quantized or sparse attention (Zhang\net al., 2025a;b; 2024), can also be used to accelerate the inference of diffusion bridge models.\nDiffusion Bridges\nDiffusion bridges (De Bortoli et al., 2021; Chen et al., 2021b; Liu et al., 2023b;\nSomnath et al., 2023; Zhou et al., 2023; Chen et al., 2023; Shi et al., 2024; Deng et al., 2024) are a\npromising generative variant of diffusion models for modeling the transport between two arbitrary\ndistributions. One line of work is the diffusion Schrodinger bridge models (De Bortoli et al., 2021;\nChen et al., 2021b; Shi et al., 2024; Deng et al., 2024), which solves an entropy-regularized opti-\nmal transport problem between two probability distributions. However, their reliance on expensive\niterative procedures has limited their application scope, particularly for high-dimensional data. Sub-\nsequent works have endeavored to enhance the tractability of the Schrodinger bridge problem by\nmaking assumptions such as paired data (Liu et al., 2023b; Somnath et al., 2023; Chen et al., 2023).\nOn the other hand, DDBMs (Zhou et al., 2023) construct diffusion bridges via Doob‚Äôs h-transform,\noffering a reverse-time perspective of a diffusion process conditioned on given endpoints. This ap-\nproach aligns the design spaces and training algorithms of DDBMs closely with those of score-based\ngenerative models, leading to state-of-the-art performance in image translation tasks. However, the\nsampling procedure of DDBMs still relies on inefficient simulations of differential equations, lack-\ning theoretical insights to develop efficient samplers. BBDM (Li et al., 2023) and I3SB (Wang\net al., 2024a) extend the concept of DDIM to the contexts of Brownian bridge and I2SB (Liu et al.,\n2023b), respectively. SinSR (Wang et al., 2024b) is also motivated by DDIM, while the application\nis concentrated on the mean-reverting diffusion process, which ends in a Gaussian instead of a delta\ndistribution. In contrast to them, our work provides the first systematic exploration of implicit sam-\npling within the broader DDBM framework, offering theoretical insights and connections while also\nproposing novel high-order diffusion bridge solvers.\nB\nPROOFS\nB.1\nPROOF OF PROPOSITION 3.1\nProof. Since q(œÅ) in Eqn. (10) is factorized as q(œÅ)(xt0:N‚àí1|xT ) = q0(x0)q(œÅ)(xt1:N‚àí1|x0, xT )\nwhere q(œÅ)(xt1:N‚àí1|x0, xT ) = QN‚àí1\nn=1 q(œÅ)(xtn|x0, xtn+1, xT ), we have q(œÅ)(x0|xT ) = q0(x0) =\nq(x0|xT ), which proves the case for n = 0. For 1 ‚â§n ‚â§N ‚àí1, we have\nq(œÅ)(xtn|xT ) =\nZ\nq(œÅ)(xtn|x0, xT )q(œÅ)(x0|xT )dx0\n(19)\nand\nq(xtn|xT ) =\nZ\nq(xtn|x0, xT )q(x0|xT )dx0\n(20)\n15\n"
    },
    {
      "page_number": 16,
      "text": "Published as a conference paper at ICLR 2025\nSince q(œÅ)(x0|xT ) = q(x0|xT ), we only need to prove q(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ).\nFirstly, when n = N ‚àí1, we have tn+1 = T. Note that œÅ is restricted by œÅN‚àí1 = ctN‚àí1, and\nEqn. (11) becomes\nq(œÅ)(xtN‚àí1|x0, xT ) = N(atN‚àí1xT + btN‚àí1x0, c2\ntN‚àí1I)\n(21)\nwhich is exactly the same as the forward transition kernel of q in Eqn. (6).\nTherefore,\nq(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ) holds for n = N ‚àí1.\nSecondly, suppose q(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ) holds for n = k, we aim to prove that it\nholds for n = k ‚àí1. Specifically, q(œÅ)(xtk‚àí1|x0, xT ) can be expressed as\nq(œÅ)(xtk‚àí1|x0, xT ) =\nZ\nq(œÅ)(xtk‚àí1|x0, xtk, xT )q(œÅ)(xtk|x0, xT )dxtk\n=\nZ\nq(œÅ)(xtk‚àí1|x0, xtk, xT )q(xtk|x0, xT )dxtk\n=\nZ\nN(xtk‚àí1; ¬µk‚àí1|k, œÅ2\nk‚àí1I)N(xtk; atkxT + btkx0, c2\ntkI)dxtk\n(22)\nwhere\n¬µk‚àí1|k = atk‚àí1xT + btk‚àí1x0 +\nq\nc2\ntk‚àí1 ‚àíœÅ2\nk‚àí1\nxtk ‚àíatkxT ‚àíbtkx0\nctk\n(23)\nFrom (Bishop & Nasrabadi, 2006) (2.115), q(œÅ)(xtk‚àí1|x0, xT ) is a Gaussian, denoted as\nN(¬µk‚àí1, Œ£k‚àí1), where\n¬µk‚àí1 = atk‚àí1xT + btk‚àí1x0 +\nq\nc2\ntk‚àí1 ‚àíœÅ2\nk‚àí1\natkxT + btkx0 ‚àíatkxT ‚àíbtkx0\nctk\n= atk‚àí1xT + btk‚àí1x0\n(24)\nand\nŒ£k‚àí1 = œÅ2\nk‚àí1I +\nq\nc2\ntk‚àí1 ‚àíœÅ2\nk‚àí1\nctk\nc2\ntk\nq\nc2\ntk‚àí1 ‚àíœÅ2\nk‚àí1\nctk\nI\n= c2\ntk‚àí1I\n(25)\nTherefore, q(œÅ)(xtk‚àí1|x0, xT ) = q(xtk‚àí1|x0, xT ) = N(atk‚àí1xT + btk‚àí1x0, c2\ntk‚àí1I). By math-\nematical deduction, q(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ) holds for every 1 ‚â§n ‚â§N ‚àí1, which\ncompletes the proof.\nB.2\nPROOF OF PROPOSITION 3.2\nProof. Substituting Eqn. (10) and the joint distribution into Eqn. (13), we have\nJ (œÅ)(Œ∏)\n=Eq(xT )Eq(œÅ)(xt0:N‚àí1|xT )\nh\nlog q(œÅ)(xt1:N‚àí1|x0, xT ) ‚àílog pŒ∏(xt0:N‚àí1|xT )\ni\n=Eq(xT )Eq(œÅ)(xt0:N‚àí1|xT )\n\"N‚àí1\nX\nn=1\nlog q(œÅ)(xtn|x0, xtn+1, xT ) ‚àí\nN‚àí1\nX\nn=0\nlog pŒ∏(xtn|xtn+1, xT )\n#\n=\nN‚àí1\nX\nn=1\nEq(xT )Eq(œÅ)(x0,xtn+1|xT )\nh\nDKL(q(œÅ)(xtn|x0, xtn+1, xT ) ‚à•pŒ∏(xtn|xtn+1, xT ))\ni\n‚àíEq(xT )Eq(œÅ)(x0,xt1|xT ) [log pŒ∏(x0|xt1, xT )]\n(26)\nwhere\nDKL(q(œÅ)(xtn|x0, xtn+1, xT ) ‚à•pŒ∏(xtn|xtn+1, xT ))\n=DKL(q(œÅ)(xtn|x0, xtn+1, xT ) ‚à•q(œÅ)(xtn|xŒ∏(xtn+1, tn+1, xT ), xtn+1, xT ))\n=d2\nn‚à•xŒ∏(xtn+1, tn+1, xT ) ‚àíx0‚à•2\n2\n2œÅ2n\n(27)\n16\n"
    },
    {
      "page_number": 17,
      "text": "Published as a conference paper at ICLR 2025\nwhere we have denoted dn := btn ‚àí\nq\nc2\ntn ‚àíœÅ2n\nbtn+1\nctn+1 . Besides, we have\nlog pŒ∏(x0|xt1, xT ) = log q(œÅ)(x0|xŒ∏(xt1, t1, xT ), xt1, xT )\n= log N(xŒ∏(xt1, t1, xT ), œÅ2\n0I)\n= ‚àí‚à•xŒ∏(xt1, t1, xT ) ‚àíx0‚à•2\n2\n2œÅ2\n0\n+ C\n(28)\nwhere C is irrelevant to Œ∏. According to Eqn. (6), the conditional score is\n‚àáxt log q(xt|x0, xT ) = ‚àíxt ‚àíatxT ‚àíbtx0\nc2\nt\n(29)\nTherefore,\n‚à•xŒ∏(xtn, tn, xT ) ‚àíx0‚à•2\n2\n=c4\ntn\nb2\ntn\n\r\r\r\r‚àíxtn ‚àíatnxT ‚àíbtnxŒ∏(xtn, tn, xT )\nc2\ntn\n‚àí\n\u0012\n‚àíxtn ‚àíatnxT ‚àíbtnx0\nc2\ntn\n\u0013\r\r\r\r\n2\n2\n=c4\ntn\nb2\ntn\n‚à•sŒ∏(xtn, tn, xT ) ‚àí‚àáxtn log q(xtn|x0, xT )‚à•2\n2\n(30)\nwhere sŒ∏ is related to xŒ∏ by\nsŒ∏(xt, t, xT ) = ‚àíxt ‚àíatxT ‚àíbtxŒ∏(xt, t, xT )\nc2\nt\n(31)\nDefine d0 = 1, the loss J (œÅ)(Œ∏) is further simplified to\nJ (œÅ)(Œ∏) ‚àíC\n=\nN‚àí1\nX\nn=0\nEq(xT )q(œÅ)(x0,xtn+1|xT )\n\u0014d2\nn‚à•xŒ∏(xtn+1, tn+1, xT ) ‚àíx0‚à•2\n2\n2œÅ2n\n\u0015\n=\nN\nX\nn=1\nd2\nn‚àí1\n2œÅ2\nn‚àí1\nEq(xT )q(x0|xT )q(xtn|x0,xT )\n\u0002\n‚à•xŒ∏(xtn, tn, xT ) ‚àíx0‚à•2\n2\n\u0003\n=\nN\nX\nn=1\nd2\nn‚àí1c4\ntn\n2œÅ2\nn‚àí1b2\ntn\nEq(xT )q(x0|xT )q(xtn|x0,xT )\n\u0002\n‚à•sŒ∏(xtn, tn, xT ) ‚àí‚àáxtn log q(xtn|x0, xT )‚à•2\n2\n\u0003\n(32)\nCompared to the training objective of DDBMs in Eqn. (9), J (œÅ)(Œ∏) is totally equivalent up to a\nconstant, by concentrating on the discretized timesteps {tn}N\nn=1, choosing q(xT )q(x0|xT ) as the\npaired data distribution and using the weighting function Œ≥ that satisfies Œ≥(tn) =\nd2\nn‚àí1c4\ntn\n2œÅ2\nn‚àí1b2\ntn .\nB.3\nPROOF OF PROPOSITION 4.1\nProof. We first represent the PF-ODE (Eqn. (8))\ndxt =\n\u0014\nf(t)xt ‚àíg2(t)\n\u00121\n2‚àáxt log q(xt|xT ) ‚àí‚àáxt log qT |t(xT |xt)\n\u0013\u0015\ndt\n(33)\nwith the data predictor xŒ∏(xt, t, xT ). We replace the bridge score ‚àáxt log q(xt|xT ) with the net-\nwork sŒ∏(xt, t, xT ), which is related to xŒ∏(xt, t, xT ) by Eqn. (14). Besides, ‚àáxt log qT |t(xT |xt)\n17\n"
    },
    {
      "page_number": 18,
      "text": "Published as a conference paper at ICLR 2025\ncan be analytically computed as\n‚àáxt log qT |t(xT |xt) = ‚àáxt log q(xt|x0, xT )q(xT |x0)\nq(xt|x0)\n= ‚àáxt log q(xt|x0, xT ) ‚àí‚àáxt log q(xt|x0)\n= ‚àíxt ‚àíatxT ‚àíbtx0\nc2\nt\n+ xt ‚àíŒ±tx0\nœÉ2\nt\n= ‚àí\nSNRT\nSNRt (xt ‚àíŒ±t\nŒ±T xT )\nœÉ2\nt (1 ‚àíSNRT\nSNRt )\n= ‚àí\nat( Œ±T\nŒ±t xt ‚àíxT )\nc2\nt\n(34)\nSubstituting Eqn. (14) and Eqn. (34) into Eqn. (33), the PF-ODE is transformed to\ndxt =\n\"\nf(t)xt ‚àíg2(t)\n \n‚àíxt ‚àíatxT ‚àíbtxŒ∏(xt, t, xT )\n2c2\nt\n+\nat( Œ±T\nŒ±t xt ‚àíxT )\nc2\nt\n!#\ndt\n=\n\u0014\u0012\nf(t) + g2(t)\n1 ‚àí2at\nŒ±T\nŒ±t\n2c2\nt\n\u0013\nxt + g2(t) at\n2c2\nt\nxT ‚àíg2(t) bt\n2c2\nt\nxŒ∏(xt, t, xT )\n\u0015\ndt\n=\n\u0014\u0012\nf(t) + g2(t)\nœÉ2\nt\n‚àíg2(t)\n2c2\nt\n\u0013\nxt + g2(t) at\n2c2\nt\nxT ‚àíg2(t) bt\n2c2\nt\nxŒ∏(xt, t, xT )\n\u0015\ndt\n(35)\nOn the other hand, the ODE corresponding to DBIMs (Eqn. (16)) can be expanded as\ndxt\nct\n‚àíc‚Ä≤\nt\nc2\nt\nxtdt =\n\"\u0012at\nct\n\u0013‚Ä≤\nxT +\n\u0012bt\nct\n\u0013‚Ä≤\nxŒ∏(xt, t, xT )\n#\ndt\n(36)\nwhere we have denoted (¬∑)‚Ä≤ := d(¬∑)\ndt . Further simplification gives\ndxt =\n\u0014c‚Ä≤\nt\nct\nxt +\n\u0012\na‚Ä≤\nt ‚àíat\nc‚Ä≤\nt\nct\n\u0013\nxT +\n\u0012\nb‚Ä≤\nt ‚àíbt\nc‚Ä≤\nt\nct\n\u0013\nxŒ∏(xt, t, xT )\n\u0015\ndt\n(37)\nThe coefficients at, bt, ct are determined by the noise schedule Œ±t, œÉt in diffusion models. Comput-\ning their derivatives will produce terms involving f(t), g(t), which are used to define the forward\nSDE. As revealed in diffusion models, f(t), g(t) are related to Œ±t, œÉt by f(t) = d log Œ±t\ndt\n,\ng2(t) =\ndœÉ2\nt\ndt ‚àí2 d log Œ±t\ndt\nœÉ2\nt . We can derive the reverse relation of Œ±t, œÉt and f(t), g(t):\nŒ±t = e\nR t\n0 f(œÑ)dœÑ,\nœÉ2\nt = Œ±2\nt\nZ t\n0\ng2(œÑ)\nŒ±2œÑ\ndœÑ\n(38)\nwhich can facilitate subsequent calculation. We first compute the derivative of a common term in\nat, bt, ct:\n\u0012\n1\nSNRt\n\u0013‚Ä≤\n=\n\u0012œÉ2\nt\nŒ±2\nt\n\u0013‚Ä≤\n= g2(t)\nŒ±2\nt\n(39)\nFor ct, since c2\nt = œÉ2\nt (1 ‚àíSNRT\nSNRt ), we have\nc‚Ä≤\nt\nct\n= (log ct)‚Ä≤ = 1\n2(log c2\nt)‚Ä≤ = 1\n2(log œÉ2\nt + log(1 ‚àíSNRT\nSNRt\n))‚Ä≤\n(40)\nwhere\n(log œÉ2\nt )‚Ä≤ = (log œÉ2\nt\nŒ±2\nt\n)‚Ä≤ + (log Œ±2\nt)‚Ä≤ = g2(t)\nŒ±2\nt\nŒ±2\nt\nœÉ2\nt\n+ 2f(t) = g2(t)\nœÉ2\nt\n+ 2f(t)\n(41)\nand\n(log(1 ‚àíSNRT\nSNRt\n))‚Ä≤ = ‚àíSNRT\n1 ‚àíSNRT\nSNRt\n\u0012\n1\nSNRt\n\u0013‚Ä≤\n= ‚àíSNRT\nc2\nt\nœÉ2\nt\ng2(t)\nŒ±2\nt\n= ‚àíg2(t)\nc2\nt\nSNRT\nSNRt\n(42)\n18\n"
    },
    {
      "page_number": 19,
      "text": "Published as a conference paper at ICLR 2025\nSubstituting Eqn. (41) and Eqn. (42) into Eqn. (40), and using the relation SNRT\nSNRt = 1 ‚àíc2\nt\nœÉ2\nt , we have\nc‚Ä≤\nt\nct\n= f(t) + g2(t)\n2œÉ2\nt\n‚àíg2(t)\n2c2\nt\nSNRT\nSNRt\n= f(t) + g2(t)\nœÉ2\nt\n‚àíg2(t)\n2c2\nt\n(43)\nFor at, since at = Œ±t\nŒ±T\nSNRT\nSNRt , we have\na‚Ä≤\nt\nat\n= (log at)‚Ä≤ = (log Œ±t)‚Ä≤ + (log SNRT\nSNRt\n)‚Ä≤ = f(t) + SNRt\ng2(t)\nŒ±2\nt\n= f(t) + g2(t)\nœÉ2\nt\n(44)\nFor bt, since bt = Œ±t(1 ‚àíSNRT\nSNRt ), we have\nb‚Ä≤\nt\nbt\n= (log bt)‚Ä≤ = (log Œ±t)‚Ä≤+(log(1‚àíSNRT\nSNRt\n))‚Ä≤ = f(t)‚àíg2(t)\nc2\nt\nSNRT\nSNRt\n= f(t)+g2(t)\nœÉ2\nt\n‚àíg2(t)\nc2\nt\n(45)\nTherefore,\na‚Ä≤\nt ‚àíat\nc‚Ä≤\nt\nct\n= at(a‚Ä≤\nt\nat\n‚àíc‚Ä≤\nt\nct\n) = g2(t)\n2c2\nt\nat\n(46)\nand\nb‚Ä≤\nt ‚àíbt\nc‚Ä≤\nt\nct\n= bt(b‚Ä≤\nt\nbt\n‚àíc‚Ä≤\nt\nct\n) = ‚àíg2(t)\n2c2\nt\nbt\n(47)\nSubstituting Eqn. (43), Eqn. (46) and Eqn. (47) into the ODE of DBIMs in Eqn. (37), we obtain\nexactly the PF-ODE in Eqn. (35).\nC\nMORE THEORETICAL DISCUSSIONS\nC.1\nMARKOV PROPERTY OF THE GENERALIZED DIFFUSION BRIDGES\nWe aim to analyze the Markov property of the forward process corresponding to our generalized\ndiffusion bridge in Section 3.1. The forward process of q(œÅ) can be induced by Bayes‚Äô rule as\nq(œÅ)(xtn+1|x0, xtn, xT ) = q(œÅ)(xtn|x0, xtn+1, xT )q(œÅ)(xtn+1|x0, xT )\nq(œÅ)(xtn|x0, xT )\n(48)\nwhere q(œÅ)(xt|x0, xT ) = q(xt|x0, xT ) is the marginal distribution of the diffusion bridge in\nEqn. (6), and q(œÅ)(xtn|x0, xtn+1, xT ) is defined in Eqn. (11) as\nq(œÅ)(xtn|x0, xtn+1, xT ) = N(atnxT + btnx0 +\nq\nc2\ntn ‚àíœÅ2n\nxtn+1 ‚àíatn+1xT ‚àíbtn+1x0\nctn+1\n, œÅ2\nnI).\n(49)\nDue to the marginal preservation property (Proposition 3.1), we have q(œÅ)(xtn+1|x0, xT ) =\nq(xtn+1|x0, xT ) and q(œÅ)(xtn|x0, xT ) = q(xtn|x0, xT ), where q(xt|x0, xT ) = N(atxT +\nbtx0, c2\ntI) is the forward transition kernel in Eqn. (6). To identify whether q(œÅ)(xtn+1|x0, xtn, xT )\nis Markovian, we only need to examine the dependence of xtn+1 on x0. To this end, we proceed to\nderive conditions under which ‚àáxtn+1 log q(œÅ)(xtn+1|x0, xtn, xT ) involves terms concerning x0.\nSpecifically, ‚àáxtn+1 log q(œÅ)(xtn+1|x0, xtn, xT ) can be calculated as\n‚àáxtn+1 log q(œÅ)(xtn+1|x0, xtn, xT )\n=‚àáxtn+1 log q(œÅ)(xtn|x0, xtn+1, xT ) + ‚àáxtn+1 log q(œÅ)(xtn+1|x0, xT )\n= ‚àí\nq\nc2\ntn ‚àíœÅ2n(atnxT + btnx0 +\nq\nc2\ntn ‚àíœÅ2n\nxtn+1‚àíatn+1xT ‚àíbtn+1x0\nctn+1\n‚àíxtn)\nctn+1œÅ2n\n‚àíxtn+1 ‚àíatn+1xT ‚àíbtn+1x0\nc2\ntn+1\n=\nbtn+1c2\ntn ‚àíbtnctn+1\nq\nc2\ntn ‚àíœÅ2n\nc2\ntn+1œÅ2n\nx0 + C(xtn, xtn+1, xT )\n(50)\n19\n"
    },
    {
      "page_number": 20,
      "text": "Published as a conference paper at ICLR 2025\nwhere C(xtn, xtn+1, xT ) are terms irrelevant to x0. Therefore,\nq(œÅ)(xtn+1|x0, xtn, xT ) is Markovian ‚áê‚áí\nbtn+1c2\ntn ‚àíbtnctn+1\nq\nc2\ntn ‚àíœÅ2n\nc2\ntn+1œÅ2n\n= 0\n‚áê‚áíœÅn = œÉtn\ns\n1 ‚àíSNRtn+1\nSNRtn\n(51)\nwhich is exactly a boundary choice of the variance parameter œÅ. Under the other boundary choice\nœÅn = 0 and intermediate ones satisfying 0 < œÅn < œÉtn\nq\n1 ‚àí\nSNRtn+1\nSNRtn , the forward process\nq(œÅ)(xtn+1|x0, xtn, xT ) is non-Markovian.\nC.2\nTHE INSIGNIFICANCE OF LOSS WEIGHTING IN TRAINING\nThe insignificance of the weighting mismatch in Proposition 3.2 can be interpreted from two aspects.\nOn the one hand, L consists of independent terms concerning individual timesteps (as long as the\nnetwork‚Äôs parameters are not shared across different t), ensuring that the global minimum remains\nthe same as minimizing the loss at each timestep, regardless of the weighting. On the other hand, L\nunder different weightings are mutually bounded by mint wt\nmaxt Œ≥t LŒ≥(Œ∏) ‚â§Lw(Œ∏) ‚â§maxt wt\nmint Œ≥t LŒ≥(Œ∏). Be-\nsides, it is widely acknowledged that in diffusion models, the weighting corresponding to variational\ninference may yield superior likelihood but suboptimal sample quality (Ho et al., 2020; Song et al.,\n2021c), which is not preferred in practice.\nC.3\nSPECIAL CASES AND RELATIONSHIP WITH PRIOR WORKS\nConnection to Flow Matching\nWhen the noise schedule Œ±t = 1, T = 1 and œÉt = ‚àöŒ≤t, the\nforward process becomes q(xt|x0, x1) = N(txT + (1 ‚àít)x0, Œ≤t(1 ‚àít)) which is a Brownian\nbridge. When Œ≤ ‚Üí0, there will be no intermediate noise and the forward process is similar to flow\nmatching (Lipman et al., 2022; Albergo et al., 2023). In this limit, the DBIM (Œ∑ = 1) updating\nrule from time t to time s < t will become xs = sxT + (1 ‚àís)xŒ∏(xt, t, xT ) =\ns\nt xt + (1 ‚àí\ns\nt )xŒ∏(xt, t) = xt ‚àí(t ‚àís)vŒ∏(xt, t). Here we define vŒ∏(xt, t) :=\nxt‚àíxŒ∏(xt,t)\nt\nas the velocity\nfunction of the probability flow (i.e., the drift of the ODE) in flow matching methods. Therefore, in\nthe flow matching case, DBIM is a simple Euler step of the flow.\nConnection to DDIM\nIn the regions where t is small and SNRT\nSNRt is close to 0, we have at ‚âà0, bt ‚âà\nŒ±t, ct ‚âàœÉt. Therefore, the forward process of DDBM in this case is approximately q(xt|x0, xT ) =\nN(Œ±tx0, œÉ2\nt I), which is the forward process of the corresponding diffusion model. Moreover, in\nthis case, the DBIM (Œ∑ = 0) step is approximately\nxs ‚âàœÉs\nœÉt\nxt + œÉs\n\u0012Œ±s\nœÉs\n‚àíŒ±t\nœÉt\n\u0013\nxŒ∏(xt, t, xT )\n(52)\nwhich is exactly DDIM (Song et al., 2021a), except that the data prediction network xŒ∏ is dependent\non xT . This indicates that when t is small so that the component of xT in xt is negligible, DBIM\nrecovers DDIM.\nConnection to Posterior Sampling\nThe previous work I2SB (Liu et al., 2023b) also employs\ndiffusion bridges with discrete timesteps, though their noise schedule is restricted to the variance\nexploding (VE) type with f(t) = 0 in the forward process. For generation, they adopt a sim-\nilar approach to DDPM (Ho et al., 2020) by iterative sampling from the posterior distribution\npŒ∏(xn‚àí1|xn), which is a parameterized and shortened diffusion bridge between the endpoints\nÀÜx0 = xŒ∏(xn, tn, xN) and xn.\nSince the posterior distribution is not conditioned on xT (ex-\ncept through the parameterized network), the corresponding forward diffusion bridge is Markovian.\nThus, the posterior sampling in I2SB is a special case of DBIM by setting Œ∑ = 0 and f(t) = 0.\nC.4\nPERSPECTIVE OF EXPONENTIAL INTEGRATORS\nExponential integrators (Calvo & Palencia, 2006; Hochbruck et al., 2009) are widely adopted in\nrecent works concerning fast sampling of diffusion models (Zhang & Chen, 2022; Zheng et al.,\n20\n"
    },
    {
      "page_number": 21,
      "text": "Published as a conference paper at ICLR 2025\n2023a; Gonzalez et al., 2023). Suppose we have an ODE\ndxt = [a(t)xt + b(t)FŒ∏(xt, t)]dt\n(53)\nwhere FŒ∏ is the parameterized prediction function that we want to approximate with Taylor ex-\npansion. The usual way of representing its analytic solution xt at time t with respect to an initial\ncondition xs at time s is\nxt = xs +\nZ t\ns\n[a(œÑ)xœÑ + b(œÑ)FŒ∏(xœÑ, œÑ)]dœÑ\n(54)\nBy approximating the involved integrals in Eqn. (54), we can obtain direct discretizations of\nEqn. (53) such as Euler‚Äôs method. The key insight of exponential integrators is that, it is often bet-\nter to utilize the ‚Äúsemi-linear‚Äù structure of Eqn. (53) and analytically cancel the linear term a(t)xt.\nThis way, we can obtain solutions that only involve integrals of FŒ∏ and result in lower discretization\nerrors. Specifically, by the ‚Äúvariation-of-constants‚Äù formula, the exact solution of Eqn. (53) can be\nalternatively given by\nxt = e\nR t\ns a(œÑ)dœÑxs +\nZ t\ns\ne\nR t\nœÑ a(r)drb(œÑ)FŒ∏(xœÑ, œÑ)dœÑ\n(55)\nWe can apply this transformation to the PF-ODE in DDBMs. By collecting the linear terms w.r.t.\nxt, Eqn. (8) can be rewritten as (already derived in Appendix B.3)\ndxt =\n\u0014\u0012\nf(t) + g2(t)\nœÉ2\nt\n‚àíg2(t)\n2c2\nt\n\u0013\nxt + g2(t) at\n2c2\nt\nxT ‚àíg2(t) bt\n2c2\nt\nxŒ∏(xt, t, xT )\n\u0015\ndt\n(56)\nBy corresponding it to Eqn. (53), we have\na(t) = f(t) + g2(t)\nœÉ2\nt\n‚àíg2(t)\n2c2\nt\n,\nb1(t) = g2(t) at\n2c2\nt\n,\nb2(t) = ‚àíg2(t) bt\n2c2\nt\n(57)\nFrom Eqn. (43), Eqn. (46) and Eqn. (47), we know\na(t) = d log ct\ndt\n,\nb1(t) = at\nd log(at/ct)\ndt\n,\nb2(t) = bt\nd log(bt/ct)\ndt\n(58)\nNote that these relations are known in advance when converting from our ODE to the PF-ODE.\nOtherwise, finding them in this inverse conversion will be challenging. The integrals in Eqn. (55)\ncan then be calculated as\nR t\ns a(œÑ)dœÑ = log ct ‚àílog cs. Thus\ne\nR t\ns a(œÑ)dœÑ = ct\ncs\n,\ne\nR t\nœÑ a(r)dr = ct\ncœÑ\n(59)\nTherefore, the exact solution in Eqn. (55) becomes\nxt = ct\ncs\nxs + ct\nZ t\ns\naœÑ\ncœÑ\nxT d log\n\u0012aœÑ\ncœÑ\n\u0013\n+ ct\nZ t\ns\nbœÑ\ncœÑ\nxŒ∏(xœÑ, œÑ, xT )d log\n\u0012bœÑ\ncœÑ\n\u0013\n= ct\ncs\nxs +\n\u0012\nat ‚àíct\ncs\nas\n\u0013\nxT + ct\nZ t\ns\nbœÑ\ncœÑ\nxŒ∏(xœÑ, œÑ, xT )d log\n\u0012bœÑ\ncœÑ\n\u0013\n(60)\nwhich is the same as Eqn. (17) after exchanging s and t and changing the time variable in the integral\nto Œªt = log\n\u0010\nbt\nct\n\u0011\n.\nLastly, we emphasize the advantage of DBIM over employing exponential integrators. First, deriving\nour ODE via exponential integrators requires the PF-ODE as preliminary. However, the PF-ODE\nalone cannot handle the singularity at the start point and presents theoretical challenges. Moreover,\nthe conversion process from the PF-ODE to our ODE is intricate, while DBIM retains the overall\nsimplicity. Additionally, DBIM supports varying levels of stochasticity during sampling, unlike the\ndeterministic nature of ODE-based methods. This stochasticity can mitigate sampling errors via the\nLangevin mechanism (Song et al., 2021c), potentially enhancing the generation quality.\n21\n"
    },
    {
      "page_number": 22,
      "text": "Published as a conference paper at ICLR 2025\nD\nDERIVATION OF OUR HIGH-ORDER NUMERICAL SOLVERS\nHigh-order solvers of Eqn. (17) can be developed by approximating xŒ∏ in the integral with Tay-\nlor expansion.\nSpecifically, as a function of Œª, we have xŒ∏(xtŒª, tŒª, xT ) ‚âàxŒ∏(xt, t, xT ) +\nPn\nk=1\n(Œª‚àíŒªt)k\nk!\nx(k)\nŒ∏ (xt, t, xT ), where x(k)\nŒ∏ (xt, t, xT ) =\ndkxŒ∏(xtŒª,tŒª,xT )\ndŒªk\n\f\f\f\nŒª=Œªt is the k-th order\nderivative w.r.t. Œª, which can be estimated with finite difference of past network outputs.\n2nd-Order Case\nWith the Taylor expansion xŒ∏(xtŒª, tŒª, xT )\n‚âà\nxŒ∏(xt, t, xT ) + (Œª ‚àí\nŒªt)x(1)\nŒ∏ (xt, t, xT ), we have\nZ Œªs\nŒªt\neŒªxŒ∏(xtŒª, tŒª, xT )dŒª ‚âà\n Z Œªs\nŒªt\neŒªdŒª\n!\nxŒ∏(xt, t, xT ) +\n Z Œªs\nŒªt\n(Œª ‚àíŒªt)eŒªdŒª\n!\nx(1)\nŒ∏ (xt, t, xT )\n‚âàeŒªs h\n(1 ‚àíe‚àíh)ÀÜxt + (h ‚àí1 + e‚àíh)ÀÜx(1)\nt\ni\n(61)\nwhere we use ÀÜxt to denote the network output at time t, and h = Œªs ‚àíŒªt > 0. Suppose we have\nused a previous timestep u (s < t < u), the first-order derivative can be estimated by\nÀÜx(1)\nt\n‚âàÀÜxt ‚àíÀÜxu\nh1\n,\nh1 = Œªt ‚àíŒªu\n(62)\n3rd-Order Case\nWith the Taylor expansion xŒ∏(xtŒª, tŒª, xT )\n‚âà\nxŒ∏(xt, t, xT ) + (Œª ‚àí\nŒªt)x(1)\nŒ∏ (xt, t, xT ) + (Œª‚àíŒªt)2\n2\nx(2)\nŒ∏ (xt, t, xT ), we have\nZ Œªs\nŒªt\neŒªxŒ∏(xtŒª, tŒª, xT )dŒª\n‚âà\n Z Œªs\nŒªt\neŒªdŒª\n!\nxŒ∏(xt, t, xT ) +\n Z Œªs\nŒªt\n(Œª ‚àíŒªt)eŒªdŒª\n!\nx(1)\nŒ∏ (xt, t, xT )\n+\n Z Œªs\nŒªt\n(Œª ‚àíŒªt)2\n2\neŒªdŒª\n!\nx(2)\nŒ∏ (xt, t, xT )\n‚âàeŒªs\n\u0014\n(1 ‚àíe‚àíh)ÀÜxt + (h ‚àí1 + e‚àíh)ÀÜx(1)\nt\n+\n\u0012h2\n2 ‚àíh + 1 ‚àíe‚àíh\n\u0013\nÀÜx(2)\nt\n\u0015\n(63)\nSimilarly, suppose we have two previous timesteps u1, u2 (s < t < u1 < u2), and denote h1 :=\nŒªt ‚àíŒªu1, h2 := Œªu1 ‚àíŒªu2, the first-order and second-order derivatives can be estimated by\nÀÜx(1)\nt\n‚âà\nÀÜxt‚àíÀÜxu1\nh1\n(2h1 + h2) ‚àí\nÀÜxu1‚àíÀÜxu2\nh2\nh1\nh1 + h2\n,\nÀÜx(2)\nt\n‚âà2\nÀÜxt‚àíÀÜxu1\nh1\n‚àí\nÀÜxu1‚àíÀÜxu2\nh2\nh1 + h2\n(64)\nThe high-order samplers for DDBMs also theoretically guarantee the order of convergence, similar\nto those for diffusion models (Zhang & Chen, 2022; Lu et al., 2022b; Zheng et al., 2023a). We omit\nthe proofs here as they deviate from our main contributions.\n22\n"
    },
    {
      "page_number": 23,
      "text": "Published as a conference paper at ICLR 2025\nE\nALGORITHM\nAlgorithm 1 DBIM (high-order)\nRequire: condition xT , timesteps 0 ‚â§t0 < t1 < ¬∑ ¬∑ ¬∑ < tN‚àí1 < tN = T, data prediction model\nxŒ∏, booting noise œµ ‚àºN(0, I), noise schedule at, bt, ct, Œªt = log(bt/ct), order o (2 or 3).\n1: ÀÜxT ‚ÜêxŒ∏(xT , T, xT )\n2: xtN‚àí1 ‚ÜêatxT + bt ÀÜxT + ctœµ\n3: for i ‚ÜêN ‚àí1 to 1 do\n4:\ns, t ‚Üêti‚àí1, ti; h ‚ÜêŒªs ‚àíŒªt\n5:\nÀÜxt ‚ÜêxŒ∏(xt, t, xT )\n6:\nif o = 2 or i = N ‚àí1 then\n7:\nu ‚Üêti+1; h1 ‚ÜêŒªt ‚àíŒªu\n8:\nEstimate ÀÜx(1)\nt\nwith Eqn. (62)\n9:\nÀÜI ‚ÜêeŒªs\nh\n(1 ‚àíe‚àíh)ÀÜxt + (h ‚àí1 + e‚àíh)ÀÜx(1)\nt\ni\n10:\nelse\n11:\nu1, u2 ‚Üêti+1, ti+2; h1 ‚ÜêŒªt ‚àíŒªu1; h2 ‚ÜêŒªu1 ‚àíŒªu2\n12:\nEstimate ÀÜx(1)\nt , ÀÜx(2)\nt\nwith Eqn. (64)\n13:\nÀÜI ‚ÜêeŒªs\nh\n(1 ‚àíe‚àíh)ÀÜxt + (h ‚àí1 + e‚àíh)ÀÜx(1)\nt\n+\n\u0010\nh2\n2 ‚àíh + 1 ‚àíe‚àíh\u0011\nÀÜx(2)\nt\ni\n14:\nend if\n15:\nxs ‚Üêcs\nct xt +\n\u0010\nas ‚àícs\nct at\n\u0011\nxT + cs ÀÜI\n16: end for\n17: return xt0\nF\nEXPERIMENT DETAILS\nF.1\nMODEL DETAILS\nDDBMs and DBIMs are assessed using the same trained diffusion bridge models. For image trans-\nlation tasks, we directly adopt the pretrained checkpoints provided by DDBMs. The data pre-\ndiction model xŒ∏(xt, t, xT ) mentioned in the main text is parameterized by the network FŒ∏ as\nxŒ∏(xt, t, xT ) = cskip(t)xt + cout(t)FŒ∏(cin(t)xt, cnoise(t), xT ), where\ncin(t) =\n1\np\na2\ntœÉ2\nT + b2\ntœÉ2\n0 + 2atbtœÉ0T + ct\n,\ncout(t) =\nq\na2\nt(œÉ2\nT œÉ2\n0 ‚àíœÉ2\n0T ) + œÉ2\n0ctcin(t)\ncskip(t) = (btœÉ2\n0 + atœÉ0T )c2\nin(t),\ncnoise(t) = 1\n4 log t\n(65)\nand\nœÉ2\n0 = Var[x0], œÉ2\nT = Var[xT ], œÉ0T = Cov[x0, xT ]\n(66)\nFor the image inpainting task on ImageNet 256√ó256 with 128√ó128 center mask, DDBMs do not\nprovide available checkpoints. Therefore, we train a new model from scratch using the noise sched-\nule of I2SB (Liu et al., 2023b). The network is initialized from the pretrained class-conditional\ndiffusion model on ImageNet 256√ó256 provided by (Dhariwal & Nichol, 2021), while addition-\nally conditioned on xT .\nThe data prediction model in this case is parameterized by the net-\nwork FŒ∏ as xŒ∏(xt, t, xT ) = xt ‚àíœÉtFŒ∏(xt, t, xT ) and trained by minimizing the loss L(Œ∏) =\nEt,x0,xT\nh\n1\nœÉ2\nt ‚à•xŒ∏(xt, t, xT ) ‚àíx0‚à•2\n2\ni\n. We train the model on 8 NVIDIA A800 GPU cards with a\nbatch size of 256 for 400k iterations, which takes around 19 days.\nF.2\nSAMPLING DETAILS\nWe elaborate on the sampling configurations of different approaches, including the choice of\ntimesteps {ti}N\ni=0 and details of the samplers. In this work, we adopt tmin = 0.0001 and tmax = 1\nfollowing (Zhou et al., 2023). For the DDBM baseline, we use the hybrid, high-order Heun sampler\n23\n"
    },
    {
      "page_number": 24,
      "text": "Published as a conference paper at ICLR 2025\nproposed in their work with an Euler step ratio of 0.33, which is the best performing configuration for\nthe image-to-image translation task. We use the same timesteps distributed according to EDM (Kar-\nras et al., 2022)‚Äôs scheduling (t1/œÅ\nmax + i\nN (t1/œÅ\nmin ‚àít1/œÅ\nmax))œÅ, consistent with the official implementation\nof DDBM. For DBIM, since the initial sampling step is distinctly forced to be stochastic, we specif-\nically set it to transition from tmax to tmax ‚àí0.0001, and employ a simple uniformly distributed\ntimestep scheme in [tmin, tmax ‚àí0.0001) for the remaining timesteps, across all settings. For inter-\npolation experiments, to enhance diversity, we increase the step size of the first step from 0.0001 to\n0.01.\nF.3\nLICENSE\nTable 7: The used datasets, codes and their licenses.\nName\nURL\nCitation\nLicense\nEdges‚ÜíHandbags\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\nIsola et al. (2017)\nBSD\nDIODE-Outdoor\nhttps://diode-dataset.org/\nVasiljevic et al. (2019)\nMIT\nImageNet\nhttps://www.image-net.org\nDeng et al. (2009)\n\\\nGuided-Diffusion\nhttps://github.com/openai/guided-diffusion\nDhariwal & Nichol (2021)\nMIT\nI2SB\nhttps://github.com/NVlabs/I2SB\nLiu et al. (2023b)\nCC-BY-NC-SA-4.0\nDDBM\nhttps://github.com/alexzhou907/DDBM\nZhou et al. (2023)\n\\\nWe list the used datasets, codes and their licenses in Table 7.\nG\nADDITIONAL RESULTS\nG.1\nRUNTIME COMPARISON\nTable 8 shows the inference time of DBIM and previous methods on a single NVIDIA A100 under\ndifferent settings. We use torch.cuda.Event and torch.cuda.synchronize to accu-\nrately compute the runtime. We evaluate the runtime on 8 batches (dropping the first batch since\nit contains extra initializations) and report the mean and std. We can see that the runtime is pro-\nportional to NFE. This is because the main computation costs are the serial evaluations of the large\nneural network, and the calculation of other coefficients requires neglectable costs. Therefore, the\nspeedup for the NFE is approximately the actual speedup of the inference time.\nTable 8: Runtime of different methods to generate a single batch (second / batch, ¬±std) on a single\nNVIDIA A100, varying the number of function evaluations (NFE).\nMethod\nNFE\n5\n10\n15\n20\nCenter 128 √ó 128 Inpainting, ImageNet 256 √ó 256 (batch size = 16)\nI2SB (Liu et al., 2023b)\n2.8128 ¬± 0.0111\n5.6049 ¬± 0.0152\n8.3919 ¬± 0.0166\n11.1494 ¬± 0.0259\nDDBM (Zhou et al., 2023)\n2.8711 ¬± 0.0318\n5.7283 ¬± 0.0572\n8.3787 ¬± 0.1667\n11.0678 ¬± 0.3061\nDBIM (Œ∑ = 0)\n2.8755 ¬± 0.0706\n5.7810 ¬± 0.1494\n8.5890 ¬± 0.2730\n11.1613 ¬± 0.3372\nDBIM (2nd-order)\n2.8859 ¬± 0.0675\n5.7884 ¬± 0.1734\n8.6284 ¬± 0.1907\n11.5898 ¬± 0.2260\nDBIM (3rd-order)\n2.9234 ¬± 0.0361\n5.8109 ¬± 0.2982\n8.6449 ¬± 0.2118\n11.3710 ¬± 0.3237\nG.2\nDIVERSITY SCORE\nWe measure the diversity score (Batzolis et al., 2021; Li et al., 2023) on the ImageNet center inpaint-\ning task. We calculate the standard deviation of 5 generated samples (numerical range 0 ‚àº255)\ngiven each observation (condition) xT , averaged over all pixels and 1000 conditions.\nAs shown in Table 9, the diversity score keeps increasing with larger NFE. DBIM (Œ∑ = 0) consis-\ntently surpasses the flow matching baseline I2SB, and DDBM‚Äôs hybrid sampler which introduces\ndiversity through SDE steps. Surprisingly, we find that the DBIM Œ∑ = 0 case exhibits a larger di-\nversity score than the Œ∑ = 1 case. This demonstrates that the booting noise can introduce enough\n24\n"
    },
    {
      "page_number": 25,
      "text": "Published as a conference paper at ICLR 2025\nTable 9:\nDiversity scores on the ImageNet center inpainting task, varying Œ∑ and the NFE. We\nexclude statistics for DDBM (NFE‚â§10) as they correspond to severely degraded nonsense samples.\nMethod\nNFE\n5\n10\n20\n50\n100\n200\n500\nI2SB (Liu et al., 2023b)\n3.27\n4.45\n5.21\n5.75\n5.95\n6.04\n6.15\nDDBM (Zhou et al., 2023)\n-\n-\n2.96\n4.03\n4.69\n5.29\n5.83\nDBIM\nŒ∑ = 0\n3.74\n4.56\n5.20\n5.80\n6.10\n6.29\n6.42\nŒ∑ = 1\n2.62\n3.40\n4.18\n5.01\n5.45\n5.81\n6.16\nstochasticity to ensure diverse generation. Moreover, the Œ∑ = 0 case tends to generate sharper\nimages, which may favor the diversity score measured by pixel-level variance.\nH\nADDITIONAL SAMPLES\n25\n"
    },
    {
      "page_number": 26,
      "text": "Published as a conference paper at ICLR 2025\n(a) Condition\n(b) Ground-truth\n(c) DDBM (NFE=20), FID 46.74\n(d) DBIM (NFE=20 , Œ∑ = 0.0), FID 1.76\n(e) DDBM (NFE=100), FID 2.40\n(f) DBIM (NFE=100 , Œ∑ = 0.0), FID 0.91\n(g) DDBM (NFE=200), FID 0.88\n(h) DBIM (NFE=200 , Œ∑ = 0.0), FID 0.75\nFigure 6: Edges‚ÜíHandbags samples on the translation task.\n26\n"
    },
    {
      "page_number": 27,
      "text": "Published as a conference paper at ICLR 2025\n(a) Condition\n(b) Ground-truth\n(c) DDBM (NFE=20), FID 41.03\n(d) DDBM (NFE=500), FID 2.26\n(e) DBIM (NFE=20 , Œ∑ = 0.0), FID 4.97\n(f) DBIM (NFE=200 , Œ∑ = 0.0), FID 2.26\nFigure 7: DIODE-Outdoor samples on the translation task.\n27\n"
    },
    {
      "page_number": 28,
      "text": "Published as a conference paper at ICLR 2025\n(a) Condition\n(b) Ground-truth\n(c) DDBM (NFE=10), FID 57.18\n(d) DDBM (NFE=500), FID 4.27\n(e) DBIM (NFE=10 , Œ∑ = 0.0), FID 4.51\n(f) DBIM (NFE=100 , Œ∑ = 0.0), FID 3.91\n(g) DBIM (NFE=100 , Œ∑ = 0.8), FID 3.88\n(h) DBIM (NFE=500 , Œ∑ = 1.0), FID 3.80\nFigure 8: ImageNet 256 √ó 256 samples on the inpainting task with center 128 √ó 128 mask.\n28\n"
    }
  ]
}