{
  "filename": "10.48550_arxiv.2201.07207.pdf",
  "total_pages": 33,
  "full_text": "Language Models as Zero-Shot Planners:\nExtracting Actionable Knowledge for Embodied Agents\nWenlong Huang\nUC Berkeley\nPieter Abbeel\nUC Berkeley\nDeepak Pathak*\nCarnegie Mellon University\nIgor Mordatch*\nGoogle\nAbstract\nCan world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of grounding\nhigh-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen\nset of actionable steps (e.g. “open fridge”). While prior work focused on learning\nfrom explicit step-by-step examples of how to act, we surprisingly ﬁnd that if\npre-trained LMs are large enough and prompted appropriately, they can effectively\ndecompose high-level tasks into mid-level plans without any further training. How-\never, the plans produced naively by LLMs often cannot map precisely to admissible\nactions. We propose a procedure that conditions on existing demonstrations and\nsemantically translates the plans to admissible actions. Our evaluation in the recent\nVirtualHome environment shows that the resulting method substantially improves\nexecutability over the LLM baseline. The conducted human evaluation reveals a\ntrade-off between executability and correctness but shows a promising sign towards\nextracting actionable knowledge from language models.\nTask: Brush teeth\nStep 1: Go to bathroom\nGPT-2 1.5B\nTask: Brush teeth\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Find toothbrush\nStep 4: Pick up toothbrush\nStep 5: Put toothbrush in mouth\nStep 6: Move brush around mouth\nfor two minutes\nStep 7: Spit out toothpaste and\nbrush into sink\nStep 8: Turn on water in sink and\nrinse brush for one minute\nStep 9: Turn off water in sink and\nreturn brush to cupboard\nCodex 12B\nTask: Brush teeth\nStep 1: Walk to bathroom\nStep 2: Open door\nStep 3: Walk to sink\nStep 4: Put pot on sink\nStep 5: Put brush on toothbrush\nStep 6: Turn to toothpaste\nStep 7: Put toothpaste on toothbrush\nStep 8: Put teeth on toothbrush\nTranslated Codex 12B\nTask: Throw away paper\nStep 1: Walk to home office\nStep 2: Walk to desk\nStep 3: Find desk\nStep 4: Turn to desk\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find check\nStep 8: Grab check\nStep 9: Squeeze check\nStep 10: Stand up\nStep 11: Walk to trashcan\nStep 12: Put check on trashcan\nHuman\nTask: Throw away paper\nStep 1: Walk to home office\nStep 2: Walk to wastebasket\nStep 3: Find wastebasket\nStep 4: Grab wastebasket\nStep 5: Walk to desk\nStep 6: Drop paper in wastebasket\nGPT-3 175B\nTask: Throw away paper\nStep 1: Walk to home office\nStep 2: Walk to table\nStep 3: Find table\nStep 4: Turn to table\nStep 5: Find paper\nStep 6: Grab paper\nStep 7: Walk to trashcan\nStep 8: Open trashcan\nStep 9: Put paper on trashcan\nStep 10: Close trashcan\nTranslated GPT-3 175B\n\u0013\n\u0015\u0013\n\u0017\u0013\n\u0019\u0013\n\u001b\u0013\n\u0014\u0013\u0013\n\b\u0003([HFXWDELOLW\\\n\u0013\n\u0015\u0013\n\u0017\u0013\n\u0019\u0013\n\u001b\u0013\n\u0014\u0013\u0013\n\b\u0003&RUUHFWQHVV\n+XPDQ\u0003\u000b\u0014\u0013\u0013\b\u0003([HFXWDELOLW\\\f\nGPT-2 1.5B\nGPT-3 13B\nCodex 12B\nGPT-3 175B\nTranslated\nCodex 12B\n(Ours)\nGPT-2 0.1B\nTranslated\nGPT-3 175B (Ours)\nTask: Get Glass of Milk\nWalk to Kitchen\nOpen Fridge\nGrab Milk\nClose Fridge\nFigure 1: Executability v.s. semantic correctness of generated plans (left), sample plans by different models\n(right), and example environment execution (bottom). Large models can produce action plans indistinguishable\nfrom those by humans, but frequently are not executable in the environment. Using our techniques, we can\nsigniﬁcantly improve executability, albeit at the cost of correctness. More samples can be found in Appendix A.5.\n*Equal advising. Correspondence to Wenlong Huang <wenlong.huang@berkeley.edu>.\nCode and videos at https://huangwl18.github.io/language-planner\narXiv:2201.07207v2  [cs.LG]  8 Mar 2022\nContents\n1\nIntroduction\n3\n2\nEvaluation Framework\n4\n2.1\nEvaluated Environment: VirtualHome . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nMetrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nMethod\n6\n3.1\nQuerying LLMs for Action Plans . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nAdmissible Action Parsing by Semantic Translation . . . . . . . . . . . . . . . . .\n6\n3.3\nAutoregressive Trajectory Correction . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.4\nDynamic Example Selection for Improved Knowledge Extraction\n. . . . . . . . .\n7\n4\nResults\n8\n4.1\nDo LLMs contain actionable knowledge for high-level tasks? . . . . . . . . . . . .\n8\n4.2\nHow executable are the LLM action plans?\n. . . . . . . . . . . . . . . . . . . . .\n9\n4.3\nCan LLM action plans be made executable by proposed procedure?\n. . . . . . . .\n9\n5\nAnalysis and Discussions\n10\n5.1\nAblation of design decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5.2\nAre the generated action plans grounded in the environment? . . . . . . . . . . . .\n10\n5.3\nEffect of Different Translation LMs\n. . . . . . . . . . . . . . . . . . . . . . . . .\n11\n5.4\nCan LLMs generate actionable programs by following step-by-step instructions?\n.\n11\n5.5\nAnalysis of program length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6\nRelated Works\n12\n7\nConclusion, Limitations & Future Work\n13\nA Appendix\n18\nA.1\nHyperparameter Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.2\nDetails of Human Evaluations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.3\nAll Evaluated Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nA.4\nNatural Language Templates for All Atomic Actions\n. . . . . . . . . . . . . . . .\n21\nA.5\nRandom Samples of Action Plans\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2\n1\nIntroduction\nLarge language models (LLMs) have made impressive advances in language generation and under-\nstanding in recent years [10, 39, 40, 5]. See [4] for a recent summary of their capabilities and impacts.\nBeing trained on large corpora of human-produced language, these models are thought to contain a\nlot of information about the world [42, 23, 3] - albeit in linguistic form.\nWe ask whether we can use such knowledge contained in LLMs not just for linguistic tasks, but to\nmake goal-driven decisions that can be enacted in interactive, embodied environments. But we are\nnot simply interested in whether we can train models on a dataset of demonstrations collected for\nsome speciﬁc environment – we are instead interested in whether LLMs already contain information\nnecessary to accomplish goals without any additional training.\nMore speciﬁcally, we ask whether world knowledge about how to perform high-level tasks (such as\n“make breakfast”) can be expanded to a series of groundable actions (such as “open fridge”, “grab\nmilk”, “close fridge”, etc) that can be executed in the environment. For our investigation, we use\nthe recently proposed VirtualHome environment [38]. It can simulate a large variety of realistic\nhuman activities in a household environment and supports the ability to perform them via embodied\nactions deﬁned with a verb-object syntax. However, due to the open-ended nature of the tasks,\nit is difﬁcult to autonomously evaluate their success. We rely on human evaluation (conducted on\nMechanical Turk) to decide whether sequences of actions meaningfully accomplish posed tasks.\nWe ﬁnd that large GPT-3 [5] and Codex [7] models, when prompted with a single ﬁxed example of a\ntask description and its associated sequence of actions, can produce very plausible action plans for\nthe task we’re interested in. Such completions reﬂect the information already stored in the model\n– no model ﬁne-tuning is involved. Additionally, we only observe this effect in the larger models.\nUnfortunately, despite their semantic correctness, the produced action plans are often not executable\nin the environment. Produced actions may not map precisely to admissible actions, or may contain\nvarious linguistic ambiguities.\nWe propose several tools to improve executability of the model’s outputs. First, we enumerate all\nadmissible actions and map the model’s output phrases to the most semantically-similar admissible\naction (we use similarity measure between sentence embeddings produced by a RoBERTa model [27]\nin this work, but other choices are possible). Second, we use the model to autoregressively generate\nactions in a plan by conditioning past actions that have been made admissible via the technique above.\nSuch on-the-ﬂy correction can keep generation anchored to admissible actions. Third, we provide\nweak supervision to the model by prompting the model with a known task example similar to the\nquery task. This is somewhat reminiscent of prompt tuning approaches but does not require access to\ngradients or internals of the model.\nUsing the above tools to bias model generation, we ﬁnd that we improve executability of action plans\nfrom 18% to 79% (see Figure 1) without any invasive modiﬁcations to model parameters or any extra\ngradient or internal information beyond what is returned from the model’s forward pass. This is\nadvantageous because it does not require any modiﬁcations to the model training procedure and can\nﬁt within existing model serving pipelines. However, we do ﬁnd there to be some drop in correctness\nof the action sequences generated with the above tools (as judged by humans), indicating a promising\nstep, but requiring more research on the topic.\nTo summarize, our paper’s contributions are as follows:\n• We show that without any training, large language models can be prompted to generate\nplausible goal-driven action plans, but such plans are frequently not executable in interactive\nenvironments.\n• We propose several tools to improve executability of the model generation without invasive\nprobing or modiﬁcations to the model.\n• We conduct a human evaluation of multiple techniques and models and report on the\ntrade-offs between executability and semantic correctness.\n3\nTask: Shave\nStep 1: Grab razor\nStep 2: Wash razor\nStep 3: Switch on razor\nTask: Apply lotion\nStep 1: Pour lotion into right hand\nStep 2:\nTask: Shave\nStep 1: Grab razor\nStep 2: Switch on razor\nStep 3: Put razor on face\nTask: Apply lotion\nPre-Trained Causal LLM\nFrozen\nPre-Trained\nMasked LLM\nFrozen\nStep 1: Squeeze out a glob of lotion\nStep 1: Pour lotion into right hand\nStep 1: Squeeze out a glob of lotion\nPre-Trained Causal LLM\nFrozen\nZero-Shot Planning via Causal LLM\nTranslation to Admissible Action\nStep-By-Step\nAutoregressive Generation\nPrompt\nPrompt\nFigure 2: We investigate the possibility of extracting actionable knowledge from pre-trained large language\nmodels (LLMs). We ﬁrst show surprising ﬁnding that pre-trained causal LLMs can decompose high-level\ntasks into sensible mid-level action plans (left). To make the plans executable, we propose to translate each step\ninto admissible action via another pre-trained masked LLM (middle). The translated action is appended to the\nprompt used for generating the remaining steps (right). All models are kept frozen without additional training.\n2\nEvaluation Framework\nSimulating open-ended tasks that resemble naturalistic human activities requires an environment to\nsupport a rich set of diverse interactions, rendering most existing embodied environments unsuitable\nfor our investigation. One exception is VirtualHome [38], which we evaluate on as it models complex\nhuman activities, though only in a household setting. To measure correctness of the generated action\nplans, for which evaluating computationally is inherently difﬁcult for these open-ended tasks, we\nconduct a human evaluation similar to Puig et al. [38]. We note that since no further training is\ninvolved throughout our investigations, the observations and ﬁndings presented in this paper should\nalso translate to similar embodied environments, likely even beyond the household domain.\n2.1\nEvaluated Environment: VirtualHome\nPreliminaries\nIn VirtualHome, activities are expressed as programs. Each program consists of a\nsequence of textual action steps, where each step is written as: [action] ⟨arg⟩(idx). Each\naction refers to one of the 42 atomic actions supported in VirtualHome, such as “walk” and “open”.\nFull list of atomic actions can be found in Appendix A.4. Different actions take in different numbers\nof arg, such as “bedroom” and “fridge”, that are necessary for specifying an interaction. Associated\nwith each arg is a unique id specifying the corresponding node in the environment graph, in case\nof multiple instances of the same object class are present in the graph. For the sake of simplicity,\nwe omit the id in the remaining discussions of this paper and allow automatic assignment by the\nenvironment. An example program is shown below for the task “Relax on sofa”:\n[WALK] ⟨living_room⟩(1)\n[WALK] ⟨television⟩(1)\n[FIND] ⟨television⟩(1)\n[SWITCHON] ⟨television⟩(1)\n[FIND] ⟨sofa⟩(1)\n[SIT] ⟨sofa⟩(1)\n[TURNTO] ⟨television⟩(1)\n[WATCH] ⟨television⟩(1)\nEvaluated Tasks\nWe use the ActivityPrograms knowledge base collected by Puig et al. [38] for\nevaluation. It contains 2821 different entries annotated by Amazon Mechanical Turk (MTurk) workers.\nEach entry contains 1) a high-level task name (e.g. “Watch TV”), 2) detailed instructions expressed\nin natural language to complete the task (e.g. “Sit on my couch directly opposite my TV, switch on\nmy TV with the remote control and watch”), and 3) an executable program containing all necessary\nsteps for a robotic agent (example above). We omit the use of detailed instructions (2) as we desire\ndirect extraction of executable programs (3) from only high-level task names (1). There are 292\ndistinct high-level tasks in the knowledge base, from which we randomly sample 88 held-out tasks\nfor evaluation. The remaining 204 tasks are used as demonstration set from which we are allowed\n4\nAlgorithm 1: Generating Action Plans from Pre-Trained Language Models\nNotation Summary:\nLMP : text completion language model (also referred as Planning LM)\nLMT : text embedding language model (also referred as Translation LM)\n{(Ti, Ei)}N\ni=1: demonstration set, where T is task name and E is example plan for T\nC: cosine similarity function\nP: mean token log probability under LMP\nInput:\nquery task name Q, e.g. “make breakfast”\nOutput: action plan consisting of admissible env actions, e.g. “open fridge”\nExtract most similar example (T ∗, E∗) whose T ∗maximizes C(LMT (T), LMT (Q))\nInitialize prompt with (T ∗+ E∗+ Q)\nwhile max step is not reached do\nSample LMP with current prompt to obtain k single-step action phrases\nfor each sample ˆa and each admissible env action ae do\nCalculate ranking score by C(LMT (ˆa), LMT (ae)) + β · P(ˆa)\nend for\nAppend highest-scoring env action a∗\ne to prompt\nAppend a∗\ne to output\nif > 50% samples are 0-length or highest score < ϵ then\nbreak\nend if\nend while\nto select as example(s) for prompting language models, or in the case of supervised ﬁne-tuning\nbaselines, they are used to ﬁne-tune pre-trained language models.\n2.2\nMetrics\nA program that commands the agent to wander around in a household environment is highly executable\nbut is mostly not correct. On the other hand, a program composed of natural language instructions\nannotated by humans is likely correct but cannot be executed, because its format is ambiguous and\nmay lack necessary common-sense actions (e.g. fridge must be opened before an agent can grab\nthings from it). We thus consider two axes for evaluation: executability and correctness.\nExecutability\nExecutability measures whether an action plan can be correctly parsed and satisﬁes\nthe common-sense constraints of the environment. To be correctly parsed, an action plan must\nbe syntactically correct and contain only allowed actions and recognizable objects. To satisfy the\ncommon-sense constraints, each action step must not violate the set of its pre-conditions (e.g. the\nagent cannot grab milk from the fridge before opening it) and post-conditions (e.g. the state of the\nfridge changes from “closed” to “open” after the agent opens it). We report the average executability\nacross all 88 tasks and all 7 VirtualHome scenes.\nCorrectness\nUnlike most embodied environments where the completion of a task can be easily\njudged, the ambiguous and multimodal nature of natural language task speciﬁcation makes it impracti-\ncal to obtain a gold-standard measurement of correctness1. Therefore, we conduct human evaluations\nfor the main methods. For the remaining analysis, we rely on a match-based metric that measures\nhow similar a generated program is to human annotations. Speciﬁcally, we follow Puig et al. [38]\nand calculate the longest common subsequence (LCS) between two programs, normalized by the\nmaximum length of the two. In the presence of multiple human-written programs for a single task,\nwe take the maximum LCS across them. However, we note that the majority of the tasks only have\none human annotation, but there are often many plausible ways to complete a certain task, making\n1One approach could be measuring the similarity of the ﬁnal environment state produced by executing\npredicted and human-written programs, but initial state must be kept ﬁxed for each task, which are not appropriate\nfor many tasks due to their open-ended nature.\n5\nthis metric imperfect at evaluation program correctness2. Although correlation between the two is\nshown by Puig et al. [38], we consider it only as a proxy metric in replacement of unscalable human\nevaluation.\n3\nMethod\nIn this section, we investigate the possibility of extracting actionable knowledge from pre-trained\nlanguage models without further training. We ﬁrst give an overview of the common approach to query\nlarge language models (LLMs) and how it may be used for embodied agents in Section 3.1. Then we\ndescribe an inference-time procedure that addresses several deﬁciencies of the LLM baseline and\noffers better executability in embodied environments. We break down the proposed procedure into\nthree individual components, each discussed in Section 3.2, 3.3, 3.4. Pseudo-code is in Algorithm 1.\nSince LMs excel at dealing with natural language text instead of the speciﬁc format required by\nVirtualHome as described in Section 2.1, we only expose natural language text to LMs. To do\nthis, we deﬁne a bi-directional mapping for each atomic action that converts between the natu-\nral language format and the program format. For instance, “walk to living room” is mapped to\n[WALK] ⟨living_room⟩(1). Full list of the mappings is in Appendix A.4.\n3.1\nQuerying LLMs for Action Plans\nPrevious works have shown that large language models pre-trained on a colossal amount of data would\ninternalize rich world knowledge that can be probed to perform various downstream tasks [39, 5].\nNotably, autoregressive LLMs can even perform in-context learning, an ability to solve tasks using\nonly contextual information without gradient updates [5]. Contextual information is given as part\nof the input prompt and LMs are asked to complete the remaining text. It often consists of natural\nlanguage instructions and/or a number of examples containing the desired input/output pairs.\nWe adopt the same approach to query LLMs to generate action plans for high-level tasks. Speciﬁcally,\nwe prepend one example high-level task and its annotated action plan from the demonstration set to\nthe query task, as shown in Figure 2. To obtain text completion results, we sample from autoregressive\nLLM using temperature sampling and nucleus sampling [18]. We refer to this LM as Planning LM\nand the approach using this LM for plan generation as Vanilla ⟨LM⟩, where ⟨LM⟩is replaced by\nspeciﬁc language model such as GPT-3.\nTo improve the generation quality, we follow Chen et al. [7] to sample multiple outputs for each query.\nHowever, unlike Chen et al. [7] who investigate program synthesis and can choose the sample with\nhighest unit test pass rate, we only consider the setting where one sample is allowed to be evaluated\nfor each task. This is because repetitive trial-and-error is equivalent to probing the environment for\nprivileged information, which should not be considered viable in our setting. For Vanilla ⟨LM⟩, to\nchoose the best action plan X∗among k samples (X1, X2, ..., Xk), each consisting of ni tokens\nXi = (xi,1, xi,2, ..., xi,ni), we select the sample with highest mean log probability as follows:\nargmax\nXi\n\u0012\nPθ(Xi) := 1\nni\nni\nX\nj=1\nlog pθ(xi,j|xi,<j)\n\u0013\nwhere θ parameterizes the Planning LM.\n(1)\n3.2\nAdmissible Action Parsing by Semantic Translation\nOne issue arises when naively following the above approach to generate action plans: the plan\nexpressed in free-form language often cannot be mapped to unambiguous actionable steps and thus is\nnot executable by a robotic agent. Many reasons can cause such failures: 1) the output does not follow\npre-deﬁned mappings of any atomic action (e.g. “I ﬁrst walk to the bedroom” is not of the format\n“walk to ⟨PLACE⟩”), 2) the output may refer to atomic action and objects using words unrecognizable\nby the environment (e.g. “microwave the chocolate milk” where “microwave” and “chocolate milk”\ncannot be mapped to precise action and object), or 3) the output contains lexically ambiguous words\n(e.g. “open TV” should instead be “switch on TV”).\n2Although LCS has a mathematical range of [0, 1], we measure the LCS between different human-written\nprograms for the same task and ﬁnd an empirical maximum of 0.489.\n6\nInstead of developing a set of rules to transform the free-form text into admissible action steps, we\npropose to again leverage world knowledge learned by language models to semantically translate the\naction. For each admissible environment action ae, we calculate its semantic distance to the predicted\naction phrase ˆa by cosine similarity:\nC(f(ˆa), f(ae)) :=\nf(ˆa) · f(ae)\n∥f(ˆa)∥∥f(ae)∥where f is an embedding function.\n(2)\nTo embed the output action phrase and environment actions, we use a BERT-style LM [10, 27]\npre-trained with Sentence-BERT [41] objective, to which we refer as Translation LM3. The action\nembedding is obtained by mean-pooling the last layer hidden states across all tokens in that action\nphrase. While the set of admissible actions in our environment is discrete and possible to exhaustively\nenumerate, sampling or projection can be employed in larger discrete or continuous action spaces.\n3.3\nAutoregressive Trajectory Correction\nTranslating each step of the program after the entire program has been synthesized lacks consideration\nof achievability of individual steps and subjects to compounding errors. In practice, LLMs might\noutput compounded instructions for a single step, even though it cannot be completed using one\nadmissible action in the environment. To this end, we can instead interleave plan generation and\naction translation to allow for automatic trajectory correction. At each step, we ﬁrst query Planning\nLM to generate k samples for a single action ( ˆa1, ˆa2, ..., ˆak). For each sample ˆa, we consider both its\nsemantic soundness and its achievability in the environment. Speciﬁcally, we aim to ﬁnd admissible\nenvironment action ae by modifying the ranking scheme described in Equation 1 as follows:\nargmax\nae\n\u0014\nmax\nˆa\nC(f(ˆa), f(ae)) + β · Pθ(ˆa)\n\u0015\nwhere β is a weighting coefﬁcient.\n(3)\nThen we append the translated environment action ae to the unﬁnished text completion. This way\nall subsequent steps will be conditioned on admissible actions instead of free-form action phrases\ngenerated by Planning LM. Furthermore, we can use Translation LM to detect out-of-distribution\nactions, those outside the capabilities of a robot, and terminate a program early instead of mapping to\na faulty action. This can be achieved by setting a threshold ϵ such that if maxˆa,ae C(f(ˆa), f(ae)) +\nβ · Pθ(ˆa) < ϵ at step t, the program is terminated early. Since we now sample Planning LM for\nindividual steps instead of an entire sequence, another termination condition we consider is when\n> 50% of current-step samples are 0-length (excluding leading or trailing non-English text tokens).\n3.4\nDynamic Example Selection for Improved Knowledge Extraction\nSo far in the text, we always give the same example in the prompt for all query tasks. However,\nconsider the task of “ordering pizza”. Prompting LLMs with this task may give the assumption that\nthe agent is initialized in front of a computer, and the LLMs may guide the agent to search for a pizza\nstore and click “checkout my cart”. Although these are reasonable and feasible in the real world,\nsuch assumption cannot always be made as these interactions may not be supported in simulated\nenvironments. In fact, the closest series of actions that human experts give in VirtualHome may be\n“walking to a computer”, “switching on the computer”, and “typing the keyboard”. Without being\nﬁne-tuned on these data, LLMs would often fail at these tasks.\nTo provide weak supervision at inference time, we propose to select the most similar task T and its\nexample plan E from the demonstration set to be used as the example in the prompt. Speciﬁcally, we\nre-use the same Translation LM introduced in Section 3.2 and select (T ∗, E∗) whose high-level task\nname T ∗maximizes C(f(T), f(Q)), where Q is the query task. This approach bears resemblance to\nseveral recent works [37, 13, 26, 43]. An example is shown in Figure 2 where “Shave” is the most\nsimilar to the query task “Apply lotion”.\nFINAL METHOD\nCombining the various improvement discussed above, we refer to the ﬁnal\nmethod as Translated ⟨LM⟩, where ⟨LM⟩is replaced by speciﬁc language model used such as GPT-3.\n3Note that this is a different LM than the GPT-style Planning LM. Using a single LM for both purposes could\nas well be possible and likely more efﬁcient, but we leave such investigation to future works.\n7\nTask: Complete Amazon\nTurk Surveys\nTask: Get Glass of Milk\nSit on Chair\nSwitch on Computer\nWalk to Home Office\nLook at Computer\nWalk to Kitchen\nOpen Fridge\nGrab Milk\nClose Fridge\nFigure 3: Visualization of VirtualHome programs generated by our approach. The top row shows the execution\nof the task “Complete Amazon Turk Surveys”, and the bottom row shows the task “Get Glass of Milk”. We show\nLLMs not only can generate sensible action plans given only high-level tasks but also contains the actionable\nknowledge that can be extracted for grounding in embodied environments.\n4\nResults\nIn this section, we ﬁrst show that language models can generate sensible action plans for many\nhigh-level tasks, even without any additional training. Then we highlight its inadequacy when naively\napplied to embodied environments and demonstrate how this can be improved by again leveraging\nworld knowledge learned by LLMs. Visualization of generated programs is shown in Figure 3.\nSampling from LMs\nPre-trained LMs are sensitive to sampling parameters and the speciﬁc exam-\nple given in the prompt. For all evaluated methods, we perform hyperparameter search over various\nsampling parameters, and for methods using a ﬁxed prompt example, we report metrics averaged\nacross three randomly chosen examples. To select the best run for each method, we rank the runs by\nthe sum of LCS and executability, each normalized by human-expert scores. Further details are in\nAppendix A.1.\nModel Choices\nFor Planning LM, we evaluate a representative set of causal language models.\nFor Translation LM, we mainly use Sentence-RoBERTa-355M and provide relevant ablations in\nSection 5.3. GPT-3 and Codex are accessed using OpenAI API, and the remaining models are accessed\nthrough open-source packages, Hugging Face Transformers [55] and SentenceTransformers [41], all\nwithout additional training (except for the ﬁne-tuning baseline).\n4.1\nDo LLMs contain actionable knowledge for high-level tasks?\nWe ﬁrst investigate whether LLMs can generate sensible action plans expressed in free-form language.\nWe use the approach described in Section 3.1 to query pre-trained LLMs. To evaluate the correctness\nof generated action plans, we conduct human evaluations. For each model, we ask 10 human\nannotators to determine – by answering “Yes” or “No” – whether each task can be completed using\nprovided action steps. To provide a reference of how humans might rate the action plans provided\nby other humans, we also ask annotators to rate the human-written action plans included in the\nVirtualHome dataset for the same set of tasks. In contrast to the free-form text output by LLMs,\nhumans wrote the plans using a graphical programming interface that enforces strict syntax and a\nchosen set of atomic action vocabulary, which limit the expressivity and the completeness of their\nanswers4. More details of our human evaluation procedure can be found in Appendix A.2.\nWe show the human evaluation results in Figure 1, where the y-axis shows correctness averaged\nacross all tasks and all annotators. Surprisingly, when LLMs are large enough and without imposed\nsyntactic constraints, they can generate highly realistic action plans whose correctness – as deemed\nby human annotators – even surpasses human-written action plans. We also observe some level of\ncorrectness for smaller models such as GPT-2. However, inspection of its produced output indicates\n4 Puig et al. [38] also conduct a human evaluation on 100 randomly sampled human-written programs and\nshow that 64% of them are complete (i.e. contain all necessary steps). Readers are encouraged to refer to Puig\net al. [38] for a more comprehensive analysis of the dataset.\n8\nLanguage Model\nExecutability\nLCS\nCorrectness\nVanilla GPT-2 117M\n18.66%\n3.19%\n15.81% (4.90%)\nVanilla GPT-2 1.5B\n39.40%\n7.78%\n29.25% (5.28%)\nVanilla Codex 2.5B\n17.62%\n15.57%\n63.08% (7.12%)\nVanilla GPT-Neo 2.7B\n29.92%\n11.52%\n65.29% (9.08%)\nVanilla Codex 12B\n18.07%\n16.97%\n64.87% (5.41%)\nVanilla GPT-3 13B\n25.87%\n13.40%\n49.44% (8.14%)\nVanilla GPT-3 175B\n7.79%\n17.82%\n77.86% (6.42%)\nHuman\n100.00%\nN/A\n70.05% (5.44%)\nFine-tuned GPT-3 13B\n66.07%\n34.08%\n64.92% (5.96%)\nOUR FINAL METHODS\nTranslated Codex 12B\n78.57%\n24.72%\n54.88% (5.90%)\nTranslated GPT-3 175B\n73.05%\n24.09%\n66.13% (8.38%)\nTable 1: Human-evaluated correctness and evaluation results in VirtualHome. Although action plans generated\nby large language models can match or even surpass human-written plans in correctness measure, they are\nrarely executable. By translating the naive action plans, we show an important step towards grounding LLMs\nin embodied environments, but we observe room to achieve this without trading executability for correctness.\nWe also observe a failure mode among smaller models that lead to high executability. For correctness measure,\nstandard error of the mean across 10 human annotators is reported in the parenthesis.\nthat it often generates shorter plans by ignoring common-sense actions or by simply rephrasing\nthe given task (e.g. the task “Go to sleep” produces only a single step “Go to bed”). These failure\nmodes sometimes mislead human annotators to mark them correct as the annotators may ignore\ncommon-sense actions in their judgment as well, resulting in a higher correctness rate than the quality\nof the output shows.\n4.2\nHow executable are the LLM action plans?\nWe analyze the executability of LLM plans by evaluating them in all 7 household scenes in Virtual-\nHome. As shown in Table 1, we ﬁnd action plans generated naively by LLMs are generally not very\nexecutable. Although smaller models seem to have higher executability, we ﬁnd that the majority of\nthese executable plans are produced by ignoring the queried task and repeating the given example of\na different task. This is validated by the fact that smaller models have lower LCS than larger models\ndespite having high executability, showing that this failure mode is prevalent among smaller models.\nIn contrast, larger models do not suffer severely from this failure mode. Yet as a result of being more\nexpressive, their generated programs are substantially less executable.\n4.3\nCan LLM action plans be made executable by proposed procedure?\nWe evaluate the effectiveness of our proposed procedure of action translation. We ﬁrst create a bank\nof all allowed 47522 action steps in the environment, including all possible combinations of atomic\nactions and allowed arguments/objects. Then we use an off-the-shelf Sentence-RoBERTa [27, 41] as\nTranslation LM to create embeddings for actions and output text. For better computational efﬁciency,\nwe pre-compute the embeddings for all allowed actions, leaving minor computation overhead for\nour procedure over the baseline methods at inference time. As shown in Table 1, executability of\ngenerated programs is signiﬁcantly improved. Furthermore, we also observe improved LCS because\nthe translated action steps precisely follow the program syntax and thus are more similar to the plans\nproduced by human experts. Sample output is shown in Figure 1 and a larger random subset of\ngenerated samples can be found in Appendix A.5.\nTo validate their correctness, we again perform human evaluations using the same procedure from\nSection 4.1. Results are shown in Table 1. We ﬁnd that despite being more similar to human-written\nplans as they follow strict syntax, the programs are deemed less correct by humans compared to their\nvanilla counterparts. By examining the output, we observe two main sources of errors. First, we\nﬁnd Translation LM is poor at mapping compounded instructions to a succinct admissible action,\ne.g. “brush teeth with toothbrush and toothpaste”. Second, we ﬁnd that the generated programs are\nsometimes terminated too early. This is partly due to the imperfect expressivity of the environment;\n9\ncertain necessary actions or objects are not implemented to fully achieve some tasks, so Translation\nLM cannot map to a sufﬁciently similar action. This is also reﬂected by our human evaluation results\nof the programs written by other humans, as only 70% of the programs are considered complete.\n5\nAnalysis and Discussions\n5.1\nAblation of design decisions\nWe perform ablation studies for the three components of our proposed procedure, described in\nSection 3.2, 3.3, and 3.4 respectively. As shown in Table 2, leaving out any of the three components\nwould all lead to decreased performance in both executability and LCS. An exception is Translated\nGPT-3 w/o Trajectory Correction, where we observe a slight improvement in LCS at the expense of a\nconsiderable drop in executability. Among the three proposed components, leaving out action transla-\ntion leads to the most signiﬁcant executability drop, showing the importance of action translation in\nextracting executable action plans from LLMs.\nMethods\nExecutability\nLCS\nTranslated Codex 12B\n78.57%\n24.72%\n- w/o Action Translation\n31.49%\n22.53%\n- w/o Dynamic Example\n50.86%\n22.84%\n- w/o Trajectory Correction\n55.19%\n24.43%\nTranslated GPT-3 175B\n73.05%\n24.09%\n- w/o Action Translation\n36.04%\n24.31%\n- w/o Dynamic Example\n60.82%\n22.92%\n- w/o Trajectory Correction\n40.10%\n24.98%\nTable 2: Ablation of three proposed techniques.\n5.2\nAre the generated action plans grounded in the environment?\nSince successful execution of correct action plans directly measures grounding, we calculate the\npercentage of generated action plans that are both correct and executable. We deem an action plan\nto be correct if 70% or more human annotators decide it is correct. Human-written plans are 100%\nexecutable, of which 65.91% are deemed correct. Results for LMs are shown in Figure 4.\nAlthough smaller LMs such as GPT-2 can generate highly executable action plans as shown in\nTable 1, these executable plans mostly are not correct, as they often repeat the given example or\ndo not contain all necessary steps. Increasing model parameters can lead to some improvement in\ngenerating plans that are both executable and correct, yet it scales poorly with the parameter count. In\nthe meantime, action translation offers a promising way towards grounding actionable knowledge by\nproducing executable and correct plans, though a large gap remains to be closed to reach human-level\nperformance (65.91%).\nTranslated GPT-3 175B\nTranslated Codex 12B\nVanilla GPT-3 175B\nVanilla GPT-3 12B\nVanilla Codex 12B\nVanilla GPT-2 1.5B\nVanilla GPT-2 0.1B\n35.23%\n27.27%\n6.82%\n2.27%\n4.55%\n1.14%\n0.0%\n% of Executable & Correct Plans   \nFigure 4: Percentage of both executable and correct action plans generated by LMs.\n10\n5.3\nEffect of Different Translation LMs\nIn this section, we study the effect of using different Translation LM. We compare two size variants of\nSentence BERT and Sentence RoBERTa [10, 27, 41] trained on the STS benchmark [6] and a baseline\nusing averaged GloVe embeddings [35]. Results are shown in Table 3. Notably, we do not observe\nsigniﬁcant differences in executability and LCS across different variants of BERT and RoBERTa. We\nhypothesize that this is because any language models trained on reasonably large datasets should be\ncapable of the single-step action phrase translation considered in this work. However, simply using\naverage GloVe embeddings would lead to signiﬁcantly reduced performance.\nTranslation LM\nParameter Count\nExecutability\nLCS\nCODEX 12B AS PLANNING LM\nAvg. GloVe embeddings\n-\n46.92%\n9.71%\nSentence Bert (base)\n110M\n73.21%\n24.10%\nSentence Bert (large)\n340M\n75.16%\n20.79%\nSentence RoBERTa (base)\n125M\n74.35%\n22.82%\nSentence RoBERTa (large)\n325M\n78.57%\n24.72%\nGPT-3 175B AS PLANNING LM\nAvg. GloVe embeddings\n-\n47.40%\n12.16%\nSentence Bert (base)\n110M\n77.60%\n24.49%\nSentence Bert (large)\n340M\n67.86%\n21.24%\nSentence RoBERTa (base)\n125M\n72.73%\n23.64%\nSentence RoBERTa (large)\n325M\n73.05%\n24.09%\nTable 3: Effect of different Translation LMs on executability and LCS.\n5.4\nCan LLMs generate actionable programs by following step-by-step instructions?\nPrior works often focus on translating step-by-step instructions into executable programs. Speciﬁcally,\ninstead of only providing a high-level task name, how-to instructions are also provided, as shown\nin Figure 5. Although this setting is easier as it does not require rich prior knowledge, how-to\ninstructions can help resolve much ambiguity of exactly how to perform a high-level task when\nmultiple solutions are possible. To investigate whether pre-trained LLMs are capable of doing this\nwithout additional training, we include these instructions in the prompt and evaluate LLMs with\nthe proposed procedure. We compare to a supervised baseline from VirtualHome that trains an\nLSTM [17] from scratch on human-annotated data. Since the code to train the baseline is not publicly\nreleased and a different train/test split is likely used, we only show results reported in Puig et al. [38]\nas a crude reference. We also cannot compare executability as it is not reported. Results are shown in\nTable 4. Surprisingly, without being ﬁne-tuned on any domain data, Translated Codex/GPT-3 can\nattain LCS close to supervised methods while generating highly executable programs.\nTask: Read book\nDescription: Walk to home office, \nturn on light, grab a book, sit in \nchair, start to read the book.\nStep 1: Walk to home office\nStep 2: Walk to light\nStep 3: Find light\nStep 4: Switch on light\nStep 5: Find novel\nStep 6: Grab novel\nStep 7: Find chair\nStep 8: Sit on chair\nStep 9: Read novel\nTask: Find dictionary\nDescription: Move towards the\nbookshelf, scan the bookshelf for\nthe dictionary, when the\ndictionary is found, pick up the\ndictionary.\nFigure 5: An example prompt containing step-by-\nstep instructions.\nMethods\nExecutability\nLCS\nTranslated Codex 12B\n78.57%\n32.87%\nTranslated GPT-3 175B\n74.15%\n31.05%\nSupervised LSTM\n-\n34.00%\nTable 4: Executability and LCS when conditioned\non step-by-step instructions.\n11\n5.5\nAnalysis of program length\nShorter programs have a natural advantage of being more executable as they need to satisfy less\npre/post-conditions, albeit being prone to incompleteness. To validate the proposed approach does\nnot simply generate very short programs, we calculate the average program length across the 88\nevaluated tasks. Results are shown in Table 5. Mirroring the observations made in Section 4.1 and\nSection 4.2, we ﬁnd smaller LMs such as GPT-2 tend to generate shorter programs than larger models\ndo while frequently repeating the given executable example. In contrast, larger models like Codex\nand GPT-3 can generate more expressive programs with high realism, yet consequently, they often\nsuffer from executability. We show proposed procedure can ﬁnd appropriate balance and is capable\nof generating programs that are highly executable while maintaining reasonable expressiveness as\nmeasured by program length.\nMethods\nExecutability\nAverage Length\nVanilla GPT-2 1.5B\n39.40%\n4.24\nVanilla Codex 12B\n18.07%\n7.22\nVanilla GPT-3 175B\n7.79%\n9.716\nTranslated Codex 12B\n78.57%\n7.13\nTranslated GPT-3 175B\n73.05%\n7.36\nHuman\n100.00%\n9.66\nTable 5: Average executability & program length of different methods.\n6\nRelated Works\nLarge-scale natural language modeling has witnessed rapid advances since the inception of the\nTransformer architecture [53]. It has been shown by recent works that large language models (LLMs)\npre-trained on large unstructured text corpus not only can perform strongly on various down-stream\nNLP tasks [10, 39, 40, 5] but the learned representations can also be used to model relations of\nentities [23], retrieve matching visual features [19], synthesize code from docstrings [15, 7], solve\nmath problems [8, 46], and even as valuable priors when applied to diverse tasks from different\nmodalities [28, 52]. Notably, by pre-training on large-scale data, these models can also internalize an\nimplicit knowledge base containing rich information about the world from which factual answers\n(e.g. “Dante was born in ⟨PLACE⟩”) can be extracted [36, 21, 9, 50, 42]. Compared to prior works in\nsingle-step knowledge extraction, we aim to extract sequential action plans to complete open-ended\nhuman activities while satisfying various constraints of an interactive environment.\nMany prior works have looked into grounding natural language in embodied environments. A series\nof them parse language instructions into formal logic or rely mainly on lexical analysis to resolve\nvarious linguistic ambiguities for embodied agents [2, 33, 34, 51]. However, they often require many\nhand-designed rules or scale inadequately to more complex tasks and environments. Recently, many\nefforts have been put into creating more realistic environments with the goal to further advances in\nthis area [38, 47, 48, 22, 44, 1]. At the same time, by leveraging the better representation power of\nneural architectures, a number of works have looked into creating instruction-following agents that\ncan perform manipulation [29, 30], navigation [11, 54, 31], or both [49, 16, 12]. Recent works also\nuse language as hierarchical abstractions to plan actions using imitation learning [45] and to guide\nexploration in reinforcement learning [32].\nNotably, many prior works do not leverage full-blown pre-trained LLMs; most investigate smaller\nLMs that require considerable domain-speciﬁc data for ﬁne-tuning to obtain reasonable performance.\nPerhaps more importantly, few works have evaluated LLMs in an embodiment setting that realizes the\nfull potential of the actionable knowledge these models already contain by pre-training on large-scale\nunstructured text: the tasks evaluated are often generated from a handful of templates, which do not\nresemble the highly diverse activities that humans perform in daily lives [14, 20]. The development\nof VirtualHome environment [38] enables such possibility. However, relevant works [38, 25] rely\non human-annotated data and perform supervised training from scratch. Due to the lack of rich\nworld knowledge, these models can only generate action plans given detailed instructions of how\nto act or video demonstrations. Concurrent work by Li et al. [24] validates similar hypothesis that\n12\nLMs contain rich actionable knowledge. They ﬁne-tune GPT-2 with demonstrations to incorporate\nenvironment context and to predict actions in VirtualHome, and evaluate on tasks that are generated\nfrom pre-deﬁned predicates. In contrast, we investigate existing knowledge in LLMs without any\nadditional training and evaluate on human activity tasks expressed in free-form language.\n7\nConclusion, Limitations & Future Work\nIn this work, we investigate actionable knowledge already contained in pre-trained LLMs without any\nadditional training. We present several techniques to extract this knowledge to perform common-sense\ngrounding by planning actions for complex human activities.\nDespite promising ﬁndings, there remain several limitations of this work which we discuss as follows:\nDrop in Correctness\nAlthough our approach can signiﬁcantly improve executability of the gen-\nerated plans, we observe a considerable drop in correctness. In addition to the errors caused by\nthe proposed action translation (discussed in Section 4.3), this is partially attributed to the limited\nexpressivity of VirtualHome, as it may not support all necessary actions to fully complete all evaluated\ntasks (correctness is judged by humans). This is also reﬂected by that Vanilla LMs can even surpass\nhuman-written plans, which are restricted by environment expressivity.\nMid-Level Grounding\nInstead of grounding the LLM generation to low-level actions by using\ndownstream data from a speciﬁc environment, we focus on high-level to mid-level grounding such\nthat we evaluate raw knowledge of LLMs as closely and broadly as possible. Hence, we only\nconsider the most prominent challenge in mid-level grounding that the generated plans must satisfy\nall common-sense constraints (characterized by executability metric). As a result, we assume there\nis a low-level controller that can execute these mid-level actions (such as “grab cup”), and we do\nnot investigate the usefulness of LLMs for low-level sensorimotor behavior grounding. To perform\nsensorimotor grounding, such as navigation and interaction mask prediction, domain-speciﬁc data\nand ﬁne-tuning are likely required.\nIgnorant of Environment Context\nWe do not incorporate observation context or feedback into\nour models. To some extent, we approach LLMs in the same way as how VirtualHome asks human\nannotators to write action plans for a given human activity by imagination, in which case humans\nsimilarly do not observe environment context. Similar to human-written plans, we assume the plans\ngenerated by LMs only refer to one instance of each object class. As a result, successful plan\ngeneration for tasks like “stack two plates on the right side of a cup” is not possible.\nEvaluation Protocol\nWe measure quality of plans by a combination of executability and correctness\ninstead of one straightforward metric. To the best of our knowledge, there isn’t a known way to\ncomputationally assess the semantic correctness of the plans due to the tasks’ open-ended and\nmulti-modal nature. Prior work also adopt similar combination of metrics [38]. We report two\nmetrics individually to shine light on the deﬁciencies of existing LLMs which we hope could provide\ninsights for future works. To provide a holistic view, we report results by combining two metrics in\nSection 5.2.\nWe believe addressing each of these shortcoming will lead to exciting future directions. We also\nhope these ﬁndings can inspire future investigations into using pre-trained LMs for goal-driven\ndecision-making problems and grounding the learned knowledge in embodied environments.\nAcknowledgment\nWe would like to thank OpenAI for providing academic access to the OpenAI API and Luke Metz for\nvaluable feedback and discussions. This work was supported in part by Berkeley Deep Drive, NSF\nIIS-2024594, and GoodAI Research Award.\n13\nReferences\n[1] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid,\nStephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environments. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 3674–3683, 2018.\n[2] Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping\ninstructions to actions. Transactions of the Association for Computational Linguistics, 1:49–62,\n2013.\n[3] BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabil-\nities of language models. In preparation, 2021. URL https://github.com/google/\nBIG-bench/.\n[4] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,\nJared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano\nErmon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard,\nSaahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte\nKhani, Omar Khattab, Pang Wei Kohd, Mark Krass, Ranjay Krishna, Rohith Kuditipudi,\nAnanya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa\nLi, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr,\nIsabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi\nRaghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack\nRyan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan\nSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang,\nWilliam Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga,\nJiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia\nZheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models,\n2021.\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[6] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\npreprint arXiv:1708.00055, 2017.\n[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri\nEdwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language\nmodels trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-\npher Hesse, and John Schulman. Training veriﬁers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\n[9] Joe Davison, Joshua Feldman, and Alexander M Rush. Commonsense knowledge mining\nfrom pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 1173–1178, 2019.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n14\n[11] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe\nMorency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-\nfollower models for vision-and-language navigation. arXiv preprint arXiv:1806.02724, 2018.\n[12] Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to\ngoals: Inverse reinforcement learning for vision-based instruction following. arXiv preprint\narXiv:1902.07742, 2019.\n[13] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723, 2020.\n[14] Brent Harrison and Mark O Riedl. Learning from stories: using crowdsourced narratives to\ntrain virtual agents. In Twelfth Artiﬁcial Intelligence and Interactive Digital Entertainment\nConference, 2016.\n[15] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,\nCollin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge\ncompetence with apps. arXiv preprint arXiv:2105.09938, 2021.\n[16] Felix Hill, Sona Mokra, Nathaniel Wong, and Tim Harley. Human instruction-following with\ndeep reinforcement learning via transfer-learning from text. arXiv preprint arXiv:2005.09382,\n2020.\n[17] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\n[18] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\ntext degeneration. arXiv preprint arXiv:1904.09751, 2019.\n[19] Gabriel Ilharco, Rowan Zellers, Ali Farhadi, and Hannaneh Hajishirzi. Probing text models for\ncommon ground with visual representations. arXiv e-prints, pages arXiv–2005, 2020.\n[20] Peter A Jansen. Visually-grounded planning without vision: Language models infer detailed\nplans from high-level instructions. arXiv preprint arXiv:2009.14259, 2020.\n[21] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? Transactions of the Association for Computational Linguistics, 8:423–438,\n2020.\n[22] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti,\nDaniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d\nenvironment for visual ai. arXiv preprint arXiv:1712.05474, 2017.\n[23] Belinda Z Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural\nlanguage models. arXiv preprint arXiv:2106.00737, 2021.\n[24] Shuang Li, Xavier Puig, Yilun Du, Clinton Wang, Ekin Akyurek, Antonio Torralba, Jacob\nAndreas, and Igor Mordatch. Pre-trained language models for interactive decision-making.\narXiv preprint arXiv:2202.01771, 2022.\n[25] Yuan-Hong Liao, Xavier Puig, Marko Boben, Antonio Torralba, and Sanja Fidler. Synthesizing\nenvironment-aware activities via activity sketches. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6291–6299, 2019.\n[26] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\nWhat makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.\n[27] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[28] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. arXiv preprint arXiv:2103.05247, 2021.\n15\n[29] Corey Lynch and Pierre Sermanet.\nGrounding language in play.\narXiv preprint\narXiv:2005.07648, 2020.\n[30] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured\ndata. Proceedings of Robotics: Science and Systems. doi, 10, 2021.\n[31] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra.\nImproving vision-and-language navigation with image-text pairs from the web. In European\nConference on Computer Vision, pages 259–274. Springer, 2020.\n[32] Suvir Mirchandani, Siddharth Karamcheti, and Dorsa Sadigh. Ella: Exploration through learned\nlanguage abstraction. arXiv preprint arXiv:2103.05825, 2021.\n[33] Dipendra Misra, Kejia Tao, Percy Liang, and Ashutosh Saxena. Environment-driven lexicon\ninduction for high-level instructions. In Proceedings of the 53rd Annual Meeting of the Asso-\nciation for Computational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pages 992–1002, 2015.\n[34] Dipendra K Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Saxena. Tell me dave: Context-\nsensitive grounding of natural language to manipulation instructions. The International Journal\nof Robotics Research, 35(1-3):281–300, 2016.\n[35] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for\nword representation. In Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 1532–1543, 2014.\n[36] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel.\nLanguage models as knowledge bases?\narXiv preprint\narXiv:1909.01066, 2019.\n[37] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek,\nand Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models.\narXiv preprint arXiv:2201.11227, 2022.\n[38] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio\nTorralba. Virtualhome: Simulating household activities via programs. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 8494–8502, 2018.\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n[41] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084, 2019.\n[42] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\n[43] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633, 2021.\n[44] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana\nJain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for\nembodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 9339–9347, 2019.\n[45] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with\nlatent language. arXiv preprint arXiv:2110.01517, 2021.\n[46] Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate\n& rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034,\n2021.\n16\n[47] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-\ntaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded\ninstructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10740–10749, 2020.\n[48] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and\nMatthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive\nlearning. arXiv preprint arXiv:2010.03768, 2020.\n[49] Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme.\nEmbodied bert: A transformer model for embodied, language-guided visual task completion.\narXiv preprint arXiv:2108.04927, 2021.\n[50] Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. olmpics-on what language\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\n743–758, 2020.\n[51] Moritz Tenorth, Daniel Nyga, and Michael Beetz. Understanding and executing instructions for\neveryday manipulation tasks from the world wide web. In 2010 ieee international conference\non robotics and automation, pages 1486–1491. IEEE, 2010.\n[52] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. arXiv preprint arXiv:2106.13884,\n2021.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pages 5998–6008, 2017.\n[54] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang,\nWilliam Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised\nimitation learning for vision-language navigation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6629–6638, 2019.\n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transform-\ners: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n17\nA\nAppendix\nA.1\nHyperparameter Search\nFor each evaluated method, we perform grid search over the following hyperparameters:\nName\nDescription\nSearch Values\nepsilon (ϵ)\nOut-of-distribution early termination threshold\n{0, 0.4, 0.8}\ntemperature\nsampling parameter adjusting relative token probabilities\n{0.1, 0.3, 0.6}\nk\nnumber of samples generated by Planning LM\n{1, 10}\nbeta (β)\nweighting coefﬁcient in action translation to trade off\nsemantic and translation correctness\n{0.3}\nfrequence_penalty\nOpenAI API only; penalize new tokens based on their\nexisting frequency in the text so far\n{0.1, 0.3, 0.6, 0.9}\npresence_penalty\nOpenAI API only; penalize new tokens based on whether\nthey appear in the text so far\n{0.3, 0.5, 0.8}\nrepetition_penalty\nHugging Face Transformers only; penalize new tokens\nbased on whether repeating existing text\n{1.0, 1.2, 1.5, 1.8}\nFor methods that use ﬁxed example across evaluated tasks, we search over the following three\nrandomly chosen examples:\nExample 1\nExample 2\nExample 3\nTask: Use computer\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find computer\nStep 6: Switch on computer\nStep 7: Turn to computer\nStep 8: Look at computer\nStep 9: Find keyboard\nStep 10: Type on keyboard\nTask: Relax on sofa\nStep 1: Walk to home ofﬁce\nStep 2: Walk to couch\nStep 3: Find couch\nStep 4: Sit on couch\nStep 5: Find pillow\nStep 6: Lie on couch\nTask: Read book\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\n18\nA.2\nDetails of Human Evaluations\nHuman evaluations are conducted on Amazon Mechanical Turk. For each method, we generate action\nplans for all 88 high-level tasks. To account for the expressivity of the VirtualHome environment [38],\nwe include action plans written by human experts from the VirtualHome dataset as references in\nour human evaluations. The evaluations are conducted in the form of questionnaires containing all\naction plans whose order is randomly shufﬂed and whose corresponding methods are unknown to\nthe annotators. Human annotators are required to answer all the questions in the questionnaire. For\neach question, the annotators need to answer either “Yes” or “No” indicating if they believe the\naction plan completes the task. For each method, we report correctness percentage averaged across\n10 participated human annotators and all 88 tasks. We further report the standard error of the mean\nacross human annotators. Screenshot can be found in Figure 6.\nFigure 6: Screenshot of human evaluation interface, conducted as a Google Forms questionnaire.\n19\nA.3\nAll Evaluated Tasks\nThe evaluated tasks are part of the ActivityPrograms dataset collected by Puig et al. [38]. Some of the\ntask names may contain misspelling(s).\n1. Apply lotion\n2. Arrange folders\n3. Breakfast\n4. Browse internet\n5. Brush teeth\n6. Change clothes\n7. Change sheets and pil-\nlow cases\n8. Collect napkin rings\n9. Complete surveys on\namazon turk\n10. Compute\n11. Decorate it\n12. Do homework\n13. Do work\n14. Draft home\n15. Draw picture\n16. Dry soap bottles\n17. Dust\n18. Eat cereal\n19. Eat cheese\n20. Eat snacks and drink\ntea\n21. Empty dishwasher and\nﬁll dishwasher\n22. Entertain\n23. Feed me\n24. Find dictionary\n25. Fix snack\n26. Get glass of milk\n27. Give milk to cat\n28. Go to sleep\n29. Grab things\n30. Hand washing\n31. Hang keys\n32. Hang pictures\n33. Iron shirt\n34. Keep cats inside while\ndoor is open\n35. Keep cats out of room\n36. Leave home\n37. Listen to music\n38. Look at mirror\n39. Look at painting\n40. Make bed\n41. Make popcorn\n42. Organize closet\n43. Organize pantry\n44. Paint ceiling\n45. Pay bills\n46. Pick up toys\n47. Play musical chairs\n48. Prepare pot of boiling\nwater\n49. Push all chairs in\n50. Push in desk chair\n51. Put alarm clock in bed-\nroom\n52. Put away groceries\n53. Put away toys\n54. Put clothes away\n55. Put mail in mail orga-\nnizer\n56. Put on your shoes\n57. Put out ﬂowers\n58. Put up decoration\n59. Read\n60. Read newspaper\n61. Read on sofa\n62. Read to child\n63. Read yourself to sleep\n64. Receive credit card\n65. Restock\n66. Scrubbing living room\ntile ﬂoor is once week\nactivity for me\n67. Style hair\n68. Switch on lamp\n69. Take jacket off\n70. Take shoes off\n71. Tale off shoes\n72. Throw away paper\n73. Try yourself off\n74. Turn off TV\n75. Turn on TV with re-\nmote\n76. Turn on radio\n77. Type up document\n78. Unload various items\nfrom pockets and place\nthem in bowl on table\n79. Use laptop\n80. Vacuum\n81. Walk to room\n82. Wash dirty dishes\n83. Wash face\n84. Wash monitor\n85. Wash teeth\n86. Watch horror movie\n87. Wipe down sink\n88. Write book\n20\nA.4\nNatural Language Templates for All Atomic Actions\nVirtualHome requires action steps speciﬁed in a speciﬁc format, yet language models are trained to\ndeal with mostly natural language. We thus deﬁne a natural language template for each atomic action\nand only expose the converted natural language text in all operations involving language models,\ni.e. autoregressive generation and action translation. After we obtain an entire generated program\nexpressed in natural language, such as those in Figure 1 and Figure 2, we then convert each action\nstep to the VirtualHome syntax. Full list of the atomic actions and their natural language templates\ncan be found below.\nAtomic Action in VirtualHome Syntax\nNatural Language Template\n[CLOSE] ⟨arg1⟩(1)\nclose ⟨arg1⟩\n[CUT] ⟨arg1⟩(1)\ncut ⟨arg1⟩\n[DRINK] ⟨arg1⟩(1)\ndrink ⟨arg1⟩\n[DROP] ⟨arg1⟩(1)\ndrop ⟨arg1⟩\n[EAT] ⟨arg1⟩(1)\neat ⟨arg1⟩\n[FIND] ⟨arg1⟩(1)\nﬁnd ⟨arg1⟩\n[GRAB] ⟨arg1⟩(1)\ngrab ⟨arg1⟩\n[GREET] ⟨arg1⟩(1)\ngreet ⟨arg1⟩\n[LIE] ⟨arg1⟩(1)\nlie on ⟨arg1⟩\n[LOOKAT] ⟨arg1⟩(1)\nlook at ⟨arg1⟩\n[MOVE] ⟨arg1⟩(1)\nmove ⟨arg1⟩\n[OPEN] ⟨arg1⟩(1)\nopen ⟨arg1⟩\n[PLUGIN] ⟨arg1⟩(1)\nplug in ⟨arg1⟩\n[PLUGOUT] ⟨arg1⟩(1)\nplug out ⟨arg1⟩\n[POINTAT] ⟨arg1⟩(1)\npoint at ⟨arg1⟩\n[POUR] ⟨arg1⟩(1) ⟨arg2⟩(1)\npour ⟨arg1⟩into ⟨arg2⟩\n[PULL] ⟨arg1⟩(1)\npull ⟨arg1⟩\n[PUSH] ⟨arg1⟩(1)\npush ⟨arg1⟩\n[PUTBACK] ⟨arg1⟩(1) ⟨arg2⟩(1)\nput ⟨arg1⟩on ⟨arg2⟩\n[PUTIN] ⟨arg1⟩(1) ⟨arg2⟩(1)\nput ⟨arg1⟩in ⟨arg2⟩\n[PUTOBJBACK] ⟨arg1⟩(1)\nput back ⟨arg1⟩\n[PUTOFF] ⟨arg1⟩(1)\ntake off ⟨arg1⟩\n[PUTON] ⟨arg1⟩(1)\nput on ⟨arg1⟩\n[READ] ⟨arg1⟩(1)\nread ⟨arg1⟩\n[RELEASE]\nrelease\n[RINSE] ⟨arg1⟩(1)\nrinse ⟨arg1⟩\n[RUN] ⟨arg1⟩(1)\nrun to ⟨arg1⟩\n[SCRUB] ⟨arg1⟩(1)\nscrub ⟨arg1⟩\n[SIT] ⟨arg1⟩(1)\nsit on ⟨arg1⟩\n[SLEEP]\nsleep\n[SQUEEZE] ⟨arg1⟩(1)\nsqueeze ⟨arg1⟩\n[STANDUP]\nstand up\n[SWITCHOFF] ⟨arg1⟩(1)\nswitch off ⟨arg1⟩\n[SWITCHON] ⟨arg1⟩(1)\nswitch on ⟨arg1⟩\n[TOUCH] ⟨arg1⟩(1)\ntouch ⟨arg1⟩\n[TURNTO] ⟨arg1⟩(1)\nturn to ⟨arg1⟩\n[TYPE] ⟨arg1⟩(1)\ntype on ⟨arg1⟩\n[WAKEUP]\nwake up\n[WALK] ⟨arg1⟩(1)\nwalk to ⟨arg1⟩\n[WASH] ⟨arg1⟩(1)\nwash ⟨arg1⟩\n[WATCH] ⟨arg1⟩(1)\nwatch ⟨arg1⟩\n[WIPE] ⟨arg1⟩(1)\nwipe ⟨arg1⟩\n21\nA.5\nRandom Samples of Action Plans\nHuman\nVanilla GPT3-175B\nTranslated GPT3-175B (Ours)\nTask: Go to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find bed\nStep 4: Turn to bed\nStep 5: Look at bed\nStep 6: Sit on bed\nStep 7: Sleep\nTask: Go to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find pajamas\nStep 4: Put on pajamas\nStep 5: Find slippers\nStep 6: Put on slippers\nStep 7: Get in bed\nStep 8: Sleep\nTask: Go to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Lie on bed\nStep 4: Sleep\nTask: Browse internet\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find computer\nStep 6: Switch on computer\nStep 7: Turn to computer\nStep 8: Look at computer\nStep 9: Find keyboard\nStep 10: Type on keyboard\nTask: Browse internet\nStep 1: Walk to home ofﬁce\nStep 2: Walk to computer\nStep 3: Find computer\nStep 4: Grab computer\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Browse internet\nTask: Browse internet\nStep 1: Walk to home ofﬁce\nStep 2: Walk to desk\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find computer\nStep 6: Switch on computer\nStep 7: Find mouse\nStep 8: Grab mouse\nStep 9: Turn to computer\nStep 10: Point at computer\nTask: Wash teeth\nStep 1: Stand up\nStep 2: Walk to bathroom\nStep 3: Walk to sink\nStep 4: Find sink\nStep 5: Turn to sink\nStep 6: Look at sink\nStep 7: Find teeth\nStep 8: Wash teeth\nTask: Wash teeth\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Turn on faucet\nStep 4: Grab toothbrush\nStep 5: Put toothpaste on tooth-\nbrush\nStep 6: Put toothbrush in mouth\nStep 7: Brush teeth\nStep 8: Rinse mouth out with wa-\nter\nStep 9: Turn off faucet\nStep 10: Walk back to bedroom\nTask: Wash teeth\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Find faucet\nStep 4: Switch on faucet\nStep 5: Walk to toothbrush\nStep 6: Grab toothbrush\nTask: Brush teeth\nStep 1: Walk to bathroom\nStep 2: Walk to tooth paste\nStep 3: Find tooth paste\nStep 4: Grab tooth paste\nStep 5: Find toothbrush\nStep 6: Grab toothbrush\nTask: Brush teeth\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Turn on water\nStep 4: Grab toothbrush\nStep 5: Put toothpaste on tooth-\nbrush\nStep 6: Put toothbrush in mouth\nStep 7: Brush teeth\nTask: Brush teeth\nStep 1: Find toothbrush\nStep 2: Turn to toothbrush\nStep 3: Point at toothbrush\nStep 4: Wash toothbrush\nTask: Vacuum\nStep 1: Walk to home ofﬁce\nStep 2: Walk to computer\nStep 3: Find vacuum cleaner\nStep 4:\nSwitch on vacuum\ncleaner\nStep 5:\nSwitch off vacuum\ncleaner\nTask: Vacuum\nStep 1: Walk to home ofﬁce\nStep 2: Walk to vacuum\nStep 3: Find vacuum\nStep 4: Grab vacuum\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Vacuum\nTask: Vacuum\nStep 1: Walk to home ofﬁce\nStep 2: Walk to dresser\nStep 3: Open dresser\nStep 4: Find vacuum cleaner\nStep 5: Grab vacuum cleaner\nStep 6: Pull vacuum cleaner\nStep 7: Plug in vacuum cleaner\nStep 8:\nSwitch on vacuum\ncleaner\n22\nTask: Change sheets and pillow\ncases\nStep 1: Find pillow\nStep 2: Turn to pillow\nStep 3: Look at pillow\nStep 4: Grab pillow\nTask: Change sheets and pillow\ncases\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Change sheets and pillow\ncases\nStep 4: Walk to kitchen\nStep 5: Walk to sink\nStep 6: Wash sheets and pillow\ncases\nStep 7: Walk to bedroom\nStep 8: Put sheets and pillow\ncases on bed\nStep 9: Walk to living room\nStep 10: Sit on couch\nTask: Change sheets and pillow\ncases\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find sheets\nStep 4: Grab sheets\nStep 5: Find pillow\nStep 6: Grab pillow\nTask: Wash dirty dishes\nStep 1: Walk to dining room\nStep 2: Walk to sink\nStep 3: Find sponge\nStep 4: Grab sponge\nStep 5: Find faucet\nStep 6: Switch on faucet\nStep 7: Put sponge on sink\nStep 8: Grab sponge\nStep 9: Find dish soap\nStep 10: Grab dish soap\nStep 11:\nPour dish soap into\nsponge\nStep 12: Put back dish soap\nStep 13: Find plate\nStep 14: Grab plate\nStep 15: Scrub plate\nStep 16: Rinse plate\nStep 17: Find dishrack\nStep 18: Put plate on dishrack\nStep 19: Find plate\nStep 20: Grab plate\nStep 21: Scrub plate\nStep 22: Rinse plate\nStep 23: Put plate on dishrack\nStep 24: Rinse sponge\nStep 25: Put back sponge\nStep 26: Switch off faucet\nTask: Wash dirty dishes\nStep 1: Walk to kitchen\nStep 2: Walk to sink\nStep 3: Wash dishes\nStep 4: Dry dishes\nStep 5: Put dishes away\nTask: Wash dirty dishes\nStep 1: Walk to dining room\nStep 2: Walk to sink\nStep 3: Find faucet\nStep 4: Switch on faucet\nStep 5: Walk to plate\nStep 6: Grab plate\n23\nTask: Feed me\nStep 1: Walk to dining room\nStep 2: Walk to plate\nStep 3: Find plate\nStep 4: Grab plate\nStep 5: Find food\nStep 6: Grab food\nStep 7: Put food on plate\nStep 8: Find food\nStep 9: Grab food\nStep 10: Put food on plate\nStep 11: Find fork\nStep 12: Grab fork\nStep 13: Eat plate\nStep 14: Eat plate\nTask: Feed me\nStep 1: Walk to kitchen\nStep 2: Walk to refrigerator\nStep 3: Open refrigerator\nStep 4: Grab food\nStep 5: Find plate\nStep 6: Put food on plate\nStep 7: Find fork\nStep 8: Put fork on plate\nStep 9: Find spoon\nStep 10: Put spoon on plate\nStep 11: Find cup\nStep 12: Pour water in cup\nStep 13: Walk to table\nStep 14: Sit at table\nStep 15: Eat food\nTask: Feed me\nStep 1: Walk to dining room\nStep 2: Walk to table\nStep 3: Find table\nStep 4: Turn to table\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find food\nStep 8: Grab food\nStep 9: Find plate\nStep 10: Put food on plate\nTask: Breakfast\nStep 1: Walk to dining room\nStep 2: Walk to food\nStep 3: Grab food\nStep 4: Walk to bedroom\nStep 5: Walk to television\nStep 6: Switch on television\nStep 7: Walk to bed\nStep 8: Lie on bed\nStep 9: Turn to television\nStep 10: Watch television\nStep 11: Eat food\nTask: Breakfast\nStep 1: Walk to kitchen\nStep 2: Find cereal\nStep 3: Pour cereal into bowl\nStep 4: Pour milk into bowl\nStep 5: Walk to living room\nStep 6: Sit on couch\nStep 7: Eat cereal\nTask: Breakfast\nStep 1: Walk to dining room\nStep 2: Walk to freezer\nStep 3: Find freezer\nStep 4: Open freezer\nStep 5: Find bread\nStep 6: Grab bread\nStep 7: Close freezer\nStep 8: Find toaster\nStep 9: Plug in toaster\nStep 10: Put bread on plate\nStep 11: Switch on toaster\nTask: Read\nStep 1: Find novel\nStep 2: Grab novel\nStep 3: Walk to bedroom\nStep 4: Walk to chair\nStep 5: Sit on chair\nStep 6: Read novel\nTask: Read\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nTask: Read\nStep 1: Walk to home ofﬁce\nStep 2: Walk to light\nStep 3: Find light\nStep 4: Switch on light\nStep 5: Find novel\nStep 6: Grab novel\nStep 7: Find chair\nStep 8: Sit on chair\nStep 9: Read novel\nTask: Style hair\nStep 1: Find towel\nStep 2: Turn to towel\nStep 3: Point at towel\nStep 4: Wash towel\nStep 5: Turn to towel\nStep 6: Look at towel\nStep 7: Squeeze towel\nStep 8: Walk to bathroom\nTask: Style hair\nStep 1: Walk to bathroom\nStep 2: Walk to mirror\nStep 3: Find hairbrush\nStep 4: Grab hairbrush\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Style hair\nTask: Style hair\nStep 1: Walk to bathroom\nStep 2: Walk to mirror\nStep 3: Find hair\nStep 4: Grab hair\nStep 5: Find comb\nStep 6: Grab comb\nStep 7: Find brush\nStep 8: Grab brush\n24\nTask: Pick up toys\nStep 1: Walk to bedroom\nStep 2: Walk to toy\nStep 3: Find toy\nStep 4: Grab toy\nStep 5: Walk to ﬁling cabinet\nStep 6: Open ﬁling cabinet\nStep 7: Put toy on ﬁling cabinet\nStep 8: Find toy\nStep 9: Grab toy\nStep 10: Walk to ﬁling cabinet\nStep 11: Put toy on ﬁling cabinet\nStep 12: Find toy\nStep 13: Grab toy\nStep 14: Walk to ﬁling cabinet\nStep 15: Put toy on ﬁling cabinet\nStep 16: Close ﬁling cabinet\nTask: Pick up toys\nStep 1: Walk to living room\nStep 2: Walk to toy box\nStep 3: Find toy box\nStep 4: Grab toy box\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Pick up toys\nTask: Pick up toys\nStep 1: Find toy\nStep 2: Turn to toy\nStep 3: Point at toy\nStep 4: Wash toy\nTask: Decorate it\nStep 1: Walk to home ofﬁce\nStep 2: Walk to centerpiece\nStep 3: Find centerpiece\nStep 4: Grab centerpiece\nStep 5: Walk to table\nStep 6: Put centerpiece on table\nStep 7: Find drawing\nStep 8: Grab drawing\nStep 9: Find wall\nStep 10: Turn to wall\nStep 11: Put drawing on wall\nStep 12: Find drawing\nStep 13: Grab drawing\nStep 14: Turn to wall\nStep 15: Put drawing on wall\nTask: Decorate it\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nStep 8: Decorate it\nTask: Decorate it\nStep 1: Walk to table\nStep 2: Walk to centerpiece\nStep 3: Look at centerpiece\nStep 4: Grab centerpiece\nStep 5: Point at centerpiece\nTask: Put away groceries\nStep 1: Walk to dining room\nStep 2: Walk to freezer\nStep 3: Find freezer\nStep 4: Open freezer\nStep 5: Find food\nStep 6: Grab food\nStep 7: Put food in freezer\nStep 8: Close freezer\nTask: Put away groceries\nStep 1: Walk to kitchen\nStep 2: Walk to refrigerator\nStep 3: Open refrigerator\nStep 4: Grab milk and eggs\nStep 5: Close refrigerator\nStep 6: Walk to counter\nStep 7: Put milk and eggs on\ncounter\nStep 8: Walk to cabinet\nStep 9: Open cabinet\nStep 10: Put away milk and eggs\nStep 11: Close cabinet\nStep 12: Walk to refrigerator\nStep 13: Grab orange juice and\nbread\nStep 14: Close refrigerator\nTask: Put away groceries\nStep 1: Walk to kitchen\nStep 2: Walk to fridge\nStep 3: Open fridge\nStep 4: Put food in fridge\nStep 5: Close fridge\nStep 6: Walk to dining room\nStep 7: Walk to livingroom\nStep 8: Walk to bedroom\nStep 9: Walk to bathroom\n25\nTask: Draw picture\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find desk\nStep 6: Open desk\nStep 7: Find check\nStep 8: Grab check\nStep 9: Put check on desk\nStep 10: Find pencil\nStep 11: Grab pencil\nStep 12: Close desk\nTask: Draw picture\nStep 1: Walk to home ofﬁce\nStep 2: Walk to pencils\nStep 3: Find pencils\nStep 4: Grab pencils\nStep 5: Find paper\nStep 6: Sit on chair\nStep 7: Draw picture\nTask: Draw picture\nStep 1: Walk to home ofﬁce\nStep 2: Walk to wall\nStep 3: Walk to drawing\nStep 4: Find drawing\nStep 5: Turn to drawing\nStep 6: Look at drawing\nStep 7: Touch drawing\nStep 8: Pull drawing\nStep 9: Push drawing\nTask: Entertain\nStep 1: Walk to home ofﬁce\nStep 2: Walk to cheese\nStep 3: Find cheese\nStep 4: Find food\nStep 5: Find couch\nStep 6: Sit on couch\nStep 7: Find wine glass\nStep 8: Grab wine glass\nStep 9: Drink wine glass\nTask: Entertain\nStep 1: Walk to living room\nStep 2: Walk to entertainment\ncenter\nStep 3: Find remote\nStep 4: Grab remote\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Entertain self\nTask: Entertain\nStep 1: Walk to livingroom\nStep 2: Walk to couch\nStep 3: Find couch\nStep 4: Sit on couch\nTask: Type up document\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find computer\nStep 6: Switch on computer\nStep 7: Turn to computer\nStep 8: Look at computer\nStep 9: Find check\nStep 10: Grab check\nStep 11: Read check\nStep 12: Find coffee cup\nStep 13: Grab coffee cup\nStep 14: Drink coffee cup\nStep 15: Put back coffee cup\nStep 16: Find keyboard\nStep 17: Type on keyboard\nTask: Type up document\nStep 1: Walk to home ofﬁce\nStep 2: Walk to keyboard\nStep 3: Find keyboard\nStep 4: Grab keyboard\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Type document\nTask: Type up document\nStep 1: Walk to home ofﬁce\nStep 2: Walk to computer\nStep 3: Find computer\nStep 4: Switch on computer\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Turn to computer\nStep 8: Look at computer\nStep 9: Find keyboard\nStep 10: Grab keyboard\nStep 11: Type on keyboard\nStep 12: Stand up\nStep 13: Find printer\nStep 14: Switch on printer\nStep 15: Find printing paper\nTask: Put on your shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Grab shoes\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find feet both\nStep 8: Put shoes on feet both\nTask: Put on your shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Grab shoes\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Put on shoes\nTask: Put on your shoes\nStep 1: Find shoes\nStep 2: Turn to shoes\nStep 3: Point at shoes\nStep 4: Wash shoes\n26\nTask: Watch horror movie\nStep 1: Walk to home ofﬁce\nStep 2: Walk to remote control\nStep 3: Find remote control\nStep 4: Grab remote control\nStep 5: Find couch\nStep 6: Sit on couch\nStep 7: Find television\nStep 8: Switch on television\nStep 9: Turn to television\nStep 10: Watch television\nTask: Watch horror movie\nStep 1: Walk to home ofﬁce\nStep 2: Walk to horror movie\nStep 3: Find horror movie\nStep 4: Grab horror movie\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Watch horror movie\nTask: Watch horror movie\nStep 1: Find television\nStep 2: Switch on television\nStep 3: Find cd player\nStep 4: Switch on cd player\nStep 5: Switch off cd player\nStep 6: Find remote control\nStep 7: Switch on remote control\nStep 8: Switch off remote control\nTask: Turn on radio\nStep 1: Walk to home ofﬁce\nStep 2: Walk to stereo\nStep 3: Find stereo\nStep 4: Find electrical outlet\nStep 5: Turn to electrical outlet\nStep 6: Plug in stereo\nStep 7: Switch on electrical outlet\nStep 8: Switch on stereo\nTask: Turn on radio\nStep 1: Walk to home ofﬁce\nStep 2: Walk to radio\nStep 3: Find radio\nStep 4: Grab radio\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Turn on radio\nTask: Turn on radio\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find remote control\nStep 6: Grab remote control\nStep 7: Find radio\nStep 8: Point at radio\nStep 9: Switch on radio\nStep 10: Put back remote control\nTask: Look at mirror\nStep 1: Walk to bathroom\nStep 2: Walk to mirror\nStep 3: Find mirror\nStep 4: Turn to mirror\nStep 5: Look at mirror\nTask: Look at mirror\nStep 1: Walk to bathroom\nStep 2: Walk to mirror\nStep 3: Look in mirror\nTask: Look at mirror\nStep 1: Walk to bedroom\nStep 2: Walk to mirror\nStep 3: Find mirror\nStep 4: Turn to mirror\nStep 5: Look at mirror\nStep 6: Find face\nStep 7: Turn to face\nStep 8: Look at face\nStep 9: Find hair\nStep 10: Turn to hair\nStep 11: Look at hair\nStep 12: Find dress\nStep 13: Turn to dress\nStep 14: Look at dress\nTask: Leave home\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Grab shoes\nStep 5: Put on shoes\nTask: Leave home\nStep 1: Walk to front door\nStep 2: Open front door\nStep 3: Walk to car\nStep 4: Open car door\nStep 5: Get in car\nStep 6: Close car door\nStep 7: Drive to work\nTask: Leave home\nStep 1: Find shoes\nStep 2: Grab shoes\nStep 3: Put on shoes\nTask: Walk to room\nStep 1: Walk to home ofﬁce\nStep 2: Walk to home ofﬁce\nTask: Walk to room\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nTask: Walk to room\nStep 1: Walk to dining room\nStep 2: Walk to livingroom\nStep 3: Walk to bedroom\n27\nTask: Hang pictures\nStep 1: Walk to bedroom\nStep 2: Walk to drawing\nStep 3: Find drawing\nStep 4: Grab drawing\nStep 5: Find wall\nStep 6: Put drawing on wall\nStep 7: Turn to drawing\nStep 8: Look at drawing\nStep 9: Find drawing\nStep 10: Grab drawing\nStep 11: Put drawing on wall\nStep 12: Turn to drawing\nStep 13: Look at drawing\nTask: Hang pictures\nStep 1: Walk to home ofﬁce\nStep 2: Walk to box of pictures\nStep 3: Find box of pictures\nStep 4: Grab box of pictures\nStep 5: Walk to wall\nStep 6: Hang pictures on wall\nTask: Hang pictures\nStep 1: Walk to home ofﬁce\nStep 2: Walk to table\nStep 3: Find keys\nStep 4: Grab keys\nStep 5: Walk to wall\nStep 6: Find hanger\nStep 7: Put keys on hanger\nTask: Wipe down sink\nStep 1: Walk to dining room\nStep 2: Walk to rag\nStep 3: Find rag\nStep 4: Grab rag\nStep 5: Walk to sink\nStep 6: Wipe sink\nStep 7: Put back rag\nTask: Wipe down sink\nStep 1: Walk to kitchen\nStep 2: Walk to sink\nStep 3: Grab sponge\nStep 4: Wipe down sink\nStep 5: Rinse sponge\nStep 6: Put sponge away\nStep 7: Wash hands\nTask: Wipe down sink\nStep 1: Walk to bathroom\nStep 2: Walk to cleaning solution\nStep 3: Find cleaning solution\nStep 4: Grab cleaning solution\nStep 5: Find sink\nStep 6: Pour cleaning solution\ninto sink\nStep 7: Find faucet\nStep 8: Switch on faucet\nStep 9: Scrub sink\nStep 10: Wash sink\nTask: Take shoes off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Find shoes\nStep 5: Take off shoes\nStep 6: Find dresser\nStep 7: Open dresser\nStep 8: Grab shoes\nStep 9: Put shoes in dresser\nStep 10: Close dresser\nTask: Take shoes off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Grab shoes\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Take shoes off\nTask: Take shoes off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nTask: Give milk to cat\nStep 1: Walk to home ofﬁce\nStep 2: Walk to cat\nStep 3: Find cat\nStep 4: Turn to cat\nStep 5: Point at cat\nStep 6: Pull cat\nStep 7: Find milk\nStep 8: Grab milk\nStep 9: Find plate\nStep 10: Pour milk into plate\nStep 11: Touch cat\nStep 12: Scrub cat\nTask: Give milk to cat\nStep 1: Walk to kitchen\nStep 2: Walk to fridge\nStep 3: Open fridge\nStep 4: Grab milk\nStep 5: Walk to cat\nStep 6: Pour milk into bowl\nStep 7: Put bowl on ﬂoor\nStep 8: Walk away from cat\nTask: Give milk to cat\nStep 1: Find cat\nStep 2: Touch cat\nStep 3: Open fridge\nStep 4: Touch milk\nStep 5: Pour milk into bowl\n28\nTask: Grab things\nStep 1: Walk to novel\nStep 2: Turn to novel\nStep 3: Grab novel\nTask: Grab things\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nStep 8: Grab things\nTask: Grab things\nStep 1: Find toy\nStep 2: Turn to toy\nStep 3: Point at toy\nStep 4: Wash toy\nTask: Use laptop\nStep 1: Walk to home ofﬁce\nStep 2: Walk to couch\nStep 3: Find couch\nStep 4: Sit on couch\nStep 5: Find laptop\nStep 6: Switch on laptop\nStep 7: Turn to laptop\nStep 8: Look at laptop\nStep 9: Find keyboard\nStep 10: Type on keyboard\nStep 11: Turn to keyboard\nStep 12: Look at keyboard\nStep 13: Type on keyboard\nStep 14: Turn to laptop\nStep 15: Look at laptop\nTask: Use laptop\nStep 1: Walk to home ofﬁce\nStep 2: Walk to laptop\nStep 3: Find laptop\nStep 4: Grab laptop\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Use laptop\nTask: Use laptop\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find laptop\nStep 6: Switch on laptop\nStep 7: Find mouse\nStep 8: Grab mouse\nStep 9: Find mousepad\nStep 10: Put mouse on mousepad\nStep 11: Turn to laptop\nStep 12: Point at laptop\nTask: Organize pantry\nStep 1: Walk to dining room\nStep 2: Walk to pantry\nStep 3: Find food\nStep 4: Grab food\nStep 5: Turn to food\nStep 6: Look at food\nStep 7: Find garbage can\nStep 8: Put food on garbage can\nStep 9: Find vegetable\nStep 10: Grab vegetable\nStep 11: Turn to vegetable\nStep 12: Look at vegetable\nStep 13:\nPut vegetable on\ngarbage can\nStep 14: Find dry pasta\nStep 15: Grab dry pasta\nStep 16: Turn to dry pasta\nStep 17: Look at dry pasta\nStep 18: Put back dry pasta\nStep 19: Find food\nStep 20: Turn to food\nStep 21: Look at food\nStep 22: Push food\nStep 23: Find noodles\nStep 24: Grab noodles\nStep 25: Turn to noodles\nStep 26: Look at noodles\nStep 27: Put back noodles\nTask: Organize pantry\nStep 1: Walk to kitchen\nStep 2: Walk to pantry\nStep 3: Find pantry\nStep 4: Grab pantry\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Organize pantry\nTask: Organize pantry\nStep 1: Walk to pantry\nStep 2: Find pantry\n29\nTask: Change clothes\nStep 1: Walk to bedroom\nStep 2: Walk to dresser\nStep 3: Open dresser\nStep 4: Find dress\nStep 5: Grab dress\nStep 6: Put on dress\nStep 7: Close dresser\nTask: Change clothes\nStep 1: Walk to bedroom\nStep 2: Walk to closet\nStep 3: Find clothes\nStep 4: Grab clothes\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Change clothes\nTask: Change clothes\nStep 1: Walk to bedroom\nStep 2: Walk to dresser\nStep 3: Find dresser\nStep 4: Open dresser\nStep 5: Find pants\nStep 6: Grab pants\nStep 7: Put on pants\nStep 8: Find shirt\nStep 9: Grab shirt\nStep 10: Put on shirt\nStep 11: Close dresser\nTask: Put up decoration\nStep 1: Walk to home ofﬁce\nStep 2: Walk to drawing\nStep 3: Find drawing\nStep 4: Find centerpiece\nStep 5: Grab centerpiece\nStep 6: Walk to wall\nStep 7: Grab drawing\nStep 8: Put drawing on wall\nStep 9: Find centerpiece\nStep 10: Grab centerpiece\nStep 11: Walk to table\nStep 12: Put centerpiece on table\nTask: Put up decoration\nStep 1: Walk to home ofﬁce\nStep 2: Walk to decoration\nStep 3: Find decoration\nStep 4: Grab decoration\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Put up decoration\nTask: Put up decoration\nStep 1: Walk to livingroom\nStep 2: Walk to table\nTask: Make bed\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find sheets\nStep 4: Grab sheets\nStep 5: Pull sheets\nTask: Make bed\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find sheets\nStep 4: Find blanket\nStep 5: Find pillow\nStep 6: Put sheets on bed\nStep 7: Put blanket on bed\nStep 8: Put pillow on bed\nTask: Make bed\nStep 1: Stand up\nStep 2: Find sheets\nStep 3: Find blanket\nStep 4: Find pillow\nStep 5: Find comforter\nStep 6: Find bed\nTask: Scrubbing living room tile\nﬂoor is once week activity for me\nStep 1: Find mop\nStep 2: Turn to mop\nStep 3: Point at mop\nStep 4: Wash mop\nStep 5: Turn to mop\nStep 6: Look at mop\nTask: Scrubbing living room tile\nﬂoor is once week activity for me\nStep 1: Walk to living room\nStep 2: Walk to tile ﬂoor\nStep 3: Find scrub brush\nStep 4: Grab scrub brush\nStep 5: Scrub tile ﬂoor\nStep 6: Put down scrub brush\nStep 7: Walk to kitchen\nTask: Scrubbing living room tile\nﬂoor is once week activity for me\nStep 1: Walk to livingroom\nStep 2: Walk to mop\nStep 3: Find mop\nStep 4: Grab mop\nStep 5: Walk to bathroom\nStep 6: Find bucket\nStep 7: Switch on faucet\nTask: Read yourself to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find bed\nStep 6: Sit on bed\nStep 7: Find bookmark\nStep 8: Grab bookmark\nStep 9: Read novel\nStep 10: Put bookmark on novel\nStep 11: Sleep\nTask: Read yourself to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find book\nStep 4: Grab book\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read book\nStep 8: Sleep\nTask: Read yourself to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Lie on bed\nStep 4: Sleep\nStep 5: Read book\n30\nTask: Hand washing\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Find faucet\nStep 4: Switch on faucet\nStep 5: Find soap\nStep 6: Grab soap\nStep 7: Find hands both\nStep 8: Scrub hands both\nStep 9: Put back soap\nStep 10: Wash hands both\nStep 11: Switch off faucet\nStep 12: Find towel\nStep 13: Grab towel\nStep 14: Wipe hands both\nTask: Hand washing\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Turn on faucet\nStep 4: Grab soap\nStep 5: Wash hands\nStep 6: Rinse hands\nStep 7: Dry hands\nTask: Hand washing\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Find faucet\nStep 4: Turn to faucet\nStep 5: Find hands both\nStep 6: Wash hands both\nTask: Throw away paper\nStep 1: Walk to home ofﬁce\nStep 2: Walk to desk\nStep 3: Find desk\nStep 4: Turn to desk\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find check\nStep 8: Grab check\nStep 9: Squeeze check\nStep 10: Stand up\nStep 11: Walk to trashcan\nStep 12: Put check on trashcan\nTask: Throw away paper\nStep 1: Walk to home ofﬁce\nStep 2: Walk to wastebasket\nStep 3: Find wastebasket\nStep 4: Grab wastebasket\nStep 5: Walk to desk\nStep 6: Drop paper in wastebas-\nket\nTask: Throw away paper\nStep 1: Walk to home ofﬁce\nStep 2: Walk to table\nStep 3: Find table\nStep 4: Turn to table\nStep 5: Find paper\nStep 6: Grab paper\nStep 7: Walk to trashcan\nStep 8: Open trashcan\nStep 9: Put paper on trashcan\nStep 10: Close trashcan\nTask: Tale off shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Find shoes\nStep 5: Take off shoes\nStep 6: Grab shoes\nStep 7: Find mat\nStep 8: Put shoes on mat\nTask: Tale off shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nTask: Tale off shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nTask: Hang keys\nStep 1: Walk to bedroom\nStep 2: Walk to dresser\nStep 3: Find purse\nStep 4: Grab purse\nStep 5: Open purse\nStep 6: Find keys\nStep 7: Grab keys\nStep 8: Find bowl\nStep 9: Put keys on bowl\nStep 10: Close purse\nStep 11: Put back purse\nTask: Hang keys\nStep 1: Walk to home ofﬁce\nStep 2: Walk to key hook\nStep 3: Find key hook\nStep 4: Grab key hook\nStep 5: Find keys\nStep 6: Hang keys on key hook\nStep 7: Walk to front door\nStep 8: Leave house\nTask: Hang keys\nStep 1: Walk to home ofﬁce\nStep 2: Walk to table\nStep 3: Find keys\nStep 4: Grab keys\nStep 5: Walk to wall\nStep 6: Find hanger\nStep 7: Put keys on hanger\n31\nTask: Make popcorn\nStep 1: Walk to dining room\nStep 2: Walk to food\nStep 3: Find food\nStep 4: Grab food\nStep 5: Walk to microwave\nStep 6: Turn to microwave\nStep 7: Open microwave\nStep 8: Put food in microwave\nStep 9: Close microwave\nStep 10: Switch on microwave\nTask: Make popcorn\nStep 1: Walk to kitchen\nStep 2: Walk to microwave\nStep 3: Open microwave\nStep 4: Put bag of popcorn in mi-\ncrowave\nStep 5: Close microwave\nStep 6:\nPush buttons on mi-\ncrowave\nStep 7: Wait for popcorn to pop\nStep 8: Get bowl out of cupboard\nStep 9: Get butter out of fridge\nStep 10: Get salt out of cupboard\nStep 11: Pour popcorn into bowl\nStep 12: Add butter and salt to\nbowl\nStep 13:\nTask: Make popcorn\nStep 1: Walk to kitchen\nStep 2: Walk to microwave\nStep 3: Find microwave\nStep 4: Open microwave\nTask: Receive credit card\nStep 1: Walk to home ofﬁce\nStep 2: Walk to envelope\nStep 3: Find envelope\nStep 4: Grab envelope\nStep 5: Walk to home ofﬁce\nStep 6: Walk to scissors\nStep 7: Find scissors\nStep 8: Open envelope\nStep 9: Find creditcard\nStep 10: Grab creditcard\nTask: Receive credit card\nStep 1: Walk to mailbox\nStep 2: Open mailbox\nStep 3: Grab credit card\nStep 4: Walk to home ofﬁce\nStep 5: Walk to desk\nStep 6: Put credit card on desk\nStep 7: Sit at desk\nStep 8: Look at credit card\nTask: Receive credit card\nStep 1: Walk to home ofﬁce\nStep 2: Walk to cabinet\nStep 3: Find cabinet\nStep 4: Open cabinet\nTask: Read to child\nStep 1: Walk to bedroom\nStep 2: Walk to child\nStep 3: Find child\nStep 4: Greet child\nStep 5: Walk to ﬁling cabinet\nStep 6: Open ﬁling cabinet\nStep 7: Find novel\nStep 8: Grab novel\nStep 9: Walk to chair\nStep 10: Pull chair\nStep 11: Sit on chair\nStep 12: Turn to novel\nStep 13: Look at novel\nStep 14: Turn to novel\nStep 15: Point at novel\nStep 16: Turn to novel\nStep 17: Point at novel\nStep 18: Read novel\nStep 19: Read novel\nStep 20: Read novel\nTask: Read to child\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nTask: Read to child\nStep 1: Walk to child\nStep 2: Read book\n32\nTask: Eat cheese\nStep 1: Walk to dining room\nStep 2: Walk to freezer\nStep 3: Open freezer\nStep 4: Find cheese\nStep 5: Grab cheese\nStep 6: Eat cheese\nTask: Eat cheese\nStep 1: Walk to kitchen\nStep 2: Walk to refrigerator\nStep 3: Open refrigerator\nStep 4: Grab cheese\nStep 5: Eat cheese\nTask: Eat cheese\nStep 1: Walk to dining room\nStep 2: Walk to table\nStep 3: Find table\nStep 4: Turn to table\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find food\nStep 8: Grab food\nStep 9: Find plate\nStep 10: Put food on plate\nTask: Unload various items from\npockets and place them in bowl\non table\nStep 1: Walk to home ofﬁce\nStep 2: Walk to table\nStep 3: Find bowl\nStep 4: Grab bowl\nStep 5: Put bowl on table\nStep 6: Find pants\nStep 7: Find bills\nStep 8: Grab bills\nStep 9: Find phone\nStep 10: Grab phone\nTask: Unload various items from\npockets and place them in bowl\non table\nStep 1: Walk to home ofﬁce\nStep 2: Walk to bowl on table\nStep 3: Find bowl on table\nStep 4: Grab bowl on table\nStep 5:\nUnload various items\nfrom pockets and place them in\nbowl on table\nTask: Unload various items from\npockets and place them in bowl\non table\nStep 1: Find bowl\nStep 2: Turn to bowl\nStep 3: Point at bowl\nStep 4: Wash bowl\nTask: Switch on lamp\nStep 1: Walk to bedroom\nStep 2: Walk to ﬂoor lamp\nStep 3: Find ﬂoor lamp\nStep 4: Switch on ﬂoor lamp\nTask: Switch on lamp\nStep 1: Walk to lamp\nStep 2: Switch on lamp\nStep 3: Walk to chair\nStep 4: Sit on chair\nStep 5: Read book\nTask: Switch on lamp\nStep 1: Walk to bedroom\nStep 2: Walk to ﬂoor lamp\nStep 3: Find ﬂoor lamp\nStep 4: Turn to ﬂoor lamp\nStep 5: Switch on ﬂoor lamp\nStep 6: Find bed\nStep 7: Lie on bed\nTask: Take jacket off\nStep 1: Walk to bedroom\nStep 2: Walk to jacket\nStep 3: Find jacket\nStep 4: Find jacket\nStep 5: Take off jacket\nStep 6: Find dresser\nStep 7: Open dresser\nStep 8: Close dresser\nTask: Take jacket off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to jacket\nStep 3: Find jacket\nStep 4: Grab jacket\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Take jacket off\nTask: Take jacket off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to jacket\nStep 3: Find jacket\n33\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Language Models as Zero-Shot Planners:\nExtracting Actionable Knowledge for Embodied Agents\nWenlong Huang\nUC Berkeley\nPieter Abbeel\nUC Berkeley\nDeepak Pathak*\nCarnegie Mellon University\nIgor Mordatch*\nGoogle\nAbstract\nCan world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of grounding\nhigh-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen\nset of actionable steps (e.g. “open fridge”). While prior work focused on learning\nfrom explicit step-by-step examples of how to act, we surprisingly ﬁnd that if\npre-trained LMs are large enough and prompted appropriately, they can effectively\ndecompose high-level tasks into mid-level plans without any further training. How-\never, the plans produced naively by LLMs often cannot map precisely to admissible\nactions. We propose a procedure that conditions on existing demonstrations and\nsemantically translates the plans to admissible actions. Our evaluation in the recent\nVirtualHome environment shows that the resulting method substantially improves\nexecutability over the LLM baseline. The conducted human evaluation reveals a\ntrade-off between executability and correctness but shows a promising sign towards\nextracting actionable knowledge from language models.\nTask: Brush teeth\nStep 1: Go to bathroom\nGPT-2 1.5B\nTask: Brush teeth\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Find toothbrush\nStep 4: Pick up toothbrush\nStep 5: Put toothbrush in mouth\nStep 6: Move brush around mouth\nfor two minutes\nStep 7: Spit out toothpaste and\nbrush into sink\nStep 8: Turn on water in sink and\nrinse brush for one minute\nStep 9: Turn off water in sink and\nreturn brush to cupboard\nCodex 12B\nTask: Brush teeth\nStep 1: Walk to bathroom\nStep 2: Open door\nStep 3: Walk to sink\nStep 4: Put pot on sink\nStep 5: Put brush on toothbrush\nStep 6: Turn to toothpaste\nStep 7: Put toothpaste on toothbrush\nStep 8: Put teeth on toothbrush\nTranslated Codex 12B\nTask: Throw away paper\nStep 1: Walk to home office\nStep 2: Walk to desk\nStep 3: Find desk\nStep 4: Turn to desk\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find check\nStep 8: Grab check\nStep 9: Squeeze check\nStep 10: Stand up\nStep 11: Walk to trashcan\nStep 12: Put check on trashcan\nHuman\nTask: Throw away paper\nStep 1: Walk to home office\nStep 2: Walk to wastebasket\nStep 3: Find wastebasket\nStep 4: Grab wastebasket\nStep 5: Walk to desk\nStep 6: Drop paper in wastebasket\nGPT-3 175B\nTask: Throw away paper\nStep 1: Walk to home office\nStep 2: Walk to table\nStep 3: Find table\nStep 4: Turn to table\nStep 5: Find paper\nStep 6: Grab paper\nStep 7: Walk to trashcan\nStep 8: Open trashcan\nStep 9: Put paper on trashcan\nStep 10: Close trashcan\nTranslated GPT-3 175B\n\u0013\n\u0015\u0013\n\u0017\u0013\n\u0019\u0013\n\u001b\u0013\n\u0014\u0013\u0013\n\b\u0003([HFXWDELOLW\\\n\u0013\n\u0015\u0013\n\u0017\u0013\n\u0019\u0013\n\u001b\u0013\n\u0014\u0013\u0013\n\b\u0003&RUUHFWQHVV\n+XPDQ\u0003\u000b\u0014\u0013\u0013\b\u0003([HFXWDELOLW\\\f\nGPT-2 1.5B\nGPT-3 13B\nCodex 12B\nGPT-3 175B\nTranslated\nCodex 12B\n(Ours)\nGPT-2 0.1B\nTranslated\nGPT-3 175B (Ours)\nTask: Get Glass of Milk\nWalk to Kitchen\nOpen Fridge\nGrab Milk\nClose Fridge\nFigure 1: Executability v.s. semantic correctness of generated plans (left), sample plans by different models\n(right), and example environment execution (bottom). Large models can produce action plans indistinguishable\nfrom those by humans, but frequently are not executable in the environment. Using our techniques, we can\nsigniﬁcantly improve executability, albeit at the cost of correctness. More samples can be found in Appendix A.5.\n*Equal advising. Correspondence to Wenlong Huang <wenlong.huang@berkeley.edu>.\nCode and videos at https://huangwl18.github.io/language-planner\narXiv:2201.07207v2  [cs.LG]  8 Mar 2022\n"
    },
    {
      "page_number": 2,
      "text": "Contents\n1\nIntroduction\n3\n2\nEvaluation Framework\n4\n2.1\nEvaluated Environment: VirtualHome . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nMetrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nMethod\n6\n3.1\nQuerying LLMs for Action Plans . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nAdmissible Action Parsing by Semantic Translation . . . . . . . . . . . . . . . . .\n6\n3.3\nAutoregressive Trajectory Correction . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.4\nDynamic Example Selection for Improved Knowledge Extraction\n. . . . . . . . .\n7\n4\nResults\n8\n4.1\nDo LLMs contain actionable knowledge for high-level tasks? . . . . . . . . . . . .\n8\n4.2\nHow executable are the LLM action plans?\n. . . . . . . . . . . . . . . . . . . . .\n9\n4.3\nCan LLM action plans be made executable by proposed procedure?\n. . . . . . . .\n9\n5\nAnalysis and Discussions\n10\n5.1\nAblation of design decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5.2\nAre the generated action plans grounded in the environment? . . . . . . . . . . . .\n10\n5.3\nEffect of Different Translation LMs\n. . . . . . . . . . . . . . . . . . . . . . . . .\n11\n5.4\nCan LLMs generate actionable programs by following step-by-step instructions?\n.\n11\n5.5\nAnalysis of program length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6\nRelated Works\n12\n7\nConclusion, Limitations & Future Work\n13\nA Appendix\n18\nA.1\nHyperparameter Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.2\nDetails of Human Evaluations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.3\nAll Evaluated Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nA.4\nNatural Language Templates for All Atomic Actions\n. . . . . . . . . . . . . . . .\n21\nA.5\nRandom Samples of Action Plans\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2\n"
    },
    {
      "page_number": 3,
      "text": "1\nIntroduction\nLarge language models (LLMs) have made impressive advances in language generation and under-\nstanding in recent years [10, 39, 40, 5]. See [4] for a recent summary of their capabilities and impacts.\nBeing trained on large corpora of human-produced language, these models are thought to contain a\nlot of information about the world [42, 23, 3] - albeit in linguistic form.\nWe ask whether we can use such knowledge contained in LLMs not just for linguistic tasks, but to\nmake goal-driven decisions that can be enacted in interactive, embodied environments. But we are\nnot simply interested in whether we can train models on a dataset of demonstrations collected for\nsome speciﬁc environment – we are instead interested in whether LLMs already contain information\nnecessary to accomplish goals without any additional training.\nMore speciﬁcally, we ask whether world knowledge about how to perform high-level tasks (such as\n“make breakfast”) can be expanded to a series of groundable actions (such as “open fridge”, “grab\nmilk”, “close fridge”, etc) that can be executed in the environment. For our investigation, we use\nthe recently proposed VirtualHome environment [38]. It can simulate a large variety of realistic\nhuman activities in a household environment and supports the ability to perform them via embodied\nactions deﬁned with a verb-object syntax. However, due to the open-ended nature of the tasks,\nit is difﬁcult to autonomously evaluate their success. We rely on human evaluation (conducted on\nMechanical Turk) to decide whether sequences of actions meaningfully accomplish posed tasks.\nWe ﬁnd that large GPT-3 [5] and Codex [7] models, when prompted with a single ﬁxed example of a\ntask description and its associated sequence of actions, can produce very plausible action plans for\nthe task we’re interested in. Such completions reﬂect the information already stored in the model\n– no model ﬁne-tuning is involved. Additionally, we only observe this effect in the larger models.\nUnfortunately, despite their semantic correctness, the produced action plans are often not executable\nin the environment. Produced actions may not map precisely to admissible actions, or may contain\nvarious linguistic ambiguities.\nWe propose several tools to improve executability of the model’s outputs. First, we enumerate all\nadmissible actions and map the model’s output phrases to the most semantically-similar admissible\naction (we use similarity measure between sentence embeddings produced by a RoBERTa model [27]\nin this work, but other choices are possible). Second, we use the model to autoregressively generate\nactions in a plan by conditioning past actions that have been made admissible via the technique above.\nSuch on-the-ﬂy correction can keep generation anchored to admissible actions. Third, we provide\nweak supervision to the model by prompting the model with a known task example similar to the\nquery task. This is somewhat reminiscent of prompt tuning approaches but does not require access to\ngradients or internals of the model.\nUsing the above tools to bias model generation, we ﬁnd that we improve executability of action plans\nfrom 18% to 79% (see Figure 1) without any invasive modiﬁcations to model parameters or any extra\ngradient or internal information beyond what is returned from the model’s forward pass. This is\nadvantageous because it does not require any modiﬁcations to the model training procedure and can\nﬁt within existing model serving pipelines. However, we do ﬁnd there to be some drop in correctness\nof the action sequences generated with the above tools (as judged by humans), indicating a promising\nstep, but requiring more research on the topic.\nTo summarize, our paper’s contributions are as follows:\n• We show that without any training, large language models can be prompted to generate\nplausible goal-driven action plans, but such plans are frequently not executable in interactive\nenvironments.\n• We propose several tools to improve executability of the model generation without invasive\nprobing or modiﬁcations to the model.\n• We conduct a human evaluation of multiple techniques and models and report on the\ntrade-offs between executability and semantic correctness.\n3\n"
    },
    {
      "page_number": 4,
      "text": "Task: Shave\nStep 1: Grab razor\nStep 2: Wash razor\nStep 3: Switch on razor\nTask: Apply lotion\nStep 1: Pour lotion into right hand\nStep 2:\nTask: Shave\nStep 1: Grab razor\nStep 2: Switch on razor\nStep 3: Put razor on face\nTask: Apply lotion\nPre-Trained Causal LLM\nFrozen\nPre-Trained\nMasked LLM\nFrozen\nStep 1: Squeeze out a glob of lotion\nStep 1: Pour lotion into right hand\nStep 1: Squeeze out a glob of lotion\nPre-Trained Causal LLM\nFrozen\nZero-Shot Planning via Causal LLM\nTranslation to Admissible Action\nStep-By-Step\nAutoregressive Generation\nPrompt\nPrompt\nFigure 2: We investigate the possibility of extracting actionable knowledge from pre-trained large language\nmodels (LLMs). We ﬁrst show surprising ﬁnding that pre-trained causal LLMs can decompose high-level\ntasks into sensible mid-level action plans (left). To make the plans executable, we propose to translate each step\ninto admissible action via another pre-trained masked LLM (middle). The translated action is appended to the\nprompt used for generating the remaining steps (right). All models are kept frozen without additional training.\n2\nEvaluation Framework\nSimulating open-ended tasks that resemble naturalistic human activities requires an environment to\nsupport a rich set of diverse interactions, rendering most existing embodied environments unsuitable\nfor our investigation. One exception is VirtualHome [38], which we evaluate on as it models complex\nhuman activities, though only in a household setting. To measure correctness of the generated action\nplans, for which evaluating computationally is inherently difﬁcult for these open-ended tasks, we\nconduct a human evaluation similar to Puig et al. [38]. We note that since no further training is\ninvolved throughout our investigations, the observations and ﬁndings presented in this paper should\nalso translate to similar embodied environments, likely even beyond the household domain.\n2.1\nEvaluated Environment: VirtualHome\nPreliminaries\nIn VirtualHome, activities are expressed as programs. Each program consists of a\nsequence of textual action steps, where each step is written as: [action] ⟨arg⟩(idx). Each\naction refers to one of the 42 atomic actions supported in VirtualHome, such as “walk” and “open”.\nFull list of atomic actions can be found in Appendix A.4. Different actions take in different numbers\nof arg, such as “bedroom” and “fridge”, that are necessary for specifying an interaction. Associated\nwith each arg is a unique id specifying the corresponding node in the environment graph, in case\nof multiple instances of the same object class are present in the graph. For the sake of simplicity,\nwe omit the id in the remaining discussions of this paper and allow automatic assignment by the\nenvironment. An example program is shown below for the task “Relax on sofa”:\n[WALK] ⟨living_room⟩(1)\n[WALK] ⟨television⟩(1)\n[FIND] ⟨television⟩(1)\n[SWITCHON] ⟨television⟩(1)\n[FIND] ⟨sofa⟩(1)\n[SIT] ⟨sofa⟩(1)\n[TURNTO] ⟨television⟩(1)\n[WATCH] ⟨television⟩(1)\nEvaluated Tasks\nWe use the ActivityPrograms knowledge base collected by Puig et al. [38] for\nevaluation. It contains 2821 different entries annotated by Amazon Mechanical Turk (MTurk) workers.\nEach entry contains 1) a high-level task name (e.g. “Watch TV”), 2) detailed instructions expressed\nin natural language to complete the task (e.g. “Sit on my couch directly opposite my TV, switch on\nmy TV with the remote control and watch”), and 3) an executable program containing all necessary\nsteps for a robotic agent (example above). We omit the use of detailed instructions (2) as we desire\ndirect extraction of executable programs (3) from only high-level task names (1). There are 292\ndistinct high-level tasks in the knowledge base, from which we randomly sample 88 held-out tasks\nfor evaluation. The remaining 204 tasks are used as demonstration set from which we are allowed\n4\n"
    },
    {
      "page_number": 5,
      "text": "Algorithm 1: Generating Action Plans from Pre-Trained Language Models\nNotation Summary:\nLMP : text completion language model (also referred as Planning LM)\nLMT : text embedding language model (also referred as Translation LM)\n{(Ti, Ei)}N\ni=1: demonstration set, where T is task name and E is example plan for T\nC: cosine similarity function\nP: mean token log probability under LMP\nInput:\nquery task name Q, e.g. “make breakfast”\nOutput: action plan consisting of admissible env actions, e.g. “open fridge”\nExtract most similar example (T ∗, E∗) whose T ∗maximizes C(LMT (T), LMT (Q))\nInitialize prompt with (T ∗+ E∗+ Q)\nwhile max step is not reached do\nSample LMP with current prompt to obtain k single-step action phrases\nfor each sample ˆa and each admissible env action ae do\nCalculate ranking score by C(LMT (ˆa), LMT (ae)) + β · P(ˆa)\nend for\nAppend highest-scoring env action a∗\ne to prompt\nAppend a∗\ne to output\nif > 50% samples are 0-length or highest score < ϵ then\nbreak\nend if\nend while\nto select as example(s) for prompting language models, or in the case of supervised ﬁne-tuning\nbaselines, they are used to ﬁne-tune pre-trained language models.\n2.2\nMetrics\nA program that commands the agent to wander around in a household environment is highly executable\nbut is mostly not correct. On the other hand, a program composed of natural language instructions\nannotated by humans is likely correct but cannot be executed, because its format is ambiguous and\nmay lack necessary common-sense actions (e.g. fridge must be opened before an agent can grab\nthings from it). We thus consider two axes for evaluation: executability and correctness.\nExecutability\nExecutability measures whether an action plan can be correctly parsed and satisﬁes\nthe common-sense constraints of the environment. To be correctly parsed, an action plan must\nbe syntactically correct and contain only allowed actions and recognizable objects. To satisfy the\ncommon-sense constraints, each action step must not violate the set of its pre-conditions (e.g. the\nagent cannot grab milk from the fridge before opening it) and post-conditions (e.g. the state of the\nfridge changes from “closed” to “open” after the agent opens it). We report the average executability\nacross all 88 tasks and all 7 VirtualHome scenes.\nCorrectness\nUnlike most embodied environments where the completion of a task can be easily\njudged, the ambiguous and multimodal nature of natural language task speciﬁcation makes it impracti-\ncal to obtain a gold-standard measurement of correctness1. Therefore, we conduct human evaluations\nfor the main methods. For the remaining analysis, we rely on a match-based metric that measures\nhow similar a generated program is to human annotations. Speciﬁcally, we follow Puig et al. [38]\nand calculate the longest common subsequence (LCS) between two programs, normalized by the\nmaximum length of the two. In the presence of multiple human-written programs for a single task,\nwe take the maximum LCS across them. However, we note that the majority of the tasks only have\none human annotation, but there are often many plausible ways to complete a certain task, making\n1One approach could be measuring the similarity of the ﬁnal environment state produced by executing\npredicted and human-written programs, but initial state must be kept ﬁxed for each task, which are not appropriate\nfor many tasks due to their open-ended nature.\n5\n"
    },
    {
      "page_number": 6,
      "text": "this metric imperfect at evaluation program correctness2. Although correlation between the two is\nshown by Puig et al. [38], we consider it only as a proxy metric in replacement of unscalable human\nevaluation.\n3\nMethod\nIn this section, we investigate the possibility of extracting actionable knowledge from pre-trained\nlanguage models without further training. We ﬁrst give an overview of the common approach to query\nlarge language models (LLMs) and how it may be used for embodied agents in Section 3.1. Then we\ndescribe an inference-time procedure that addresses several deﬁciencies of the LLM baseline and\noffers better executability in embodied environments. We break down the proposed procedure into\nthree individual components, each discussed in Section 3.2, 3.3, 3.4. Pseudo-code is in Algorithm 1.\nSince LMs excel at dealing with natural language text instead of the speciﬁc format required by\nVirtualHome as described in Section 2.1, we only expose natural language text to LMs. To do\nthis, we deﬁne a bi-directional mapping for each atomic action that converts between the natu-\nral language format and the program format. For instance, “walk to living room” is mapped to\n[WALK] ⟨living_room⟩(1). Full list of the mappings is in Appendix A.4.\n3.1\nQuerying LLMs for Action Plans\nPrevious works have shown that large language models pre-trained on a colossal amount of data would\ninternalize rich world knowledge that can be probed to perform various downstream tasks [39, 5].\nNotably, autoregressive LLMs can even perform in-context learning, an ability to solve tasks using\nonly contextual information without gradient updates [5]. Contextual information is given as part\nof the input prompt and LMs are asked to complete the remaining text. It often consists of natural\nlanguage instructions and/or a number of examples containing the desired input/output pairs.\nWe adopt the same approach to query LLMs to generate action plans for high-level tasks. Speciﬁcally,\nwe prepend one example high-level task and its annotated action plan from the demonstration set to\nthe query task, as shown in Figure 2. To obtain text completion results, we sample from autoregressive\nLLM using temperature sampling and nucleus sampling [18]. We refer to this LM as Planning LM\nand the approach using this LM for plan generation as Vanilla ⟨LM⟩, where ⟨LM⟩is replaced by\nspeciﬁc language model such as GPT-3.\nTo improve the generation quality, we follow Chen et al. [7] to sample multiple outputs for each query.\nHowever, unlike Chen et al. [7] who investigate program synthesis and can choose the sample with\nhighest unit test pass rate, we only consider the setting where one sample is allowed to be evaluated\nfor each task. This is because repetitive trial-and-error is equivalent to probing the environment for\nprivileged information, which should not be considered viable in our setting. For Vanilla ⟨LM⟩, to\nchoose the best action plan X∗among k samples (X1, X2, ..., Xk), each consisting of ni tokens\nXi = (xi,1, xi,2, ..., xi,ni), we select the sample with highest mean log probability as follows:\nargmax\nXi\n\u0012\nPθ(Xi) := 1\nni\nni\nX\nj=1\nlog pθ(xi,j|xi,<j)\n\u0013\nwhere θ parameterizes the Planning LM.\n(1)\n3.2\nAdmissible Action Parsing by Semantic Translation\nOne issue arises when naively following the above approach to generate action plans: the plan\nexpressed in free-form language often cannot be mapped to unambiguous actionable steps and thus is\nnot executable by a robotic agent. Many reasons can cause such failures: 1) the output does not follow\npre-deﬁned mappings of any atomic action (e.g. “I ﬁrst walk to the bedroom” is not of the format\n“walk to ⟨PLACE⟩”), 2) the output may refer to atomic action and objects using words unrecognizable\nby the environment (e.g. “microwave the chocolate milk” where “microwave” and “chocolate milk”\ncannot be mapped to precise action and object), or 3) the output contains lexically ambiguous words\n(e.g. “open TV” should instead be “switch on TV”).\n2Although LCS has a mathematical range of [0, 1], we measure the LCS between different human-written\nprograms for the same task and ﬁnd an empirical maximum of 0.489.\n6\n"
    },
    {
      "page_number": 7,
      "text": "Instead of developing a set of rules to transform the free-form text into admissible action steps, we\npropose to again leverage world knowledge learned by language models to semantically translate the\naction. For each admissible environment action ae, we calculate its semantic distance to the predicted\naction phrase ˆa by cosine similarity:\nC(f(ˆa), f(ae)) :=\nf(ˆa) · f(ae)\n∥f(ˆa)∥∥f(ae)∥where f is an embedding function.\n(2)\nTo embed the output action phrase and environment actions, we use a BERT-style LM [10, 27]\npre-trained with Sentence-BERT [41] objective, to which we refer as Translation LM3. The action\nembedding is obtained by mean-pooling the last layer hidden states across all tokens in that action\nphrase. While the set of admissible actions in our environment is discrete and possible to exhaustively\nenumerate, sampling or projection can be employed in larger discrete or continuous action spaces.\n3.3\nAutoregressive Trajectory Correction\nTranslating each step of the program after the entire program has been synthesized lacks consideration\nof achievability of individual steps and subjects to compounding errors. In practice, LLMs might\noutput compounded instructions for a single step, even though it cannot be completed using one\nadmissible action in the environment. To this end, we can instead interleave plan generation and\naction translation to allow for automatic trajectory correction. At each step, we ﬁrst query Planning\nLM to generate k samples for a single action ( ˆa1, ˆa2, ..., ˆak). For each sample ˆa, we consider both its\nsemantic soundness and its achievability in the environment. Speciﬁcally, we aim to ﬁnd admissible\nenvironment action ae by modifying the ranking scheme described in Equation 1 as follows:\nargmax\nae\n\u0014\nmax\nˆa\nC(f(ˆa), f(ae)) + β · Pθ(ˆa)\n\u0015\nwhere β is a weighting coefﬁcient.\n(3)\nThen we append the translated environment action ae to the unﬁnished text completion. This way\nall subsequent steps will be conditioned on admissible actions instead of free-form action phrases\ngenerated by Planning LM. Furthermore, we can use Translation LM to detect out-of-distribution\nactions, those outside the capabilities of a robot, and terminate a program early instead of mapping to\na faulty action. This can be achieved by setting a threshold ϵ such that if maxˆa,ae C(f(ˆa), f(ae)) +\nβ · Pθ(ˆa) < ϵ at step t, the program is terminated early. Since we now sample Planning LM for\nindividual steps instead of an entire sequence, another termination condition we consider is when\n> 50% of current-step samples are 0-length (excluding leading or trailing non-English text tokens).\n3.4\nDynamic Example Selection for Improved Knowledge Extraction\nSo far in the text, we always give the same example in the prompt for all query tasks. However,\nconsider the task of “ordering pizza”. Prompting LLMs with this task may give the assumption that\nthe agent is initialized in front of a computer, and the LLMs may guide the agent to search for a pizza\nstore and click “checkout my cart”. Although these are reasonable and feasible in the real world,\nsuch assumption cannot always be made as these interactions may not be supported in simulated\nenvironments. In fact, the closest series of actions that human experts give in VirtualHome may be\n“walking to a computer”, “switching on the computer”, and “typing the keyboard”. Without being\nﬁne-tuned on these data, LLMs would often fail at these tasks.\nTo provide weak supervision at inference time, we propose to select the most similar task T and its\nexample plan E from the demonstration set to be used as the example in the prompt. Speciﬁcally, we\nre-use the same Translation LM introduced in Section 3.2 and select (T ∗, E∗) whose high-level task\nname T ∗maximizes C(f(T), f(Q)), where Q is the query task. This approach bears resemblance to\nseveral recent works [37, 13, 26, 43]. An example is shown in Figure 2 where “Shave” is the most\nsimilar to the query task “Apply lotion”.\nFINAL METHOD\nCombining the various improvement discussed above, we refer to the ﬁnal\nmethod as Translated ⟨LM⟩, where ⟨LM⟩is replaced by speciﬁc language model used such as GPT-3.\n3Note that this is a different LM than the GPT-style Planning LM. Using a single LM for both purposes could\nas well be possible and likely more efﬁcient, but we leave such investigation to future works.\n7\n"
    },
    {
      "page_number": 8,
      "text": "Task: Complete Amazon\nTurk Surveys\nTask: Get Glass of Milk\nSit on Chair\nSwitch on Computer\nWalk to Home Office\nLook at Computer\nWalk to Kitchen\nOpen Fridge\nGrab Milk\nClose Fridge\nFigure 3: Visualization of VirtualHome programs generated by our approach. The top row shows the execution\nof the task “Complete Amazon Turk Surveys”, and the bottom row shows the task “Get Glass of Milk”. We show\nLLMs not only can generate sensible action plans given only high-level tasks but also contains the actionable\nknowledge that can be extracted for grounding in embodied environments.\n4\nResults\nIn this section, we ﬁrst show that language models can generate sensible action plans for many\nhigh-level tasks, even without any additional training. Then we highlight its inadequacy when naively\napplied to embodied environments and demonstrate how this can be improved by again leveraging\nworld knowledge learned by LLMs. Visualization of generated programs is shown in Figure 3.\nSampling from LMs\nPre-trained LMs are sensitive to sampling parameters and the speciﬁc exam-\nple given in the prompt. For all evaluated methods, we perform hyperparameter search over various\nsampling parameters, and for methods using a ﬁxed prompt example, we report metrics averaged\nacross three randomly chosen examples. To select the best run for each method, we rank the runs by\nthe sum of LCS and executability, each normalized by human-expert scores. Further details are in\nAppendix A.1.\nModel Choices\nFor Planning LM, we evaluate a representative set of causal language models.\nFor Translation LM, we mainly use Sentence-RoBERTa-355M and provide relevant ablations in\nSection 5.3. GPT-3 and Codex are accessed using OpenAI API, and the remaining models are accessed\nthrough open-source packages, Hugging Face Transformers [55] and SentenceTransformers [41], all\nwithout additional training (except for the ﬁne-tuning baseline).\n4.1\nDo LLMs contain actionable knowledge for high-level tasks?\nWe ﬁrst investigate whether LLMs can generate sensible action plans expressed in free-form language.\nWe use the approach described in Section 3.1 to query pre-trained LLMs. To evaluate the correctness\nof generated action plans, we conduct human evaluations. For each model, we ask 10 human\nannotators to determine – by answering “Yes” or “No” – whether each task can be completed using\nprovided action steps. To provide a reference of how humans might rate the action plans provided\nby other humans, we also ask annotators to rate the human-written action plans included in the\nVirtualHome dataset for the same set of tasks. In contrast to the free-form text output by LLMs,\nhumans wrote the plans using a graphical programming interface that enforces strict syntax and a\nchosen set of atomic action vocabulary, which limit the expressivity and the completeness of their\nanswers4. More details of our human evaluation procedure can be found in Appendix A.2.\nWe show the human evaluation results in Figure 1, where the y-axis shows correctness averaged\nacross all tasks and all annotators. Surprisingly, when LLMs are large enough and without imposed\nsyntactic constraints, they can generate highly realistic action plans whose correctness – as deemed\nby human annotators – even surpasses human-written action plans. We also observe some level of\ncorrectness for smaller models such as GPT-2. However, inspection of its produced output indicates\n4 Puig et al. [38] also conduct a human evaluation on 100 randomly sampled human-written programs and\nshow that 64% of them are complete (i.e. contain all necessary steps). Readers are encouraged to refer to Puig\net al. [38] for a more comprehensive analysis of the dataset.\n8\n"
    },
    {
      "page_number": 9,
      "text": "Language Model\nExecutability\nLCS\nCorrectness\nVanilla GPT-2 117M\n18.66%\n3.19%\n15.81% (4.90%)\nVanilla GPT-2 1.5B\n39.40%\n7.78%\n29.25% (5.28%)\nVanilla Codex 2.5B\n17.62%\n15.57%\n63.08% (7.12%)\nVanilla GPT-Neo 2.7B\n29.92%\n11.52%\n65.29% (9.08%)\nVanilla Codex 12B\n18.07%\n16.97%\n64.87% (5.41%)\nVanilla GPT-3 13B\n25.87%\n13.40%\n49.44% (8.14%)\nVanilla GPT-3 175B\n7.79%\n17.82%\n77.86% (6.42%)\nHuman\n100.00%\nN/A\n70.05% (5.44%)\nFine-tuned GPT-3 13B\n66.07%\n34.08%\n64.92% (5.96%)\nOUR FINAL METHODS\nTranslated Codex 12B\n78.57%\n24.72%\n54.88% (5.90%)\nTranslated GPT-3 175B\n73.05%\n24.09%\n66.13% (8.38%)\nTable 1: Human-evaluated correctness and evaluation results in VirtualHome. Although action plans generated\nby large language models can match or even surpass human-written plans in correctness measure, they are\nrarely executable. By translating the naive action plans, we show an important step towards grounding LLMs\nin embodied environments, but we observe room to achieve this without trading executability for correctness.\nWe also observe a failure mode among smaller models that lead to high executability. For correctness measure,\nstandard error of the mean across 10 human annotators is reported in the parenthesis.\nthat it often generates shorter plans by ignoring common-sense actions or by simply rephrasing\nthe given task (e.g. the task “Go to sleep” produces only a single step “Go to bed”). These failure\nmodes sometimes mislead human annotators to mark them correct as the annotators may ignore\ncommon-sense actions in their judgment as well, resulting in a higher correctness rate than the quality\nof the output shows.\n4.2\nHow executable are the LLM action plans?\nWe analyze the executability of LLM plans by evaluating them in all 7 household scenes in Virtual-\nHome. As shown in Table 1, we ﬁnd action plans generated naively by LLMs are generally not very\nexecutable. Although smaller models seem to have higher executability, we ﬁnd that the majority of\nthese executable plans are produced by ignoring the queried task and repeating the given example of\na different task. This is validated by the fact that smaller models have lower LCS than larger models\ndespite having high executability, showing that this failure mode is prevalent among smaller models.\nIn contrast, larger models do not suffer severely from this failure mode. Yet as a result of being more\nexpressive, their generated programs are substantially less executable.\n4.3\nCan LLM action plans be made executable by proposed procedure?\nWe evaluate the effectiveness of our proposed procedure of action translation. We ﬁrst create a bank\nof all allowed 47522 action steps in the environment, including all possible combinations of atomic\nactions and allowed arguments/objects. Then we use an off-the-shelf Sentence-RoBERTa [27, 41] as\nTranslation LM to create embeddings for actions and output text. For better computational efﬁciency,\nwe pre-compute the embeddings for all allowed actions, leaving minor computation overhead for\nour procedure over the baseline methods at inference time. As shown in Table 1, executability of\ngenerated programs is signiﬁcantly improved. Furthermore, we also observe improved LCS because\nthe translated action steps precisely follow the program syntax and thus are more similar to the plans\nproduced by human experts. Sample output is shown in Figure 1 and a larger random subset of\ngenerated samples can be found in Appendix A.5.\nTo validate their correctness, we again perform human evaluations using the same procedure from\nSection 4.1. Results are shown in Table 1. We ﬁnd that despite being more similar to human-written\nplans as they follow strict syntax, the programs are deemed less correct by humans compared to their\nvanilla counterparts. By examining the output, we observe two main sources of errors. First, we\nﬁnd Translation LM is poor at mapping compounded instructions to a succinct admissible action,\ne.g. “brush teeth with toothbrush and toothpaste”. Second, we ﬁnd that the generated programs are\nsometimes terminated too early. This is partly due to the imperfect expressivity of the environment;\n9\n"
    },
    {
      "page_number": 10,
      "text": "certain necessary actions or objects are not implemented to fully achieve some tasks, so Translation\nLM cannot map to a sufﬁciently similar action. This is also reﬂected by our human evaluation results\nof the programs written by other humans, as only 70% of the programs are considered complete.\n5\nAnalysis and Discussions\n5.1\nAblation of design decisions\nWe perform ablation studies for the three components of our proposed procedure, described in\nSection 3.2, 3.3, and 3.4 respectively. As shown in Table 2, leaving out any of the three components\nwould all lead to decreased performance in both executability and LCS. An exception is Translated\nGPT-3 w/o Trajectory Correction, where we observe a slight improvement in LCS at the expense of a\nconsiderable drop in executability. Among the three proposed components, leaving out action transla-\ntion leads to the most signiﬁcant executability drop, showing the importance of action translation in\nextracting executable action plans from LLMs.\nMethods\nExecutability\nLCS\nTranslated Codex 12B\n78.57%\n24.72%\n- w/o Action Translation\n31.49%\n22.53%\n- w/o Dynamic Example\n50.86%\n22.84%\n- w/o Trajectory Correction\n55.19%\n24.43%\nTranslated GPT-3 175B\n73.05%\n24.09%\n- w/o Action Translation\n36.04%\n24.31%\n- w/o Dynamic Example\n60.82%\n22.92%\n- w/o Trajectory Correction\n40.10%\n24.98%\nTable 2: Ablation of three proposed techniques.\n5.2\nAre the generated action plans grounded in the environment?\nSince successful execution of correct action plans directly measures grounding, we calculate the\npercentage of generated action plans that are both correct and executable. We deem an action plan\nto be correct if 70% or more human annotators decide it is correct. Human-written plans are 100%\nexecutable, of which 65.91% are deemed correct. Results for LMs are shown in Figure 4.\nAlthough smaller LMs such as GPT-2 can generate highly executable action plans as shown in\nTable 1, these executable plans mostly are not correct, as they often repeat the given example or\ndo not contain all necessary steps. Increasing model parameters can lead to some improvement in\ngenerating plans that are both executable and correct, yet it scales poorly with the parameter count. In\nthe meantime, action translation offers a promising way towards grounding actionable knowledge by\nproducing executable and correct plans, though a large gap remains to be closed to reach human-level\nperformance (65.91%).\nTranslated GPT-3 175B\nTranslated Codex 12B\nVanilla GPT-3 175B\nVanilla GPT-3 12B\nVanilla Codex 12B\nVanilla GPT-2 1.5B\nVanilla GPT-2 0.1B\n35.23%\n27.27%\n6.82%\n2.27%\n4.55%\n1.14%\n0.0%\n% of Executable & Correct Plans   \nFigure 4: Percentage of both executable and correct action plans generated by LMs.\n10\n"
    },
    {
      "page_number": 11,
      "text": "5.3\nEffect of Different Translation LMs\nIn this section, we study the effect of using different Translation LM. We compare two size variants of\nSentence BERT and Sentence RoBERTa [10, 27, 41] trained on the STS benchmark [6] and a baseline\nusing averaged GloVe embeddings [35]. Results are shown in Table 3. Notably, we do not observe\nsigniﬁcant differences in executability and LCS across different variants of BERT and RoBERTa. We\nhypothesize that this is because any language models trained on reasonably large datasets should be\ncapable of the single-step action phrase translation considered in this work. However, simply using\naverage GloVe embeddings would lead to signiﬁcantly reduced performance.\nTranslation LM\nParameter Count\nExecutability\nLCS\nCODEX 12B AS PLANNING LM\nAvg. GloVe embeddings\n-\n46.92%\n9.71%\nSentence Bert (base)\n110M\n73.21%\n24.10%\nSentence Bert (large)\n340M\n75.16%\n20.79%\nSentence RoBERTa (base)\n125M\n74.35%\n22.82%\nSentence RoBERTa (large)\n325M\n78.57%\n24.72%\nGPT-3 175B AS PLANNING LM\nAvg. GloVe embeddings\n-\n47.40%\n12.16%\nSentence Bert (base)\n110M\n77.60%\n24.49%\nSentence Bert (large)\n340M\n67.86%\n21.24%\nSentence RoBERTa (base)\n125M\n72.73%\n23.64%\nSentence RoBERTa (large)\n325M\n73.05%\n24.09%\nTable 3: Effect of different Translation LMs on executability and LCS.\n5.4\nCan LLMs generate actionable programs by following step-by-step instructions?\nPrior works often focus on translating step-by-step instructions into executable programs. Speciﬁcally,\ninstead of only providing a high-level task name, how-to instructions are also provided, as shown\nin Figure 5. Although this setting is easier as it does not require rich prior knowledge, how-to\ninstructions can help resolve much ambiguity of exactly how to perform a high-level task when\nmultiple solutions are possible. To investigate whether pre-trained LLMs are capable of doing this\nwithout additional training, we include these instructions in the prompt and evaluate LLMs with\nthe proposed procedure. We compare to a supervised baseline from VirtualHome that trains an\nLSTM [17] from scratch on human-annotated data. Since the code to train the baseline is not publicly\nreleased and a different train/test split is likely used, we only show results reported in Puig et al. [38]\nas a crude reference. We also cannot compare executability as it is not reported. Results are shown in\nTable 4. Surprisingly, without being ﬁne-tuned on any domain data, Translated Codex/GPT-3 can\nattain LCS close to supervised methods while generating highly executable programs.\nTask: Read book\nDescription: Walk to home office, \nturn on light, grab a book, sit in \nchair, start to read the book.\nStep 1: Walk to home office\nStep 2: Walk to light\nStep 3: Find light\nStep 4: Switch on light\nStep 5: Find novel\nStep 6: Grab novel\nStep 7: Find chair\nStep 8: Sit on chair\nStep 9: Read novel\nTask: Find dictionary\nDescription: Move towards the\nbookshelf, scan the bookshelf for\nthe dictionary, when the\ndictionary is found, pick up the\ndictionary.\nFigure 5: An example prompt containing step-by-\nstep instructions.\nMethods\nExecutability\nLCS\nTranslated Codex 12B\n78.57%\n32.87%\nTranslated GPT-3 175B\n74.15%\n31.05%\nSupervised LSTM\n-\n34.00%\nTable 4: Executability and LCS when conditioned\non step-by-step instructions.\n11\n"
    },
    {
      "page_number": 12,
      "text": "5.5\nAnalysis of program length\nShorter programs have a natural advantage of being more executable as they need to satisfy less\npre/post-conditions, albeit being prone to incompleteness. To validate the proposed approach does\nnot simply generate very short programs, we calculate the average program length across the 88\nevaluated tasks. Results are shown in Table 5. Mirroring the observations made in Section 4.1 and\nSection 4.2, we ﬁnd smaller LMs such as GPT-2 tend to generate shorter programs than larger models\ndo while frequently repeating the given executable example. In contrast, larger models like Codex\nand GPT-3 can generate more expressive programs with high realism, yet consequently, they often\nsuffer from executability. We show proposed procedure can ﬁnd appropriate balance and is capable\nof generating programs that are highly executable while maintaining reasonable expressiveness as\nmeasured by program length.\nMethods\nExecutability\nAverage Length\nVanilla GPT-2 1.5B\n39.40%\n4.24\nVanilla Codex 12B\n18.07%\n7.22\nVanilla GPT-3 175B\n7.79%\n9.716\nTranslated Codex 12B\n78.57%\n7.13\nTranslated GPT-3 175B\n73.05%\n7.36\nHuman\n100.00%\n9.66\nTable 5: Average executability & program length of different methods.\n6\nRelated Works\nLarge-scale natural language modeling has witnessed rapid advances since the inception of the\nTransformer architecture [53]. It has been shown by recent works that large language models (LLMs)\npre-trained on large unstructured text corpus not only can perform strongly on various down-stream\nNLP tasks [10, 39, 40, 5] but the learned representations can also be used to model relations of\nentities [23], retrieve matching visual features [19], synthesize code from docstrings [15, 7], solve\nmath problems [8, 46], and even as valuable priors when applied to diverse tasks from different\nmodalities [28, 52]. Notably, by pre-training on large-scale data, these models can also internalize an\nimplicit knowledge base containing rich information about the world from which factual answers\n(e.g. “Dante was born in ⟨PLACE⟩”) can be extracted [36, 21, 9, 50, 42]. Compared to prior works in\nsingle-step knowledge extraction, we aim to extract sequential action plans to complete open-ended\nhuman activities while satisfying various constraints of an interactive environment.\nMany prior works have looked into grounding natural language in embodied environments. A series\nof them parse language instructions into formal logic or rely mainly on lexical analysis to resolve\nvarious linguistic ambiguities for embodied agents [2, 33, 34, 51]. However, they often require many\nhand-designed rules or scale inadequately to more complex tasks and environments. Recently, many\nefforts have been put into creating more realistic environments with the goal to further advances in\nthis area [38, 47, 48, 22, 44, 1]. At the same time, by leveraging the better representation power of\nneural architectures, a number of works have looked into creating instruction-following agents that\ncan perform manipulation [29, 30], navigation [11, 54, 31], or both [49, 16, 12]. Recent works also\nuse language as hierarchical abstractions to plan actions using imitation learning [45] and to guide\nexploration in reinforcement learning [32].\nNotably, many prior works do not leverage full-blown pre-trained LLMs; most investigate smaller\nLMs that require considerable domain-speciﬁc data for ﬁne-tuning to obtain reasonable performance.\nPerhaps more importantly, few works have evaluated LLMs in an embodiment setting that realizes the\nfull potential of the actionable knowledge these models already contain by pre-training on large-scale\nunstructured text: the tasks evaluated are often generated from a handful of templates, which do not\nresemble the highly diverse activities that humans perform in daily lives [14, 20]. The development\nof VirtualHome environment [38] enables such possibility. However, relevant works [38, 25] rely\non human-annotated data and perform supervised training from scratch. Due to the lack of rich\nworld knowledge, these models can only generate action plans given detailed instructions of how\nto act or video demonstrations. Concurrent work by Li et al. [24] validates similar hypothesis that\n12\n"
    },
    {
      "page_number": 13,
      "text": "LMs contain rich actionable knowledge. They ﬁne-tune GPT-2 with demonstrations to incorporate\nenvironment context and to predict actions in VirtualHome, and evaluate on tasks that are generated\nfrom pre-deﬁned predicates. In contrast, we investigate existing knowledge in LLMs without any\nadditional training and evaluate on human activity tasks expressed in free-form language.\n7\nConclusion, Limitations & Future Work\nIn this work, we investigate actionable knowledge already contained in pre-trained LLMs without any\nadditional training. We present several techniques to extract this knowledge to perform common-sense\ngrounding by planning actions for complex human activities.\nDespite promising ﬁndings, there remain several limitations of this work which we discuss as follows:\nDrop in Correctness\nAlthough our approach can signiﬁcantly improve executability of the gen-\nerated plans, we observe a considerable drop in correctness. In addition to the errors caused by\nthe proposed action translation (discussed in Section 4.3), this is partially attributed to the limited\nexpressivity of VirtualHome, as it may not support all necessary actions to fully complete all evaluated\ntasks (correctness is judged by humans). This is also reﬂected by that Vanilla LMs can even surpass\nhuman-written plans, which are restricted by environment expressivity.\nMid-Level Grounding\nInstead of grounding the LLM generation to low-level actions by using\ndownstream data from a speciﬁc environment, we focus on high-level to mid-level grounding such\nthat we evaluate raw knowledge of LLMs as closely and broadly as possible. Hence, we only\nconsider the most prominent challenge in mid-level grounding that the generated plans must satisfy\nall common-sense constraints (characterized by executability metric). As a result, we assume there\nis a low-level controller that can execute these mid-level actions (such as “grab cup”), and we do\nnot investigate the usefulness of LLMs for low-level sensorimotor behavior grounding. To perform\nsensorimotor grounding, such as navigation and interaction mask prediction, domain-speciﬁc data\nand ﬁne-tuning are likely required.\nIgnorant of Environment Context\nWe do not incorporate observation context or feedback into\nour models. To some extent, we approach LLMs in the same way as how VirtualHome asks human\nannotators to write action plans for a given human activity by imagination, in which case humans\nsimilarly do not observe environment context. Similar to human-written plans, we assume the plans\ngenerated by LMs only refer to one instance of each object class. As a result, successful plan\ngeneration for tasks like “stack two plates on the right side of a cup” is not possible.\nEvaluation Protocol\nWe measure quality of plans by a combination of executability and correctness\ninstead of one straightforward metric. To the best of our knowledge, there isn’t a known way to\ncomputationally assess the semantic correctness of the plans due to the tasks’ open-ended and\nmulti-modal nature. Prior work also adopt similar combination of metrics [38]. We report two\nmetrics individually to shine light on the deﬁciencies of existing LLMs which we hope could provide\ninsights for future works. To provide a holistic view, we report results by combining two metrics in\nSection 5.2.\nWe believe addressing each of these shortcoming will lead to exciting future directions. We also\nhope these ﬁndings can inspire future investigations into using pre-trained LMs for goal-driven\ndecision-making problems and grounding the learned knowledge in embodied environments.\nAcknowledgment\nWe would like to thank OpenAI for providing academic access to the OpenAI API and Luke Metz for\nvaluable feedback and discussions. This work was supported in part by Berkeley Deep Drive, NSF\nIIS-2024594, and GoodAI Research Award.\n13\n"
    },
    {
      "page_number": 14,
      "text": "References\n[1] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid,\nStephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environments. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 3674–3683, 2018.\n[2] Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping\ninstructions to actions. Transactions of the Association for Computational Linguistics, 1:49–62,\n2013.\n[3] BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabil-\nities of language models. In preparation, 2021. URL https://github.com/google/\nBIG-bench/.\n[4] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,\nJared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano\nErmon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard,\nSaahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte\nKhani, Omar Khattab, Pang Wei Kohd, Mark Krass, Ranjay Krishna, Rohith Kuditipudi,\nAnanya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa\nLi, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr,\nIsabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi\nRaghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack\nRyan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan\nSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang,\nWilliam Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga,\nJiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia\nZheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models,\n2021.\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[6] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\npreprint arXiv:1708.00055, 2017.\n[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri\nEdwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language\nmodels trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-\npher Hesse, and John Schulman. Training veriﬁers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\n[9] Joe Davison, Joshua Feldman, and Alexander M Rush. Commonsense knowledge mining\nfrom pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 1173–1178, 2019.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n14\n"
    },
    {
      "page_number": 15,
      "text": "[11] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe\nMorency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-\nfollower models for vision-and-language navigation. arXiv preprint arXiv:1806.02724, 2018.\n[12] Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to\ngoals: Inverse reinforcement learning for vision-based instruction following. arXiv preprint\narXiv:1902.07742, 2019.\n[13] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723, 2020.\n[14] Brent Harrison and Mark O Riedl. Learning from stories: using crowdsourced narratives to\ntrain virtual agents. In Twelfth Artiﬁcial Intelligence and Interactive Digital Entertainment\nConference, 2016.\n[15] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,\nCollin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge\ncompetence with apps. arXiv preprint arXiv:2105.09938, 2021.\n[16] Felix Hill, Sona Mokra, Nathaniel Wong, and Tim Harley. Human instruction-following with\ndeep reinforcement learning via transfer-learning from text. arXiv preprint arXiv:2005.09382,\n2020.\n[17] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\n[18] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\ntext degeneration. arXiv preprint arXiv:1904.09751, 2019.\n[19] Gabriel Ilharco, Rowan Zellers, Ali Farhadi, and Hannaneh Hajishirzi. Probing text models for\ncommon ground with visual representations. arXiv e-prints, pages arXiv–2005, 2020.\n[20] Peter A Jansen. Visually-grounded planning without vision: Language models infer detailed\nplans from high-level instructions. arXiv preprint arXiv:2009.14259, 2020.\n[21] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? Transactions of the Association for Computational Linguistics, 8:423–438,\n2020.\n[22] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti,\nDaniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d\nenvironment for visual ai. arXiv preprint arXiv:1712.05474, 2017.\n[23] Belinda Z Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural\nlanguage models. arXiv preprint arXiv:2106.00737, 2021.\n[24] Shuang Li, Xavier Puig, Yilun Du, Clinton Wang, Ekin Akyurek, Antonio Torralba, Jacob\nAndreas, and Igor Mordatch. Pre-trained language models for interactive decision-making.\narXiv preprint arXiv:2202.01771, 2022.\n[25] Yuan-Hong Liao, Xavier Puig, Marko Boben, Antonio Torralba, and Sanja Fidler. Synthesizing\nenvironment-aware activities via activity sketches. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6291–6299, 2019.\n[26] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\nWhat makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.\n[27] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[28] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. arXiv preprint arXiv:2103.05247, 2021.\n15\n"
    },
    {
      "page_number": 16,
      "text": "[29] Corey Lynch and Pierre Sermanet.\nGrounding language in play.\narXiv preprint\narXiv:2005.07648, 2020.\n[30] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured\ndata. Proceedings of Robotics: Science and Systems. doi, 10, 2021.\n[31] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra.\nImproving vision-and-language navigation with image-text pairs from the web. In European\nConference on Computer Vision, pages 259–274. Springer, 2020.\n[32] Suvir Mirchandani, Siddharth Karamcheti, and Dorsa Sadigh. Ella: Exploration through learned\nlanguage abstraction. arXiv preprint arXiv:2103.05825, 2021.\n[33] Dipendra Misra, Kejia Tao, Percy Liang, and Ashutosh Saxena. Environment-driven lexicon\ninduction for high-level instructions. In Proceedings of the 53rd Annual Meeting of the Asso-\nciation for Computational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pages 992–1002, 2015.\n[34] Dipendra K Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Saxena. Tell me dave: Context-\nsensitive grounding of natural language to manipulation instructions. The International Journal\nof Robotics Research, 35(1-3):281–300, 2016.\n[35] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for\nword representation. In Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 1532–1543, 2014.\n[36] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel.\nLanguage models as knowledge bases?\narXiv preprint\narXiv:1909.01066, 2019.\n[37] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek,\nand Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models.\narXiv preprint arXiv:2201.11227, 2022.\n[38] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio\nTorralba. Virtualhome: Simulating household activities via programs. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 8494–8502, 2018.\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n[41] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084, 2019.\n[42] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\n[43] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633, 2021.\n[44] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana\nJain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for\nembodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 9339–9347, 2019.\n[45] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with\nlatent language. arXiv preprint arXiv:2110.01517, 2021.\n[46] Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate\n& rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034,\n2021.\n16\n"
    },
    {
      "page_number": 17,
      "text": "[47] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-\ntaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded\ninstructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10740–10749, 2020.\n[48] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and\nMatthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive\nlearning. arXiv preprint arXiv:2010.03768, 2020.\n[49] Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme.\nEmbodied bert: A transformer model for embodied, language-guided visual task completion.\narXiv preprint arXiv:2108.04927, 2021.\n[50] Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. olmpics-on what language\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\n743–758, 2020.\n[51] Moritz Tenorth, Daniel Nyga, and Michael Beetz. Understanding and executing instructions for\neveryday manipulation tasks from the world wide web. In 2010 ieee international conference\non robotics and automation, pages 1486–1491. IEEE, 2010.\n[52] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. arXiv preprint arXiv:2106.13884,\n2021.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pages 5998–6008, 2017.\n[54] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang,\nWilliam Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised\nimitation learning for vision-language navigation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6629–6638, 2019.\n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transform-\ners: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n17\n"
    },
    {
      "page_number": 18,
      "text": "A\nAppendix\nA.1\nHyperparameter Search\nFor each evaluated method, we perform grid search over the following hyperparameters:\nName\nDescription\nSearch Values\nepsilon (ϵ)\nOut-of-distribution early termination threshold\n{0, 0.4, 0.8}\ntemperature\nsampling parameter adjusting relative token probabilities\n{0.1, 0.3, 0.6}\nk\nnumber of samples generated by Planning LM\n{1, 10}\nbeta (β)\nweighting coefﬁcient in action translation to trade off\nsemantic and translation correctness\n{0.3}\nfrequence_penalty\nOpenAI API only; penalize new tokens based on their\nexisting frequency in the text so far\n{0.1, 0.3, 0.6, 0.9}\npresence_penalty\nOpenAI API only; penalize new tokens based on whether\nthey appear in the text so far\n{0.3, 0.5, 0.8}\nrepetition_penalty\nHugging Face Transformers only; penalize new tokens\nbased on whether repeating existing text\n{1.0, 1.2, 1.5, 1.8}\nFor methods that use ﬁxed example across evaluated tasks, we search over the following three\nrandomly chosen examples:\nExample 1\nExample 2\nExample 3\nTask: Use computer\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find computer\nStep 6: Switch on computer\nStep 7: Turn to computer\nStep 8: Look at computer\nStep 9: Find keyboard\nStep 10: Type on keyboard\nTask: Relax on sofa\nStep 1: Walk to home ofﬁce\nStep 2: Walk to couch\nStep 3: Find couch\nStep 4: Sit on couch\nStep 5: Find pillow\nStep 6: Lie on couch\nTask: Read book\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\n18\n"
    },
    {
      "page_number": 19,
      "text": "A.2\nDetails of Human Evaluations\nHuman evaluations are conducted on Amazon Mechanical Turk. For each method, we generate action\nplans for all 88 high-level tasks. To account for the expressivity of the VirtualHome environment [38],\nwe include action plans written by human experts from the VirtualHome dataset as references in\nour human evaluations. The evaluations are conducted in the form of questionnaires containing all\naction plans whose order is randomly shufﬂed and whose corresponding methods are unknown to\nthe annotators. Human annotators are required to answer all the questions in the questionnaire. For\neach question, the annotators need to answer either “Yes” or “No” indicating if they believe the\naction plan completes the task. For each method, we report correctness percentage averaged across\n10 participated human annotators and all 88 tasks. We further report the standard error of the mean\nacross human annotators. Screenshot can be found in Figure 6.\nFigure 6: Screenshot of human evaluation interface, conducted as a Google Forms questionnaire.\n19\n"
    },
    {
      "page_number": 20,
      "text": "A.3\nAll Evaluated Tasks\nThe evaluated tasks are part of the ActivityPrograms dataset collected by Puig et al. [38]. Some of the\ntask names may contain misspelling(s).\n1. Apply lotion\n2. Arrange folders\n3. Breakfast\n4. Browse internet\n5. Brush teeth\n6. Change clothes\n7. Change sheets and pil-\nlow cases\n8. Collect napkin rings\n9. Complete surveys on\namazon turk\n10. Compute\n11. Decorate it\n12. Do homework\n13. Do work\n14. Draft home\n15. Draw picture\n16. Dry soap bottles\n17. Dust\n18. Eat cereal\n19. Eat cheese\n20. Eat snacks and drink\ntea\n21. Empty dishwasher and\nﬁll dishwasher\n22. Entertain\n23. Feed me\n24. Find dictionary\n25. Fix snack\n26. Get glass of milk\n27. Give milk to cat\n28. Go to sleep\n29. Grab things\n30. Hand washing\n31. Hang keys\n32. Hang pictures\n33. Iron shirt\n34. Keep cats inside while\ndoor is open\n35. Keep cats out of room\n36. Leave home\n37. Listen to music\n38. Look at mirror\n39. Look at painting\n40. Make bed\n41. Make popcorn\n42. Organize closet\n43. Organize pantry\n44. Paint ceiling\n45. Pay bills\n46. Pick up toys\n47. Play musical chairs\n48. Prepare pot of boiling\nwater\n49. Push all chairs in\n50. Push in desk chair\n51. Put alarm clock in bed-\nroom\n52. Put away groceries\n53. Put away toys\n54. Put clothes away\n55. Put mail in mail orga-\nnizer\n56. Put on your shoes\n57. Put out ﬂowers\n58. Put up decoration\n59. Read\n60. Read newspaper\n61. Read on sofa\n62. Read to child\n63. Read yourself to sleep\n64. Receive credit card\n65. Restock\n66. Scrubbing living room\ntile ﬂoor is once week\nactivity for me\n67. Style hair\n68. Switch on lamp\n69. Take jacket off\n70. Take shoes off\n71. Tale off shoes\n72. Throw away paper\n73. Try yourself off\n74. Turn off TV\n75. Turn on TV with re-\nmote\n76. Turn on radio\n77. Type up document\n78. Unload various items\nfrom pockets and place\nthem in bowl on table\n79. Use laptop\n80. Vacuum\n81. Walk to room\n82. Wash dirty dishes\n83. Wash face\n84. Wash monitor\n85. Wash teeth\n86. Watch horror movie\n87. Wipe down sink\n88. Write book\n20\n"
    },
    {
      "page_number": 21,
      "text": "A.4\nNatural Language Templates for All Atomic Actions\nVirtualHome requires action steps speciﬁed in a speciﬁc format, yet language models are trained to\ndeal with mostly natural language. We thus deﬁne a natural language template for each atomic action\nand only expose the converted natural language text in all operations involving language models,\ni.e. autoregressive generation and action translation. After we obtain an entire generated program\nexpressed in natural language, such as those in Figure 1 and Figure 2, we then convert each action\nstep to the VirtualHome syntax. Full list of the atomic actions and their natural language templates\ncan be found below.\nAtomic Action in VirtualHome Syntax\nNatural Language Template\n[CLOSE] ⟨arg1⟩(1)\nclose ⟨arg1⟩\n[CUT] ⟨arg1⟩(1)\ncut ⟨arg1⟩\n[DRINK] ⟨arg1⟩(1)\ndrink ⟨arg1⟩\n[DROP] ⟨arg1⟩(1)\ndrop ⟨arg1⟩\n[EAT] ⟨arg1⟩(1)\neat ⟨arg1⟩\n[FIND] ⟨arg1⟩(1)\nﬁnd ⟨arg1⟩\n[GRAB] ⟨arg1⟩(1)\ngrab ⟨arg1⟩\n[GREET] ⟨arg1⟩(1)\ngreet ⟨arg1⟩\n[LIE] ⟨arg1⟩(1)\nlie on ⟨arg1⟩\n[LOOKAT] ⟨arg1⟩(1)\nlook at ⟨arg1⟩\n[MOVE] ⟨arg1⟩(1)\nmove ⟨arg1⟩\n[OPEN] ⟨arg1⟩(1)\nopen ⟨arg1⟩\n[PLUGIN] ⟨arg1⟩(1)\nplug in ⟨arg1⟩\n[PLUGOUT] ⟨arg1⟩(1)\nplug out ⟨arg1⟩\n[POINTAT] ⟨arg1⟩(1)\npoint at ⟨arg1⟩\n[POUR] ⟨arg1⟩(1) ⟨arg2⟩(1)\npour ⟨arg1⟩into ⟨arg2⟩\n[PULL] ⟨arg1⟩(1)\npull ⟨arg1⟩\n[PUSH] ⟨arg1⟩(1)\npush ⟨arg1⟩\n[PUTBACK] ⟨arg1⟩(1) ⟨arg2⟩(1)\nput ⟨arg1⟩on ⟨arg2⟩\n[PUTIN] ⟨arg1⟩(1) ⟨arg2⟩(1)\nput ⟨arg1⟩in ⟨arg2⟩\n[PUTOBJBACK] ⟨arg1⟩(1)\nput back ⟨arg1⟩\n[PUTOFF] ⟨arg1⟩(1)\ntake off ⟨arg1⟩\n[PUTON] ⟨arg1⟩(1)\nput on ⟨arg1⟩\n[READ] ⟨arg1⟩(1)\nread ⟨arg1⟩\n[RELEASE]\nrelease\n[RINSE] ⟨arg1⟩(1)\nrinse ⟨arg1⟩\n[RUN] ⟨arg1⟩(1)\nrun to ⟨arg1⟩\n[SCRUB] ⟨arg1⟩(1)\nscrub ⟨arg1⟩\n[SIT] ⟨arg1⟩(1)\nsit on ⟨arg1⟩\n[SLEEP]\nsleep\n[SQUEEZE] ⟨arg1⟩(1)\nsqueeze ⟨arg1⟩\n[STANDUP]\nstand up\n[SWITCHOFF] ⟨arg1⟩(1)\nswitch off ⟨arg1⟩\n[SWITCHON] ⟨arg1⟩(1)\nswitch on ⟨arg1⟩\n[TOUCH] ⟨arg1⟩(1)\ntouch ⟨arg1⟩\n[TURNTO] ⟨arg1⟩(1)\nturn to ⟨arg1⟩\n[TYPE] ⟨arg1⟩(1)\ntype on ⟨arg1⟩\n[WAKEUP]\nwake up\n[WALK] ⟨arg1⟩(1)\nwalk to ⟨arg1⟩\n[WASH] ⟨arg1⟩(1)\nwash ⟨arg1⟩\n[WATCH] ⟨arg1⟩(1)\nwatch ⟨arg1⟩\n[WIPE] ⟨arg1⟩(1)\nwipe ⟨arg1⟩\n21\n"
    },
    {
      "page_number": 22,
      "text": "A.5\nRandom Samples of Action Plans\nHuman\nVanilla GPT3-175B\nTranslated GPT3-175B (Ours)\nTask: Go to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find bed\nStep 4: Turn to bed\nStep 5: Look at bed\nStep 6: Sit on bed\nStep 7: Sleep\nTask: Go to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find pajamas\nStep 4: Put on pajamas\nStep 5: Find slippers\nStep 6: Put on slippers\nStep 7: Get in bed\nStep 8: Sleep\nTask: Go to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Lie on bed\nStep 4: Sleep\nTask: Browse internet\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find computer\nStep 6: Switch on computer\nStep 7: Turn to computer\nStep 8: Look at computer\nStep 9: Find keyboard\nStep 10: Type on keyboard\nTask: Browse internet\nStep 1: Walk to home ofﬁce\nStep 2: Walk to computer\nStep 3: Find computer\nStep 4: Grab computer\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Browse internet\nTask: Browse internet\nStep 1: Walk to home ofﬁce\nStep 2: Walk to desk\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find computer\nStep 6: Switch on computer\nStep 7: Find mouse\nStep 8: Grab mouse\nStep 9: Turn to computer\nStep 10: Point at computer\nTask: Wash teeth\nStep 1: Stand up\nStep 2: Walk to bathroom\nStep 3: Walk to sink\nStep 4: Find sink\nStep 5: Turn to sink\nStep 6: Look at sink\nStep 7: Find teeth\nStep 8: Wash teeth\nTask: Wash teeth\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Turn on faucet\nStep 4: Grab toothbrush\nStep 5: Put toothpaste on tooth-\nbrush\nStep 6: Put toothbrush in mouth\nStep 7: Brush teeth\nStep 8: Rinse mouth out with wa-\nter\nStep 9: Turn off faucet\nStep 10: Walk back to bedroom\nTask: Wash teeth\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Find faucet\nStep 4: Switch on faucet\nStep 5: Walk to toothbrush\nStep 6: Grab toothbrush\nTask: Brush teeth\nStep 1: Walk to bathroom\nStep 2: Walk to tooth paste\nStep 3: Find tooth paste\nStep 4: Grab tooth paste\nStep 5: Find toothbrush\nStep 6: Grab toothbrush\nTask: Brush teeth\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Turn on water\nStep 4: Grab toothbrush\nStep 5: Put toothpaste on tooth-\nbrush\nStep 6: Put toothbrush in mouth\nStep 7: Brush teeth\nTask: Brush teeth\nStep 1: Find toothbrush\nStep 2: Turn to toothbrush\nStep 3: Point at toothbrush\nStep 4: Wash toothbrush\nTask: Vacuum\nStep 1: Walk to home ofﬁce\nStep 2: Walk to computer\nStep 3: Find vacuum cleaner\nStep 4:\nSwitch on vacuum\ncleaner\nStep 5:\nSwitch off vacuum\ncleaner\nTask: Vacuum\nStep 1: Walk to home ofﬁce\nStep 2: Walk to vacuum\nStep 3: Find vacuum\nStep 4: Grab vacuum\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Vacuum\nTask: Vacuum\nStep 1: Walk to home ofﬁce\nStep 2: Walk to dresser\nStep 3: Open dresser\nStep 4: Find vacuum cleaner\nStep 5: Grab vacuum cleaner\nStep 6: Pull vacuum cleaner\nStep 7: Plug in vacuum cleaner\nStep 8:\nSwitch on vacuum\ncleaner\n22\n"
    },
    {
      "page_number": 23,
      "text": "Task: Change sheets and pillow\ncases\nStep 1: Find pillow\nStep 2: Turn to pillow\nStep 3: Look at pillow\nStep 4: Grab pillow\nTask: Change sheets and pillow\ncases\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Change sheets and pillow\ncases\nStep 4: Walk to kitchen\nStep 5: Walk to sink\nStep 6: Wash sheets and pillow\ncases\nStep 7: Walk to bedroom\nStep 8: Put sheets and pillow\ncases on bed\nStep 9: Walk to living room\nStep 10: Sit on couch\nTask: Change sheets and pillow\ncases\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find sheets\nStep 4: Grab sheets\nStep 5: Find pillow\nStep 6: Grab pillow\nTask: Wash dirty dishes\nStep 1: Walk to dining room\nStep 2: Walk to sink\nStep 3: Find sponge\nStep 4: Grab sponge\nStep 5: Find faucet\nStep 6: Switch on faucet\nStep 7: Put sponge on sink\nStep 8: Grab sponge\nStep 9: Find dish soap\nStep 10: Grab dish soap\nStep 11:\nPour dish soap into\nsponge\nStep 12: Put back dish soap\nStep 13: Find plate\nStep 14: Grab plate\nStep 15: Scrub plate\nStep 16: Rinse plate\nStep 17: Find dishrack\nStep 18: Put plate on dishrack\nStep 19: Find plate\nStep 20: Grab plate\nStep 21: Scrub plate\nStep 22: Rinse plate\nStep 23: Put plate on dishrack\nStep 24: Rinse sponge\nStep 25: Put back sponge\nStep 26: Switch off faucet\nTask: Wash dirty dishes\nStep 1: Walk to kitchen\nStep 2: Walk to sink\nStep 3: Wash dishes\nStep 4: Dry dishes\nStep 5: Put dishes away\nTask: Wash dirty dishes\nStep 1: Walk to dining room\nStep 2: Walk to sink\nStep 3: Find faucet\nStep 4: Switch on faucet\nStep 5: Walk to plate\nStep 6: Grab plate\n23\n"
    },
    {
      "page_number": 24,
      "text": "Task: Feed me\nStep 1: Walk to dining room\nStep 2: Walk to plate\nStep 3: Find plate\nStep 4: Grab plate\nStep 5: Find food\nStep 6: Grab food\nStep 7: Put food on plate\nStep 8: Find food\nStep 9: Grab food\nStep 10: Put food on plate\nStep 11: Find fork\nStep 12: Grab fork\nStep 13: Eat plate\nStep 14: Eat plate\nTask: Feed me\nStep 1: Walk to kitchen\nStep 2: Walk to refrigerator\nStep 3: Open refrigerator\nStep 4: Grab food\nStep 5: Find plate\nStep 6: Put food on plate\nStep 7: Find fork\nStep 8: Put fork on plate\nStep 9: Find spoon\nStep 10: Put spoon on plate\nStep 11: Find cup\nStep 12: Pour water in cup\nStep 13: Walk to table\nStep 14: Sit at table\nStep 15: Eat food\nTask: Feed me\nStep 1: Walk to dining room\nStep 2: Walk to table\nStep 3: Find table\nStep 4: Turn to table\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find food\nStep 8: Grab food\nStep 9: Find plate\nStep 10: Put food on plate\nTask: Breakfast\nStep 1: Walk to dining room\nStep 2: Walk to food\nStep 3: Grab food\nStep 4: Walk to bedroom\nStep 5: Walk to television\nStep 6: Switch on television\nStep 7: Walk to bed\nStep 8: Lie on bed\nStep 9: Turn to television\nStep 10: Watch television\nStep 11: Eat food\nTask: Breakfast\nStep 1: Walk to kitchen\nStep 2: Find cereal\nStep 3: Pour cereal into bowl\nStep 4: Pour milk into bowl\nStep 5: Walk to living room\nStep 6: Sit on couch\nStep 7: Eat cereal\nTask: Breakfast\nStep 1: Walk to dining room\nStep 2: Walk to freezer\nStep 3: Find freezer\nStep 4: Open freezer\nStep 5: Find bread\nStep 6: Grab bread\nStep 7: Close freezer\nStep 8: Find toaster\nStep 9: Plug in toaster\nStep 10: Put bread on plate\nStep 11: Switch on toaster\nTask: Read\nStep 1: Find novel\nStep 2: Grab novel\nStep 3: Walk to bedroom\nStep 4: Walk to chair\nStep 5: Sit on chair\nStep 6: Read novel\nTask: Read\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nTask: Read\nStep 1: Walk to home ofﬁce\nStep 2: Walk to light\nStep 3: Find light\nStep 4: Switch on light\nStep 5: Find novel\nStep 6: Grab novel\nStep 7: Find chair\nStep 8: Sit on chair\nStep 9: Read novel\nTask: Style hair\nStep 1: Find towel\nStep 2: Turn to towel\nStep 3: Point at towel\nStep 4: Wash towel\nStep 5: Turn to towel\nStep 6: Look at towel\nStep 7: Squeeze towel\nStep 8: Walk to bathroom\nTask: Style hair\nStep 1: Walk to bathroom\nStep 2: Walk to mirror\nStep 3: Find hairbrush\nStep 4: Grab hairbrush\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Style hair\nTask: Style hair\nStep 1: Walk to bathroom\nStep 2: Walk to mirror\nStep 3: Find hair\nStep 4: Grab hair\nStep 5: Find comb\nStep 6: Grab comb\nStep 7: Find brush\nStep 8: Grab brush\n24\n"
    },
    {
      "page_number": 25,
      "text": "Task: Pick up toys\nStep 1: Walk to bedroom\nStep 2: Walk to toy\nStep 3: Find toy\nStep 4: Grab toy\nStep 5: Walk to ﬁling cabinet\nStep 6: Open ﬁling cabinet\nStep 7: Put toy on ﬁling cabinet\nStep 8: Find toy\nStep 9: Grab toy\nStep 10: Walk to ﬁling cabinet\nStep 11: Put toy on ﬁling cabinet\nStep 12: Find toy\nStep 13: Grab toy\nStep 14: Walk to ﬁling cabinet\nStep 15: Put toy on ﬁling cabinet\nStep 16: Close ﬁling cabinet\nTask: Pick up toys\nStep 1: Walk to living room\nStep 2: Walk to toy box\nStep 3: Find toy box\nStep 4: Grab toy box\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Pick up toys\nTask: Pick up toys\nStep 1: Find toy\nStep 2: Turn to toy\nStep 3: Point at toy\nStep 4: Wash toy\nTask: Decorate it\nStep 1: Walk to home ofﬁce\nStep 2: Walk to centerpiece\nStep 3: Find centerpiece\nStep 4: Grab centerpiece\nStep 5: Walk to table\nStep 6: Put centerpiece on table\nStep 7: Find drawing\nStep 8: Grab drawing\nStep 9: Find wall\nStep 10: Turn to wall\nStep 11: Put drawing on wall\nStep 12: Find drawing\nStep 13: Grab drawing\nStep 14: Turn to wall\nStep 15: Put drawing on wall\nTask: Decorate it\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nStep 8: Decorate it\nTask: Decorate it\nStep 1: Walk to table\nStep 2: Walk to centerpiece\nStep 3: Look at centerpiece\nStep 4: Grab centerpiece\nStep 5: Point at centerpiece\nTask: Put away groceries\nStep 1: Walk to dining room\nStep 2: Walk to freezer\nStep 3: Find freezer\nStep 4: Open freezer\nStep 5: Find food\nStep 6: Grab food\nStep 7: Put food in freezer\nStep 8: Close freezer\nTask: Put away groceries\nStep 1: Walk to kitchen\nStep 2: Walk to refrigerator\nStep 3: Open refrigerator\nStep 4: Grab milk and eggs\nStep 5: Close refrigerator\nStep 6: Walk to counter\nStep 7: Put milk and eggs on\ncounter\nStep 8: Walk to cabinet\nStep 9: Open cabinet\nStep 10: Put away milk and eggs\nStep 11: Close cabinet\nStep 12: Walk to refrigerator\nStep 13: Grab orange juice and\nbread\nStep 14: Close refrigerator\nTask: Put away groceries\nStep 1: Walk to kitchen\nStep 2: Walk to fridge\nStep 3: Open fridge\nStep 4: Put food in fridge\nStep 5: Close fridge\nStep 6: Walk to dining room\nStep 7: Walk to livingroom\nStep 8: Walk to bedroom\nStep 9: Walk to bathroom\n25\n"
    },
    {
      "page_number": 26,
      "text": "Task: Draw picture\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find desk\nStep 6: Open desk\nStep 7: Find check\nStep 8: Grab check\nStep 9: Put check on desk\nStep 10: Find pencil\nStep 11: Grab pencil\nStep 12: Close desk\nTask: Draw picture\nStep 1: Walk to home ofﬁce\nStep 2: Walk to pencils\nStep 3: Find pencils\nStep 4: Grab pencils\nStep 5: Find paper\nStep 6: Sit on chair\nStep 7: Draw picture\nTask: Draw picture\nStep 1: Walk to home ofﬁce\nStep 2: Walk to wall\nStep 3: Walk to drawing\nStep 4: Find drawing\nStep 5: Turn to drawing\nStep 6: Look at drawing\nStep 7: Touch drawing\nStep 8: Pull drawing\nStep 9: Push drawing\nTask: Entertain\nStep 1: Walk to home ofﬁce\nStep 2: Walk to cheese\nStep 3: Find cheese\nStep 4: Find food\nStep 5: Find couch\nStep 6: Sit on couch\nStep 7: Find wine glass\nStep 8: Grab wine glass\nStep 9: Drink wine glass\nTask: Entertain\nStep 1: Walk to living room\nStep 2: Walk to entertainment\ncenter\nStep 3: Find remote\nStep 4: Grab remote\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Entertain self\nTask: Entertain\nStep 1: Walk to livingroom\nStep 2: Walk to couch\nStep 3: Find couch\nStep 4: Sit on couch\nTask: Type up document\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find computer\nStep 6: Switch on computer\nStep 7: Turn to computer\nStep 8: Look at computer\nStep 9: Find check\nStep 10: Grab check\nStep 11: Read check\nStep 12: Find coffee cup\nStep 13: Grab coffee cup\nStep 14: Drink coffee cup\nStep 15: Put back coffee cup\nStep 16: Find keyboard\nStep 17: Type on keyboard\nTask: Type up document\nStep 1: Walk to home ofﬁce\nStep 2: Walk to keyboard\nStep 3: Find keyboard\nStep 4: Grab keyboard\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Type document\nTask: Type up document\nStep 1: Walk to home ofﬁce\nStep 2: Walk to computer\nStep 3: Find computer\nStep 4: Switch on computer\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Turn to computer\nStep 8: Look at computer\nStep 9: Find keyboard\nStep 10: Grab keyboard\nStep 11: Type on keyboard\nStep 12: Stand up\nStep 13: Find printer\nStep 14: Switch on printer\nStep 15: Find printing paper\nTask: Put on your shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Grab shoes\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find feet both\nStep 8: Put shoes on feet both\nTask: Put on your shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Grab shoes\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Put on shoes\nTask: Put on your shoes\nStep 1: Find shoes\nStep 2: Turn to shoes\nStep 3: Point at shoes\nStep 4: Wash shoes\n26\n"
    },
    {
      "page_number": 27,
      "text": "Task: Watch horror movie\nStep 1: Walk to home ofﬁce\nStep 2: Walk to remote control\nStep 3: Find remote control\nStep 4: Grab remote control\nStep 5: Find couch\nStep 6: Sit on couch\nStep 7: Find television\nStep 8: Switch on television\nStep 9: Turn to television\nStep 10: Watch television\nTask: Watch horror movie\nStep 1: Walk to home ofﬁce\nStep 2: Walk to horror movie\nStep 3: Find horror movie\nStep 4: Grab horror movie\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Watch horror movie\nTask: Watch horror movie\nStep 1: Find television\nStep 2: Switch on television\nStep 3: Find cd player\nStep 4: Switch on cd player\nStep 5: Switch off cd player\nStep 6: Find remote control\nStep 7: Switch on remote control\nStep 8: Switch off remote control\nTask: Turn on radio\nStep 1: Walk to home ofﬁce\nStep 2: Walk to stereo\nStep 3: Find stereo\nStep 4: Find electrical outlet\nStep 5: Turn to electrical outlet\nStep 6: Plug in stereo\nStep 7: Switch on electrical outlet\nStep 8: Switch on stereo\nTask: Turn on radio\nStep 1: Walk to home ofﬁce\nStep 2: Walk to radio\nStep 3: Find radio\nStep 4: Grab radio\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Turn on radio\nTask: Turn on radio\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find remote control\nStep 6: Grab remote control\nStep 7: Find radio\nStep 8: Point at radio\nStep 9: Switch on radio\nStep 10: Put back remote control\nTask: Look at mirror\nStep 1: Walk to bathroom\nStep 2: Walk to mirror\nStep 3: Find mirror\nStep 4: Turn to mirror\nStep 5: Look at mirror\nTask: Look at mirror\nStep 1: Walk to bathroom\nStep 2: Walk to mirror\nStep 3: Look in mirror\nTask: Look at mirror\nStep 1: Walk to bedroom\nStep 2: Walk to mirror\nStep 3: Find mirror\nStep 4: Turn to mirror\nStep 5: Look at mirror\nStep 6: Find face\nStep 7: Turn to face\nStep 8: Look at face\nStep 9: Find hair\nStep 10: Turn to hair\nStep 11: Look at hair\nStep 12: Find dress\nStep 13: Turn to dress\nStep 14: Look at dress\nTask: Leave home\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Grab shoes\nStep 5: Put on shoes\nTask: Leave home\nStep 1: Walk to front door\nStep 2: Open front door\nStep 3: Walk to car\nStep 4: Open car door\nStep 5: Get in car\nStep 6: Close car door\nStep 7: Drive to work\nTask: Leave home\nStep 1: Find shoes\nStep 2: Grab shoes\nStep 3: Put on shoes\nTask: Walk to room\nStep 1: Walk to home ofﬁce\nStep 2: Walk to home ofﬁce\nTask: Walk to room\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nTask: Walk to room\nStep 1: Walk to dining room\nStep 2: Walk to livingroom\nStep 3: Walk to bedroom\n27\n"
    },
    {
      "page_number": 28,
      "text": "Task: Hang pictures\nStep 1: Walk to bedroom\nStep 2: Walk to drawing\nStep 3: Find drawing\nStep 4: Grab drawing\nStep 5: Find wall\nStep 6: Put drawing on wall\nStep 7: Turn to drawing\nStep 8: Look at drawing\nStep 9: Find drawing\nStep 10: Grab drawing\nStep 11: Put drawing on wall\nStep 12: Turn to drawing\nStep 13: Look at drawing\nTask: Hang pictures\nStep 1: Walk to home ofﬁce\nStep 2: Walk to box of pictures\nStep 3: Find box of pictures\nStep 4: Grab box of pictures\nStep 5: Walk to wall\nStep 6: Hang pictures on wall\nTask: Hang pictures\nStep 1: Walk to home ofﬁce\nStep 2: Walk to table\nStep 3: Find keys\nStep 4: Grab keys\nStep 5: Walk to wall\nStep 6: Find hanger\nStep 7: Put keys on hanger\nTask: Wipe down sink\nStep 1: Walk to dining room\nStep 2: Walk to rag\nStep 3: Find rag\nStep 4: Grab rag\nStep 5: Walk to sink\nStep 6: Wipe sink\nStep 7: Put back rag\nTask: Wipe down sink\nStep 1: Walk to kitchen\nStep 2: Walk to sink\nStep 3: Grab sponge\nStep 4: Wipe down sink\nStep 5: Rinse sponge\nStep 6: Put sponge away\nStep 7: Wash hands\nTask: Wipe down sink\nStep 1: Walk to bathroom\nStep 2: Walk to cleaning solution\nStep 3: Find cleaning solution\nStep 4: Grab cleaning solution\nStep 5: Find sink\nStep 6: Pour cleaning solution\ninto sink\nStep 7: Find faucet\nStep 8: Switch on faucet\nStep 9: Scrub sink\nStep 10: Wash sink\nTask: Take shoes off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Find shoes\nStep 5: Take off shoes\nStep 6: Find dresser\nStep 7: Open dresser\nStep 8: Grab shoes\nStep 9: Put shoes in dresser\nStep 10: Close dresser\nTask: Take shoes off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Grab shoes\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Take shoes off\nTask: Take shoes off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nTask: Give milk to cat\nStep 1: Walk to home ofﬁce\nStep 2: Walk to cat\nStep 3: Find cat\nStep 4: Turn to cat\nStep 5: Point at cat\nStep 6: Pull cat\nStep 7: Find milk\nStep 8: Grab milk\nStep 9: Find plate\nStep 10: Pour milk into plate\nStep 11: Touch cat\nStep 12: Scrub cat\nTask: Give milk to cat\nStep 1: Walk to kitchen\nStep 2: Walk to fridge\nStep 3: Open fridge\nStep 4: Grab milk\nStep 5: Walk to cat\nStep 6: Pour milk into bowl\nStep 7: Put bowl on ﬂoor\nStep 8: Walk away from cat\nTask: Give milk to cat\nStep 1: Find cat\nStep 2: Touch cat\nStep 3: Open fridge\nStep 4: Touch milk\nStep 5: Pour milk into bowl\n28\n"
    },
    {
      "page_number": 29,
      "text": "Task: Grab things\nStep 1: Walk to novel\nStep 2: Turn to novel\nStep 3: Grab novel\nTask: Grab things\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nStep 8: Grab things\nTask: Grab things\nStep 1: Find toy\nStep 2: Turn to toy\nStep 3: Point at toy\nStep 4: Wash toy\nTask: Use laptop\nStep 1: Walk to home ofﬁce\nStep 2: Walk to couch\nStep 3: Find couch\nStep 4: Sit on couch\nStep 5: Find laptop\nStep 6: Switch on laptop\nStep 7: Turn to laptop\nStep 8: Look at laptop\nStep 9: Find keyboard\nStep 10: Type on keyboard\nStep 11: Turn to keyboard\nStep 12: Look at keyboard\nStep 13: Type on keyboard\nStep 14: Turn to laptop\nStep 15: Look at laptop\nTask: Use laptop\nStep 1: Walk to home ofﬁce\nStep 2: Walk to laptop\nStep 3: Find laptop\nStep 4: Grab laptop\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Use laptop\nTask: Use laptop\nStep 1: Walk to home ofﬁce\nStep 2: Walk to chair\nStep 3: Find chair\nStep 4: Sit on chair\nStep 5: Find laptop\nStep 6: Switch on laptop\nStep 7: Find mouse\nStep 8: Grab mouse\nStep 9: Find mousepad\nStep 10: Put mouse on mousepad\nStep 11: Turn to laptop\nStep 12: Point at laptop\nTask: Organize pantry\nStep 1: Walk to dining room\nStep 2: Walk to pantry\nStep 3: Find food\nStep 4: Grab food\nStep 5: Turn to food\nStep 6: Look at food\nStep 7: Find garbage can\nStep 8: Put food on garbage can\nStep 9: Find vegetable\nStep 10: Grab vegetable\nStep 11: Turn to vegetable\nStep 12: Look at vegetable\nStep 13:\nPut vegetable on\ngarbage can\nStep 14: Find dry pasta\nStep 15: Grab dry pasta\nStep 16: Turn to dry pasta\nStep 17: Look at dry pasta\nStep 18: Put back dry pasta\nStep 19: Find food\nStep 20: Turn to food\nStep 21: Look at food\nStep 22: Push food\nStep 23: Find noodles\nStep 24: Grab noodles\nStep 25: Turn to noodles\nStep 26: Look at noodles\nStep 27: Put back noodles\nTask: Organize pantry\nStep 1: Walk to kitchen\nStep 2: Walk to pantry\nStep 3: Find pantry\nStep 4: Grab pantry\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Organize pantry\nTask: Organize pantry\nStep 1: Walk to pantry\nStep 2: Find pantry\n29\n"
    },
    {
      "page_number": 30,
      "text": "Task: Change clothes\nStep 1: Walk to bedroom\nStep 2: Walk to dresser\nStep 3: Open dresser\nStep 4: Find dress\nStep 5: Grab dress\nStep 6: Put on dress\nStep 7: Close dresser\nTask: Change clothes\nStep 1: Walk to bedroom\nStep 2: Walk to closet\nStep 3: Find clothes\nStep 4: Grab clothes\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Change clothes\nTask: Change clothes\nStep 1: Walk to bedroom\nStep 2: Walk to dresser\nStep 3: Find dresser\nStep 4: Open dresser\nStep 5: Find pants\nStep 6: Grab pants\nStep 7: Put on pants\nStep 8: Find shirt\nStep 9: Grab shirt\nStep 10: Put on shirt\nStep 11: Close dresser\nTask: Put up decoration\nStep 1: Walk to home ofﬁce\nStep 2: Walk to drawing\nStep 3: Find drawing\nStep 4: Find centerpiece\nStep 5: Grab centerpiece\nStep 6: Walk to wall\nStep 7: Grab drawing\nStep 8: Put drawing on wall\nStep 9: Find centerpiece\nStep 10: Grab centerpiece\nStep 11: Walk to table\nStep 12: Put centerpiece on table\nTask: Put up decoration\nStep 1: Walk to home ofﬁce\nStep 2: Walk to decoration\nStep 3: Find decoration\nStep 4: Grab decoration\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Put up decoration\nTask: Put up decoration\nStep 1: Walk to livingroom\nStep 2: Walk to table\nTask: Make bed\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find sheets\nStep 4: Grab sheets\nStep 5: Pull sheets\nTask: Make bed\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find sheets\nStep 4: Find blanket\nStep 5: Find pillow\nStep 6: Put sheets on bed\nStep 7: Put blanket on bed\nStep 8: Put pillow on bed\nTask: Make bed\nStep 1: Stand up\nStep 2: Find sheets\nStep 3: Find blanket\nStep 4: Find pillow\nStep 5: Find comforter\nStep 6: Find bed\nTask: Scrubbing living room tile\nﬂoor is once week activity for me\nStep 1: Find mop\nStep 2: Turn to mop\nStep 3: Point at mop\nStep 4: Wash mop\nStep 5: Turn to mop\nStep 6: Look at mop\nTask: Scrubbing living room tile\nﬂoor is once week activity for me\nStep 1: Walk to living room\nStep 2: Walk to tile ﬂoor\nStep 3: Find scrub brush\nStep 4: Grab scrub brush\nStep 5: Scrub tile ﬂoor\nStep 6: Put down scrub brush\nStep 7: Walk to kitchen\nTask: Scrubbing living room tile\nﬂoor is once week activity for me\nStep 1: Walk to livingroom\nStep 2: Walk to mop\nStep 3: Find mop\nStep 4: Grab mop\nStep 5: Walk to bathroom\nStep 6: Find bucket\nStep 7: Switch on faucet\nTask: Read yourself to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find bed\nStep 6: Sit on bed\nStep 7: Find bookmark\nStep 8: Grab bookmark\nStep 9: Read novel\nStep 10: Put bookmark on novel\nStep 11: Sleep\nTask: Read yourself to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Find book\nStep 4: Grab book\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read book\nStep 8: Sleep\nTask: Read yourself to sleep\nStep 1: Walk to bedroom\nStep 2: Walk to bed\nStep 3: Lie on bed\nStep 4: Sleep\nStep 5: Read book\n30\n"
    },
    {
      "page_number": 31,
      "text": "Task: Hand washing\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Find faucet\nStep 4: Switch on faucet\nStep 5: Find soap\nStep 6: Grab soap\nStep 7: Find hands both\nStep 8: Scrub hands both\nStep 9: Put back soap\nStep 10: Wash hands both\nStep 11: Switch off faucet\nStep 12: Find towel\nStep 13: Grab towel\nStep 14: Wipe hands both\nTask: Hand washing\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Turn on faucet\nStep 4: Grab soap\nStep 5: Wash hands\nStep 6: Rinse hands\nStep 7: Dry hands\nTask: Hand washing\nStep 1: Walk to bathroom\nStep 2: Walk to sink\nStep 3: Find faucet\nStep 4: Turn to faucet\nStep 5: Find hands both\nStep 6: Wash hands both\nTask: Throw away paper\nStep 1: Walk to home ofﬁce\nStep 2: Walk to desk\nStep 3: Find desk\nStep 4: Turn to desk\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find check\nStep 8: Grab check\nStep 9: Squeeze check\nStep 10: Stand up\nStep 11: Walk to trashcan\nStep 12: Put check on trashcan\nTask: Throw away paper\nStep 1: Walk to home ofﬁce\nStep 2: Walk to wastebasket\nStep 3: Find wastebasket\nStep 4: Grab wastebasket\nStep 5: Walk to desk\nStep 6: Drop paper in wastebas-\nket\nTask: Throw away paper\nStep 1: Walk to home ofﬁce\nStep 2: Walk to table\nStep 3: Find table\nStep 4: Turn to table\nStep 5: Find paper\nStep 6: Grab paper\nStep 7: Walk to trashcan\nStep 8: Open trashcan\nStep 9: Put paper on trashcan\nStep 10: Close trashcan\nTask: Tale off shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nStep 4: Find shoes\nStep 5: Take off shoes\nStep 6: Grab shoes\nStep 7: Find mat\nStep 8: Put shoes on mat\nTask: Tale off shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nTask: Tale off shoes\nStep 1: Walk to home ofﬁce\nStep 2: Walk to shoes\nStep 3: Find shoes\nTask: Hang keys\nStep 1: Walk to bedroom\nStep 2: Walk to dresser\nStep 3: Find purse\nStep 4: Grab purse\nStep 5: Open purse\nStep 6: Find keys\nStep 7: Grab keys\nStep 8: Find bowl\nStep 9: Put keys on bowl\nStep 10: Close purse\nStep 11: Put back purse\nTask: Hang keys\nStep 1: Walk to home ofﬁce\nStep 2: Walk to key hook\nStep 3: Find key hook\nStep 4: Grab key hook\nStep 5: Find keys\nStep 6: Hang keys on key hook\nStep 7: Walk to front door\nStep 8: Leave house\nTask: Hang keys\nStep 1: Walk to home ofﬁce\nStep 2: Walk to table\nStep 3: Find keys\nStep 4: Grab keys\nStep 5: Walk to wall\nStep 6: Find hanger\nStep 7: Put keys on hanger\n31\n"
    },
    {
      "page_number": 32,
      "text": "Task: Make popcorn\nStep 1: Walk to dining room\nStep 2: Walk to food\nStep 3: Find food\nStep 4: Grab food\nStep 5: Walk to microwave\nStep 6: Turn to microwave\nStep 7: Open microwave\nStep 8: Put food in microwave\nStep 9: Close microwave\nStep 10: Switch on microwave\nTask: Make popcorn\nStep 1: Walk to kitchen\nStep 2: Walk to microwave\nStep 3: Open microwave\nStep 4: Put bag of popcorn in mi-\ncrowave\nStep 5: Close microwave\nStep 6:\nPush buttons on mi-\ncrowave\nStep 7: Wait for popcorn to pop\nStep 8: Get bowl out of cupboard\nStep 9: Get butter out of fridge\nStep 10: Get salt out of cupboard\nStep 11: Pour popcorn into bowl\nStep 12: Add butter and salt to\nbowl\nStep 13:\nTask: Make popcorn\nStep 1: Walk to kitchen\nStep 2: Walk to microwave\nStep 3: Find microwave\nStep 4: Open microwave\nTask: Receive credit card\nStep 1: Walk to home ofﬁce\nStep 2: Walk to envelope\nStep 3: Find envelope\nStep 4: Grab envelope\nStep 5: Walk to home ofﬁce\nStep 6: Walk to scissors\nStep 7: Find scissors\nStep 8: Open envelope\nStep 9: Find creditcard\nStep 10: Grab creditcard\nTask: Receive credit card\nStep 1: Walk to mailbox\nStep 2: Open mailbox\nStep 3: Grab credit card\nStep 4: Walk to home ofﬁce\nStep 5: Walk to desk\nStep 6: Put credit card on desk\nStep 7: Sit at desk\nStep 8: Look at credit card\nTask: Receive credit card\nStep 1: Walk to home ofﬁce\nStep 2: Walk to cabinet\nStep 3: Find cabinet\nStep 4: Open cabinet\nTask: Read to child\nStep 1: Walk to bedroom\nStep 2: Walk to child\nStep 3: Find child\nStep 4: Greet child\nStep 5: Walk to ﬁling cabinet\nStep 6: Open ﬁling cabinet\nStep 7: Find novel\nStep 8: Grab novel\nStep 9: Walk to chair\nStep 10: Pull chair\nStep 11: Sit on chair\nStep 12: Turn to novel\nStep 13: Look at novel\nStep 14: Turn to novel\nStep 15: Point at novel\nStep 16: Turn to novel\nStep 17: Point at novel\nStep 18: Read novel\nStep 19: Read novel\nStep 20: Read novel\nTask: Read to child\nStep 1: Walk to home ofﬁce\nStep 2: Walk to novel\nStep 3: Find novel\nStep 4: Grab novel\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Read novel\nTask: Read to child\nStep 1: Walk to child\nStep 2: Read book\n32\n"
    },
    {
      "page_number": 33,
      "text": "Task: Eat cheese\nStep 1: Walk to dining room\nStep 2: Walk to freezer\nStep 3: Open freezer\nStep 4: Find cheese\nStep 5: Grab cheese\nStep 6: Eat cheese\nTask: Eat cheese\nStep 1: Walk to kitchen\nStep 2: Walk to refrigerator\nStep 3: Open refrigerator\nStep 4: Grab cheese\nStep 5: Eat cheese\nTask: Eat cheese\nStep 1: Walk to dining room\nStep 2: Walk to table\nStep 3: Find table\nStep 4: Turn to table\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Find food\nStep 8: Grab food\nStep 9: Find plate\nStep 10: Put food on plate\nTask: Unload various items from\npockets and place them in bowl\non table\nStep 1: Walk to home ofﬁce\nStep 2: Walk to table\nStep 3: Find bowl\nStep 4: Grab bowl\nStep 5: Put bowl on table\nStep 6: Find pants\nStep 7: Find bills\nStep 8: Grab bills\nStep 9: Find phone\nStep 10: Grab phone\nTask: Unload various items from\npockets and place them in bowl\non table\nStep 1: Walk to home ofﬁce\nStep 2: Walk to bowl on table\nStep 3: Find bowl on table\nStep 4: Grab bowl on table\nStep 5:\nUnload various items\nfrom pockets and place them in\nbowl on table\nTask: Unload various items from\npockets and place them in bowl\non table\nStep 1: Find bowl\nStep 2: Turn to bowl\nStep 3: Point at bowl\nStep 4: Wash bowl\nTask: Switch on lamp\nStep 1: Walk to bedroom\nStep 2: Walk to ﬂoor lamp\nStep 3: Find ﬂoor lamp\nStep 4: Switch on ﬂoor lamp\nTask: Switch on lamp\nStep 1: Walk to lamp\nStep 2: Switch on lamp\nStep 3: Walk to chair\nStep 4: Sit on chair\nStep 5: Read book\nTask: Switch on lamp\nStep 1: Walk to bedroom\nStep 2: Walk to ﬂoor lamp\nStep 3: Find ﬂoor lamp\nStep 4: Turn to ﬂoor lamp\nStep 5: Switch on ﬂoor lamp\nStep 6: Find bed\nStep 7: Lie on bed\nTask: Take jacket off\nStep 1: Walk to bedroom\nStep 2: Walk to jacket\nStep 3: Find jacket\nStep 4: Find jacket\nStep 5: Take off jacket\nStep 6: Find dresser\nStep 7: Open dresser\nStep 8: Close dresser\nTask: Take jacket off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to jacket\nStep 3: Find jacket\nStep 4: Grab jacket\nStep 5: Find chair\nStep 6: Sit on chair\nStep 7: Take jacket off\nTask: Take jacket off\nStep 1: Walk to home ofﬁce\nStep 2: Walk to jacket\nStep 3: Find jacket\n33\n"
    }
  ]
}