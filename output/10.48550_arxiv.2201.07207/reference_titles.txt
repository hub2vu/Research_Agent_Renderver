Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments
Weakly supervised learning of semantic parsers for mapping instructions to actions
Beyond the imitation game: Measuring and extrapolating the capabilities of language models
Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Language models are few-shot learners
Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation
Evaluating large language models trained on code
Training veriﬁers to solve math word problems
Commonsense knowledge mining from pretrained models
Bert: Pre-training of deep bidirectional transformers for language understanding
Speakerfollower models for vision-and-language navigation
From language to goals: Inverse reinforcement learning for vision-based instruction following
Making pre-trained language models better few-shot learners
Learning from stories: using crowdsourced narratives to train virtual agents
Measuring coding challenge competence with apps
Human instruction-following with deep reinforcement learning via transfer-learning from text
Long short-term memory
The curious case of neural text degeneration
Probing text models for common ground with visual representations
Visually-grounded planning without vision: Language models infer detailed plans from high-level instructions
Ai2-thor: An interactive 3d environment for visual ai
Implicit representations of meaning in neural language models
Pre-trained language models for interactive decision-making
Synthesizing environment-aware activities via activity sketches
Roberta: A robustly optimized bert pretraining approach
Pretrained transformers as universal computation engines
Grounding language in play
Language conditioned imitation learning over unstructured data
Improving vision-and-language navigation with image-text pairs from the web
Ella: Exploration through learned language abstraction
Environment-driven lexicon induction for high-level instructions
Tell me dave: Contextsensitive grounding of natural language to manipulation instructions
Glove: Global vectors for word representation
Language models as knowledge bases? arXiv preprint arXiv:
Synchromesh: Reliable code generation from pre-trained language models
Virtualhome: Simulating household activities via programs
Language models are unsupervised multitask learners
Exploring the limits of transfer learning with a uniﬁed text-to-text transformer
Sentence-bert: Sentence embeddings using siamese bertnetworks
How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:
Learning to retrieve prompts for in-context learning
Habitat: A platform for embodied ai research
Skill induction and planning with latent language
Generate & rank: A multi-task framework for math word problems
Alfred: A benchmark for interpreting grounded instructions for everyday tasks
Alfworld: Aligning text and embodied environments for interactive learning
Embodied bert: A transformer model for embodied, language-guided visual task completion
olmpics-on what language model pre-training captures
Understanding and executing instructions for everyday manipulation tasks from the world wide web
Multimodal few-shot learning with frozen language models
Attention is all you need
Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation
Huggingface’s transformers: State-of-the-art natural language processing