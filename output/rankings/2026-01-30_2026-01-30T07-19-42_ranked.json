{
  "success": true,
  "error": null,
  "summary": {
    "input_count": 96,
    "filtered_count": 0,
    "scored_count": 96,
    "output_count": 10,
    "purpose": "general",
    "ranking_mode": "balanced",
    "profile_used": "users/profile.json",
    "llm_verification_used": false,
    "llm_calls_made": 0
  },
  "ranked_papers": [
    {
      "rank": 1,
      "paper_id": "n5PrId7pk5",
      "title": "Linear combinations of latents in generative models: subspaces and beyond",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.7850455361474136,
        "breakdown": {
          "semantic_relevance": 0.7481788722619988,
          "must_keywords": 0.75,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.3888888888888889
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding_high"
      },
      "tags": [
        "SEMANTIC_HIGH_MATCH",
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "n5PrId7pk5",
        "title": "Linear combinations of latents in generative models: subspaces and beyond",
        "abstract": "Sampling from generative models has become a crucial tool for applications like data synthesis and augmentation. Diffusion, Flow Matching and Continuous Normalising Flows have shown effectiveness across various modalities, and rely on latent variables for generation. For experimental design or creative applications that require more control over the generation process, it has become common to manipulate the latent variable directly. However, existing approaches for performing such manipulations (e.g. interpolation or forming low-dimensional representations) only work well in special cases or are network or data-modality specific. \nWe propose Latent Optimal Linear combinations (LOL) as a general-purpose method to form linear combinations of latent variables that adhere to the assumptions of the generative model. As LOL is easy to implement and naturally addresses the broader task of forming any linear combinations, e.g. the construction of subspaces of the latent space, LOL dramatically simplifies the creation of expressive low-dimensional representations of high-dimensional objects.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=n5PrId7pk5",
        "github_url": null,
        "semantic_score": 0.6812313795089722,
        "keyword_score": 0.3888888888888889,
        "combined_score": 0.47659163607491384
      }
    },
    {
      "rank": 2,
      "paper_id": "k03mB41vyM",
      "title": "Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.7455918744688139,
        "breakdown": {
          "semantic_relevance": 0.7,
          "must_keywords": 0.5,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.5
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding+llm"
      },
      "tags": [
        "SEMANTIC_HIGH_MATCH",
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "k03mB41vyM",
        "title": "Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning",
        "abstract": "Identifying latent representations or causal structures is important for good generalization and downstream task performance. However, both fields developed rather independently.\nWe observe that several structure and representation identifiability methods, particularly those that require multiple environments, rely on \nexchangeable non--i.i.d. (independent and identically distributed) data.\nTo formalize this connection, \nwe propose the Identifiable Exchangeable Mechanisms (IEM) framework to unify key representation and causal structure learning methods. IEM provides a unified probabilistic graphical model encompassing causal discovery, Independent Component Analysis, and Causal Representation Learning.\nWith the help of the IEM model, we generalize the Causal de Finetti theorem of Guo et al., 2022 by relaxing the necessary conditions for causal structure identification in exchangeable data.\nWe term these conditions cause and mechanism variability, and show how they imply a duality condition in identifiable representation learning, leading to new identifiability results.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=k03mB41vyM",
        "github_url": null,
        "semantic_score": 0.6029552817344666,
        "keyword_score": 0.5,
        "combined_score": 0.5308865845203399
      }
    },
    {
      "rank": 3,
      "paper_id": "MxALfOAnXv",
      "title": "Continuity-Preserving  Convolutional Autoencoders for Learning Continuous Latent Dynamical Models from Images",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.7234226455934325,
        "breakdown": {
          "semantic_relevance": 0.7094359037487288,
          "must_keywords": 0.25,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.5
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding_high"
      },
      "tags": [
        "SEMANTIC_HIGH_MATCH",
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "MxALfOAnXv",
        "title": "Continuity-Preserving  Convolutional Autoencoders for Learning Continuous Latent Dynamical Models from Images",
        "abstract": "Continuous dynamical systems are cornerstones of many scientific and engineering disciplines.\nWhile machine learning offers powerful tools to model these systems from trajectory data, challenges arise when these trajectories are captured as images, resulting in pixel-level observations that are discrete in nature.\nConsequently, a naive application of a convolutional autoencoder can result in latent coordinates that are discontinuous in time.\nTo resolve this, we propose continuity-preserving convolutional autoencoders (CpAEs) to learn continuous latent states and their corresponding continuous latent dynamical models from discrete image frames. \nWe present a mathematical formulation for learning dynamics from image frames, which illustrates issues with previous approaches and motivates our methodology based on promoting the continuity of convolution filters, thereby preserving the continuity of the latent states.\nThis approach enables CpAEs to produce latent states that evolve continuously with the underlying dynamics, leading to more accurate latent dynamical models.\nExtensive experiments across various scenarios demonstrate the effectiveness of CpAEs.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=MxALfOAnXv",
        "github_url": null,
        "semantic_score": 0.6680076718330383,
        "keyword_score": 0.5,
        "combined_score": 0.5504023015499114
      }
    },
    {
      "rank": 4,
      "paper_id": "vFanHFE4Qv",
      "title": "Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.7211770151973351,
        "breakdown": {
          "semantic_relevance": 0.701950469095071,
          "must_keywords": 0.25,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.4444444444444444
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding_high"
      },
      "tags": [
        "SEMANTIC_HIGH_MATCH",
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "vFanHFE4Qv",
        "title": "Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning",
        "abstract": "The Platonic Representation Hypothesis posits that behind different modalities of data (what we sense or detect), there exists a universal, modality-independent representation of reality. Inspired by this, we treat each neuron as a system, where we can detect the neuron’s multi-segment activity data under different peripheral conditions. We believe that, similar to the Platonic idea, there exists a time-invariant representation behind the different segments of the same neuron, which reflects the intrinsic properties of the neuron’s system. Intrinsic properties include the molecular profiles, brain regions and morphological structure, etc. The optimization objective for obtaining the intrinsic representation of neurons should satisfy two criteria: (I) segments from the same neuron should have a higher similarity than segments from different neurons; (II) the representations should generalize well to out-of-domain data. To achieve this, we employ contrastive learning, treating different segments from the same neuron as positive pairs and segments from different neurons as negative pairs. During the implementation, we chose the VICReg, which uses only positive pairs for optimization but indirectly separates dissimilar samples via regularization terms. To validate the efficacy of our method, we first applied it to simulated neuron population dynamics data generated using the Izhikevich model. We successfully confirmed that our approach captures the type of each neuron as defined by preset hyperparameters. We then applied our method to two real-world neuron dynamics datasets, including spatial transcriptomics-derived neuron type annotations and the brain regions where each neuron is located. The learned representations from our model not only predict neuron type and location but also show robustness when tested on out-of-domain data (unseen animals). This demonstrates the potential of our approach in advancing the understanding of neuronal systems and offers valuable insights for future neuroscience research.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=vFanHFE4Qv",
        "github_url": null,
        "semantic_score": 0.5935531854629517,
        "keyword_score": 0.4444444444444444,
        "combined_score": 0.48917706674999656
      }
    },
    {
      "rank": 5,
      "paper_id": "WfaQrKCr4X",
      "title": "I-Con: A Unifying Framework for Representation Learning",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.690591874468814,
        "breakdown": {
          "semantic_relevance": 0.6,
          "must_keywords": 0.25,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.4444444444444444
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding+llm"
      },
      "tags": [
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "WfaQrKCr4X",
        "title": "I-Con: A Unifying Framework for Representation Learning",
        "abstract": "As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of mod- ern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality re- duction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=WfaQrKCr4X",
        "github_url": null,
        "semantic_score": 0.6735424995422363,
        "keyword_score": 0.4444444444444444,
        "combined_score": 0.5131738609737819
      }
    },
    {
      "rank": 6,
      "paper_id": "DShqJA1Z64",
      "title": "Towards a learning theory of representation alignment",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.690591874468814,
        "breakdown": {
          "semantic_relevance": 0.6,
          "must_keywords": 0.25,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.4444444444444444
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding+llm"
      },
      "tags": [
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "DShqJA1Z64",
        "title": "Towards a learning theory of representation alignment",
        "abstract": "It has recently been argued that AI models' representations are becoming aligned as their scale and performance increase. Empirical analyses have been designed to support this idea and conjecture the possible alignment of different representations toward a shared statistical model of reality. In this paper, we propose a learning-theoretic perspective to representation alignment. First, we review and connect different notions of alignment based on metric, probabilistic, and spectral ideas. Then, we focus on stitching, a particular approach to understanding the interplay between different representations in the context of a task. Our main contribution here is to relate the properties of stitching to the kernel alignment of the underlying representation. Our results can be seen as a first step toward casting representation alignment as a learning-theoretic problem.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=DShqJA1Z64",
        "github_url": null,
        "semantic_score": 0.6555238962173462,
        "keyword_score": 0.4444444444444444,
        "combined_score": 0.5077682799763149
      }
    },
    {
      "rank": 7,
      "paper_id": "pPQPQ7Yd58",
      "title": "Control-oriented Clustering of Visual Latent Representation",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.6855918744688139,
        "breakdown": {
          "semantic_relevance": 0.5,
          "must_keywords": 0.5,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.4444444444444444
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding+llm"
      },
      "tags": [
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "pPQPQ7Yd58",
        "title": "Control-oriented Clustering of Visual Latent Representation",
        "abstract": "We initiate a study of the geometry of the visual representation space ---the information channel from the vision encoder to the action decoder--- in an image-based control pipeline learned from behavior cloning. Inspired by the phenomenon of *neural collapse* (NC) in image classification, we empirically demonstrate the prevalent emergence of a similar *law of clustering* in the visual representation space. Specifically, \n\n- In discrete image-based control (e.g., Lunar Lander), the visual representations cluster according to the natural discrete action labels;\n\n- In continuous image-based control (e.g., Planar Pushing and Block Stacking), the clustering emerges according to ``control-oriented'' classes that are based on (a) the relative pose between the object and the target in the input or (b) the relative pose of the object induced by expert actions in the output. Each of the classes corresponds to one relative pose orthant (REPO).\n\nBeyond empirical observation, we show such a law of clustering can be leveraged as an algorithmic tool to improve test-time performance when training a policy with limited expert demonstrations. Particularly, we pretrain the vision encoder using NC as a regularization to encourage control-oriented clustering of the visual features. Surprisingly, such an NC-pretrained vision encoder, when finetuned end-to-end with the action decoder, boosts the test-time performance by 10% to 35%. Real-world vision-based planar pushing experiments confirmed the surprising advantage of control-oriented visual representation pretraining.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=pPQPQ7Yd58",
        "github_url": null,
        "semantic_score": 0.6183115243911743,
        "keyword_score": 0.4444444444444444,
        "combined_score": 0.49660456842846334
      }
    },
    {
      "rank": 8,
      "paper_id": "ehr4oTe6XI",
      "title": "Disentangled Representation Learning with the Gromov-Monge Gap",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.660591874468814,
        "breakdown": {
          "semantic_relevance": 0.5,
          "must_keywords": 0.25,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.4444444444444444
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding+llm"
      },
      "tags": [
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "ehr4oTe6XI",
        "title": "Disentangled Representation Learning with the Gromov-Monge Gap",
        "abstract": "Learning disentangled representations from unlabelled data is a fundamental challenge in machine learning. Solving it may unlock other problems, such as generalization, interpretability, or fairness. Although remarkably challenging to solve in theory, disentanglement is often achieved in practice through prior matching. Furthermore, recent works have shown that prior matching approaches can be enhanced by leveraging geometrical considerations, e.g., by learning representations that preserve geometric features of the data, such as distances or angles between points. However, matching the prior while preserving geometric features is challenging, as a mapping that *fully* preserves these features while aligning the data distribution with the prior does not exist in general. To address these challenges, we introduce a novel approach to disentangled representation learning based on quadratic optimal transport. We formulate the problem using Gromov-Monge maps that transport one distribution onto another with minimal distortion of predefined geometric features, preserving them *as much as can be achieved*. To compute such maps, we propose the Gromov-Monge-Gap (GMG), a regularizer quantifying whether a map moves a reference distribution with minimal geometry distortion. We demonstrate the effectiveness of our approach for disentanglement across four standard benchmarks, outperforming other methods leveraging geometric considerations.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=ehr4oTe6XI",
        "github_url": null,
        "semantic_score": 0.6122022271156311,
        "keyword_score": 0.4444444444444444,
        "combined_score": 0.4947717792458004
      }
    },
    {
      "rank": 9,
      "paper_id": "wkbx7BRAsM",
      "title": "Video In-context Learning: Autoregressive Transformers are Zero-Shot Video Imitators",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.660591874468814,
        "breakdown": {
          "semantic_relevance": 0.5,
          "must_keywords": 0.25,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.3888888888888889
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding+llm"
      },
      "tags": [
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "wkbx7BRAsM",
        "title": "Video In-context Learning: Autoregressive Transformers are Zero-Shot Video Imitators",
        "abstract": "People interact with the real-world largely dependent on visual signal, which are ubiquitous and illustrate detailed demonstrations. In this paper, we explore utilizing visual signals as a new interface for models to interact with the environment. Specifically, we choose videos as a representative visual signal. And by training autoregressive Transformers on video datasets in a self-supervised objective, we find that the model emerges a zero-shot capability to infer the semantics from a demonstration video, and imitate the semantics to an unseen scenario. This allows the models to perform unseen tasks by watching the demonstration video in an in-context manner, without further fine-tuning. To validate the imitation capacity, we design various evaluation metrics including both objective and subjective measures. The results show that our models can generate high-quality video clips that accurately align with the semantic guidance provided by the demonstration videos, and we also show that the imitation capacity follows the scaling law.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=wkbx7BRAsM",
        "github_url": null,
        "semantic_score": 0.64753258228302,
        "keyword_score": 0.3888888888888889,
        "combined_score": 0.4664819969071282
      }
    },
    {
      "rank": 10,
      "paper_id": "FVuqJt3c4L",
      "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
      "authors": [],
      "published": "2025-05-01",
      "score": {
        "final": 0.6555918744688138,
        "breakdown": {
          "semantic_relevance": 0.4,
          "must_keywords": 0.5,
          "author_trust": 1,
          "institution_trust": 0,
          "recency": 0.17795937234406978,
          "practicality": 0,
          "keyword_score": 0.5
        },
        "soft_penalty": 0.3,
        "penalty_keywords": [],
        "evaluation_method": "embedding+llm"
      },
      "tags": [
        "PREFERRED_AUTHOR"
      ],
      "local_status": {
        "already_downloaded": false,
        "local_path": null
      },
      "original_data": {
        "paper_id": "FVuqJt3c4L",
        "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
        "abstract": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer.",
        "authors": [],
        "published": "2025-05-01",
        "categories": [
          "ICLR 2025"
        ],
        "pdf_url": "https://openreview.net/pdf?id=FVuqJt3c4L",
        "github_url": null,
        "semantic_score": 0.6496423482894897,
        "keyword_score": 0.5,
        "combined_score": 0.5448927044868469
      }
    }
  ],
  "filtered_papers": [],
  "contrastive_paper": null,
  "comparison_notes": [],
  "output_path": "/data/output/rankings/2026-01-30_2026-01-30T07-19-42_ranked.json",
  "generated_at": "2026-01-30T07:19:42.686441"
}