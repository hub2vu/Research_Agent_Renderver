{
  "filename": "116466_Self-Evolving Pseudo-Rehearsal for Catastrophic Forgetting with Task Similarity in LLMs.pdf",
  "total_pages": 24,
  "full_text": "Self-Evolving Pseudo-Rehearsal for Catastrophic\nForgetting with Task Similarity in LLMs\nJun Wang1∗Liang Ding2∗Shuai Wang1\nHongyu Li3\nYong Luo1† Huangxuan Zhao1†\nHan Hu4\nBo Du1\n1School of Computer Science, National Engineering Research Center of Multimedia Software\nand Hubei Key Laboratory of Multimedia and Network Communication Engineering,\nWuhan University, Wuhan, China\n2The University of Sydney, Australia\n3Individual Researcher\n4School of Information and Electronics, Beijing Institute of Technology, Beijing, China\n{junwang_ai, wangshuai123, luoyong, zhaohuangxuan, dubo}@whu.edu.cn\n{liangding.liam, hongyuli102799}@gmail.com\nhhu@bit.edu.cn\nAbstract\nContinual learning for large language models (LLMs) demands a precise balance\nbetween plasticity - the ability to absorb new tasks - and stability - the preservation\nof previously learned knowledge. Conventional rehearsal methods, which replay\nstored examples, are limited by long-term data inaccessibility; earlier pseudo-\nrehearsal methods require additional generation modules, while self-synthesis\napproaches often generate samples that poorly align with real tasks, suffer from\nunstable outputs, and ignore task relationships. We present Self-Evolving Pseudo-\nRehearsal for Catastrophic Forgetting with Task Similarity (SERS), a lightweight\nframework that 1) decouples pseudo-input synthesis from label creation, using\nsemantic masking and template guidance to produce diverse, task-relevant prompts\nwithout extra modules; 2) applies label self-evolution, blending base-model pri-\nors with fine-tuned outputs to prevent over-specialization; and 3) introduces a\ndynamic regularizer driven by the Wasserstein distance between task distributions,\nautomatically relaxing or strengthening constraints in proportion to task similarity.\nExperiments across diverse tasks on different LLMs show that our SERS reduces\nforgetting by over 2% points against strong pseudo-rehearsal baselines, by ensuring\nefficient data utilization and wisely transferring knowledge. The code will be\nreleased at https://github.com/JerryWangJun/LLM_CL_SERS/.\n1\nIntroduction\nEnabling large language models (LLMs) to acquire new knowledge continuously (Wu et al., 2024;\nZheng et al., 2025b) holds significant importance for developing artificial intelligence systems with\nlifelong learning abilities. While practical applications demand LLMs continually adapt to evolving\ndownstream tasks, conventional learning methods (Hu et al., 2022; Han et al., 2024) often struggle to\npreserve existing capabilities during such situations. Continual learning enables LLMs to flexibly\nintegrate new and existing knowledge as tasks increase, addressing the limitations of static training\nin preserving prior performance while incorporating new information. The core challenge lies in\nachieving an optimal balance between plasticity and stability (Mermillod et al., 2013). Excessive\nplasticity will result in catastrophic forgetting, whereas overly strong stability may prevent efficient\nand effective knowledge transfer.\n∗Equal contribution\n†Corresponding authors.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n60\n40\n20\n0\n20\n40\n60\n60\n40\n20\n0\n20\n40\n60\nReal Samples\nSelf-Synthesis\nOur Method\nQA\nQG\nSA\nSUM\nTRANS\nFigure 1: Clustering analysis of pseudo samples generated by our method (SERS) and Self-\nSynthesized Rehearsal (SSR) approaches across five tasks, alongside real samples. It can be observed\nthat the pseudo samples generated by our method are closer to the real samples than SSR, indicating\nthat SERS produces more similar pseudo samples that better reflect knowledge from previous tasks.\nA series of works (Zhao et al., 2024; Zheng et al., 2025a; Wang et al., 2024a; Sun and Gao, 2024)\nhave been proposed to mitigate this balancing challenge. Rehearsal-based methods (Yin et al., 2022;\nde Masson D’Autume et al., 2019; Rolnick et al., 2019) preserve model capabilities on previous tasks\nby utilizing real samples from prior training processes, which are not always consistently available in\npractice. To tackle the challenge of limited access to get historical data, existing solutions (Sun et al.,\n2020; Zhao et al., 2024) apply pseudo-sample generation, yet the additional generation modules\nincrease the number of trainable parameters. Huang et al. (2024) leverage in-context learning capacity\nof LLMs for self-synthesis rehearsal, effectively alleviating parameter burdens. We unexpectedly\nfound that self-synthesized samples often exhibit low similarity to real data, failing to adequately\nreflect the knowledge structure and thus undermining the effectiveness of rehearsals, as shown\nby the clustering analysis in Figure 1. Regularization-based approaches (Guo et al., 2024; Wang\net al., 2023) impose constraints on loss functions to penalize parameter updates that affect prior\ntask knowledge. However, traditional static constraint methods, with their fixed trade-off between\nfacilitating knowledge transfer and preventing forgetting, lack consideration for task diversity.\nTo generate pseudo samples that better support knowledge consolidation during rehearsal, while\nflexibly balancing knowledge transfer and forgetting prevention across tasks, we propose Self-\nEvolving Pseudo-Rehearsal with Task Similarity (SERS). Specifically, we generate pseudo inputs\nusing template guidance and semantic masking, eliminating task-specific instructions, where dynamic\nguidance and mask ratios ensure the diversity. After generating the pseudo inputs, to supplement\npseudo labels, over-specialized samples are selected via label self-questioning and ease the demand\nfor task-specific knowledge through label self-evolution. In the rehearsal stage, to fully promote\npermissible knowledge transfer, we design a regularization loss function based on task similarity.\nWhen tasks are similar, the regularization is relaxed to encourage the integration of new and old\nknowledge; otherwise, constraints are strengthened to alleviate the forgetting of previous knowledge.\nWe conducted extensive experiments on the SuperNI dataset (Wang et al., 2022) using\nLLaMA2-7B (Touvron et al., 2023) and ChatGLM-6B (GLM et al., 2024) to evaluate the per-\nformance of SERS across varying task chains. Results show that SERS consistently outperforms\nexisting methods and is more stable across a variety of task orders. On LLaMA2-7B, it achieved a\n2.16% relative improvement over advanced pseudo-sample rehearsal approaches, closely matching\nMulti-Task Learning (MTL) performance; on ChatGLM-6B, it even surpassed MTL.\nThe main contributions of our work are as follows:\n• We propose SERS, a continual learning framework for LLMs that decouples input and label\nsynthesis. SERS generates pseudo inputs via template guidance and semantic masking and uses a\n2\nlabel self-evolution module to prevent over-specialization in pseudo labels, brings human learning\nstrategies into machine learning.\n• We introduce a task similarity-based dynamic regularization to effectively balance stability and\nplasticity, reducing the sensitivity of knowledge transfer to task order.\n• Experiments show that SERS significantly improves learning accuracy under task-incremental\nconditions, alleviates catastrophic forgetting, and even facilitates additional knowledge transfer.\n2\nRelated Work\n2.1\nSelf-Evolution Learning\nSelf-evolution (Zhong et al., 2022; Peng et al., 2023; Zhong et al., 2023; Zheng et al., 2023; Tao\net al., 2024; Song et al., 2025) is a paradigm that enables models to learn and improve through\nself-generated knowledge, inspired by human learning from experience. In this process, LLMs create\nnew tasks and solutions based on predefined goals, collect feedback from the environment, refine the\nacquired experience to eliminate errors, and update their parameters or context accordingly.\nZhong et al. (2022) improve pretraining efficiency through a two-stage process of self-questioning and\nself-evolution. In the first stage, the model uses masking to detect tokens it struggles to understand;\nin the second, it generates soft labels with richer knowledge patterns to enhance training. Similarly,\nSingh et al. (2023) apply reinforcement learning to actively generate new samples, evaluate them\nusing a binary reward function, and select high-quality data for model updates.\nMotivated by these strategies and aiming to address the instability of pseudo-sample generation\nin LLMs, we propose a label-level self-evolution method.\nBy imitating self-questioning and\nself-evolution structure, our approach detects over-specialized pseudo labels and smooths them\nwith general knowledge, mitigating the local overfitting to a specific task caused by rehearsal.\n2.2\nContinual Instruction Tuning for LLMs\nContinual instruction tuning for LLMs extends traditional LLMs tuning by enabling LLMs to\nincrementally absorb new tasks and feedback without forgetting prior knowledge. Compared to\nstandard continual learning, it introduces unique challenges due to generative outputs, global semantic\nrelationships, and model scale. Existing methods can be broadly categorized into:\n(1) Architecture-based (Ren et al., 2024; Zhao et al., 2024; Ke et al., 2023): These methods adjust\nmodel architecture or parameter distribution to separate the knowledge of new and old tasks, thus\nmitigating forgetting caused by parameter interference. For example, Ren et al. (2024) use fast and\nslow learners to balance stability and plasticity. Zhao et al. (2024) introduce an Attentive Learning &\nSelection module by combining multiple PET blocks in different ways to fit different tasks. However,\nas task numbers increase, adding new modules raises computational costs, and separate architecture\nadjustments limit flexibility and universality.\n(2) Rehearsal-based methods (Wang et al., 2024b; Huang et al., 2024; Maekawa et al., 2023): These\nmethods involve real or pseudo-sample rehearsal. Real sample rehearsal, as in Wang et al. (2024b),\nhelps recall past knowledge but relies on access to original data during each training stage, which\nis often impractical. Pseudo-sample rehearsal typically necessitates an extra generation module,\nincreasing trainable parameters. To our knowledge, Huang et al. (2024) are the first to use self-\nsynthesis to generate pseudo samples from a few real samples, solving storage issues, but still facing\nchallenges with pseudo samples instability and task comprehension.\n(3) Regularization-based methods (Wang et al., 2023; Jin et al., 2021; Li et al., 2024; Guo et al.,\n2024): These methods constrain excessive parameter updates with regularization. For example, Wang\net al. (2023) learn tasks in different low-rank vector subspaces and keep these subspaces orthogonal\nto minimize interference. However, orthogonal subspaces limit the knowledge transfer between tasks.\nOur method combines pseudo-sample rehearsal and regularization. For pseudo-sample generation,\nwe leverage template guidance and semantic masking to ensure the stability and real-sample similarity\nof synthesized pseudo samples, while varying templates and masking ratios promote diversity. For\nregularization, we dynamically adjust the regularization strength based on task similarity and account\n3\n���0\n��\n① Semantic-Guided Pseudo Input Generation\n���\n���(���, ���）\n���\n② Label Self-Evolution\n����\n���0\n③ Rehearsal with Similarity Regularization\n��+1\n���(���, ���）\n��1...�−1\n����\nTask Similarity W\n����+1\n����\nFigure 2: The overall framework of our SERS method. In the Semantic-Guided Pseudo-Input\nGeneration stage, a small set of real samples produces pseudo inputs Xi\np. Then, the Label Self-\nEvolution module refines these inputs by integrating knowledge from LLM 0 and LLM i, yielding\nrehearsal pseudo samples Di\np(Xi\np, Y i\np). Finally, in the Rehearsal with Similarity Regularization\nstage, the rehearsal samples and the new training data Di+1 are combined for fine-tuning, with\nregularization applied based on task similarity.\nfor the impact of task order on parameter updates, effectively balancing knowledge transfer and\nresistance to catastrophic forgetting.\n3\nMethodology\n3.1\nProblem Definition\nWe consider the problem of Task-Incremental Continual Instruction Tuning. Given a sequence of N\ninstruction-following tasks T1, T2, . . . , TN, each associated with a dataset Di = (Xi, Y i), the goal is\nto continually fine-tune a pre-trained language model LLM0 on these tasks in sequence. At each step\ni, the model receives only the current task dataset Di and fine-tunes the model LLM i−1 to obtain\nLLM i. The objective is to learn each new task while maintaining performance on all previous tasks,\nwithout requiring large-scale retraining.\n3.2\nFramework Overview\nIn this paper, we propose a continual learning framework for LLMs that combines pseudo-sample\nrehearsal with regularization. As shown in Figure 2, our approach consists of three main compo-\nnents: semantic-guided pseudo-input generation, label self-evolution, and rehearsal with similarity\nregularization. In the following sections, we provide a detailed explanation of each module.\n3.3\nSemantic-Guided Pseudo-Input Generation\nThe self-synthesis approach proposed by Huang et al. (2024) effectively addresses the limitations\ndiscussed above, but still faces key challenges: the generated pseudo samples cannot well reflect the\noriginal knowledge structure and thus provide limited support for rehearsal. Additionally, appending\ntask instructions and labels increases the model’s comprehension burden, while the generated labels\nlose meaning after further refinement. To address these problems, we propose a Semantic-Guided\nPseudo-Input Generation module. As shown in Figure 3, real examples are masked in two roles:\nas Example Template that providing structure guidance, and as Semantic Guidance that offering\nsemantic context. Experiments in Wang et al. (2024c) indicate that models not fine-tuned on a specific\ntask have stronger contextual understanding, so that we use LLM 0 to fill in the masks to get pseudo\ninputs Xi\np without extra generating block. Varying mask ratios and example templates enhance\ndiversity, while removing task instructions and labels reduces cognitive load. Figure 1 shows the\nclustering of pseudo samples from our method, the self-synthesis approach, and real data. Our pseudo\n4\n Example Template\n \n \n \n \n Semantic Guidanc\n(a) Semantic-Guided Pseudo-Input Generation\ne\nWhat is the largest <mask> \nin our solar system?\nWhat is the capital of <mask>?\nWhat is the capital of France?\nWhen and <mask> was <mask> born？\nWhen and where was Einstein born？\nWhat is the largest \n<mask> in <mask>?\n���0\n Generated Pseudo Input\nWhat is the largest planet in our solar system?\n(b) Label Self-Evolution\nTop k% \ndifferences\n���\n���0\n����\nStage1: Self-Questioning\nWhat is the largest object in our solar system?\nWhat is the largest ocean on Earth?\nWhat is the largest largest country in Africa?\nOthers\n��_���������\n�\n��_��ℎ���\n�\nStage2: Self-Evolution\n���(���, ���)\n Over-Specialized Input: \nWhat is the largest \nocean on Earth?\nThe biggest ocean is the one \nbetween America and Asia\nThe largest ocean is the Pacific \nOcean with 165,250,000 km²\nLabel \nSelf-Evolution\nThe Pacific Ocean is \nEarth's largest ocean\n���0\n����\n����\n�∗\n+(�−�) ∗\n���0\n����\nFigure 3: Detailed illustration of the core modules. (a) Semantic-Guided Pseudo-Input Generation:\nA real sample and its masked version serve as an example template, while an additional masked\nsample provides semantic guidance. These are combined and passed through LLM 0 to generate\npseudo inputs. Varying combinations and mask ratios promote diversity. (b) Label Self-Evolution:\n(Top) In self-questioning stage, the top-k% pseudo inputs with high domain dependence are identified\nas over-specialized samples. In self-evolution stage, these are relabeled by blending knowledge from\nLLM 0 and LLM i; others directly use the output of LLM i as labels. (Bottom) An example shows\nthat an over-specialized input leads to poor output from LLM 0 and an overly detailed output from\nLLM i. After label self-evolution, the final label becomes more acceptable, with reduced reliance on\ndomain-specific knowledge.\nsamples are more similar to real ones, preserving knowledge structure while maintaining diversity.\nExamples in real-task settings are shown in Appendix A.\n3.4\nLabel Self-Evolution\nConsidering that randomness in pseudo-input generation can lead to over-specialized instances\nrequiring excessive expertise, rehearsing with such samples may cause large parameter shifts and\ndisrupt existing knowledge. We therefore introduce a label self-evolution method inspired by human\nreview. As shown at the top of Figure 3, the process consists of two stages: self-questioning and\nself-evolution. In self-questioning, both the base model LLM 0 and fine-tuned model LLM i generate\nlabels. Samples with the top-k% output differences are treated as over-specialized. In the self-\nevolution stage, regular samples adopt LLM i’s outputs as labels, while over-specialized samples are\nrelabeled by integrating the outputs from both models using a weighted combination. The coefficient\nα adjusts the contributions of LLM 0 and LLM i to balance general and task-specific knowledge.\n5\nAs shown in the lower part of Figure 3, over-specialized inputs produce vague outputs on LLM 0 and\nhighly specific ones on LLM i. The label self-evolution module merges these to produce acceptable\nlabels, mitigating overfitting during rehearsal. Examples in Figure 8 illustrate the effectiveness and\nreliability of this process on real tasks.\n3.5\nRehearsal with Similarity Regularization\nThis section explores how task similarity, reflected through task order, influences training results.\nAfter generating pseudo samples avoiding over-specialization, pseudo samples are used for rehearsal\ntraining. At the training stage of T i+1, the model is updated using pseudo samples of old tasks\nD1...i−1\np\n, Di\np and new task training data Di+1. Previous works (Huang et al., 2024; Zhao et al., 2024)\nfine-tune using LoRA (Hu et al., 2022) without adapting to task characteristics, making the results\nhighly sensitive to task order. Since the model already contains knowledge from earlier tasks, similar\nnew tasks should allow more knowledge transfer, whereas dissimilar ones require stronger constraints\nto maintain prior knowledge. To achieve this, we design a regularization loss based on task similarity,\nwhich is incorporated into the original cross-entropy loss to adjust LoRA fine-tuning:\nL = Lce + λ · Lreg,\n(1)\nwhere Lce is the standard cross-entropy loss, λ controls regularization strength, and Lreg is the\nregularization term, balancing knowledge sharing and parameter stability. Specifically, we impose\nthe regularization constraint on all training target parameters during optimization, as formulated in\nEquation 2:\nLreg = 1\n2\nX\ni\nE\n\u0002\n∥θi∥2\u0003\n.\n(2)\nWe modulate regularization strength λ based on task similarity W and rehearsal ratio rreplay. When\nrreplay is low or W is large, indicating limited rehearsal or low task similarity, stronger constraints\nare applied to stabilize knowledge retention. In contrast, higher rreplay or smaller W suggests task\nalignment, allowing more relaxed regularization to facilitate knowledge transfer. This dynamic\nadjustment enables SERS to maintain a balance between stability and plasticity, supporting more\nrobust continual learning. Accordingly, λ is formally defined in Equation 3:\nλ = [λmin +\n\u0000λmax −λmin\n\u0001\u0010\n1 −e−\nW\nWth\n\u0011\n] ∗\n\u00001 −rreplay\n\u0001\n,\n(3)\nwhere λmin and λmax control the range of regularization strength, and Wth adjusts the curvature of\nthe scaling function. The Wasserstein Distance (Chen et al., 2022; Liu et al., 2025), a representative\nof the optimal transport framework (Alvarez-Melis and Fusi, 2020), provides a metric for assessing\nthe similarity between the distributions of two datasets, which is defined as in Equation 4:\nW(P, Q) =\ninf\nγ∈Π(P,Q) E(X,Y )∼γ [∥X −Y ∥] .\n(4)\n4\nExperiment\n4.1\nDataset and Metrics\nOur experiments are conducted on the SuperNI (Wang et al., 2022) dataset, a large and comprehensive\nbenchmark for instruction tuning. For fair comparison, we adopt the same ten tasks as Huang et al.\n(2024), divided into two groups: one with five tasks and the other with ten. Each group is evaluated\nunder three different task orders. Experiments were carried out on LLaMA2-7B(Touvron et al., 2023)\nand ChatGLM-6B(GLM et al., 2024). For more details about the dataset, please refer to Appendix C.\nWe adopt the Rouge-L score (Lin, 2004) to assess generation quality, where Ri\nj denotes the model’s\nperformance on task j at stage i. To evaluate overall performance, knowledge transfer ability, and\nretention of prior knowledge, the following commonly used continual learning metrics are selected:\n(1) Average Rouge-L (AR).\nAfter training on the final task, the average performance across all\ntasks is computed as shown in Equation 5:\nAR = 1\nN\nN\nX\ni=1\nRN\ni .\n(5)\n6\nTable 1: Results on LLaMA2-7B and ChatGLM-6B under different task orders and settings.\nModel\nOrder 1\nOrder 2\nOrder 3\nAvg.\nAR ↑\nBWT ↑\nAR ↑\nBWT ↑\nAR ↑\nBWT ↑\nAR ↑\nBWT ↑\nLLaMA2-7B 5Tasks\nMTL\n53.07\n–\n53.07\n–\n53.07\n–\n53.07\n–\nKMeansSel(1%)\n49.73\n-5.22\n50.14\n-4.17\n50.12\n-3.61\n50.00\n-4.33\nL2\n28.62\n-28.99\n29.22\n-28.45\n28.33\n-30.71\n28.72\n-29.38\nSAPT\n50.47\n-3.75\n51.04\n-2.98\n50.22\n-4.37\n50.58\n-3.70\nSSR\n51.33\n-1.97\n52.41\n-1.18\n52.02\n-1.01\n51.92\n-1.39\nSERS\n52.90\n-0.55\n53.01\n-0.27\n52.84\n-0.63\n52.92\n-0.48\nChatGLM-6B 5Tasks\nMTL\n48.92\n–\n48.92\n–\n48.92\n–\n48.92\n–\nKMeansSel(1%)\n43.72\n-5.64\n43.74\n-5.07\n45.13\n-4.37\n44.19\n-5.02\nL2\n25.19\n-35.32\n26.46\n-32.47\n26.18\n-34.92\n25.94\n-34.24\nSAPT\n49.01\n-1.54\n48.65\n-2.11\n49.23\n-1.89\n48.96\n-1.84\nSSR\n48.95\n-2.12\n49.02\n-1.94\n49.38\n-0.51\n49.11\n-1.52\nSERS\n49.97\n-0.89\n49.86\n-1.17\n50.04\n-0.48\n49.98\n-0.85\nLLaMA2-7B 10Tasks\nMTL\n64.72\n–\n64.72\n–\n64.72\n–\n64.72\n–\nKMeansSel(1%)\n59.13\n-5.88\n60.71\n-5.39\n60.44\n-7.17\n60.09\n-6.15\nL2\n33.13\n-28.99\n34.71\n-25.12\n37.02\n-22.71\n34.95\n-25.42\nSAPT\n62.51\n-2.06\n61.90\n-2.81\n62.29\n-2.30\n62.23\n-2.39\nSSR\n62.29\n-1.84\n62.64\n-1.86\n62.36\n-3.95\n62.43\n-2.55\nSERS\n63.42\n-1.72\n63.45\n-1.11\n64.46\n-2.27\n63.78\n-1.7\nChatGLM-6B 10Tasks\nMTL\n62.04\n–\n62.04\n–\n62.04\n–\n62.04\n–\nKMeansSel(1%)\n60.84\n-5.41\n61.24\n-4.77\n61.04\n-5.27\n61.04\n-5.15\nL2\n40.18\n-29.71\n41.37\n-28.66\n41.99\n-26.12\n41.18\n-28.16\nSAPT\n61.30\n-3.44\n61.56\n-2.73\n60.87\n-2.92\n61.24\n-3.03\nSSR\n62.68\n-1.79\n62.27\n-2.42\n61.80\n-1.56\n62.25\n-1.92\nSERS\n63.30\n-1.17\n63.22\n-1.52\n63.16\n-1.49\n63.23\n-1.39\n(2) Backward Transfer (BWT).\nBWT measures the degree to which the learning of subsequent\ntasks affects the performance of the learned tasks, which is defined as:\nBWT =\n1\nN −1\nN−1\nX\ni=1\n(RN\ni −Ri\ni).\n(6)\n4.2\nExperiment Details\nAll experiments were conducted on a single A100 GPU. For pseudo-sample generation, 1% of\nreal samples are used to create pseudo samples equivalent to 10% of the training data. In the self-\nquestioning stage, we set k = 20, α = 0.5 for LLaMA2-7B, and k = 10, α = 0.6 for ChatGLM-6B\nto control the selection and evolution of over-specialized samples. LoRA is used for fine-tuning, and\nWasserstein distance based on model embeddings guides the regularization process.\n4.3\nExperiment Results\nWe compare our SERS method with several representative baselines, including the classic rehearsal-\nbased KMeansSel, which selects real samples via KMeans clustering; the advanced pesudo-sample\nrehearsal approach SSR (Huang et al., 2024), which leverages self-synthesis to generate pseudo\nsamples for rehearsal; advanced structure-based method SAPT (Zhao et al., 2024), which employs\na Shared Attentive Learning & Selection module to align the PET learning and selection; and the\nregularization-based L2 method. A multi-task learning (MTL) baseline, which jointly trains all tasks\nwithout considering forgetting, is also included for reference. As shown in Table 1, SERS consistently\n7\nTable 2: The ablation studies on each proposed module. SGG refers to our Semantic-Guided Pseudo-\nInput Generation. LSE denotes Label Self-Evolution strategy. SR represents Similarity Regularization.\nA “✓“ indicates that our module is applied, while a“–“ denotes the use of a corresponding strategy\nfrom existing advanced pseudo-rehearsal approaches.\nAblation Setting\nLLaMA-7B AR (%) ↑\nChatGLM-6B AR (%) ↑\nSGG\nLSE\nSR\nOrder 1\nOrder 2\nOrder 3\nOrder 1\nOrde r2\nOrder 3\n–\n–\n–\n51.33\n52.41\n52.02\n48.95\n49.02\n49.38\n✓\n–\n–\n52.37\n52.63\n52.54\n49.87\n49.58\n49.35\n–\n✓\n–\n51.99\n52.83\n52.33\n49.25\n49.40\n49.02\n–\n–\n✓\n52.40\n52.42\n52.61\n49.68\n49.62\n49.50\n✓\n✓\n–\n52.47\n52.78\n52.71\n49.78\n49.33\n49.80\n–\n✓\n✓\n52.56\n52.92\n52.73\n49.08\n49.43\n49.21\n✓\n–\n✓\n52.59\n52.95\n52.56\n49.70\n49.80\n49.62\n✓\n✓\n✓\n52.90\n53.01\n52.84\n49.97\n49.86\n50.04\n1\n2\n3\n4\n5\nTraining Stage\n61.5\n62.0\n62.5\n63.0\n63.5\n64.0\n64.5\n65.0\n65.5\n66.0\n66.5\nAR(%)\nTRANS\n1\n2\n3\n4\n5\nTraining Stage\n97.5\n98.0\n98.5\n99.0\n99.5\n100.0\nSA\n1\n2\n3\n4\n5\nTraining Stage\n33.0\n33.5\n34.0\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\nQA\n1\n2\n3\n4\n5\nTraining Stage\n25.0\n25.5\n26.0\n26.5\n27.0\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n38.5\n39.0\n39.5\nQG\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\nFigure 4: Ablation results detailing the performance variations of the LLaMA2-7B model across a\n5tasks sequence under Order 1 (TRANS →SA →QA →SUM →QG). More details of ablation\nresults are shown in Figure 9\noutperforms these baselines and maintains stable performance across task orders, demonstrating\nthe effectiveness of similarity regularization. On LLaMA2-7B, it achieves results close to MTL,\nsurpassing the next best method by 2.16% and exhibiting significantly lower BWT. Notably, in\nChatGLM-6B, where MTL suffers from task confusion due to global attention and 2D positional\nencoding, SERS surpasses MTL by incrementally refining decision boundaries through rehearsal\nwith staged updates.\n5\nAblation and Comparison Experiments\n5.1\nModule Ablation\nIn this section, we carry out ablation studies to verify the effectiveness of each module. All experi-\nments are on 5tasks, and we measure performance with the AR metric. The results appear in Table 2,\nand detailed ablation results are provided in Figure 4. SERS introduces three core improvements:\nSemantic-Guided Generation, Label Self-Evolution, and Similarity Regularization. We evaluate\nthe effectiveness of these components with different configuration settings. For settings that do not\ninclude the SERS modules, we adopt corresponding strategies from Huang et al. (2024), where pseudo\nsamples are generated via in-context learning, pseudo labels are directly refined on the task-specific\nmodel, and no regularization method is applied. The settings with semantic-guided generation\nachieve higher overall performance compared to those using existing advanced method to generate\npseudo samples. The curves with task similarity are more likely to exhibit task-level performance\n8\nimprovement during training, and those incorporating label self-evolution tend to perform better on\nnew tasks, demonstrating the effectiveness of our proposed improvements.\n5.2\nData Utilization Efficiency\nIn our experiments, we first employed 1% of real samples to synthesize 10% pseudo samples,\nachieving strong performance with efficient data utilization. We then extended the analysis by\ngenerating different amounts of pseudo samples for rehearsal using various proportions of real data\n(1%, 0.75%, 0.5%, and 5%) to investigate the trade-off between data efficiency and pseudo-sample\nredundancy. As shown in Figure 5, even a small number of real samples can produce pseudo\nsamples that are diverse and capable of capturing the underlying task knowledge. However, as the\npseudo-sample ratio increases, the improvement in AR gradually saturates. When generating 20%\npseudo samples from 1% real data, performance begins to decline due to excessive redundancy and\ninterference with learning new tasks.\nWhen further reducing the number of real samples, we observe that using 0.75% or 0.5% of real data\nyields slightly better performance than 1% real data when synthesizing a small proportion of pseudo\nsamples. Nevertheless, the performance degrades notably as the pseudo-sample ratio grows. This\nsuggests that with a small pseudo-sample ratio, fewer real samples can better capture the essential\ntask knowledge and improve synthesis quality. In contrast, when a larger number of pseudo samples\nare generated, the limited diversity of real samples leads to higher redundancy, which hinders learning\neffectiveness. Moreover, pseudo samples generated from a larger real dataset tend to form more\ncluster centers. Under low pseudo-sample ratios, this results in less coherent knowledge structures\nand slightly worse performance than using 1% real data. Yet, as the pseudo-sample ratio increases to\n20%, the performance improves substantially and approaches that of multi-task learning (MTL).\nWe also compare pseudo-sample rehearsal with real-sample rehearsal, as presented in Table 5 of\nthe Appendix E. The results are consistent with findings from SSR, showing that even when 10%\nof real samples are replayed, the performance remains inferior to that of pseudo-sample rehearsal.\nThis is because labels synthesized by the old model facilitate learning, improving the new model’s\ntask adaptation. In contrast, real-sample rehearsal is directly constrained by the limited proportion of\navailable real data, whereas pseudo-sample rehearsal can flexibly expand data diversity by synthesiz-\ning new samples from a fixed real set. Consequently, the 1% real-sample rehearsal fails to match\nthe performance achieved with 5% real samples, as the smaller rehearsal ratio restricts the model’s\nability to preserve prior knowledge.\n5.3\nAnalysis of Parameters\nWe analyze the impact of two key parameters in label self-evolution. The proportion threshold k\ndetermines the selection of over-specialized samples during self-questioning. As shown in Figure 6,\nsetting k too high omits valuable specialized knowledge, while a low k allows too many over-\nspecialized samples for rehearsal, causing bias in model parameters and affecting overall performance.\nα controls the balance between general (LLM 0) and task-specific (LLM i) knowledge when refining\nlabels. Figure 6 illustrates that a high α may tend to less accurate labels, whereas a low α reduces the\nsmoothing effect, weakening the integration of general and specialized knowledge.\nParameter adjustment should consider model capability. Stronger mask-filling models better preserve\nprior knowledge in pseudo samples, allowing for a smaller k; weaker models require a larger k to\navoid excessive specialization. Similarly, models with stronger downstream abilities benefit from a\nhigher α to increase the general knowledge in over-specialized samples, while less capable models\nrequire a lower α to avoid inaccurate labels.\n6\nConclusion\nIn this work, we propose SERS for catastrophic forgetting mitigation in LLMs.\nSERS gen-\nerates pseudo samples that better reflect the structural knowledge of previous tasks, prevents\nover-specialization on rehearsal pseudo samples from harming overall performance and dynamically\nadjusts regularization strength based on the similarity between previous and new tasks. Extensive\nexperiments demonstrate that, compared to various representative methods, SERS achieves more\n9\n1%\n5%\n10%\n15%\n20%\nPseudo Sample Ratio\n51.5\n52.0\n52.5\n53.0\nAR(%)\nData Utilization Efficiency Study\nMTL\n0.5% Real Sample\n0.75% Real Sample\n1% Real Sample\n5% Real Sample\nFigure 5: Rehearsal Analysis. We generate\nvarious proportions of pseudo samples us-\ning different amounts of real samples rang-\ning from 0.5% to 5% on LLaMA2-7B to\nevaluate data utilization efficiency.\n0.1\n0.2\n0.3\n0.4\n0.5\nk values\n51\n52\n53\nAR (%)\nComparison Study of k (α=0.5)\n0.3\n0.4\n0.5\n0.6\n0.7\nα values\n51\n52\n53\nAR (%)\nComparison Study of α (k=0.2)\nOrder1\nOrder2\nOrder3\nFigure 6: Parameter Analysis. We evaluate SERS\nperformance on LLaMA2-7B by varying k and α\nwhile fixing the other parameter respectively.\neffective forgetting mitigation and enhanced performance stability, underscoring SERS’s potential as\na general solution for continual learning in LLMs.\nLimitations\nAlthough ablations confirm each module’s contribution to AR and show score performance during\ntraining, the complex relationships between tasks make it hard to pinpoint why some tasks improve\nor decline. A deeper analysis of how new tasks affect previous tasks may unlock further gains in\ncontinual accuracy. Moreover, while pseudo-sample rehearsal boosts review of past knowledge, it\nremains unclear whether these synthetic examples can introduce knowledge beyond the original data.\nExploring the ability of pseudo samples to enrich the model with unseen knowledge could be key to\nsurpassing MTL in future continual learning work.\nAcknowledgments and Disclosure of Funding\nThis work is supported by the National Key Research and Development Program of China\n(2023YFC2705700), the National Natural Science Foundation of China (Grant No. 62225113,\nU23A20318, U2336211 and 62276195), the Foundation for Innovative Research Groups of Hubei\nProvince (Grant No. 2024AFA017) and the Science and Technology Major Project of Hubei Province\n(Grant No. 2024BAB046). The numerical calculations in this paper have been done on the supercom-\nputing system in the Supercomputing Center of Wuhan University.\nReferences\nDavid Alvarez-Melis and Nicolo Fusi. Geometric dataset distances via optimal transport. Advances\nin Neural Information Processing Systems, 33:21428–21439, 2020.\nYao Chen, Qingyi Gao, and Xiao Wang. Inferential wasserstein generative adversarial networks.\nJournal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):83–113, 2022.\nCyprien de Masson D’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic\nmemory in lifelong language learning. Advances in Neural Information Processing Systems, 32,\n2019.\nTeam GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas,\nGuanyu Feng, Hanlin Zhao, et al. Chatglm: A family of large language models from glm-130b to\nglm-4 all tools. arXiv preprint arXiv:2406.12793, 2024.\n10\nYanhui Guo, Shaoyuan Xu, Jinmiao Fu, Jia Liu, Chaosheng Dong, and Bryan Wang. Q-tuning:\nQueue-based prompt tuning for lifelong few-shot language learning. In Findings of the Association\nfor Computational Linguistics: NAACL 2024, pages 2595–2622, 2024.\nZeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning\nfor large models: A comprehensive survey. arXiv preprint arXiv:2403.14608, 2024.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?id=\nnZeVKeeFYf9.\nJianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao,\nand Jinsong Su. Mitigating catastrophic forgetting in large language models with self-synthesized\nrehearsal. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1416–1428, 2024.\nXisen Jin, Bill Yuchen Lin, Mohammad Rostami, and Xiang Ren. Learn continually, generalize\nrapidly: Lifelong knowledge accumulation for few-shot learning. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages 714–729, 2021.\nZixuan Ke, Bing Liu, Wenhan Xiong, Asli Celikyilmaz, and Haoran Li. Sub-network discovery\nand soft-masking for continual learning of mixed tasks.\nIn Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages 15090–15107, 2023.\nHongyu Li, Liang Ding, Meng Fang, and Dacheng Tao. Revisiting catastrophic forgetting in large\nlanguage model tuning. In Findings of the Association for Computational Linguistics: EMNLP\n2024, pages 4297–4308, 2024.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pages 74–81, 2004.\nXinran Liu, Yikun Bai, Yuzhe Lu, Andrea Soltoggio, and Soheil Kolouri. Wasserstein task embedding\nfor measuring task similarities. Neural Networks, 181:106796, 2025.\nAru Maekawa, Hidetaka Kamigaito, Kotaro Funakoshi, and Manabu Okumura. Generative replay\ninspired by hippocampal memory indexing for continual language learning. In Proceedings of the\n17th Conference of the European Chapter of the Association for Computational Linguistics, pages\n930–942, 2023.\nMartial Mermillod, Aurélia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investi-\ngating the continuum from catastrophic forgetting to age-limited learning effects, 2013.\nKeqin Peng, Liang Ding, Qihuang Zhong, Yuanxin Ouyang, Wenge Rong, Zhang Xiong, and Dacheng\nTao. Token-level self-evolution training for sequence-to-sequence learning. In Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),\npages 841–850, 2023.\nWeijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin. Analyzing and reducing\ncatastrophic forgetting in parameter efficient tuning. arXiv preprint arXiv:2402.18865, 2024.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience\nreplay for continual learning. Advances in neural information processing systems, 32, 2019.\nAvi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J\nLiu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for\nproblem-solving with language models. Transactions on Machine Learning Research, 2023.\nYuncheng Song, Liang Ding, Changtong Zan, and Shujian Huang.\nSelf-evolution knowledge\ndistillation for llm-based machine translation. In Proceedings of the 31st International Conference\non Computational Linguistics, pages 10298–10308, 2025.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. Lamol: Language modeling for lifelong language\nlearning. In International Conference on Learning Representations, 2020.\n11\nHuashan Sun and Yang Gao. Reviving dormant memories: Investigating catastrophic forgetting in\nlanguage models through rationale-guidance difficulty. arXiv preprint arXiv:2411.11932, 2024.\nZhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang,\nDacheng Tao, and Jingren Zhou. A survey on self-evolution of large language models. CoRR,\n2024.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nMingyang Wang, Heike Adel, Lukas Lange, Jannik Strötgen, and Hinrich Schütze. Rehearsal-free\nmodular and compositional continual learning for language models. In Proceedings of the 2024\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 2: Short Papers), pages 469–480, 2024a.\nXiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and\nXuanjing Huang. Orthogonal subspace learning for language model continual learning. In The\n2023 Conference on Empirical Methods in Natural Language Processing, 2023.\nYifan Wang, Yafei Liu, Chufan Shi, Haoling Li, Chen Chen, Haonan Lu, and Yujiu Yang. Inscl: A\ndata-efficient continual learning paradigm for fine-tuning large language models with instructions.\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\n663–677, 2024b.\nYihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit S Dhillon, and\nSanjiv Kumar. Two-stage llm fine-tuning with less specialization and more generalization. In The\nTwelfth International Conference on Learning Representations, 2024c.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al.\nSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages\n5085–5109, 2022.\nTongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari.\nContinual learning for large language models: A survey. CoRR, 2024.\nWenpeng Yin, Jia Li, and Caiming Xiong. Contintin: Continual learning from task instructions. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 3062–3072, 2022.\nWeixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu Zhang, Qing Yang,\nDongliang Xu, and Wanxiang Che. Sapt: A shared attention framework for parameter-efficient\ncontinual learning of large language models. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 11641–11661, 2024.\nHaoqi Zheng, Qihuang Zhong, Liang Ding, Zhiliang Tian, Xin Niu, Changjian Wang, Dongsheng\nLi, and Dacheng Tao. Self-evolution learning for mixup: Enhance data augmentation on few-shot\ntext classification tasks. In The 2023 Conference on Empirical Methods in Natural Language\nProcessing, 2023.\nJunhao Zheng, Xidi Cai, Shengjie Qiu, and Qianli Ma. Spurious forgetting in continual learning\nof language models. In The Thirteenth International Conference on Learning Representations,\n2025a.\nJunhao Zheng, Shengjie Qiu, Chengming Shi, and Qianli Ma. Towards lifelong learning of large\nlanguage models: A survey. ACM Computing Surveys, 57(8):1–35, 2025b.\nQihuang Zhong, Liang Ding, Yibing Zhan, Yu Qiao, Yonggang Wen, Li Shen, Juhua Liu, Baosheng\nYu, Bo Du, Yixin Chen, et al. Toward efficient language model pretraining and downstream\nadaptation via self-evolution: A case study on superglue. arXiv preprint arXiv:2212.01853, 2022.\n12\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. Self-evolution learning for\ndiscriminative language model pretraining. In Findings of the Association for Computational\nLinguistics: ACL 2023, pages 4130–4145, 2023.\n13\nA\nComparative Case Study on Pseudo-Sample Generation\nTo illustrate the effectiveness of our proposed method, we present a comparison between the traditional\nICL approach and the Semantic-Guided Pseudo-Input Generation we introduced. Using two samples\nfrom the same task, we generate pseudo data in two ways for two retries: the ICL method generates\nboth inputs and outputs with 2-shot prompting, while our method focuses on generating only pseudo\ninputs. As shown in Figure 7, it is evident that the 2-shot ICL approach fails to capture the essence of\nthe “cosmosqa“ task. The generated questions resemble generic QA samples, lacking the implicit\ncues that emphasize common-sense reasoning. In contrast, our method is able to produce samples that\nclosely resemble real ones under a low masking ratio. When the masking ratio is high, the generated\noutputs are diverse but still retain the core features of the“cosmosqa“ task. This not only ensures the\ndiversity of pseudo samples, but also supports the model’s review of previous knowledge.\nReal Sample:\nInput:\nEmmy isn’t feeling well. I woke up this morning with a little bit of vom on the bed near her mouth. Dog’s\npuke all the time though, so no big deal. I let her out and she took a big old solid 2-point conversion. But\nshe didn’t run or scuff her feet afterwards, she just ambled back inside.\nQuestion: What did I mean when I said she took a solid 2-point conversion?\nOutput: She pooped.\nInput:\nblaine was only able to stay for a little while since he had to work sunday and while i wanted to go home\nwith him to NOT face the party the rest of the evening, i stayed with nick and brenna. i had such a good\ntime though. there was making out involved among some of the party guests. oh, might i add that the party\ntook place on the rooftop of this lovely apartment building.\nQuestion: Where might James have gone after leaving the party?\nOutput: He went home.\nIn-Context Learning Output:\nInput:\nThe best thing about being single is you can eat\nwhatever you want whenever you feel like eating\nit!\nQuestion:\nWhen would Katie most likely be\nthinking these thoughts?\nOutput: After having an argument over dinner at\nsomeone else’s house.\nInput:\nWhenever we meet up at a cafe or something, he\nalways seems very keen to see me again, and wants\nus to do things together all the time! But then when\nI’m ready to leave, his eyes get all glassy and sad\nlooking and he says Ï don’t want you to think badly\nabout me.Ït makes me feel like maybe my presence\nisn’t actually welcome - I can tell he doesn’t really\nenjoy being around me, even though he tries hard\nto hide it.\nQuestion: What does the speaker most likely think\nabout herself?\nOutput: She thinks highly of herself.\nSemantic-Guided Generation:\nInput:\nBlaine was reluctant to leave early because he had\nto work Sunday and wanted to spend more time\nwith Nick and Brenna. He decided not to go home\nwith them but instead to return later tonight. I\nwas having such a good time at the party as were\nmany others who made it into the exclusive venue\nlocated on top of this amazing apartment complex.\nOh, might I add that the party location was right\noutside our living room window!\nQuestion: Where else could James have gone after\nleaving the party?\nInput:\nBlaine seemed like such a nice guy since it came\ndown to him being the last person standing at the\nbar Sunday night. He wanted me to stay for just\na little longer; however, I didn’t want to face the\nremainder of the evening alone. So I went home\nwith Nick and Brenna instead. They were very\nkind and offered us plenty of space even if we\nmade our own drinks or snacks from what they\nprovided. Oh! And by the way, their roof-top patio\nwas absolutely gorgeous!\nQuestion: What would you think about James\nbased upon his behavior\nFigure 7: A case study for Pseudo-Sample Generation\n14\nB\nComparative Case Study on Output Refinement\nThis part demonstrates the value of self-evolution in label refinement through real examples. By\nadjusting the balance between general knowledge from LLM 0 and task-specific knowledge from\nLLM i, the model can reduce overly specialization and produce more acceptable responses. In the\nQA example shown in Figure 8, the output from LLM 0 contains hallucinations and fails to provide a\ncorrect answer. In contrast, the output from LLM i correctly identifies the key term \"family\". The\nfinal response refined through self-evolution also delivers a precise and reasonable answer.\nInput: A small part of her face lit up when she heard the news. Her eyes glistened but there\nwasn’t any trace of joy on them or happiness even though this was great news. She simply\nlooked relieved because finally things were starting to go well for her family. And they had been\nthrough hell lately so every little thing counted nowadays. So she couldn’t help feeling grateful.\nQuestion: What made the person feel relief?\nLLM 0 Output: Things went wrong recently!\nLLM i Output: They got good news for their family.\nOutput after Self-Evolution: The person was told that things was going to be okay.\nFigure 8: A case study for output refinement\nC\nAdditional Experimental Details\nAll samples are selected from the SuperNI dataset. To ensure fair comparison, we follow the same\ntask settings as advanced pseudo-sample rehearsal methods (Huang et al., 2024). The selected tasks\nare shown in Table 3. During pseudo-sample generation, we begin with 1% of the real samples\nand apply the Semantic-Guided Pseudo-Input Generation method with three retries to create a large\nnumber of pseudo samples. Then, we use the K-Means algorithm to select 10% of these for rehearsal.\nDuring the training phase, we selected two numbers of tasks and two task sequences to form four\ntask chains, with the task chains as shown in Table 4.\nTable 3: Details of task names and abbreviations\nAbbreviation\nTask Name\nQA\ntask024_cosmosqa_answer_generation\nQG\ntask074_squad1.1_question_generation\nSA\ntask1312_amazonreview_polarity_classification\nSUM\ntask511_reddit_tifu_long_text_summarization\nTRANS\ntask1219_ted_translation_en_es\nDSG\ntask574_air_dialogue_sentence_generation\nEXPL\ntask192_hotpotqa_sentence_generation\nPARA\ntask177_para-nmt_paraphrasing\nPOS\ntask346_hybridqa_classification\nPE\ntask064_all_elements_except_first_i\nTable 4: Details of Task Chains under Different Task Numbers and Orders\nSettings\nTask Chain\n5Tasks Order 1\nTRANS →SA →QA →SUM →QG\n5Tasks Order 2\nQA →QG →SA →SUM →TRANS\n5Tasks Order 3\nSUM →QG →TRANS →QA →SA\n10Tasks Order 1\nTRANS →SA →QA →SUM →QG →PE →PARA →POS →DSG →EXPL\n10Tasks Order 2\nQA →QG →SA →SUM →TRANS →DSG →EXPL →PARA →PE →POS\n10Tasks Order 3\nSUM →QG →TRANS →QA →SA →PARA →DSG →POS →EXPL →PE\n15\nD\nAblation Study Details\nIn this section, we present the detailed performance of each ablation setting across different task chains\nand models. The results show that our generation strategy consistently leads to better outcomes, label\nself-evolution generally benefits the learning of new tasks, and similarity regularization facilitates\nknowledge transfer, increasing the likelihood of performance gains throughout training. Details are\nshown in Figure 9.\n1\n2\n3\n4\n5\nTraining Stage\n32.0\n32.5\n33.0\n33.5\n34.0\n34.5\n35.0\n35.5\n36.0\nAR(%)\nQA\n1\n2\n3\n4\n5\nTraining Stage\n37.5\n38.0\n38.5\n39.0\n39.5\n40.0\n40.5\nQG\n1\n2\n3\n4\n5\nTraining Stage\n98.0\n98.5\n99.0\n99.5\n100.0\nSA\n1\n2\n3\n4\n5\nTraining Stage\n26.0\n26.5\n27.0\n27.5\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n64.5\n65.0\n65.5\n66.0\nTRANS\nAblation Study on LLaMA2-7B Order2\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\n1\n2\n3\n4\n5\nTraining Stage\n24.5\n25.0\n25.5\n26.0\n26.5\n27.0\n27.5\nAR(%)\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n36.0\n36.5\n37.0\n37.5\n38.0\n38.5\n39.0\n39.5\n40.0\nQG\n1\n2\n3\n4\n5\nTraining Stage\n63.0\n63.5\n64.0\n64.5\n65.0\n65.5\n66.0\nTRANS\n1\n2\n3\n4\n5\nTraining Stage\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\nQA\n1\n2\n3\n4\n5\nTraining Stage\n99.0\n99.5\n100.0\nSA\nAblation Study on LLaMA2-7B Order3\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\n1\n2\n3\n4\n5\nTraining Stage\n53.5\n54.0\n54.5\n55.0\n55.5\n56.0\n56.5\n57.0\n57.5\n58.0\n58.5\n59.0\n59.5\n60.0\n60.5\nAR(%)\nTRANS\n1\n2\n3\n4\n5\nTraining Stage\n98.0\n98.5\n99.0\n99.5\nSA\n1\n2\n3\n4\n5\nTraining Stage\n31.5\n32.0\n32.5\n33.0\n33.5\n34.0\n34.5\nQA\n1\n2\n3\n4\n5\nTraining Stage\n21.0\n21.5\n22.0\n22.5\n23.0\n23.5\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n36.5\n37.0\n37.5\n38.0\nQG\nAblation Study on ChatGLM-6B Order1\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\n16\n1\n2\n3\n4\n5\nTraining Stage\n30.0\n30.5\n31.0\n31.5\n32.0\n32.5\n33.0\n33.5\n34.0\n34.5\nAR(%)\nQA\n1\n2\n3\n4\n5\nTraining Stage\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\n37.5\n38.0\n38.5\n39.0\nQG\n1\n2\n3\n4\n5\nTraining Stage\n98.0\n98.5\n99.0\n99.5\nSA\n1\n2\n3\n4\n5\nTraining Stage\n21.5\n22.0\n22.5\n23.0\n23.5\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n58.5\n59.0\n59.5\n60.0\nTRANS\nAblation Study on ChatGLM-6B Order2\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\n1\n2\n3\n4\n5\nTraining Stage\n21.5\n22.0\n22.5\n23.0\n23.5\n24.0\nAR(%)\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\n37.5\n38.0\nQG\n1\n2\n3\n4\n5\nTraining Stage\n56.5\n57.0\n57.5\n58.0\n58.5\n59.0\n59.5\n60.0\n60.5\nTRANS\n1\n2\n3\n4\n5\nTraining Stage\n32.0\n32.5\n33.0\n33.5\n34.0\nQA\n1\n2\n3\n4\n5\nTraining Stage\n98.0\n98.5\n99.0\n99.5\nSA\nAblation Study on ChatGLM-6B Order3\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\nFigure 9: Ablation results detailing the performance variations of different models across different\ntask chains\nE\nReal-Sample Rehearsal Details\nThis section presents additional results on real-sample rehearsal for comparison with pseudo-sample\nrehearsal. As shown in Table E, even when 10% of real samples are used for rehearsal under the same\ncontinual learning setup, the performance remains lower than that of pseudo-sample rehearsal. This\nobservation can be explained by the fact that labels synthesized by the old model facilitate learning,\nimproving the new model’s task adaptation. In contrast, real-sample rehearsal is constrained by the\nlimited number of available samples, resulting in reduced diversity and weaker knowledge coverage.\nConsequently, its performance degrades more noticeably under low rehearsal ratios.\nTable 5: Real-Sample Rehearsal Results on LLaMA2-7B\nData Rehearsal\nOrder1\nOrder2\nOrder3\nAvg\n1% real samples\n48.11\n49.02\n48.74\n48.62\n5% real samples\n50.18\n50.65\n50.02\n50.28\n10% real samples\n50.24\n51.09\n50.84\n50.73\n1% real samples synthesis 10% pseudo samples\n52.90\n53.01\n52.84\n52.92\nF\nComparison Study Details\nIn this section, we provide additional experimental details on the ChatGLM-6B model to demonstrate\nthe impact of the hyperparameters k and α on the SERS framework.\n17\n0.1\n0.2\n0.3\n0.4\n0.5\nk values\n46\n47\n48\n49\n50\nAR(%)\nComparison Study of k on ChatGLM-6B\nOrder1\nOrder2\nOrder3\nFigure 10: Comparison study of k (α=0.6) on ChatGLM-6B\n0.3\n0.4\n0.5\n0.6\n0.7\n values\n48.5\n49.0\n49.5\n50.0\nAR(%)\nComparison Study of  on ChatGLM-6B\nOrder1\nOrder2\nOrder3\nFigure 11: Comparison study of α (k=0.1) on ChatGLM-6B\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: In the abstract and introduction, we clearly demonstrate the contribution and\nscope of this paper.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\n18\nAnswer: [Yes] ,\nJustification: In Section 6, we have thoroughly discussed the limitations of our article,\nhoping to guide more future work.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: In Section 3, we elaborated on the motivation and theoretical derivation of our\nmethod, with a complete proof process in place.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\n19\nJustification: We have provided detailed descriptions of the experimental details in section\n4.2 and methods in section 3 to ensure that our experiment can be reproduced.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: Our datasets are derived from publicly available datasets, and our code will\nalso be fully open-sourced.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n20\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: In Section 4.2, we presented the experimental setup and the selection of key\nparameters. Additional details could refer to our code.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [No]\nJustification: Due to the computational cost of continual learning, we did not perform\nmultiple runs for each experiment.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\n21\nJustification: In Section 4.2, we have provided sufficient information on the computer\nresources needed to reproduce the experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We guarantee that the research conducted in the paper complies with NeurIPS\nCode of Ethics in all aspects.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We outlined the societal benefits of continual learning research in Section 1,\nand highlighted the limitations and challenges of existing techniques.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n22\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper poses no such risks.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: The creators or original owners of the assets used in the paper, such as code,\ndata, and models, have been appropriately recognized, and the licenses and terms of use\nhave been clearly mentioned and properly respected.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: The paper does not release new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n23\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [Yes]\nJustification: This study consistently adheres to relevant policies governing the use of LLMs\nand provides a detailed description of their application.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n24\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Self-Evolving Pseudo-Rehearsal for Catastrophic\nForgetting with Task Similarity in LLMs\nJun Wang1∗Liang Ding2∗Shuai Wang1\nHongyu Li3\nYong Luo1† Huangxuan Zhao1†\nHan Hu4\nBo Du1\n1School of Computer Science, National Engineering Research Center of Multimedia Software\nand Hubei Key Laboratory of Multimedia and Network Communication Engineering,\nWuhan University, Wuhan, China\n2The University of Sydney, Australia\n3Individual Researcher\n4School of Information and Electronics, Beijing Institute of Technology, Beijing, China\n{junwang_ai, wangshuai123, luoyong, zhaohuangxuan, dubo}@whu.edu.cn\n{liangding.liam, hongyuli102799}@gmail.com\nhhu@bit.edu.cn\nAbstract\nContinual learning for large language models (LLMs) demands a precise balance\nbetween plasticity - the ability to absorb new tasks - and stability - the preservation\nof previously learned knowledge. Conventional rehearsal methods, which replay\nstored examples, are limited by long-term data inaccessibility; earlier pseudo-\nrehearsal methods require additional generation modules, while self-synthesis\napproaches often generate samples that poorly align with real tasks, suffer from\nunstable outputs, and ignore task relationships. We present Self-Evolving Pseudo-\nRehearsal for Catastrophic Forgetting with Task Similarity (SERS), a lightweight\nframework that 1) decouples pseudo-input synthesis from label creation, using\nsemantic masking and template guidance to produce diverse, task-relevant prompts\nwithout extra modules; 2) applies label self-evolution, blending base-model pri-\nors with fine-tuned outputs to prevent over-specialization; and 3) introduces a\ndynamic regularizer driven by the Wasserstein distance between task distributions,\nautomatically relaxing or strengthening constraints in proportion to task similarity.\nExperiments across diverse tasks on different LLMs show that our SERS reduces\nforgetting by over 2% points against strong pseudo-rehearsal baselines, by ensuring\nefficient data utilization and wisely transferring knowledge. The code will be\nreleased at https://github.com/JerryWangJun/LLM_CL_SERS/.\n1\nIntroduction\nEnabling large language models (LLMs) to acquire new knowledge continuously (Wu et al., 2024;\nZheng et al., 2025b) holds significant importance for developing artificial intelligence systems with\nlifelong learning abilities. While practical applications demand LLMs continually adapt to evolving\ndownstream tasks, conventional learning methods (Hu et al., 2022; Han et al., 2024) often struggle to\npreserve existing capabilities during such situations. Continual learning enables LLMs to flexibly\nintegrate new and existing knowledge as tasks increase, addressing the limitations of static training\nin preserving prior performance while incorporating new information. The core challenge lies in\nachieving an optimal balance between plasticity and stability (Mermillod et al., 2013). Excessive\nplasticity will result in catastrophic forgetting, whereas overly strong stability may prevent efficient\nand effective knowledge transfer.\n∗Equal contribution\n†Corresponding authors.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n"
    },
    {
      "page_number": 2,
      "text": "60\n40\n20\n0\n20\n40\n60\n60\n40\n20\n0\n20\n40\n60\nReal Samples\nSelf-Synthesis\nOur Method\nQA\nQG\nSA\nSUM\nTRANS\nFigure 1: Clustering analysis of pseudo samples generated by our method (SERS) and Self-\nSynthesized Rehearsal (SSR) approaches across five tasks, alongside real samples. It can be observed\nthat the pseudo samples generated by our method are closer to the real samples than SSR, indicating\nthat SERS produces more similar pseudo samples that better reflect knowledge from previous tasks.\nA series of works (Zhao et al., 2024; Zheng et al., 2025a; Wang et al., 2024a; Sun and Gao, 2024)\nhave been proposed to mitigate this balancing challenge. Rehearsal-based methods (Yin et al., 2022;\nde Masson D’Autume et al., 2019; Rolnick et al., 2019) preserve model capabilities on previous tasks\nby utilizing real samples from prior training processes, which are not always consistently available in\npractice. To tackle the challenge of limited access to get historical data, existing solutions (Sun et al.,\n2020; Zhao et al., 2024) apply pseudo-sample generation, yet the additional generation modules\nincrease the number of trainable parameters. Huang et al. (2024) leverage in-context learning capacity\nof LLMs for self-synthesis rehearsal, effectively alleviating parameter burdens. We unexpectedly\nfound that self-synthesized samples often exhibit low similarity to real data, failing to adequately\nreflect the knowledge structure and thus undermining the effectiveness of rehearsals, as shown\nby the clustering analysis in Figure 1. Regularization-based approaches (Guo et al., 2024; Wang\net al., 2023) impose constraints on loss functions to penalize parameter updates that affect prior\ntask knowledge. However, traditional static constraint methods, with their fixed trade-off between\nfacilitating knowledge transfer and preventing forgetting, lack consideration for task diversity.\nTo generate pseudo samples that better support knowledge consolidation during rehearsal, while\nflexibly balancing knowledge transfer and forgetting prevention across tasks, we propose Self-\nEvolving Pseudo-Rehearsal with Task Similarity (SERS). Specifically, we generate pseudo inputs\nusing template guidance and semantic masking, eliminating task-specific instructions, where dynamic\nguidance and mask ratios ensure the diversity. After generating the pseudo inputs, to supplement\npseudo labels, over-specialized samples are selected via label self-questioning and ease the demand\nfor task-specific knowledge through label self-evolution. In the rehearsal stage, to fully promote\npermissible knowledge transfer, we design a regularization loss function based on task similarity.\nWhen tasks are similar, the regularization is relaxed to encourage the integration of new and old\nknowledge; otherwise, constraints are strengthened to alleviate the forgetting of previous knowledge.\nWe conducted extensive experiments on the SuperNI dataset (Wang et al., 2022) using\nLLaMA2-7B (Touvron et al., 2023) and ChatGLM-6B (GLM et al., 2024) to evaluate the per-\nformance of SERS across varying task chains. Results show that SERS consistently outperforms\nexisting methods and is more stable across a variety of task orders. On LLaMA2-7B, it achieved a\n2.16% relative improvement over advanced pseudo-sample rehearsal approaches, closely matching\nMulti-Task Learning (MTL) performance; on ChatGLM-6B, it even surpassed MTL.\nThe main contributions of our work are as follows:\n• We propose SERS, a continual learning framework for LLMs that decouples input and label\nsynthesis. SERS generates pseudo inputs via template guidance and semantic masking and uses a\n2\n"
    },
    {
      "page_number": 3,
      "text": "label self-evolution module to prevent over-specialization in pseudo labels, brings human learning\nstrategies into machine learning.\n• We introduce a task similarity-based dynamic regularization to effectively balance stability and\nplasticity, reducing the sensitivity of knowledge transfer to task order.\n• Experiments show that SERS significantly improves learning accuracy under task-incremental\nconditions, alleviates catastrophic forgetting, and even facilitates additional knowledge transfer.\n2\nRelated Work\n2.1\nSelf-Evolution Learning\nSelf-evolution (Zhong et al., 2022; Peng et al., 2023; Zhong et al., 2023; Zheng et al., 2023; Tao\net al., 2024; Song et al., 2025) is a paradigm that enables models to learn and improve through\nself-generated knowledge, inspired by human learning from experience. In this process, LLMs create\nnew tasks and solutions based on predefined goals, collect feedback from the environment, refine the\nacquired experience to eliminate errors, and update their parameters or context accordingly.\nZhong et al. (2022) improve pretraining efficiency through a two-stage process of self-questioning and\nself-evolution. In the first stage, the model uses masking to detect tokens it struggles to understand;\nin the second, it generates soft labels with richer knowledge patterns to enhance training. Similarly,\nSingh et al. (2023) apply reinforcement learning to actively generate new samples, evaluate them\nusing a binary reward function, and select high-quality data for model updates.\nMotivated by these strategies and aiming to address the instability of pseudo-sample generation\nin LLMs, we propose a label-level self-evolution method.\nBy imitating self-questioning and\nself-evolution structure, our approach detects over-specialized pseudo labels and smooths them\nwith general knowledge, mitigating the local overfitting to a specific task caused by rehearsal.\n2.2\nContinual Instruction Tuning for LLMs\nContinual instruction tuning for LLMs extends traditional LLMs tuning by enabling LLMs to\nincrementally absorb new tasks and feedback without forgetting prior knowledge. Compared to\nstandard continual learning, it introduces unique challenges due to generative outputs, global semantic\nrelationships, and model scale. Existing methods can be broadly categorized into:\n(1) Architecture-based (Ren et al., 2024; Zhao et al., 2024; Ke et al., 2023): These methods adjust\nmodel architecture or parameter distribution to separate the knowledge of new and old tasks, thus\nmitigating forgetting caused by parameter interference. For example, Ren et al. (2024) use fast and\nslow learners to balance stability and plasticity. Zhao et al. (2024) introduce an Attentive Learning &\nSelection module by combining multiple PET blocks in different ways to fit different tasks. However,\nas task numbers increase, adding new modules raises computational costs, and separate architecture\nadjustments limit flexibility and universality.\n(2) Rehearsal-based methods (Wang et al., 2024b; Huang et al., 2024; Maekawa et al., 2023): These\nmethods involve real or pseudo-sample rehearsal. Real sample rehearsal, as in Wang et al. (2024b),\nhelps recall past knowledge but relies on access to original data during each training stage, which\nis often impractical. Pseudo-sample rehearsal typically necessitates an extra generation module,\nincreasing trainable parameters. To our knowledge, Huang et al. (2024) are the first to use self-\nsynthesis to generate pseudo samples from a few real samples, solving storage issues, but still facing\nchallenges with pseudo samples instability and task comprehension.\n(3) Regularization-based methods (Wang et al., 2023; Jin et al., 2021; Li et al., 2024; Guo et al.,\n2024): These methods constrain excessive parameter updates with regularization. For example, Wang\net al. (2023) learn tasks in different low-rank vector subspaces and keep these subspaces orthogonal\nto minimize interference. However, orthogonal subspaces limit the knowledge transfer between tasks.\nOur method combines pseudo-sample rehearsal and regularization. For pseudo-sample generation,\nwe leverage template guidance and semantic masking to ensure the stability and real-sample similarity\nof synthesized pseudo samples, while varying templates and masking ratios promote diversity. For\nregularization, we dynamically adjust the regularization strength based on task similarity and account\n3\n"
    },
    {
      "page_number": 4,
      "text": "���0\n��\n① Semantic-Guided Pseudo Input Generation\n���\n���(���, ���）\n���\n② Label Self-Evolution\n����\n���0\n③ Rehearsal with Similarity Regularization\n��+1\n���(���, ���）\n��1...�−1\n����\nTask Similarity W\n����+1\n����\nFigure 2: The overall framework of our SERS method. In the Semantic-Guided Pseudo-Input\nGeneration stage, a small set of real samples produces pseudo inputs Xi\np. Then, the Label Self-\nEvolution module refines these inputs by integrating knowledge from LLM 0 and LLM i, yielding\nrehearsal pseudo samples Di\np(Xi\np, Y i\np). Finally, in the Rehearsal with Similarity Regularization\nstage, the rehearsal samples and the new training data Di+1 are combined for fine-tuning, with\nregularization applied based on task similarity.\nfor the impact of task order on parameter updates, effectively balancing knowledge transfer and\nresistance to catastrophic forgetting.\n3\nMethodology\n3.1\nProblem Definition\nWe consider the problem of Task-Incremental Continual Instruction Tuning. Given a sequence of N\ninstruction-following tasks T1, T2, . . . , TN, each associated with a dataset Di = (Xi, Y i), the goal is\nto continually fine-tune a pre-trained language model LLM0 on these tasks in sequence. At each step\ni, the model receives only the current task dataset Di and fine-tunes the model LLM i−1 to obtain\nLLM i. The objective is to learn each new task while maintaining performance on all previous tasks,\nwithout requiring large-scale retraining.\n3.2\nFramework Overview\nIn this paper, we propose a continual learning framework for LLMs that combines pseudo-sample\nrehearsal with regularization. As shown in Figure 2, our approach consists of three main compo-\nnents: semantic-guided pseudo-input generation, label self-evolution, and rehearsal with similarity\nregularization. In the following sections, we provide a detailed explanation of each module.\n3.3\nSemantic-Guided Pseudo-Input Generation\nThe self-synthesis approach proposed by Huang et al. (2024) effectively addresses the limitations\ndiscussed above, but still faces key challenges: the generated pseudo samples cannot well reflect the\noriginal knowledge structure and thus provide limited support for rehearsal. Additionally, appending\ntask instructions and labels increases the model’s comprehension burden, while the generated labels\nlose meaning after further refinement. To address these problems, we propose a Semantic-Guided\nPseudo-Input Generation module. As shown in Figure 3, real examples are masked in two roles:\nas Example Template that providing structure guidance, and as Semantic Guidance that offering\nsemantic context. Experiments in Wang et al. (2024c) indicate that models not fine-tuned on a specific\ntask have stronger contextual understanding, so that we use LLM 0 to fill in the masks to get pseudo\ninputs Xi\np without extra generating block. Varying mask ratios and example templates enhance\ndiversity, while removing task instructions and labels reduces cognitive load. Figure 1 shows the\nclustering of pseudo samples from our method, the self-synthesis approach, and real data. Our pseudo\n4\n"
    },
    {
      "page_number": 5,
      "text": " Example Template\n \n \n \n \n Semantic Guidanc\n(a) Semantic-Guided Pseudo-Input Generation\ne\nWhat is the largest <mask> \nin our solar system?\nWhat is the capital of <mask>?\nWhat is the capital of France?\nWhen and <mask> was <mask> born？\nWhen and where was Einstein born？\nWhat is the largest \n<mask> in <mask>?\n���0\n Generated Pseudo Input\nWhat is the largest planet in our solar system?\n(b) Label Self-Evolution\nTop k% \ndifferences\n���\n���0\n����\nStage1: Self-Questioning\nWhat is the largest object in our solar system?\nWhat is the largest ocean on Earth?\nWhat is the largest largest country in Africa?\nOthers\n��_���������\n�\n��_��ℎ���\n�\nStage2: Self-Evolution\n���(���, ���)\n Over-Specialized Input: \nWhat is the largest \nocean on Earth?\nThe biggest ocean is the one \nbetween America and Asia\nThe largest ocean is the Pacific \nOcean with 165,250,000 km²\nLabel \nSelf-Evolution\nThe Pacific Ocean is \nEarth's largest ocean\n���0\n����\n����\n�∗\n+(�−�) ∗\n���0\n����\nFigure 3: Detailed illustration of the core modules. (a) Semantic-Guided Pseudo-Input Generation:\nA real sample and its masked version serve as an example template, while an additional masked\nsample provides semantic guidance. These are combined and passed through LLM 0 to generate\npseudo inputs. Varying combinations and mask ratios promote diversity. (b) Label Self-Evolution:\n(Top) In self-questioning stage, the top-k% pseudo inputs with high domain dependence are identified\nas over-specialized samples. In self-evolution stage, these are relabeled by blending knowledge from\nLLM 0 and LLM i; others directly use the output of LLM i as labels. (Bottom) An example shows\nthat an over-specialized input leads to poor output from LLM 0 and an overly detailed output from\nLLM i. After label self-evolution, the final label becomes more acceptable, with reduced reliance on\ndomain-specific knowledge.\nsamples are more similar to real ones, preserving knowledge structure while maintaining diversity.\nExamples in real-task settings are shown in Appendix A.\n3.4\nLabel Self-Evolution\nConsidering that randomness in pseudo-input generation can lead to over-specialized instances\nrequiring excessive expertise, rehearsing with such samples may cause large parameter shifts and\ndisrupt existing knowledge. We therefore introduce a label self-evolution method inspired by human\nreview. As shown at the top of Figure 3, the process consists of two stages: self-questioning and\nself-evolution. In self-questioning, both the base model LLM 0 and fine-tuned model LLM i generate\nlabels. Samples with the top-k% output differences are treated as over-specialized. In the self-\nevolution stage, regular samples adopt LLM i’s outputs as labels, while over-specialized samples are\nrelabeled by integrating the outputs from both models using a weighted combination. The coefficient\nα adjusts the contributions of LLM 0 and LLM i to balance general and task-specific knowledge.\n5\n"
    },
    {
      "page_number": 6,
      "text": "As shown in the lower part of Figure 3, over-specialized inputs produce vague outputs on LLM 0 and\nhighly specific ones on LLM i. The label self-evolution module merges these to produce acceptable\nlabels, mitigating overfitting during rehearsal. Examples in Figure 8 illustrate the effectiveness and\nreliability of this process on real tasks.\n3.5\nRehearsal with Similarity Regularization\nThis section explores how task similarity, reflected through task order, influences training results.\nAfter generating pseudo samples avoiding over-specialization, pseudo samples are used for rehearsal\ntraining. At the training stage of T i+1, the model is updated using pseudo samples of old tasks\nD1...i−1\np\n, Di\np and new task training data Di+1. Previous works (Huang et al., 2024; Zhao et al., 2024)\nfine-tune using LoRA (Hu et al., 2022) without adapting to task characteristics, making the results\nhighly sensitive to task order. Since the model already contains knowledge from earlier tasks, similar\nnew tasks should allow more knowledge transfer, whereas dissimilar ones require stronger constraints\nto maintain prior knowledge. To achieve this, we design a regularization loss based on task similarity,\nwhich is incorporated into the original cross-entropy loss to adjust LoRA fine-tuning:\nL = Lce + λ · Lreg,\n(1)\nwhere Lce is the standard cross-entropy loss, λ controls regularization strength, and Lreg is the\nregularization term, balancing knowledge sharing and parameter stability. Specifically, we impose\nthe regularization constraint on all training target parameters during optimization, as formulated in\nEquation 2:\nLreg = 1\n2\nX\ni\nE\n\u0002\n∥θi∥2\u0003\n.\n(2)\nWe modulate regularization strength λ based on task similarity W and rehearsal ratio rreplay. When\nrreplay is low or W is large, indicating limited rehearsal or low task similarity, stronger constraints\nare applied to stabilize knowledge retention. In contrast, higher rreplay or smaller W suggests task\nalignment, allowing more relaxed regularization to facilitate knowledge transfer. This dynamic\nadjustment enables SERS to maintain a balance between stability and plasticity, supporting more\nrobust continual learning. Accordingly, λ is formally defined in Equation 3:\nλ = [λmin +\n\u0000λmax −λmin\n\u0001\u0010\n1 −e−\nW\nWth\n\u0011\n] ∗\n\u00001 −rreplay\n\u0001\n,\n(3)\nwhere λmin and λmax control the range of regularization strength, and Wth adjusts the curvature of\nthe scaling function. The Wasserstein Distance (Chen et al., 2022; Liu et al., 2025), a representative\nof the optimal transport framework (Alvarez-Melis and Fusi, 2020), provides a metric for assessing\nthe similarity between the distributions of two datasets, which is defined as in Equation 4:\nW(P, Q) =\ninf\nγ∈Π(P,Q) E(X,Y )∼γ [∥X −Y ∥] .\n(4)\n4\nExperiment\n4.1\nDataset and Metrics\nOur experiments are conducted on the SuperNI (Wang et al., 2022) dataset, a large and comprehensive\nbenchmark for instruction tuning. For fair comparison, we adopt the same ten tasks as Huang et al.\n(2024), divided into two groups: one with five tasks and the other with ten. Each group is evaluated\nunder three different task orders. Experiments were carried out on LLaMA2-7B(Touvron et al., 2023)\nand ChatGLM-6B(GLM et al., 2024). For more details about the dataset, please refer to Appendix C.\nWe adopt the Rouge-L score (Lin, 2004) to assess generation quality, where Ri\nj denotes the model’s\nperformance on task j at stage i. To evaluate overall performance, knowledge transfer ability, and\nretention of prior knowledge, the following commonly used continual learning metrics are selected:\n(1) Average Rouge-L (AR).\nAfter training on the final task, the average performance across all\ntasks is computed as shown in Equation 5:\nAR = 1\nN\nN\nX\ni=1\nRN\ni .\n(5)\n6\n"
    },
    {
      "page_number": 7,
      "text": "Table 1: Results on LLaMA2-7B and ChatGLM-6B under different task orders and settings.\nModel\nOrder 1\nOrder 2\nOrder 3\nAvg.\nAR ↑\nBWT ↑\nAR ↑\nBWT ↑\nAR ↑\nBWT ↑\nAR ↑\nBWT ↑\nLLaMA2-7B 5Tasks\nMTL\n53.07\n–\n53.07\n–\n53.07\n–\n53.07\n–\nKMeansSel(1%)\n49.73\n-5.22\n50.14\n-4.17\n50.12\n-3.61\n50.00\n-4.33\nL2\n28.62\n-28.99\n29.22\n-28.45\n28.33\n-30.71\n28.72\n-29.38\nSAPT\n50.47\n-3.75\n51.04\n-2.98\n50.22\n-4.37\n50.58\n-3.70\nSSR\n51.33\n-1.97\n52.41\n-1.18\n52.02\n-1.01\n51.92\n-1.39\nSERS\n52.90\n-0.55\n53.01\n-0.27\n52.84\n-0.63\n52.92\n-0.48\nChatGLM-6B 5Tasks\nMTL\n48.92\n–\n48.92\n–\n48.92\n–\n48.92\n–\nKMeansSel(1%)\n43.72\n-5.64\n43.74\n-5.07\n45.13\n-4.37\n44.19\n-5.02\nL2\n25.19\n-35.32\n26.46\n-32.47\n26.18\n-34.92\n25.94\n-34.24\nSAPT\n49.01\n-1.54\n48.65\n-2.11\n49.23\n-1.89\n48.96\n-1.84\nSSR\n48.95\n-2.12\n49.02\n-1.94\n49.38\n-0.51\n49.11\n-1.52\nSERS\n49.97\n-0.89\n49.86\n-1.17\n50.04\n-0.48\n49.98\n-0.85\nLLaMA2-7B 10Tasks\nMTL\n64.72\n–\n64.72\n–\n64.72\n–\n64.72\n–\nKMeansSel(1%)\n59.13\n-5.88\n60.71\n-5.39\n60.44\n-7.17\n60.09\n-6.15\nL2\n33.13\n-28.99\n34.71\n-25.12\n37.02\n-22.71\n34.95\n-25.42\nSAPT\n62.51\n-2.06\n61.90\n-2.81\n62.29\n-2.30\n62.23\n-2.39\nSSR\n62.29\n-1.84\n62.64\n-1.86\n62.36\n-3.95\n62.43\n-2.55\nSERS\n63.42\n-1.72\n63.45\n-1.11\n64.46\n-2.27\n63.78\n-1.7\nChatGLM-6B 10Tasks\nMTL\n62.04\n–\n62.04\n–\n62.04\n–\n62.04\n–\nKMeansSel(1%)\n60.84\n-5.41\n61.24\n-4.77\n61.04\n-5.27\n61.04\n-5.15\nL2\n40.18\n-29.71\n41.37\n-28.66\n41.99\n-26.12\n41.18\n-28.16\nSAPT\n61.30\n-3.44\n61.56\n-2.73\n60.87\n-2.92\n61.24\n-3.03\nSSR\n62.68\n-1.79\n62.27\n-2.42\n61.80\n-1.56\n62.25\n-1.92\nSERS\n63.30\n-1.17\n63.22\n-1.52\n63.16\n-1.49\n63.23\n-1.39\n(2) Backward Transfer (BWT).\nBWT measures the degree to which the learning of subsequent\ntasks affects the performance of the learned tasks, which is defined as:\nBWT =\n1\nN −1\nN−1\nX\ni=1\n(RN\ni −Ri\ni).\n(6)\n4.2\nExperiment Details\nAll experiments were conducted on a single A100 GPU. For pseudo-sample generation, 1% of\nreal samples are used to create pseudo samples equivalent to 10% of the training data. In the self-\nquestioning stage, we set k = 20, α = 0.5 for LLaMA2-7B, and k = 10, α = 0.6 for ChatGLM-6B\nto control the selection and evolution of over-specialized samples. LoRA is used for fine-tuning, and\nWasserstein distance based on model embeddings guides the regularization process.\n4.3\nExperiment Results\nWe compare our SERS method with several representative baselines, including the classic rehearsal-\nbased KMeansSel, which selects real samples via KMeans clustering; the advanced pesudo-sample\nrehearsal approach SSR (Huang et al., 2024), which leverages self-synthesis to generate pseudo\nsamples for rehearsal; advanced structure-based method SAPT (Zhao et al., 2024), which employs\na Shared Attentive Learning & Selection module to align the PET learning and selection; and the\nregularization-based L2 method. A multi-task learning (MTL) baseline, which jointly trains all tasks\nwithout considering forgetting, is also included for reference. As shown in Table 1, SERS consistently\n7\n"
    },
    {
      "page_number": 8,
      "text": "Table 2: The ablation studies on each proposed module. SGG refers to our Semantic-Guided Pseudo-\nInput Generation. LSE denotes Label Self-Evolution strategy. SR represents Similarity Regularization.\nA “✓“ indicates that our module is applied, while a“–“ denotes the use of a corresponding strategy\nfrom existing advanced pseudo-rehearsal approaches.\nAblation Setting\nLLaMA-7B AR (%) ↑\nChatGLM-6B AR (%) ↑\nSGG\nLSE\nSR\nOrder 1\nOrder 2\nOrder 3\nOrder 1\nOrde r2\nOrder 3\n–\n–\n–\n51.33\n52.41\n52.02\n48.95\n49.02\n49.38\n✓\n–\n–\n52.37\n52.63\n52.54\n49.87\n49.58\n49.35\n–\n✓\n–\n51.99\n52.83\n52.33\n49.25\n49.40\n49.02\n–\n–\n✓\n52.40\n52.42\n52.61\n49.68\n49.62\n49.50\n✓\n✓\n–\n52.47\n52.78\n52.71\n49.78\n49.33\n49.80\n–\n✓\n✓\n52.56\n52.92\n52.73\n49.08\n49.43\n49.21\n✓\n–\n✓\n52.59\n52.95\n52.56\n49.70\n49.80\n49.62\n✓\n✓\n✓\n52.90\n53.01\n52.84\n49.97\n49.86\n50.04\n1\n2\n3\n4\n5\nTraining Stage\n61.5\n62.0\n62.5\n63.0\n63.5\n64.0\n64.5\n65.0\n65.5\n66.0\n66.5\nAR(%)\nTRANS\n1\n2\n3\n4\n5\nTraining Stage\n97.5\n98.0\n98.5\n99.0\n99.5\n100.0\nSA\n1\n2\n3\n4\n5\nTraining Stage\n33.0\n33.5\n34.0\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\nQA\n1\n2\n3\n4\n5\nTraining Stage\n25.0\n25.5\n26.0\n26.5\n27.0\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n38.5\n39.0\n39.5\nQG\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\nFigure 4: Ablation results detailing the performance variations of the LLaMA2-7B model across a\n5tasks sequence under Order 1 (TRANS →SA →QA →SUM →QG). More details of ablation\nresults are shown in Figure 9\noutperforms these baselines and maintains stable performance across task orders, demonstrating\nthe effectiveness of similarity regularization. On LLaMA2-7B, it achieves results close to MTL,\nsurpassing the next best method by 2.16% and exhibiting significantly lower BWT. Notably, in\nChatGLM-6B, where MTL suffers from task confusion due to global attention and 2D positional\nencoding, SERS surpasses MTL by incrementally refining decision boundaries through rehearsal\nwith staged updates.\n5\nAblation and Comparison Experiments\n5.1\nModule Ablation\nIn this section, we carry out ablation studies to verify the effectiveness of each module. All experi-\nments are on 5tasks, and we measure performance with the AR metric. The results appear in Table 2,\nand detailed ablation results are provided in Figure 4. SERS introduces three core improvements:\nSemantic-Guided Generation, Label Self-Evolution, and Similarity Regularization. We evaluate\nthe effectiveness of these components with different configuration settings. For settings that do not\ninclude the SERS modules, we adopt corresponding strategies from Huang et al. (2024), where pseudo\nsamples are generated via in-context learning, pseudo labels are directly refined on the task-specific\nmodel, and no regularization method is applied. The settings with semantic-guided generation\nachieve higher overall performance compared to those using existing advanced method to generate\npseudo samples. The curves with task similarity are more likely to exhibit task-level performance\n8\n"
    },
    {
      "page_number": 9,
      "text": "improvement during training, and those incorporating label self-evolution tend to perform better on\nnew tasks, demonstrating the effectiveness of our proposed improvements.\n5.2\nData Utilization Efficiency\nIn our experiments, we first employed 1% of real samples to synthesize 10% pseudo samples,\nachieving strong performance with efficient data utilization. We then extended the analysis by\ngenerating different amounts of pseudo samples for rehearsal using various proportions of real data\n(1%, 0.75%, 0.5%, and 5%) to investigate the trade-off between data efficiency and pseudo-sample\nredundancy. As shown in Figure 5, even a small number of real samples can produce pseudo\nsamples that are diverse and capable of capturing the underlying task knowledge. However, as the\npseudo-sample ratio increases, the improvement in AR gradually saturates. When generating 20%\npseudo samples from 1% real data, performance begins to decline due to excessive redundancy and\ninterference with learning new tasks.\nWhen further reducing the number of real samples, we observe that using 0.75% or 0.5% of real data\nyields slightly better performance than 1% real data when synthesizing a small proportion of pseudo\nsamples. Nevertheless, the performance degrades notably as the pseudo-sample ratio grows. This\nsuggests that with a small pseudo-sample ratio, fewer real samples can better capture the essential\ntask knowledge and improve synthesis quality. In contrast, when a larger number of pseudo samples\nare generated, the limited diversity of real samples leads to higher redundancy, which hinders learning\neffectiveness. Moreover, pseudo samples generated from a larger real dataset tend to form more\ncluster centers. Under low pseudo-sample ratios, this results in less coherent knowledge structures\nand slightly worse performance than using 1% real data. Yet, as the pseudo-sample ratio increases to\n20%, the performance improves substantially and approaches that of multi-task learning (MTL).\nWe also compare pseudo-sample rehearsal with real-sample rehearsal, as presented in Table 5 of\nthe Appendix E. The results are consistent with findings from SSR, showing that even when 10%\nof real samples are replayed, the performance remains inferior to that of pseudo-sample rehearsal.\nThis is because labels synthesized by the old model facilitate learning, improving the new model’s\ntask adaptation. In contrast, real-sample rehearsal is directly constrained by the limited proportion of\navailable real data, whereas pseudo-sample rehearsal can flexibly expand data diversity by synthesiz-\ning new samples from a fixed real set. Consequently, the 1% real-sample rehearsal fails to match\nthe performance achieved with 5% real samples, as the smaller rehearsal ratio restricts the model’s\nability to preserve prior knowledge.\n5.3\nAnalysis of Parameters\nWe analyze the impact of two key parameters in label self-evolution. The proportion threshold k\ndetermines the selection of over-specialized samples during self-questioning. As shown in Figure 6,\nsetting k too high omits valuable specialized knowledge, while a low k allows too many over-\nspecialized samples for rehearsal, causing bias in model parameters and affecting overall performance.\nα controls the balance between general (LLM 0) and task-specific (LLM i) knowledge when refining\nlabels. Figure 6 illustrates that a high α may tend to less accurate labels, whereas a low α reduces the\nsmoothing effect, weakening the integration of general and specialized knowledge.\nParameter adjustment should consider model capability. Stronger mask-filling models better preserve\nprior knowledge in pseudo samples, allowing for a smaller k; weaker models require a larger k to\navoid excessive specialization. Similarly, models with stronger downstream abilities benefit from a\nhigher α to increase the general knowledge in over-specialized samples, while less capable models\nrequire a lower α to avoid inaccurate labels.\n6\nConclusion\nIn this work, we propose SERS for catastrophic forgetting mitigation in LLMs.\nSERS gen-\nerates pseudo samples that better reflect the structural knowledge of previous tasks, prevents\nover-specialization on rehearsal pseudo samples from harming overall performance and dynamically\nadjusts regularization strength based on the similarity between previous and new tasks. Extensive\nexperiments demonstrate that, compared to various representative methods, SERS achieves more\n9\n"
    },
    {
      "page_number": 10,
      "text": "1%\n5%\n10%\n15%\n20%\nPseudo Sample Ratio\n51.5\n52.0\n52.5\n53.0\nAR(%)\nData Utilization Efficiency Study\nMTL\n0.5% Real Sample\n0.75% Real Sample\n1% Real Sample\n5% Real Sample\nFigure 5: Rehearsal Analysis. We generate\nvarious proportions of pseudo samples us-\ning different amounts of real samples rang-\ning from 0.5% to 5% on LLaMA2-7B to\nevaluate data utilization efficiency.\n0.1\n0.2\n0.3\n0.4\n0.5\nk values\n51\n52\n53\nAR (%)\nComparison Study of k (α=0.5)\n0.3\n0.4\n0.5\n0.6\n0.7\nα values\n51\n52\n53\nAR (%)\nComparison Study of α (k=0.2)\nOrder1\nOrder2\nOrder3\nFigure 6: Parameter Analysis. We evaluate SERS\nperformance on LLaMA2-7B by varying k and α\nwhile fixing the other parameter respectively.\neffective forgetting mitigation and enhanced performance stability, underscoring SERS’s potential as\na general solution for continual learning in LLMs.\nLimitations\nAlthough ablations confirm each module’s contribution to AR and show score performance during\ntraining, the complex relationships between tasks make it hard to pinpoint why some tasks improve\nor decline. A deeper analysis of how new tasks affect previous tasks may unlock further gains in\ncontinual accuracy. Moreover, while pseudo-sample rehearsal boosts review of past knowledge, it\nremains unclear whether these synthetic examples can introduce knowledge beyond the original data.\nExploring the ability of pseudo samples to enrich the model with unseen knowledge could be key to\nsurpassing MTL in future continual learning work.\nAcknowledgments and Disclosure of Funding\nThis work is supported by the National Key Research and Development Program of China\n(2023YFC2705700), the National Natural Science Foundation of China (Grant No. 62225113,\nU23A20318, U2336211 and 62276195), the Foundation for Innovative Research Groups of Hubei\nProvince (Grant No. 2024AFA017) and the Science and Technology Major Project of Hubei Province\n(Grant No. 2024BAB046). The numerical calculations in this paper have been done on the supercom-\nputing system in the Supercomputing Center of Wuhan University.\nReferences\nDavid Alvarez-Melis and Nicolo Fusi. Geometric dataset distances via optimal transport. Advances\nin Neural Information Processing Systems, 33:21428–21439, 2020.\nYao Chen, Qingyi Gao, and Xiao Wang. Inferential wasserstein generative adversarial networks.\nJournal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):83–113, 2022.\nCyprien de Masson D’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic\nmemory in lifelong language learning. Advances in Neural Information Processing Systems, 32,\n2019.\nTeam GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas,\nGuanyu Feng, Hanlin Zhao, et al. Chatglm: A family of large language models from glm-130b to\nglm-4 all tools. arXiv preprint arXiv:2406.12793, 2024.\n10\n"
    },
    {
      "page_number": 11,
      "text": "Yanhui Guo, Shaoyuan Xu, Jinmiao Fu, Jia Liu, Chaosheng Dong, and Bryan Wang. Q-tuning:\nQueue-based prompt tuning for lifelong few-shot language learning. In Findings of the Association\nfor Computational Linguistics: NAACL 2024, pages 2595–2622, 2024.\nZeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning\nfor large models: A comprehensive survey. arXiv preprint arXiv:2403.14608, 2024.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?id=\nnZeVKeeFYf9.\nJianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao,\nand Jinsong Su. Mitigating catastrophic forgetting in large language models with self-synthesized\nrehearsal. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1416–1428, 2024.\nXisen Jin, Bill Yuchen Lin, Mohammad Rostami, and Xiang Ren. Learn continually, generalize\nrapidly: Lifelong knowledge accumulation for few-shot learning. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages 714–729, 2021.\nZixuan Ke, Bing Liu, Wenhan Xiong, Asli Celikyilmaz, and Haoran Li. Sub-network discovery\nand soft-masking for continual learning of mixed tasks.\nIn Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages 15090–15107, 2023.\nHongyu Li, Liang Ding, Meng Fang, and Dacheng Tao. Revisiting catastrophic forgetting in large\nlanguage model tuning. In Findings of the Association for Computational Linguistics: EMNLP\n2024, pages 4297–4308, 2024.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pages 74–81, 2004.\nXinran Liu, Yikun Bai, Yuzhe Lu, Andrea Soltoggio, and Soheil Kolouri. Wasserstein task embedding\nfor measuring task similarities. Neural Networks, 181:106796, 2025.\nAru Maekawa, Hidetaka Kamigaito, Kotaro Funakoshi, and Manabu Okumura. Generative replay\ninspired by hippocampal memory indexing for continual language learning. In Proceedings of the\n17th Conference of the European Chapter of the Association for Computational Linguistics, pages\n930–942, 2023.\nMartial Mermillod, Aurélia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investi-\ngating the continuum from catastrophic forgetting to age-limited learning effects, 2013.\nKeqin Peng, Liang Ding, Qihuang Zhong, Yuanxin Ouyang, Wenge Rong, Zhang Xiong, and Dacheng\nTao. Token-level self-evolution training for sequence-to-sequence learning. In Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),\npages 841–850, 2023.\nWeijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin. Analyzing and reducing\ncatastrophic forgetting in parameter efficient tuning. arXiv preprint arXiv:2402.18865, 2024.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience\nreplay for continual learning. Advances in neural information processing systems, 32, 2019.\nAvi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J\nLiu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for\nproblem-solving with language models. Transactions on Machine Learning Research, 2023.\nYuncheng Song, Liang Ding, Changtong Zan, and Shujian Huang.\nSelf-evolution knowledge\ndistillation for llm-based machine translation. In Proceedings of the 31st International Conference\non Computational Linguistics, pages 10298–10308, 2025.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. Lamol: Language modeling for lifelong language\nlearning. In International Conference on Learning Representations, 2020.\n11\n"
    },
    {
      "page_number": 12,
      "text": "Huashan Sun and Yang Gao. Reviving dormant memories: Investigating catastrophic forgetting in\nlanguage models through rationale-guidance difficulty. arXiv preprint arXiv:2411.11932, 2024.\nZhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang,\nDacheng Tao, and Jingren Zhou. A survey on self-evolution of large language models. CoRR,\n2024.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nMingyang Wang, Heike Adel, Lukas Lange, Jannik Strötgen, and Hinrich Schütze. Rehearsal-free\nmodular and compositional continual learning for language models. In Proceedings of the 2024\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 2: Short Papers), pages 469–480, 2024a.\nXiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and\nXuanjing Huang. Orthogonal subspace learning for language model continual learning. In The\n2023 Conference on Empirical Methods in Natural Language Processing, 2023.\nYifan Wang, Yafei Liu, Chufan Shi, Haoling Li, Chen Chen, Haonan Lu, and Yujiu Yang. Inscl: A\ndata-efficient continual learning paradigm for fine-tuning large language models with instructions.\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\n663–677, 2024b.\nYihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit S Dhillon, and\nSanjiv Kumar. Two-stage llm fine-tuning with less specialization and more generalization. In The\nTwelfth International Conference on Learning Representations, 2024c.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al.\nSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages\n5085–5109, 2022.\nTongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari.\nContinual learning for large language models: A survey. CoRR, 2024.\nWenpeng Yin, Jia Li, and Caiming Xiong. Contintin: Continual learning from task instructions. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 3062–3072, 2022.\nWeixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu Zhang, Qing Yang,\nDongliang Xu, and Wanxiang Che. Sapt: A shared attention framework for parameter-efficient\ncontinual learning of large language models. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 11641–11661, 2024.\nHaoqi Zheng, Qihuang Zhong, Liang Ding, Zhiliang Tian, Xin Niu, Changjian Wang, Dongsheng\nLi, and Dacheng Tao. Self-evolution learning for mixup: Enhance data augmentation on few-shot\ntext classification tasks. In The 2023 Conference on Empirical Methods in Natural Language\nProcessing, 2023.\nJunhao Zheng, Xidi Cai, Shengjie Qiu, and Qianli Ma. Spurious forgetting in continual learning\nof language models. In The Thirteenth International Conference on Learning Representations,\n2025a.\nJunhao Zheng, Shengjie Qiu, Chengming Shi, and Qianli Ma. Towards lifelong learning of large\nlanguage models: A survey. ACM Computing Surveys, 57(8):1–35, 2025b.\nQihuang Zhong, Liang Ding, Yibing Zhan, Yu Qiao, Yonggang Wen, Li Shen, Juhua Liu, Baosheng\nYu, Bo Du, Yixin Chen, et al. Toward efficient language model pretraining and downstream\nadaptation via self-evolution: A case study on superglue. arXiv preprint arXiv:2212.01853, 2022.\n12\n"
    },
    {
      "page_number": 13,
      "text": "Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. Self-evolution learning for\ndiscriminative language model pretraining. In Findings of the Association for Computational\nLinguistics: ACL 2023, pages 4130–4145, 2023.\n13\n"
    },
    {
      "page_number": 14,
      "text": "A\nComparative Case Study on Pseudo-Sample Generation\nTo illustrate the effectiveness of our proposed method, we present a comparison between the traditional\nICL approach and the Semantic-Guided Pseudo-Input Generation we introduced. Using two samples\nfrom the same task, we generate pseudo data in two ways for two retries: the ICL method generates\nboth inputs and outputs with 2-shot prompting, while our method focuses on generating only pseudo\ninputs. As shown in Figure 7, it is evident that the 2-shot ICL approach fails to capture the essence of\nthe “cosmosqa“ task. The generated questions resemble generic QA samples, lacking the implicit\ncues that emphasize common-sense reasoning. In contrast, our method is able to produce samples that\nclosely resemble real ones under a low masking ratio. When the masking ratio is high, the generated\noutputs are diverse but still retain the core features of the“cosmosqa“ task. This not only ensures the\ndiversity of pseudo samples, but also supports the model’s review of previous knowledge.\nReal Sample:\nInput:\nEmmy isn’t feeling well. I woke up this morning with a little bit of vom on the bed near her mouth. Dog’s\npuke all the time though, so no big deal. I let her out and she took a big old solid 2-point conversion. But\nshe didn’t run or scuff her feet afterwards, she just ambled back inside.\nQuestion: What did I mean when I said she took a solid 2-point conversion?\nOutput: She pooped.\nInput:\nblaine was only able to stay for a little while since he had to work sunday and while i wanted to go home\nwith him to NOT face the party the rest of the evening, i stayed with nick and brenna. i had such a good\ntime though. there was making out involved among some of the party guests. oh, might i add that the party\ntook place on the rooftop of this lovely apartment building.\nQuestion: Where might James have gone after leaving the party?\nOutput: He went home.\nIn-Context Learning Output:\nInput:\nThe best thing about being single is you can eat\nwhatever you want whenever you feel like eating\nit!\nQuestion:\nWhen would Katie most likely be\nthinking these thoughts?\nOutput: After having an argument over dinner at\nsomeone else’s house.\nInput:\nWhenever we meet up at a cafe or something, he\nalways seems very keen to see me again, and wants\nus to do things together all the time! But then when\nI’m ready to leave, his eyes get all glassy and sad\nlooking and he says Ï don’t want you to think badly\nabout me.Ït makes me feel like maybe my presence\nisn’t actually welcome - I can tell he doesn’t really\nenjoy being around me, even though he tries hard\nto hide it.\nQuestion: What does the speaker most likely think\nabout herself?\nOutput: She thinks highly of herself.\nSemantic-Guided Generation:\nInput:\nBlaine was reluctant to leave early because he had\nto work Sunday and wanted to spend more time\nwith Nick and Brenna. He decided not to go home\nwith them but instead to return later tonight. I\nwas having such a good time at the party as were\nmany others who made it into the exclusive venue\nlocated on top of this amazing apartment complex.\nOh, might I add that the party location was right\noutside our living room window!\nQuestion: Where else could James have gone after\nleaving the party?\nInput:\nBlaine seemed like such a nice guy since it came\ndown to him being the last person standing at the\nbar Sunday night. He wanted me to stay for just\na little longer; however, I didn’t want to face the\nremainder of the evening alone. So I went home\nwith Nick and Brenna instead. They were very\nkind and offered us plenty of space even if we\nmade our own drinks or snacks from what they\nprovided. Oh! And by the way, their roof-top patio\nwas absolutely gorgeous!\nQuestion: What would you think about James\nbased upon his behavior\nFigure 7: A case study for Pseudo-Sample Generation\n14\n"
    },
    {
      "page_number": 15,
      "text": "B\nComparative Case Study on Output Refinement\nThis part demonstrates the value of self-evolution in label refinement through real examples. By\nadjusting the balance between general knowledge from LLM 0 and task-specific knowledge from\nLLM i, the model can reduce overly specialization and produce more acceptable responses. In the\nQA example shown in Figure 8, the output from LLM 0 contains hallucinations and fails to provide a\ncorrect answer. In contrast, the output from LLM i correctly identifies the key term \"family\". The\nfinal response refined through self-evolution also delivers a precise and reasonable answer.\nInput: A small part of her face lit up when she heard the news. Her eyes glistened but there\nwasn’t any trace of joy on them or happiness even though this was great news. She simply\nlooked relieved because finally things were starting to go well for her family. And they had been\nthrough hell lately so every little thing counted nowadays. So she couldn’t help feeling grateful.\nQuestion: What made the person feel relief?\nLLM 0 Output: Things went wrong recently!\nLLM i Output: They got good news for their family.\nOutput after Self-Evolution: The person was told that things was going to be okay.\nFigure 8: A case study for output refinement\nC\nAdditional Experimental Details\nAll samples are selected from the SuperNI dataset. To ensure fair comparison, we follow the same\ntask settings as advanced pseudo-sample rehearsal methods (Huang et al., 2024). The selected tasks\nare shown in Table 3. During pseudo-sample generation, we begin with 1% of the real samples\nand apply the Semantic-Guided Pseudo-Input Generation method with three retries to create a large\nnumber of pseudo samples. Then, we use the K-Means algorithm to select 10% of these for rehearsal.\nDuring the training phase, we selected two numbers of tasks and two task sequences to form four\ntask chains, with the task chains as shown in Table 4.\nTable 3: Details of task names and abbreviations\nAbbreviation\nTask Name\nQA\ntask024_cosmosqa_answer_generation\nQG\ntask074_squad1.1_question_generation\nSA\ntask1312_amazonreview_polarity_classification\nSUM\ntask511_reddit_tifu_long_text_summarization\nTRANS\ntask1219_ted_translation_en_es\nDSG\ntask574_air_dialogue_sentence_generation\nEXPL\ntask192_hotpotqa_sentence_generation\nPARA\ntask177_para-nmt_paraphrasing\nPOS\ntask346_hybridqa_classification\nPE\ntask064_all_elements_except_first_i\nTable 4: Details of Task Chains under Different Task Numbers and Orders\nSettings\nTask Chain\n5Tasks Order 1\nTRANS →SA →QA →SUM →QG\n5Tasks Order 2\nQA →QG →SA →SUM →TRANS\n5Tasks Order 3\nSUM →QG →TRANS →QA →SA\n10Tasks Order 1\nTRANS →SA →QA →SUM →QG →PE →PARA →POS →DSG →EXPL\n10Tasks Order 2\nQA →QG →SA →SUM →TRANS →DSG →EXPL →PARA →PE →POS\n10Tasks Order 3\nSUM →QG →TRANS →QA →SA →PARA →DSG →POS →EXPL →PE\n15\n"
    },
    {
      "page_number": 16,
      "text": "D\nAblation Study Details\nIn this section, we present the detailed performance of each ablation setting across different task chains\nand models. The results show that our generation strategy consistently leads to better outcomes, label\nself-evolution generally benefits the learning of new tasks, and similarity regularization facilitates\nknowledge transfer, increasing the likelihood of performance gains throughout training. Details are\nshown in Figure 9.\n1\n2\n3\n4\n5\nTraining Stage\n32.0\n32.5\n33.0\n33.5\n34.0\n34.5\n35.0\n35.5\n36.0\nAR(%)\nQA\n1\n2\n3\n4\n5\nTraining Stage\n37.5\n38.0\n38.5\n39.0\n39.5\n40.0\n40.5\nQG\n1\n2\n3\n4\n5\nTraining Stage\n98.0\n98.5\n99.0\n99.5\n100.0\nSA\n1\n2\n3\n4\n5\nTraining Stage\n26.0\n26.5\n27.0\n27.5\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n64.5\n65.0\n65.5\n66.0\nTRANS\nAblation Study on LLaMA2-7B Order2\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\n1\n2\n3\n4\n5\nTraining Stage\n24.5\n25.0\n25.5\n26.0\n26.5\n27.0\n27.5\nAR(%)\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n36.0\n36.5\n37.0\n37.5\n38.0\n38.5\n39.0\n39.5\n40.0\nQG\n1\n2\n3\n4\n5\nTraining Stage\n63.0\n63.5\n64.0\n64.5\n65.0\n65.5\n66.0\nTRANS\n1\n2\n3\n4\n5\nTraining Stage\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\nQA\n1\n2\n3\n4\n5\nTraining Stage\n99.0\n99.5\n100.0\nSA\nAblation Study on LLaMA2-7B Order3\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\n1\n2\n3\n4\n5\nTraining Stage\n53.5\n54.0\n54.5\n55.0\n55.5\n56.0\n56.5\n57.0\n57.5\n58.0\n58.5\n59.0\n59.5\n60.0\n60.5\nAR(%)\nTRANS\n1\n2\n3\n4\n5\nTraining Stage\n98.0\n98.5\n99.0\n99.5\nSA\n1\n2\n3\n4\n5\nTraining Stage\n31.5\n32.0\n32.5\n33.0\n33.5\n34.0\n34.5\nQA\n1\n2\n3\n4\n5\nTraining Stage\n21.0\n21.5\n22.0\n22.5\n23.0\n23.5\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n36.5\n37.0\n37.5\n38.0\nQG\nAblation Study on ChatGLM-6B Order1\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\n16\n"
    },
    {
      "page_number": 17,
      "text": "1\n2\n3\n4\n5\nTraining Stage\n30.0\n30.5\n31.0\n31.5\n32.0\n32.5\n33.0\n33.5\n34.0\n34.5\nAR(%)\nQA\n1\n2\n3\n4\n5\nTraining Stage\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\n37.5\n38.0\n38.5\n39.0\nQG\n1\n2\n3\n4\n5\nTraining Stage\n98.0\n98.5\n99.0\n99.5\nSA\n1\n2\n3\n4\n5\nTraining Stage\n21.5\n22.0\n22.5\n23.0\n23.5\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n58.5\n59.0\n59.5\n60.0\nTRANS\nAblation Study on ChatGLM-6B Order2\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\n1\n2\n3\n4\n5\nTraining Stage\n21.5\n22.0\n22.5\n23.0\n23.5\n24.0\nAR(%)\nSUM\n1\n2\n3\n4\n5\nTraining Stage\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\n37.5\n38.0\nQG\n1\n2\n3\n4\n5\nTraining Stage\n56.5\n57.0\n57.5\n58.0\n58.5\n59.0\n59.5\n60.0\n60.5\nTRANS\n1\n2\n3\n4\n5\nTraining Stage\n32.0\n32.5\n33.0\n33.5\n34.0\nQA\n1\n2\n3\n4\n5\nTraining Stage\n98.0\n98.5\n99.0\n99.5\nSA\nAblation Study on ChatGLM-6B Order3\nSGG+LES+SR\nSGG\nLSE\nSR\nSGG+LSE\nSGG+SR\nLSE+SR\nAdvanced Pseudo Rehearsal Method\nFigure 9: Ablation results detailing the performance variations of different models across different\ntask chains\nE\nReal-Sample Rehearsal Details\nThis section presents additional results on real-sample rehearsal for comparison with pseudo-sample\nrehearsal. As shown in Table E, even when 10% of real samples are used for rehearsal under the same\ncontinual learning setup, the performance remains lower than that of pseudo-sample rehearsal. This\nobservation can be explained by the fact that labels synthesized by the old model facilitate learning,\nimproving the new model’s task adaptation. In contrast, real-sample rehearsal is constrained by the\nlimited number of available samples, resulting in reduced diversity and weaker knowledge coverage.\nConsequently, its performance degrades more noticeably under low rehearsal ratios.\nTable 5: Real-Sample Rehearsal Results on LLaMA2-7B\nData Rehearsal\nOrder1\nOrder2\nOrder3\nAvg\n1% real samples\n48.11\n49.02\n48.74\n48.62\n5% real samples\n50.18\n50.65\n50.02\n50.28\n10% real samples\n50.24\n51.09\n50.84\n50.73\n1% real samples synthesis 10% pseudo samples\n52.90\n53.01\n52.84\n52.92\nF\nComparison Study Details\nIn this section, we provide additional experimental details on the ChatGLM-6B model to demonstrate\nthe impact of the hyperparameters k and α on the SERS framework.\n17\n"
    },
    {
      "page_number": 18,
      "text": "0.1\n0.2\n0.3\n0.4\n0.5\nk values\n46\n47\n48\n49\n50\nAR(%)\nComparison Study of k on ChatGLM-6B\nOrder1\nOrder2\nOrder3\nFigure 10: Comparison study of k (α=0.6) on ChatGLM-6B\n0.3\n0.4\n0.5\n0.6\n0.7\n values\n48.5\n49.0\n49.5\n50.0\nAR(%)\nComparison Study of  on ChatGLM-6B\nOrder1\nOrder2\nOrder3\nFigure 11: Comparison study of α (k=0.1) on ChatGLM-6B\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: In the abstract and introduction, we clearly demonstrate the contribution and\nscope of this paper.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\n18\n"
    },
    {
      "page_number": 19,
      "text": "Answer: [Yes] ,\nJustification: In Section 6, we have thoroughly discussed the limitations of our article,\nhoping to guide more future work.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: In Section 3, we elaborated on the motivation and theoretical derivation of our\nmethod, with a complete proof process in place.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\n19\n"
    },
    {
      "page_number": 20,
      "text": "Justification: We have provided detailed descriptions of the experimental details in section\n4.2 and methods in section 3 to ensure that our experiment can be reproduced.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: Our datasets are derived from publicly available datasets, and our code will\nalso be fully open-sourced.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n20\n"
    },
    {
      "page_number": 21,
      "text": "• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: In Section 4.2, we presented the experimental setup and the selection of key\nparameters. Additional details could refer to our code.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [No]\nJustification: Due to the computational cost of continual learning, we did not perform\nmultiple runs for each experiment.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\n21\n"
    },
    {
      "page_number": 22,
      "text": "Justification: In Section 4.2, we have provided sufficient information on the computer\nresources needed to reproduce the experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We guarantee that the research conducted in the paper complies with NeurIPS\nCode of Ethics in all aspects.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We outlined the societal benefits of continual learning research in Section 1,\nand highlighted the limitations and challenges of existing techniques.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n22\n"
    },
    {
      "page_number": 23,
      "text": "11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper poses no such risks.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: The creators or original owners of the assets used in the paper, such as code,\ndata, and models, have been appropriately recognized, and the licenses and terms of use\nhave been clearly mentioned and properly respected.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: The paper does not release new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n23\n"
    },
    {
      "page_number": 24,
      "text": "• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [Yes]\nJustification: This study consistently adheres to relevant policies governing the use of LLMs\nand provides a detailed description of their application.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n24\n"
    }
  ]
}