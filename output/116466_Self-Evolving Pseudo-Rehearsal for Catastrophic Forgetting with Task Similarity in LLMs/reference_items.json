[
  {
    "ref_no": 2019,
    "title": "Chatglm: A family of large language models from glm-130b to glm-4 all tools",
    "ids": {
      "arxiv": "2406.12793",
      "url": "https://openreview.net/forum?id=nZeVKeeFYf9.Jianheng",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2406.12793",
    "raw_text": "Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: A family of large language models from glm-130b to glm-4 all tools. a..."
  },
  {
    "ref_no": 2024,
    "title": "cosmosqa“ task. The generated questions resemble generic QA samples, lacking the implicit cues that emphasize common-sense reasoning. In contrast, our method is able to produce samples that closely resemble real ones under a low masking ratio. When the masking ratio is high, the generated outputs are diverse but still retain the core features of the“cosmosqa“ task. This not only ensures the diversity of pseudo samples, but also supports the model’s review of previous knowledge. Real Sample: Input: Emmy isn’t feeling well. I woke up this morning with a little bit of vom on the bed near her mouth. Dog’s puke all the time though, so no big deal. I let her out and she took a big old solid 2-point conversion. But she didn’t run or scuff her feet afterwards, she just ambled back inside. Question: What did I mean when I said she took a solid 2-point conversion? Output: She pooped. Input: blaine was only able to stay for a little while since he had to work sunday and while i wanted to go home with him to NOT face the party the rest of the evening, i stayed with nick and brenna. i had such a good time though. there was making out involved among some of the party guests. oh, might i add that the party took place on the rooftop of this lovely apartment building. Question: Where might James have gone after leaving the party? Output: He went home. In-Context Learning Output: Input: The best thing about being single is you can eat whatever you want whenever you feel like eating it! Question: When would Katie most likely be thinking these thoughts? Output: After having an argument over dinner at someone else’s house. Input: Whenever we meet up at a cafe or something, he always seems very keen to see me again, and wants us to do things together all the time! But then when I’m ready to leave, his eyes get all glassy and sad looking and he says Ï don’t want you to think badly about me.Ït makes me feel like maybe my presence isn’t actually welcome - I can tell he doesn’t really enjoy being around me, even though he tries hard to hide it. Question: What does the speaker most likely think about herself? Output: She thinks highly of herself. Semantic-Guided Generation: Input: Blaine was reluctant to leave early because he had to work Sunday and wanted to spend more time with Nick and Brenna. He decided not to go home with them but instead to return later tonight. I was having such a good time at the party as were many others who made it into the exclusive venue located on top of this amazing apartment complex. Oh, might I add that the party location was right outside our living room window! Question: Where else could James have gone after leaving the party? Input: Blaine seemed like such a nice guy since it came down to him being the last person standing at the bar Sunday night. He wanted me to stay for just a little longer; however, I didn’t want to face the remainder of the evening alone. So I went home with Nick and Brenna instead. They were very kind and offered us plenty of space even if we made our own drinks or snacks from what they provided. Oh! And by the way, their roof-top patio was absolutely gorgeous! Question: What would you think about James based upon his behavior Figure 7: A case study for Pseudo-Sample Generation B Comparative Case Study on Output Refinement This part demonstrates the value of self-evolution in label refinement through real examples. By adjusting the balance between general knowledge from LLM 0 and task-specific knowledge from LLM i, the model can reduce overly specialization and produce more acceptable responses. In the QA example shown in Figure 8, the output from LLM 0 contains hallucinations and fails to provide a correct answer. In contrast, the output from LLM i correctly identifies the key term",
    "ids": {
      "arxiv": "2307.09288",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2307.09288",
    "raw_text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tun..."
  },
  {
    "ref_no": 1,
    "title": "• The answer NA means that the abstract and introduction do not include the claims made in the paper",
    "ids": {},
    "graph_id": "",
    "raw_text": "Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Justification: In the abstract and introduction, we clearly demonstrat..."
  },
  {
    "ref_no": 2,
    "title": "Limitations",
    "ids": {},
    "graph_id": "",
    "raw_text": "Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] , Justification: In Section 6, we have thoroughly discussed the limitations of our arti..."
  },
  {
    "ref_no": 3,
    "title": "• The answer NA means that the paper does not include theoretical results",
    "ids": {},
    "graph_id": "",
    "raw_text": "Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Justification: In Section 3, we elaborated o..."
  }
]