{
  "filename": "1809.04281.pdf",
  "total_pages": 14,
  "full_text": "MUSIC TRANSFORMER:\nGENERATING MUSIC WITH LONG-TERM STRUCTURE\nCheng-Zhi Anna Huang∗Ashish Vaswani\nJakob Uszkoreit\nNoam Shazeer\nIan Simon\nCurtis Hawthorne\nAndrew M. Dai\nMatthew D. Hoffman\nMonica Dinculescu\nDouglas Eck\nGoogle Brain\nABSTRACT\nMusic relies heavily on repetition to build structure and meaning. Self-reference\noccurs on multiple timescales, from motifs to phrases to reusing of entire sections\nof music, such as in pieces with ABA structure. The Transformer (Vaswani\net al., 2017), a sequence model based on self-attention, has achieved compelling\nresults in many generation tasks that require maintaining long-range coherence.\nThis suggests that self-attention might also be well-suited to modeling music.\nIn musical composition and performance, however, relative timing is critically\nimportant. Existing approaches for representing relative positional information\nin the Transformer modulate attention based on pairwise distance (Shaw et al.,\n2018). This is impractical for long sequences such as musical compositions since\ntheir memory complexity for intermediate relative information is quadratic in the\nsequence length. We propose an algorithm that reduces their intermediate memory\nrequirement to linear in the sequence length. This enables us to demonstrate that a\nTransformer with our modiﬁed relative attention mechanism can generate minute-\nlong compositions (thousands of steps, four times the length modeled in Oore et al.\n(2018)) with compelling structure, generate continuations that coherently elaborate\non a given motif, and in a seq2seq setup generate accompaniments conditioned on\nmelodies1. We evaluate the Transformer with our relative attention mechanism on\ntwo datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art\nresults on the latter.\n1\nINTRODUCTION\nA musical piece often consists of recurring elements at various levels, from motifs to phrases to\nsections such as verse-chorus. To generate a coherent piece, a model needs to reference elements\nthat came before, sometimes in the distant past, repeating, varying, and further developing them to\ncreate contrast and surprise. Intuitively, self-attention (Parikh et al., 2016) appears to be a good match\nfor this task. Self-attention over its own previous outputs allows an autoregressive model to access\nany part of the previously generated output at every step of generation. By contrast, recurrent neural\nnetworks have to learn to proactively store elements to be referenced in a ﬁxed size state or memory,\npotentially making training much more difﬁcult. We believe that repeating self-attention in multiple,\nsuccessive layers of a Transformer decoder (Vaswani et al., 2017) helps capture the multiple levels at\nwhich self-referential phenomena exist in music.\nIn its original formulation, the Transformer relies on absolute position representations, using either\npositional sinusoids or learned position embeddings that are added to the per-position input repre-\nsentations. Recurrent and convolutional neural networks instead model position in relative terms:\nRNNs through their recurrence over the positions in their input, and CNNs by applying kernels that\neffectively choose which parameters to apply based on the relative position of the covered input\nrepresentations.\n∗Google AI Resident. Correspondence to: Cheng-Zhi Anna Huang <annahuang@google.com>\n1Samples are available for listening at\nhttps://storage.googleapis.com/music-transformer/index.html\n1\narXiv:1809.04281v3  [cs.LG]  12 Dec 2018\nMusic has multiple dimensions along which relative differences arguably matter more than their\nabsolute values; the two most prominent are timing and pitch. To capture such pairwise relations\nbetween representations, Shaw et al. (2018) introduce a relation-aware version of self-attention which\nthey use successfully to modulate self-attention by the distance between two positions. We extend\nthis approach to capture relative timing and optionally also pitch, which yields improvement in both\nsample quality and perplexity for JSB Chorales. As opposed to the original Transformer, samples\nfrom a Transformer with our relative attention mechanism maintain the regular timing grid present in\nthis dataset. The model furthermore captures global timing, giving rise to regular phrases.\nThe original formulation of relative attention (Shaw et al., 2018) requires O(L2D) memory where L\nis the sequence length and D is the dimension of the model’s hidden state. This is prohibitive for long\nsequences such as those found in the Piano-e-Competition dataset of human-performed virtuosic,\nclassical piano music. In Section 3.4, we show how to reduce the memory requirements to O(LD),\nmaking it practical to apply relative attention to long sequences.\nThe Piano-e-Competition dataset consists of MIDI recorded from performances of competition partic-\nipants, bearing expressive dynamics and timing on the granularity of < 10 miliseconds. Discretizing\ntime on a ﬁxed grid that would yield unnecessarily long sequences as not all events change on the\nsame timescale. We hence adopt a sparse, MIDI-like, event-based representation from (Oore et al.,\n2018), allowing a minute of music with 10 milisecond resolution to be represented at lengths around\n2K, as opposed to 6K to 18K on a ﬁxed-grid representation with multiple performance attributes. As\nposition in sequence no longer corresponds to time, a priori it is not obvious that relative attention\nshould work as well with such a representation. However, we will show in Section 4.2 that it does\nimprove perplexity and sample quality over strong baselines.\nWe speculate that idiomatic piano gestures such as scales, arpeggios and other motifs all exhibit a\ncertain grammar and recur periodically, hence knowing their relative positional distances makes it\neasier to model this regularity. This inductive bias towards learning relational information, as opposed\nto patterns based on absolute position, suggests that the Transformers with relative attention could\ngeneralize beyond the lengths it was trained on, which our experiments in Section 4.2.1 conﬁrm.\n1.1\nCONTRIBUTIONS\nDomain contributions\nWe show the ﬁrst successful use of Transformers in generating music that\nexhibits long-term structure. Before our work, LSTMs were used at timescales of 15s (~500 tokens)\non the Piano-e-Competition dataset (Oore et al., 2018). Our work show that Transformers not only\nachieve state-of-the-art perplexity on modeling these complex expressive piano performances, but\ncan also generate them at the scale of 60s (~2000 tokens) with remarkable internal consistency. Our\nrelative attention mechanism is essential to the model’s quality. In listening tests (see Section 4.2.3),\nsamples from models with relative self-attention were perceived as more coherent than the baseline\nTransformer model from Vaswani et al. (2017). Relative attention not only enables Transformers to\ngenerate continuations that elaborate on a given motif, but also to generalize and generate in consistent\nfashion beyond the length it was trained on (see Section 4.2.1). In a seq2seq setup, Transformers can\ngenerate accompaniments conditioned on melodies, enabling users to interact with the model.\nAlgorithmic contributions\nThe space complexity of the relative self attention mechanism in its\noriginal formulation (Shaw et al., 2018) made it infeasible to train on sequences of sufﬁcient length\nto capture long-range structure in longer musical compositions. Addressing this we present a crucial\nalgorithmic improvement to the relative self attention mechanism, dramatically reducing its memory\nrequirements from O(L2D) to O(LD). For example, we reduce the memory consumption per layer\nfrom 8.5 GB to 4.2 MB (per head from 1.1 GB to 0.52 MB) for a sequence of length L = 2048 and\nhidden-state size D = 512 (per head Dh = D\nH = 64, where number of heads is H = 8) (see Table 1),\nallowing us to use GPUs to train the relative self-attention Transformer on long sequences.\n2\nRELATED WORK\nSequence models have been the canonical choice for modeling music, from Hidden Markov Models\nto RNNs and Long Short Term Memory networks (e.g., Eck & Schmidhuber, 2002; Liang, 2016;\nOore et al., 2018), to bidirectional LSTMs (e.g., Hadjeres et al., 2017). Successful application of\nsequential models to polyphonic music often requires serializing the musical score or performance\n2\ninto a single sequence, for example by interleaving different instruments or voices. Alternatively,\na 2D pianoroll-like representation (see A.1 for more details) can be decomposed into a sequence\nof multi-hot pitch vectors, and their joint probability distributions can be captured using Restricted\nBoltzmann Machines (Smolensky, 1986; Hinton et al., 2006) or Neural Autoregressive Distribution\nEstimators (NADE; Larochelle & Murray, 2011). Pianorolls are also image-like and can be modeled\nby CNNs trained either as generative adversarial networks (e.g., Dong et al., 2018) or as orderless\nNADEs (Uria et al., 2014; 2016) (e.g., Huang et al., 2017).\nLattner et al. (2018) use self-similarity in style-transfer fashion, where the self-similarity structure of\na piece serves as a template objective for gradient descent to impose similar repetition structure on an\ninput score. Self-attention can be seen as a generalization of self-similarity; the former maps the input\nthrough different projections to queries and keys, and the latter uses the same projection for both.\nDot-product self-attention is the mechanism at the core of the Transformer, and several recent works\nhave focused on applying and improving it for image generation, speech, and summarization (Parmar\net al., 2018; Povey et al., 2018; Liu et al., 2018). A key challenge encountered by each of these efforts\nis scaling attention computationally to long sequences. This is because the time and space complexity\nof self-attention grows quadratically in the sequence length. For relative self-attention (Shaw et al.,\n2018) this is particularly problematic as the space complexity also grows linearly in the dimension,\nor depth, of the per-position representations.\n3\nMODEL\n3.1\nDATA REPRESENTATION\nWe take a language-modeling approach to training generative models for symbolic music. Hence\nwe represent music as a sequence of discrete tokens, with the vocabulary determined by the dataset.\nDatasets in different genres call for different ways of serializing polyphonic music into a single\nstream and also discretizing time.\nThe JSB Chorale dataset consists of four-part scored choral music, which can be represented as a\nmatrix where rows correspond to voices and columns to time discretized to sixteenth notes. The\nmatrix’s entries are integers that denote which pitch is being played. This matrix can than be\nserialized in raster-scan fashion by ﬁrst going down the rows and then moving right through the\ncolumns (see A.1 for more details). Compared to JSB Chorale, the piano performance data in the\nPiano-e-Competition dataset includes expressive timing information at much ﬁner granularity and\nmore voices. For the Piano-e-Competition we therefore use the performance encoding proposed\nby Oore et al. (2018) which consists of a vocabulary of 128 NOTE_ON events, 128 NOTE_OFFs,\n100 TIME_SHIFTs allowing for expressive timing at 10ms and 32 VELOCITY bins for expressive\ndynamics (see A.2 for more details).\n3.2\nBACKGROUND: SELF-ATTENTION IN TRANSFORMER\nThe Transformer decoder is a autoregressive generative model that uses primarily self-attention\nmechanisms, and learned or sinusoidal position information. Each layer consists of a self-attention\nsub-layer followed by a feedforward sub-layer.\nThe attention layer ﬁrst transforms a sequence of L D-dimensional vectors X = (x1, x2, . . . , xL)\ninto queries Q = XW Q, keys K = XW K, and values V = XW V , where W Q, W K, and W V are\neach D × D square matrices. Each L × D query, key, and value matrix is then split into H L × Dh\nparts or attention heads, indexed by h, and with dimension Dh = D\nH , which allow the model to focus\non different parts of the history. The scaled dot-product attention computes a sequence of vector\noutputs for each head as\nZh = Attention(Qh, Kh, V h) = Softmax\n\u0012QhKh⊤\n√Dh\n\u0013\nV h.\n(1)\nThe attention outputs for each head are concatenated and linearly transformed to get Z, a L by D\ndimensional matrix. A upper triangular mask ensures that queries cannot attend to keys later in the\nsequence. For other details of the Transfomer model, such as residual connections and learning rates,\nthe reader can refer Vaswani et al. (2017). The feedforward (FF) sub-layer then takes the output Z\n3\nfrom the previous attention sub-layer, and performs two layers of point-wise dense layers on the depth\nD dimension, as shown in Equation 2. W1, W2, b1, b2 are weights and biases of those two layers.\nFF(Z) = ReLU(ZW1 + b1)W2 + b2\n(2)\n3.3\nRELATIVE POSITIONAL SELF-ATTENTION\nAs the Transformer model relies solely on positional sinusoids to represent timing information, Shaw\net al. (2018) introduced relative position representations to allow attention to be informed by how far\ntwo positions are apart in a sequence. This involves learning a separate relative position embedding\nEr of shape (H, L, Dh), which has an embedding for each possible pairwise distance r = jk −iq\nbetween a query and key in position iq and jk respectively. The embeddings are ordered from distance\n−L + 1 to 0, and are learned separately for each head. In Shaw et al. (2018), the relative embeddings\ninteract with queries and give rise to a Srel, an L × L dimensional logits matrix which modulates the\nattention probabilities for each head as:\nRelativeAttention = Softmax\n\u0012QK⊤+ Srel\n√Dh\n\u0013\nV.\n(3)\nWe dropped head indices for clarity. Our work uses the same approach to infuse relative distance\ninformation in the attention computation, while signiﬁcantly improving upon the memory footprint\nfor computing Srel. For each head, Shaw et al. (2018) instantiate an intermediate tensor R of shape\n(L, L, Dh), containing the embeddings that correspond to the relative distances between all keys and\nqueries. Q is then reshaped to an (L, 1, Dh) tensor, and Srel = QR⊤.2 This incurs a total space\ncomplexity of O(L2D), restricting its application to long sequences.\n3.4\nMEMORY EFFICIENT IMPLEMENTATION OF RELATIVE POSITION-BASED ATTENTION\nWe improve the implementation of relative attention by reducing its intermediate memory requirement\nfrom O(L2D) to O(LD), with example lengths shown in Table 1. We observe that all of the terms\nwe need from QR⊤are already available if we directly multiply Q with Er, the relative position\nembedding. After we compute QEr⊤, its (iq, r) entry contains the dot product of the query in\nposition iq with the embedding of relative distance r. However, each relative logit (iq, jk) in the\nmatrix Srel from Equation 3 should be the dot product of the query in position iq and the embedding\nof the relative distance jk −iq, to match up with the indexing in QK⊤. We therefore need to “skew”\nQEr⊤so as to move the relative logits to their correct positions, as illustrated in Figure 1 and detailed\nin the next section. The time complexity for both methods are O(L2D), while in practice our method\nis 6x faster at length 650.\nFigure 1: Relative global attention: the bottom row describes our memory-efﬁcient “skewing”\nalgorithm, which does not require instantiating R (top row, which is O(L2D)). Gray indicates\nmasked or padded positions. Each color corresponds to a different relative distance.\n2We assume that the batch size is 1 here. With a batch size of B, Q would be reshaped to (L, B, Dh) and\nSrel would be computed with a batch matrix–matrix product.\n4\nTable 1: Comparing the overall relative memory complexity (intermediate relative embeddings (R or\nEr) + relative logits Srel), the maximal training lengths that can ﬁt in a GPU with 16GB memory\nassuming Dh = 64, and the memory usage per layer per head (in MB).\nImplementation\nRelative memory\nMaximal L\nL = 650\nL = 2048\nL = 3500\nShaw et al. (2018)\nO(L2D + L2)\n650\n108 + 1.7\n1100 + 16\n3100 + 49\nOurs\nO(LD + L2)\n3500\n0.17 + 1.7\n0.52 + 16\n0.90 + 49\n3.4.1\nTHE “SKEWING” PROCEDURE\nHence, we propose a “skewing” procedure to transform an absolute-by-relative (iq, r) indexed matrix\ninto an absolute-by-absolute (iq, jk) indexed matrix. The row indices iq stay the same while the\ncolumns indices are shifted according to the following equation: jk = r −(L −1) + iq. For example\nin Figure 1 the upper right green dot in position (0, 2) of QEr⊤after skewing has a column index of\n2 −(3 −1) + 0 = 0, resulting in a position of (0, 0) in Srel.\nWe outline the steps illustrated in Figure 1 below.\n1. Pad a dummy column vector of length L before the leftmost column.\n2. Reshape the matrix to have shape (L+1, L). (This step assumes NumPy-style row-major ordering.)\n3. Slice that matrix to retain only the last l rows and all the columns, resulting in a (L, L) matrix\nagain, but now absolute-by-absolute indexed, which is the Srel that we need.\n3.5\nRELATIVE LOCAL ATTENTION\nFor very long sequences, the quadratic memory requirement of even baseline Transformer is imprac-\ntical. Local attention has been used for example in Wikipedia and image generation (Liu et al., 2018;\nParmar et al., 2018) by chunking the input sequence into non-overlapping blocks. Each block then\nattends to itself and the one before, as shown by the smaller thumbnail on the top right corner of\nFigure 2.\nTo extend relative attention to the local case, we ﬁrst note that the right block has the same conﬁgura-\ntion as in the global case (see Figure 1) but much smaller: ( L\nM )2 (where M is the number of blocks,\nand N be the resulting block length) as opposed to L2. The left block is unmasked with relative\nindices running from -1 (top right) to -2N + 1 (bottom left). Hence, the learned Er for the local case\nhas shape (2N −1, N).\nSimilar to the global case, we ﬁrst compute QEr⊤and then use the following procedure to skew it to\nhave the same indexing as QK⊤, as illustrated in Figure 2.\n1. Pad a dummy column vector of length N after the rightmost column.\n2. Flatten the matrix and then pad with a dummy row of length N −1.\n3. Reshape the matrix to have shape (N + 1, 2N −1).\n4. Slice that matrix to retain only the ﬁrst N rows and last N columns, resulting in a (N, N) matrix.\nFigure 2: Relative local attention: the thumbnail on the right shows the desired conﬁguration for Srel.\nThe “skewing” procedure is shown from left to right.\n5\n4\nEXPERIMENTS\n4.1\nJ.S. BACH CHORALES\nJ.S. Bach chorales is a canonical dataset used for evaluating generative models for music 3 (e.g.,\nAllan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Liang, 2016; Hadjeres et al., 2016;\nHuang et al., 2017). It consists of score-based four-part chorales. We ﬁrst discretize the scores\nonto a 16th-note grid, and then serialize it by iterating through all the voices within a time step\nand then advancing time (see A.1 for more details). As there is a direct correspondence between\nposition in sequence and position on the timing/instrument grid in a piece, adding relative position\nrepresentations could make it easier to learn this grammar. We indeed see relative attention drastically\nimprove negative log-likelihood (NLL) over baseline Transformer (Table 2). This improvement is\nalso reﬂected in sample quality. The samples now maintain the necessary timing/instrument grid,\nalways advancing four steps before advancing in time. As local timing is maintained, the model is\nable to capture timing on a more global level, giving rise to regular phrasing, as shown in Figure 3.\nFigure 3: Unconditioned samples from Transformer without (left) and with (right) relative self-\nattention. Green vertical boxes indicate the endings of (sub)phrases where cadences are held.\nIn addition to relative attention, we also explored enhancing absolute timing through concatenating\ninstead of adding the sinusoids to the input embeddings. This allows the model to more directly\nlearn its absolute positional mapping. This further improves performance for both the baseline and\nrelative transformer (Table 2). We compare against COCONET as it is one of the best-performing\nmodels that has also been evaluated on the 16-note grid using the canonical dataset split. To\ndirectly compare, we re-evaluated COCONET to obtain note-wise losses on the validation set 4. For\nthe Transformer models (abbreviated as TF), we implemented our attention mechanisms in the\nTensor2Tensor framework (Vaswani et al., 2018). We use 8 heads, and keep the query, key (att) and\nvalue hidden size (hs) ﬁxed within a conﬁg. We tuned number of layers (L in {4,5,6}), attention\nhidden size (att in {256, 512}) and pointwise feedforward hidden size (ff in {512, 1024}).\n4.1.1\nGENERALIZING RELATIVE ATTENTION TO CAPTURE RELATIONAL INFORMATION\nA musical event bears multiple attributes, such as timing, pitch, instrument etc. To capture more\nrelational information, we extend relative attention to capture pairwise distances on additional\nattributes. We learn separate relative embeddings for timing Et and also pitch Ep. Et has entries\ncorresponding to how many sixteenth notes apart are two positions, while Ep embeds the pairwise\npitch interval. However this approach is not directly scalable beyond J.S. Bach Chorales because it\ninvolves explicitly gathering relative embeddings for Rt and Rp, resulting in a memory complexity\nof O(L2D) as in Shaw et al. (2018). This is due to relative information being computed based on\ncontent as opposed to content-invariant information such as position in sequence. It was sufﬁcient to\nadd the extra timing signals to the ﬁrst layer, perhaps because it is closest to the raw input content.\nHere, the relative logits are computed from three terms, Srel = Skew(QEr) + Q(Rt + Rp) in\ncontrast with other layers that only have one term, Skew(QEr).\n4.2\nPIANO-E-COMPETITION\nWe use the ﬁrst 6 years of of Piano-e-Competition because these years have corresponding MIDI data\nreleased 5, resulting in about 1100 pieces, split 80/10/10. Each piece is MIDI data capturing a classical\npiano performance with expressive dynamics and timing, encoded with the MIDI-like representation\n3J.S. Bach chorales dataset: https://github.com/czhuang/JSB-Chorales-dataset\n4Some earlier papers report frame-wise losses to compare to models such as RNN-RBM which model\n“chords”. Coconet can be evaluated under note-wise or frame-wise losses.\n5Piano-e-Competition dataset (competition history): http://www.piano-e-competition.com/\n6\ndescribed in Section A.2. We trained on random crops of 2000-token sequences and employed\ntwo kinds of data augmentation: pitch transpositions uniformly sampled from {−3, −2, . . . , 2, 3}\nhalf-steps, and time stretches uniformly sampled from the set {0.95, 0.975, 1.0, 1.025, 1.05}.\nWe compare to Magenta’s PerformanceRNN (LSTM, which ﬁrst used this dataset) (Oore et al.,\n2018) and LookBack RNN (LSTM with attention) (Waite, 2016). LookBack RNN uses an input\nrepresentation that requires monophonic music with barlines which is information that is not present\nin performed polyphonic music data, hence we simply adopt their architecture. Table 3 shows that\nTransformer-based architectures ﬁts this dataset better than LSTM-based models.\nTable 2: Note-wise validation NLL on J.S.Bach Chorales at 16th notes. Relative attention, more\ntiming and relational information improve performance.\nModel variation\nValidation NLL\nCOCONET (CNN, chronological, 64L, 128 3x3f)\n0.436\nCOCONET (CNN, orderless, 64L, 128 3x3f)\n≤0.238 6\nTransformer (TF) baseline (Vaswani et al., 2017) (5L, 256hs, 256att, 1024ff, 8h)\n0.417\nTF baseline + concat positional sinusoids (cps)\n0.398\nTF baseline + concat positional sinusoids, instrument labels (cpsi)\n0.370\nRelative Transformer (Shaw et al., 2018) (5L, 512hs, 512att, 512ff, 256r, 8h)\n0.357\nRelative Transformer + concat positional sinusoids, instrument labels (cpsi)\n0.347\nRelative Transformer + cpsi + relative pitch and time\n0.335\nTable 3: Validation NLL for Piano-e-Competition dataset, with event-based representation with\nlengths L = 2048. Transformer with relative attention (with our efﬁcient formulation) achieves\nstate-of-the-art performance.\nModel variation\nValidation NLL\nPERFORMANCE RNN (LSTM) (3L, 1024hs)\n1.969\nLSTM with attention (3L, 1024hs, 1024att)\n1.959\nTransformer (TF) baseline (6L, 256hs, 512att, 2048fs, 1024r, 8h)\n1.861\nTF with local attention (Liu et al., 2018) (8L, 1024fs, 512bs)\n1.863\nTF with relative global attention (our efﬁcient formulation) (6L, 2048fs, 1024r)\n1.835\nTF with relative local attention (ours) (6L, 1024fs, 2048r, 512bs)\n1.840\nWe implemented our attention mechanisms in the Tensor2Tensor framework (Vaswani et al., 2018),\nand used the default hyperparameters for training, with 0.1 learning rate, 0.1 dropout, and early\nstopping. We compare four architectures, varying on two axes: global versus local, and regular versus\nrelative attention. We found that reducing the query and key hidden size (att) to half the hidden size\n(hs) works well and use this relationship for all of the models, while tuning on number of layers\n(L) and ﬁlter size (fs). We use block size (bs) 512 for local attention. We set the maximum relative\ndistance to consider to half the training sequence length for relative global attention, and to the full\nmemory length (which is two blocks) for relative local attention. Table 3 show that relative attention\n(global or local) outperforms regular self-attention (global or local). All else being equal, local and\nglobal attention perform similarly. Each though local attention does not see all the history at once, it\ncan build up a larger receptive ﬁeld across layers. This can be an advantage in the future for training\non much longer sequences, as local attention requires much less memory.\n6COCONET is an instance of OrderlessNADE, an ensemble over orderings. The chronological loss evaluates\nthe model as autoregressive, from left to right. We can also evaluate the model as a mixture, by averaging its\nlosses over multiple random orderings. This is a lower bound on log-likelihood. It is intractable to sample from\nexactly but can be approximated through Gibbs sampling.\n7\nFigure 4: Comparing how models continue a prime (top left). Repeated motives and structure are seen\nin samples from Transformer with relative attention (top row), but less so from baseline Transformer\n(middle row) and PerformanceRNN (LSTM) (bottom row).\n4.2.1\nQUALITATIVE PRIMING EXPERIMENTS\nWhen primed with an initial motif (Chopin’s Étude Op. 10, No. 5) shown in the top left corner of\nFigure 4, we see the models perform qualitatively differently. Transformer with relative attention\nelaborates the motif and creates phrases with clear contour which are repeated and varied. Baseline\nTransformer uses the motif in a more uniform fashion, while LSTM uses the motif initially but soon\ndrifts off to other material. Note that the generated samples are twice as long as the training sequences.\nRelative attention was able to generalize to lengths longer than trained but baseline Transformer\ndeteriorates beyond its training length. See Appendix C for visualizations of how the our Relative\nTransformer attends to past motifs.\n4.2.2\nHARMONIZATION: CONDITIONING ON MELODY\nTo explore the sequence-to-sequence setup of Transformers,\nwe experimented with a conditioned generation task where\nthe encoder takes in a given melody and the decoder has to\nrealize the entire performance, i.e. melody plus accompani-\nment. The melody is encoded as a sequence of tokens as in\nWaite (2016), quantized to a 100ms grid, while the decoder\nuses the performance encoding described in Section 3.1 (and\nfurther illustrated in A.2). We use relative attention on the de-\ncoder side and show in Table 4 it also improves performance.\nTable 4:\nValidation conditional\nNLL given groundtruth melody from\nPiano-e-Competition.\nModel variation\nNLL\nBaseline Transformer\n2.066\nRelative Transformer (ours)\n1.786\n4.2.3\nHUMAN EVALUATIONS\nTo compare the perceived sample quality of models trained on the Piano-e-Competition dataset,\nand their ability to generate a continuation for a priming sequence, we carried out a listening test\nstudy comparing the baseline Transformer, our Transformer with relative-attention, PerformanceRNN\n(LSTM), and the validation set. Participants were presented with two musical excerpts (from two\ndifferent models that were given the same priming sequence) and asked to rate which one is more\nmusical on a Likert scale. For each model, we generated 10 samples each with a different prime, and\ncompared them to three other models, resulting in 60 pairwise comparisons. Each pair was rated by 3\ndifferent participants, yielding a total of 180 comparisons.\nFigure 5 shows the number of comparisons in which an excerpt from each model was selected as\nmore musical. The improvement in sample quality from using relative attention over the baseline\nTransformer model was statistically signiﬁcant (see Appendix B for the analysis), both in aggregate\nand between the pair. Even though in aggregate LSTMs performed better in the study than the\nTransformer, despite having higher perplexity, but when compared against each other head to head,\nthe results were not statistically signiﬁcant (see Table 5 in Appendix B).\n8\n(Ours)\nFigure 5: Number of wins for each model. Error bars show standard deviations of mean.\n5\nCONCLUSION\nIn this work we demonstrated that the Transformer equipped with relative attention is very well-suited\nfor generative modeling of symbolic music. The compelling long-term structure in the samples from\nour model leaves us enthusiastic about this direction of research. Moreover, the ability to expand\nupon a primer, in particular, suggests potential applications as creative tool.\nThe signiﬁcant improvement from relative attention highlights a shortcoming of the original Trans-\nformer that might also limit its performance in other domains. Improving the Transformer’s ability to\ncapture periodicity at various time scales, for instance, or relations between scalar features akin to\npitch could improve time-series models. Our memory-efﬁcient implementation enables the appli-\ncation of relative attention to much longer sequences such as long texts or even audio waveforms,\nwhich signiﬁcantly broadens the range of problems to which it could be applied.\n6\nACKNOWLEDGEMENT\nWe thank many colleagues from the Transformer (Vaswani et al., 2017) and Tensor2Tensor (Vaswani\net al., 2018) papers for helping us along the way: Lukasz Kaiser, Ryan Sepassi, Niki Parmar and Llion\nJones. Many thanks to Magenta and friends for their support throughout and for many insightful\ndiscussions: Jesse Engel, Adam Roberts, Fred Bertsch, Erich Elsen, Sander Dieleman, Sageev Oore,\nCarey Radebaugh, Natasha Jaques, Daphne Ippolito, Sherol Chan, Vida Vakilotojar, Dustin Tran,\nBen Poole and Tim Cooijmans.\nREFERENCES\nMoray Allan and Christopher KI Williams. Harmonising chorales by probabilistic inference. Advances\nin neural information processing systems, 17:25–32, 2005.\nNicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependen-\ncies in high-dimensional sequences: Application to polyphonic music generation and transcription.\nInternational Conference on Machine Learning, 2012.\nHao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. Musegan: Multi-track sequential\ngenerative adversarial networks for symbolic music generation and accompaniment. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, 2018.\nDouglas Eck and Juergen Schmidhuber. Finding temporal structure in music: Blues improvisation\nwith lstm recurrent networks. In Proceedings of the 12th IEEE Workshop on Neural Networks for\nSignal Processing, 2002.\nGaëtan Hadjeres, Jason Sakellariou, and François Pachet. Style imitation and chord invention in\npolyphonic music with exponential families. arXiv preprint arXiv:1609.05152, 2016.\nGaëtan Hadjeres, François Pachet, and Frank Nielsen. Deepbach: a steerable model for bach chorales\ngeneration. In International Conference on Machine Learning, pp. 1362–1371, 2017.\nGeoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural computation, 18(7):1527–1554, 2006.\n9\nCheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, and Doug Eck. Coun-\nterpoint by convolution. In Proceedings of the International Conference on Music Information\nRetrieval, 2017.\nHugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In AISTATS,\nvolume 1, pp. 2, 2011.\nStefan Lattner, Maarten Grachten, and Gerhard Widmer. Imposing higher-level structure in poly-\nphonic music generation using convolutional restricted boltzmann machines and constraints.\nJournal of Creative Music Systems, 2(2), 2018.\nFeynman Liang. Bachbot: Automatic composition in the style of bach chorales. Masters thesis,\nUniversity of Cambridge, 2016.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating wikipedia by summarizing long sequences. In Proceedings of the International\nConference on Learning Representations, 2018.\nSageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and Karen Simonyan. This time with\nfeeling: Learning expressive musical performance. arXiv preprint arXiv:1808.03715, 2018.\nAnkur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel for natural language inference. In Proceedings of the Conference on Empirical Methods in\nNatural Language Processing, 2016.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, and Alexander Ku.\nImage transformer. In Proceedings of the International Conference on Machine Learning, 2018.\nDaniel Povey, Hossein Hadian, Pegah Ghahremani, Ke Li, and Sanjeev Khudanpur. A time-restricted\nself-attention layer for ASR. In Proceddings of the IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2018.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-\ntions. In Proceedings of the Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, volume 2, 2018.\nPaul Smolensky. Information processing in dynamical systems: Foundations of harmony theory.\nTechnical report, DTIC Document, 1986.\nBenigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In\nInternational Conference on Machine Learning, pp. 467–475, 2014.\nBenigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural\nautoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–\n7220, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems, 2017.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and\nJakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.\nElliot Waite.\nGenerating long-term structure in songs and stories.\nhttps://magenta.\ntensorflow.org/2016/07/15/lookback-rnn-attention-rnn, 2016.\n10\nA\nDOMAIN-SPECIFIC REPRESENTATIONS\nAdapting sequence models for music requires making decisions on how to serialize a polyphonic\ntexture. The data type, whether score or performance, makes certain representations more natural for\nencoding all the information needed while still resulting in reasonable sequence lengths.\nA.1\nSERIALIZED INSTRUMENT/TIME GRID (J.S.BACH CHORALES)\nThe ﬁrst dataset, J.S. Bach Chorales, consists of four-part score-based choral music. The time\nresolution is sixteenth notes, making it possible to use a serialized grid-like representation. Figure 6\nshows how a pianoroll (left) can be represented as a grid (right), following (Huang et al., 2017). The\nrows show the MIDI pitch number of each of the four voices, from top to bottom being soprano (S),\nalto (A), tenor (T) and bass (B), while the columns is discretized time, advancing in sixteenth notes.\nHere longer notes such as quarter notes are broken down into multiple repetitions. To serialize the\ngrid into a sequence, we interleave the parts by ﬁrst iterating through all the voices at time step 1, and\nthen move to the next column, and then iterate again from top to bottom, and so on. The resulting\nsequence is S1A1T1B1S2A2T2B2..., where the subscript gives the time step. After serialization, the\nmost common sequence length is 1024. Each token is represented as onehot in pitch.\nS: 67, 67, 67, 67\nA: 62, 62, 62, 62\nT: 59, 59, 57, 57\nB: 43, 43, 45, 45\nFigure 6: The opening measure of BWV 428 is visualized as a pianoroll (left, where the x-axis is\ndiscretized time and y-axis is MIDI pitch number), and encoded in grid representation with sixteenth\nnote resolution (right). The soprano and alto voices have quarter notes at pitches G4 (67) and D4 (62),\nthe tenor has eighth notes at pitches B3 (59) and A3 (57), and the bass has eighth notes at pitches A2\n(45) and G2 (43).\nA.2\nMIDI-LIKE EVENT-BASED (PIANO-E-COMPETITION)\nThe second dataset, Piano-e-Competition, consists of polyphonic piano performances with expressive\ntiming and dynamics. The time resolution here is on the millisecond level, so a grid representation\nwould result in sequences that are too long. Instead, the polyphonic performance is serialized into a\nsequence of one hot encoded events as proposed in (Oore et al., 2018).\nFirst, the input MIDI ﬁles are preprocessed to extend note durations based on sustain pedal control\nevents. The sustain pedal is considered to be down whenever a sustain control change is encountered\nwith a value >= 64; the sustain pedal is then considered up after a control change with a value < 64.\nWithin a period where the sustain pedal is down, the duration of each note is extended to either the\nbeginning of the next note of the same pitch or the end of the sustain period, whichever happens ﬁrst.\nIf the original duration extends beyond the time when the sustain pedal is down, that original duration\nis used.\nNext, the MIDI note events are converted into a sequence from the following set of vocabulary: 128\nNOTE_ON events for starting a note of with one of the 128 MIDI pitches, 128 NOTE_OFF events\nfor ending a note with one of the 128 MIDI pitches, 100 TIME_SHIFT events representing forward\ntime shifts in 10ms increments from 10ms to 1s, and 32 SET_VELOCITY events representing the\nvelocity for future NOTE_ON events in the form of the 128 possible MIDI velocities quantized into\n32 bins. An example performance encoding is illustrated in Figure 7.\n11\nSET_VELOCITY<80>, NOTE_ON<60>\nTIME_SHIFT<500>, NOTE_ON<64>\nTIME_SHIFT<500>, NOTE_ON<67>\nTIME_SHIFT<1000>, NOTE_OFF<60>, NOTE_OFF<64>,\nNOTE_OFF<67>\nTIME_SHIFT<500>, SET_VELOCITY<100>, NOTE_ON<65>\nTIME_SHIFT<500>, NOTE_OFF<65>\nFigure 7: A snippet of a piano performance visualized as a pianoroll (left) and encoded as performance\nevents (right, serialized from left to right and then down the rows). A C Major chord is arpeggiated\nwith the sustain pedal active. At the 2-second mark, the pedal is released, ending all of the notes. At\nthe 3-second mark, an F is played for .5 seconds. The C chord is played at velocity 80 and the F is\nplayed at velocity 100.\nB\nSUPPLEMENT OF LISTENING TEST\nB.1\nSTUDY PROCEDURE\nParticipants were presented with two musical excerpts that shared a common priming sequence. For\neach excerpt, the priming sequence was played, followed by 2.5 seconds of silence, followed by the\npriming sequence again and a continuation of that sequence. The continuations were either sampled\nfrom one of the models or extracted from our validation set. We evaluated all possible pairs in the\nspace of data and model samples, except from the same model. Each continuation had a length of 512\nevents using the encoding described in Section A.2. This corresponds to the length the models were\ntrained on to remove the deteriorating effect that happens with baseline Transformer when asked to\ngenerate beyond the length it was trained on. Participants were asked which excerpt they thought was\nmore musical on a Likert scale of 1 to 5. The pair is laid out left versus right, with 1 indicating the left\nis much more musical, 2 the left is slightly more musical, 3 being a tie, 4 being the right is slightly\nmore musical, and 5 the right is much more musical. For each model, we generated 10 samples\neach with a different prime, and compared them to three other models, resulting in 60 pairwise\ncomparisons. Each pair was rated by 3 different participants, yielding a total of 180 comparisons.\nB.2\nANALYSIS\nA Kruskal-Wallis H test of the ratings showed that there was a statistically signiﬁcant difference\nbetween the models: χ2(2) = 63.84, p = 8.86e-14< 0.01. Table 5 show a post-hoc analysis on the\ncomparisons within each pair, using the Wilcoxon signed-rank test for matched samples. Table 6\nshows a post-hoc analysis of how well each model performed when compared to all pairs, and\ncompares each model’s aggregate against each other, using the Mann–Whitney U test for independent\nsamples. We use a Bonferroni correction on both to correct for multiple comparisons. The win and\nloss counts bucket scores 4, 5 and scores 1, 2 respectively, while the tieing score is 3.\nBoth within pairs and between aggregates, participants rated samples from our relative Transformer\nas more musical than the baseline Transformer with p < 0.01/6.\nFor within pairs, we did not observe a consistent statistically signiﬁcant difference between the other\nmodel pairs, baseline transformer versus LSTM and LSTM versus relative Transformer.\nWhen comparing between aggregates, LSTM was overall perceived as more musical than baseline\nTransformer. Relative Transformer came a bit close to outperforming LSTM with p = 0.018. When\nwe listen to the samples from the two, they do sound qualitatively different. Relative Transformer\noften exhibits much more structure (as shown in Figure 4), but the effects were probably less\npronounced in the listening test because we used samples around 10s to 15s, which is half the length\nof those shown in Figure 4 to prevent the baseline Transformer from deteriorating. This weakens the\ncomparison on long-term structure.\nWhen compared to real music from the validation set, we see that in aggregates, real music was better\nthan LSTM and baseline Transformer. There was no statistical signiﬁcant difference between real\nmusic and relative Transformer. This is probably again due to the samples being too short as real\nmusic is deﬁnitely still better.\n12\nTable 5: A post-hoc comparison of each pair on their pairwise comparisons with each other, using the\nWilcoxon signed-rank test for matched samples. p value less than 0.01/6=0.0016 yields a statistically\nsigniﬁcant difference and is marked by asterisk.\nPairs\nwins\nties\nlosses\np value\nOur relative transformer\nreal music\n11\n4\n15\n0.243\nOur relative transformer\nBaseline transformer\n23\n1\n6\n0.0006*\nOur relative transformer\nLSTM\n18\n1\n11\n0.204\nBaseline transformer\nLSTM\n5\n3\n22\n0.006\nBaseline transformer\nreal music\n6\n0\n24\n0.0004*\nLSTM\nreal music\n6\n2\n22\n0.0014\nTable 6: Comparing each pair on their aggregates (comparisons with all models) in (wins, ties, losses),\nusing the Mann–Whitney U test for independent samples.\nModel\nModel\np value\nOur relative transformer\n(52, 6, 32)\nreal music\n(61, 6, 23)\n0.020\nOur relative transformer\n(52, 6, 32)\nBaseline transformer\n(17, 4, 69)\n1.26e-9*\nOur relative transformer\n(52, 6, 32)\nLSTM\n(39, 6, 45)\n0.018\nBaseline transformer\n(17, 4, 69)\nLSTM\n(39, 6, 45)\n3.70e-5*\nBaseline transformer\n(17, 4, 69)\nreal music\n(61, 6, 23)\n6.73e-14*\nLSTM\n(39, 6, 45)\nreal music\n(61, 6, 23)\n4.06e-5*\nC\nVISUALIZING SOFTMAX ATTENTION\nOne advantage of attention-based models is that we can visualize its attention distribution 3. This\ngives us a glimpse of how the model might be building up recurring structures and how far it is\nattending back. The pianorolls in the visualizations below is a sample generated from Transformer\nwith relative attention. Each ﬁgure shows a query (the source of all the attention lines) and previous\nmemories being attended to (the notes that are receiving more softmax probabiliy is highlighted in).\nThe coloring of the attention lines correspond to different heads and the width to the weight of the\nsoftmax probability.\nFigure 8: This piece has a recurring triangular contour. The query is at one of the latter peaks and it\nattends to all of the previous high notes on the peak, all the way to beginning of the piece.\nFigure 9: The query a note in the left-hand, and it attends to its immediate past neighbors and mostly\nto the earlier left hand chords, with most attention lines distributed in the lower half of the pianoroll.\n13\nD\nPREVIOUS FIGURES FOR THE “SKEWING” PROCEDURE\n0\n0\n.\n.\n.\n0\n0\n0\n.\n.\n.\n0\n0\n0\n0\n.\n.\n.\nSteps 1\nSteps 2,3:\nFigure 10: Relative global attention: Steps (from left to right) for “skewing” an absolute-by-relative\n(iq, r) indexed matrix into absolute-by-absolute (iq, jk). Grey indicates self-attention masks or entries\nintroduced by the skewing procedure. Positions with relative distance zero are marked. Entries\noutlined by purple are removed in step 3.\n(N+1, 2N-1)\n-1\n-N\n-2N+1\n-N\n(N, N)\n0\n0\n0\n.\n.\n.\n-1\n-N\n-2N+1\n-N\n-1\n-2N+1\n(N, 2N-1)\nPad N-1 after flatten \nSteps 1, 2\nSteps 3\nSteps 4\nFigure 11: Relative local attention: Steps (from left to right) for “skewing” an (iq, r) indexed matrix\nwith 2N −1 ranged relative indices r into (iq, jk indexed. Shapes are indicated above the boxes,\nwhile indices in the boxes give relative distances.\n14\n",
  "pages": [
    {
      "page_number": 1,
      "text": "MUSIC TRANSFORMER:\nGENERATING MUSIC WITH LONG-TERM STRUCTURE\nCheng-Zhi Anna Huang∗Ashish Vaswani\nJakob Uszkoreit\nNoam Shazeer\nIan Simon\nCurtis Hawthorne\nAndrew M. Dai\nMatthew D. Hoffman\nMonica Dinculescu\nDouglas Eck\nGoogle Brain\nABSTRACT\nMusic relies heavily on repetition to build structure and meaning. Self-reference\noccurs on multiple timescales, from motifs to phrases to reusing of entire sections\nof music, such as in pieces with ABA structure. The Transformer (Vaswani\net al., 2017), a sequence model based on self-attention, has achieved compelling\nresults in many generation tasks that require maintaining long-range coherence.\nThis suggests that self-attention might also be well-suited to modeling music.\nIn musical composition and performance, however, relative timing is critically\nimportant. Existing approaches for representing relative positional information\nin the Transformer modulate attention based on pairwise distance (Shaw et al.,\n2018). This is impractical for long sequences such as musical compositions since\ntheir memory complexity for intermediate relative information is quadratic in the\nsequence length. We propose an algorithm that reduces their intermediate memory\nrequirement to linear in the sequence length. This enables us to demonstrate that a\nTransformer with our modiﬁed relative attention mechanism can generate minute-\nlong compositions (thousands of steps, four times the length modeled in Oore et al.\n(2018)) with compelling structure, generate continuations that coherently elaborate\non a given motif, and in a seq2seq setup generate accompaniments conditioned on\nmelodies1. We evaluate the Transformer with our relative attention mechanism on\ntwo datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art\nresults on the latter.\n1\nINTRODUCTION\nA musical piece often consists of recurring elements at various levels, from motifs to phrases to\nsections such as verse-chorus. To generate a coherent piece, a model needs to reference elements\nthat came before, sometimes in the distant past, repeating, varying, and further developing them to\ncreate contrast and surprise. Intuitively, self-attention (Parikh et al., 2016) appears to be a good match\nfor this task. Self-attention over its own previous outputs allows an autoregressive model to access\nany part of the previously generated output at every step of generation. By contrast, recurrent neural\nnetworks have to learn to proactively store elements to be referenced in a ﬁxed size state or memory,\npotentially making training much more difﬁcult. We believe that repeating self-attention in multiple,\nsuccessive layers of a Transformer decoder (Vaswani et al., 2017) helps capture the multiple levels at\nwhich self-referential phenomena exist in music.\nIn its original formulation, the Transformer relies on absolute position representations, using either\npositional sinusoids or learned position embeddings that are added to the per-position input repre-\nsentations. Recurrent and convolutional neural networks instead model position in relative terms:\nRNNs through their recurrence over the positions in their input, and CNNs by applying kernels that\neffectively choose which parameters to apply based on the relative position of the covered input\nrepresentations.\n∗Google AI Resident. Correspondence to: Cheng-Zhi Anna Huang <annahuang@google.com>\n1Samples are available for listening at\nhttps://storage.googleapis.com/music-transformer/index.html\n1\narXiv:1809.04281v3  [cs.LG]  12 Dec 2018\n"
    },
    {
      "page_number": 2,
      "text": "Music has multiple dimensions along which relative differences arguably matter more than their\nabsolute values; the two most prominent are timing and pitch. To capture such pairwise relations\nbetween representations, Shaw et al. (2018) introduce a relation-aware version of self-attention which\nthey use successfully to modulate self-attention by the distance between two positions. We extend\nthis approach to capture relative timing and optionally also pitch, which yields improvement in both\nsample quality and perplexity for JSB Chorales. As opposed to the original Transformer, samples\nfrom a Transformer with our relative attention mechanism maintain the regular timing grid present in\nthis dataset. The model furthermore captures global timing, giving rise to regular phrases.\nThe original formulation of relative attention (Shaw et al., 2018) requires O(L2D) memory where L\nis the sequence length and D is the dimension of the model’s hidden state. This is prohibitive for long\nsequences such as those found in the Piano-e-Competition dataset of human-performed virtuosic,\nclassical piano music. In Section 3.4, we show how to reduce the memory requirements to O(LD),\nmaking it practical to apply relative attention to long sequences.\nThe Piano-e-Competition dataset consists of MIDI recorded from performances of competition partic-\nipants, bearing expressive dynamics and timing on the granularity of < 10 miliseconds. Discretizing\ntime on a ﬁxed grid that would yield unnecessarily long sequences as not all events change on the\nsame timescale. We hence adopt a sparse, MIDI-like, event-based representation from (Oore et al.,\n2018), allowing a minute of music with 10 milisecond resolution to be represented at lengths around\n2K, as opposed to 6K to 18K on a ﬁxed-grid representation with multiple performance attributes. As\nposition in sequence no longer corresponds to time, a priori it is not obvious that relative attention\nshould work as well with such a representation. However, we will show in Section 4.2 that it does\nimprove perplexity and sample quality over strong baselines.\nWe speculate that idiomatic piano gestures such as scales, arpeggios and other motifs all exhibit a\ncertain grammar and recur periodically, hence knowing their relative positional distances makes it\neasier to model this regularity. This inductive bias towards learning relational information, as opposed\nto patterns based on absolute position, suggests that the Transformers with relative attention could\ngeneralize beyond the lengths it was trained on, which our experiments in Section 4.2.1 conﬁrm.\n1.1\nCONTRIBUTIONS\nDomain contributions\nWe show the ﬁrst successful use of Transformers in generating music that\nexhibits long-term structure. Before our work, LSTMs were used at timescales of 15s (~500 tokens)\non the Piano-e-Competition dataset (Oore et al., 2018). Our work show that Transformers not only\nachieve state-of-the-art perplexity on modeling these complex expressive piano performances, but\ncan also generate them at the scale of 60s (~2000 tokens) with remarkable internal consistency. Our\nrelative attention mechanism is essential to the model’s quality. In listening tests (see Section 4.2.3),\nsamples from models with relative self-attention were perceived as more coherent than the baseline\nTransformer model from Vaswani et al. (2017). Relative attention not only enables Transformers to\ngenerate continuations that elaborate on a given motif, but also to generalize and generate in consistent\nfashion beyond the length it was trained on (see Section 4.2.1). In a seq2seq setup, Transformers can\ngenerate accompaniments conditioned on melodies, enabling users to interact with the model.\nAlgorithmic contributions\nThe space complexity of the relative self attention mechanism in its\noriginal formulation (Shaw et al., 2018) made it infeasible to train on sequences of sufﬁcient length\nto capture long-range structure in longer musical compositions. Addressing this we present a crucial\nalgorithmic improvement to the relative self attention mechanism, dramatically reducing its memory\nrequirements from O(L2D) to O(LD). For example, we reduce the memory consumption per layer\nfrom 8.5 GB to 4.2 MB (per head from 1.1 GB to 0.52 MB) for a sequence of length L = 2048 and\nhidden-state size D = 512 (per head Dh = D\nH = 64, where number of heads is H = 8) (see Table 1),\nallowing us to use GPUs to train the relative self-attention Transformer on long sequences.\n2\nRELATED WORK\nSequence models have been the canonical choice for modeling music, from Hidden Markov Models\nto RNNs and Long Short Term Memory networks (e.g., Eck & Schmidhuber, 2002; Liang, 2016;\nOore et al., 2018), to bidirectional LSTMs (e.g., Hadjeres et al., 2017). Successful application of\nsequential models to polyphonic music often requires serializing the musical score or performance\n2\n"
    },
    {
      "page_number": 3,
      "text": "into a single sequence, for example by interleaving different instruments or voices. Alternatively,\na 2D pianoroll-like representation (see A.1 for more details) can be decomposed into a sequence\nof multi-hot pitch vectors, and their joint probability distributions can be captured using Restricted\nBoltzmann Machines (Smolensky, 1986; Hinton et al., 2006) or Neural Autoregressive Distribution\nEstimators (NADE; Larochelle & Murray, 2011). Pianorolls are also image-like and can be modeled\nby CNNs trained either as generative adversarial networks (e.g., Dong et al., 2018) or as orderless\nNADEs (Uria et al., 2014; 2016) (e.g., Huang et al., 2017).\nLattner et al. (2018) use self-similarity in style-transfer fashion, where the self-similarity structure of\na piece serves as a template objective for gradient descent to impose similar repetition structure on an\ninput score. Self-attention can be seen as a generalization of self-similarity; the former maps the input\nthrough different projections to queries and keys, and the latter uses the same projection for both.\nDot-product self-attention is the mechanism at the core of the Transformer, and several recent works\nhave focused on applying and improving it for image generation, speech, and summarization (Parmar\net al., 2018; Povey et al., 2018; Liu et al., 2018). A key challenge encountered by each of these efforts\nis scaling attention computationally to long sequences. This is because the time and space complexity\nof self-attention grows quadratically in the sequence length. For relative self-attention (Shaw et al.,\n2018) this is particularly problematic as the space complexity also grows linearly in the dimension,\nor depth, of the per-position representations.\n3\nMODEL\n3.1\nDATA REPRESENTATION\nWe take a language-modeling approach to training generative models for symbolic music. Hence\nwe represent music as a sequence of discrete tokens, with the vocabulary determined by the dataset.\nDatasets in different genres call for different ways of serializing polyphonic music into a single\nstream and also discretizing time.\nThe JSB Chorale dataset consists of four-part scored choral music, which can be represented as a\nmatrix where rows correspond to voices and columns to time discretized to sixteenth notes. The\nmatrix’s entries are integers that denote which pitch is being played. This matrix can than be\nserialized in raster-scan fashion by ﬁrst going down the rows and then moving right through the\ncolumns (see A.1 for more details). Compared to JSB Chorale, the piano performance data in the\nPiano-e-Competition dataset includes expressive timing information at much ﬁner granularity and\nmore voices. For the Piano-e-Competition we therefore use the performance encoding proposed\nby Oore et al. (2018) which consists of a vocabulary of 128 NOTE_ON events, 128 NOTE_OFFs,\n100 TIME_SHIFTs allowing for expressive timing at 10ms and 32 VELOCITY bins for expressive\ndynamics (see A.2 for more details).\n3.2\nBACKGROUND: SELF-ATTENTION IN TRANSFORMER\nThe Transformer decoder is a autoregressive generative model that uses primarily self-attention\nmechanisms, and learned or sinusoidal position information. Each layer consists of a self-attention\nsub-layer followed by a feedforward sub-layer.\nThe attention layer ﬁrst transforms a sequence of L D-dimensional vectors X = (x1, x2, . . . , xL)\ninto queries Q = XW Q, keys K = XW K, and values V = XW V , where W Q, W K, and W V are\neach D × D square matrices. Each L × D query, key, and value matrix is then split into H L × Dh\nparts or attention heads, indexed by h, and with dimension Dh = D\nH , which allow the model to focus\non different parts of the history. The scaled dot-product attention computes a sequence of vector\noutputs for each head as\nZh = Attention(Qh, Kh, V h) = Softmax\n\u0012QhKh⊤\n√Dh\n\u0013\nV h.\n(1)\nThe attention outputs for each head are concatenated and linearly transformed to get Z, a L by D\ndimensional matrix. A upper triangular mask ensures that queries cannot attend to keys later in the\nsequence. For other details of the Transfomer model, such as residual connections and learning rates,\nthe reader can refer Vaswani et al. (2017). The feedforward (FF) sub-layer then takes the output Z\n3\n"
    },
    {
      "page_number": 4,
      "text": "from the previous attention sub-layer, and performs two layers of point-wise dense layers on the depth\nD dimension, as shown in Equation 2. W1, W2, b1, b2 are weights and biases of those two layers.\nFF(Z) = ReLU(ZW1 + b1)W2 + b2\n(2)\n3.3\nRELATIVE POSITIONAL SELF-ATTENTION\nAs the Transformer model relies solely on positional sinusoids to represent timing information, Shaw\net al. (2018) introduced relative position representations to allow attention to be informed by how far\ntwo positions are apart in a sequence. This involves learning a separate relative position embedding\nEr of shape (H, L, Dh), which has an embedding for each possible pairwise distance r = jk −iq\nbetween a query and key in position iq and jk respectively. The embeddings are ordered from distance\n−L + 1 to 0, and are learned separately for each head. In Shaw et al. (2018), the relative embeddings\ninteract with queries and give rise to a Srel, an L × L dimensional logits matrix which modulates the\nattention probabilities for each head as:\nRelativeAttention = Softmax\n\u0012QK⊤+ Srel\n√Dh\n\u0013\nV.\n(3)\nWe dropped head indices for clarity. Our work uses the same approach to infuse relative distance\ninformation in the attention computation, while signiﬁcantly improving upon the memory footprint\nfor computing Srel. For each head, Shaw et al. (2018) instantiate an intermediate tensor R of shape\n(L, L, Dh), containing the embeddings that correspond to the relative distances between all keys and\nqueries. Q is then reshaped to an (L, 1, Dh) tensor, and Srel = QR⊤.2 This incurs a total space\ncomplexity of O(L2D), restricting its application to long sequences.\n3.4\nMEMORY EFFICIENT IMPLEMENTATION OF RELATIVE POSITION-BASED ATTENTION\nWe improve the implementation of relative attention by reducing its intermediate memory requirement\nfrom O(L2D) to O(LD), with example lengths shown in Table 1. We observe that all of the terms\nwe need from QR⊤are already available if we directly multiply Q with Er, the relative position\nembedding. After we compute QEr⊤, its (iq, r) entry contains the dot product of the query in\nposition iq with the embedding of relative distance r. However, each relative logit (iq, jk) in the\nmatrix Srel from Equation 3 should be the dot product of the query in position iq and the embedding\nof the relative distance jk −iq, to match up with the indexing in QK⊤. We therefore need to “skew”\nQEr⊤so as to move the relative logits to their correct positions, as illustrated in Figure 1 and detailed\nin the next section. The time complexity for both methods are O(L2D), while in practice our method\nis 6x faster at length 650.\nFigure 1: Relative global attention: the bottom row describes our memory-efﬁcient “skewing”\nalgorithm, which does not require instantiating R (top row, which is O(L2D)). Gray indicates\nmasked or padded positions. Each color corresponds to a different relative distance.\n2We assume that the batch size is 1 here. With a batch size of B, Q would be reshaped to (L, B, Dh) and\nSrel would be computed with a batch matrix–matrix product.\n4\n"
    },
    {
      "page_number": 5,
      "text": "Table 1: Comparing the overall relative memory complexity (intermediate relative embeddings (R or\nEr) + relative logits Srel), the maximal training lengths that can ﬁt in a GPU with 16GB memory\nassuming Dh = 64, and the memory usage per layer per head (in MB).\nImplementation\nRelative memory\nMaximal L\nL = 650\nL = 2048\nL = 3500\nShaw et al. (2018)\nO(L2D + L2)\n650\n108 + 1.7\n1100 + 16\n3100 + 49\nOurs\nO(LD + L2)\n3500\n0.17 + 1.7\n0.52 + 16\n0.90 + 49\n3.4.1\nTHE “SKEWING” PROCEDURE\nHence, we propose a “skewing” procedure to transform an absolute-by-relative (iq, r) indexed matrix\ninto an absolute-by-absolute (iq, jk) indexed matrix. The row indices iq stay the same while the\ncolumns indices are shifted according to the following equation: jk = r −(L −1) + iq. For example\nin Figure 1 the upper right green dot in position (0, 2) of QEr⊤after skewing has a column index of\n2 −(3 −1) + 0 = 0, resulting in a position of (0, 0) in Srel.\nWe outline the steps illustrated in Figure 1 below.\n1. Pad a dummy column vector of length L before the leftmost column.\n2. Reshape the matrix to have shape (L+1, L). (This step assumes NumPy-style row-major ordering.)\n3. Slice that matrix to retain only the last l rows and all the columns, resulting in a (L, L) matrix\nagain, but now absolute-by-absolute indexed, which is the Srel that we need.\n3.5\nRELATIVE LOCAL ATTENTION\nFor very long sequences, the quadratic memory requirement of even baseline Transformer is imprac-\ntical. Local attention has been used for example in Wikipedia and image generation (Liu et al., 2018;\nParmar et al., 2018) by chunking the input sequence into non-overlapping blocks. Each block then\nattends to itself and the one before, as shown by the smaller thumbnail on the top right corner of\nFigure 2.\nTo extend relative attention to the local case, we ﬁrst note that the right block has the same conﬁgura-\ntion as in the global case (see Figure 1) but much smaller: ( L\nM )2 (where M is the number of blocks,\nand N be the resulting block length) as opposed to L2. The left block is unmasked with relative\nindices running from -1 (top right) to -2N + 1 (bottom left). Hence, the learned Er for the local case\nhas shape (2N −1, N).\nSimilar to the global case, we ﬁrst compute QEr⊤and then use the following procedure to skew it to\nhave the same indexing as QK⊤, as illustrated in Figure 2.\n1. Pad a dummy column vector of length N after the rightmost column.\n2. Flatten the matrix and then pad with a dummy row of length N −1.\n3. Reshape the matrix to have shape (N + 1, 2N −1).\n4. Slice that matrix to retain only the ﬁrst N rows and last N columns, resulting in a (N, N) matrix.\nFigure 2: Relative local attention: the thumbnail on the right shows the desired conﬁguration for Srel.\nThe “skewing” procedure is shown from left to right.\n5\n"
    },
    {
      "page_number": 6,
      "text": "4\nEXPERIMENTS\n4.1\nJ.S. BACH CHORALES\nJ.S. Bach chorales is a canonical dataset used for evaluating generative models for music 3 (e.g.,\nAllan & Williams, 2005; Boulanger-Lewandowski et al., 2012; Liang, 2016; Hadjeres et al., 2016;\nHuang et al., 2017). It consists of score-based four-part chorales. We ﬁrst discretize the scores\nonto a 16th-note grid, and then serialize it by iterating through all the voices within a time step\nand then advancing time (see A.1 for more details). As there is a direct correspondence between\nposition in sequence and position on the timing/instrument grid in a piece, adding relative position\nrepresentations could make it easier to learn this grammar. We indeed see relative attention drastically\nimprove negative log-likelihood (NLL) over baseline Transformer (Table 2). This improvement is\nalso reﬂected in sample quality. The samples now maintain the necessary timing/instrument grid,\nalways advancing four steps before advancing in time. As local timing is maintained, the model is\nable to capture timing on a more global level, giving rise to regular phrasing, as shown in Figure 3.\nFigure 3: Unconditioned samples from Transformer without (left) and with (right) relative self-\nattention. Green vertical boxes indicate the endings of (sub)phrases where cadences are held.\nIn addition to relative attention, we also explored enhancing absolute timing through concatenating\ninstead of adding the sinusoids to the input embeddings. This allows the model to more directly\nlearn its absolute positional mapping. This further improves performance for both the baseline and\nrelative transformer (Table 2). We compare against COCONET as it is one of the best-performing\nmodels that has also been evaluated on the 16-note grid using the canonical dataset split. To\ndirectly compare, we re-evaluated COCONET to obtain note-wise losses on the validation set 4. For\nthe Transformer models (abbreviated as TF), we implemented our attention mechanisms in the\nTensor2Tensor framework (Vaswani et al., 2018). We use 8 heads, and keep the query, key (att) and\nvalue hidden size (hs) ﬁxed within a conﬁg. We tuned number of layers (L in {4,5,6}), attention\nhidden size (att in {256, 512}) and pointwise feedforward hidden size (ff in {512, 1024}).\n4.1.1\nGENERALIZING RELATIVE ATTENTION TO CAPTURE RELATIONAL INFORMATION\nA musical event bears multiple attributes, such as timing, pitch, instrument etc. To capture more\nrelational information, we extend relative attention to capture pairwise distances on additional\nattributes. We learn separate relative embeddings for timing Et and also pitch Ep. Et has entries\ncorresponding to how many sixteenth notes apart are two positions, while Ep embeds the pairwise\npitch interval. However this approach is not directly scalable beyond J.S. Bach Chorales because it\ninvolves explicitly gathering relative embeddings for Rt and Rp, resulting in a memory complexity\nof O(L2D) as in Shaw et al. (2018). This is due to relative information being computed based on\ncontent as opposed to content-invariant information such as position in sequence. It was sufﬁcient to\nadd the extra timing signals to the ﬁrst layer, perhaps because it is closest to the raw input content.\nHere, the relative logits are computed from three terms, Srel = Skew(QEr) + Q(Rt + Rp) in\ncontrast with other layers that only have one term, Skew(QEr).\n4.2\nPIANO-E-COMPETITION\nWe use the ﬁrst 6 years of of Piano-e-Competition because these years have corresponding MIDI data\nreleased 5, resulting in about 1100 pieces, split 80/10/10. Each piece is MIDI data capturing a classical\npiano performance with expressive dynamics and timing, encoded with the MIDI-like representation\n3J.S. Bach chorales dataset: https://github.com/czhuang/JSB-Chorales-dataset\n4Some earlier papers report frame-wise losses to compare to models such as RNN-RBM which model\n“chords”. Coconet can be evaluated under note-wise or frame-wise losses.\n5Piano-e-Competition dataset (competition history): http://www.piano-e-competition.com/\n6\n"
    },
    {
      "page_number": 7,
      "text": "described in Section A.2. We trained on random crops of 2000-token sequences and employed\ntwo kinds of data augmentation: pitch transpositions uniformly sampled from {−3, −2, . . . , 2, 3}\nhalf-steps, and time stretches uniformly sampled from the set {0.95, 0.975, 1.0, 1.025, 1.05}.\nWe compare to Magenta’s PerformanceRNN (LSTM, which ﬁrst used this dataset) (Oore et al.,\n2018) and LookBack RNN (LSTM with attention) (Waite, 2016). LookBack RNN uses an input\nrepresentation that requires monophonic music with barlines which is information that is not present\nin performed polyphonic music data, hence we simply adopt their architecture. Table 3 shows that\nTransformer-based architectures ﬁts this dataset better than LSTM-based models.\nTable 2: Note-wise validation NLL on J.S.Bach Chorales at 16th notes. Relative attention, more\ntiming and relational information improve performance.\nModel variation\nValidation NLL\nCOCONET (CNN, chronological, 64L, 128 3x3f)\n0.436\nCOCONET (CNN, orderless, 64L, 128 3x3f)\n≤0.238 6\nTransformer (TF) baseline (Vaswani et al., 2017) (5L, 256hs, 256att, 1024ff, 8h)\n0.417\nTF baseline + concat positional sinusoids (cps)\n0.398\nTF baseline + concat positional sinusoids, instrument labels (cpsi)\n0.370\nRelative Transformer (Shaw et al., 2018) (5L, 512hs, 512att, 512ff, 256r, 8h)\n0.357\nRelative Transformer + concat positional sinusoids, instrument labels (cpsi)\n0.347\nRelative Transformer + cpsi + relative pitch and time\n0.335\nTable 3: Validation NLL for Piano-e-Competition dataset, with event-based representation with\nlengths L = 2048. Transformer with relative attention (with our efﬁcient formulation) achieves\nstate-of-the-art performance.\nModel variation\nValidation NLL\nPERFORMANCE RNN (LSTM) (3L, 1024hs)\n1.969\nLSTM with attention (3L, 1024hs, 1024att)\n1.959\nTransformer (TF) baseline (6L, 256hs, 512att, 2048fs, 1024r, 8h)\n1.861\nTF with local attention (Liu et al., 2018) (8L, 1024fs, 512bs)\n1.863\nTF with relative global attention (our efﬁcient formulation) (6L, 2048fs, 1024r)\n1.835\nTF with relative local attention (ours) (6L, 1024fs, 2048r, 512bs)\n1.840\nWe implemented our attention mechanisms in the Tensor2Tensor framework (Vaswani et al., 2018),\nand used the default hyperparameters for training, with 0.1 learning rate, 0.1 dropout, and early\nstopping. We compare four architectures, varying on two axes: global versus local, and regular versus\nrelative attention. We found that reducing the query and key hidden size (att) to half the hidden size\n(hs) works well and use this relationship for all of the models, while tuning on number of layers\n(L) and ﬁlter size (fs). We use block size (bs) 512 for local attention. We set the maximum relative\ndistance to consider to half the training sequence length for relative global attention, and to the full\nmemory length (which is two blocks) for relative local attention. Table 3 show that relative attention\n(global or local) outperforms regular self-attention (global or local). All else being equal, local and\nglobal attention perform similarly. Each though local attention does not see all the history at once, it\ncan build up a larger receptive ﬁeld across layers. This can be an advantage in the future for training\non much longer sequences, as local attention requires much less memory.\n6COCONET is an instance of OrderlessNADE, an ensemble over orderings. The chronological loss evaluates\nthe model as autoregressive, from left to right. We can also evaluate the model as a mixture, by averaging its\nlosses over multiple random orderings. This is a lower bound on log-likelihood. It is intractable to sample from\nexactly but can be approximated through Gibbs sampling.\n7\n"
    },
    {
      "page_number": 8,
      "text": "Figure 4: Comparing how models continue a prime (top left). Repeated motives and structure are seen\nin samples from Transformer with relative attention (top row), but less so from baseline Transformer\n(middle row) and PerformanceRNN (LSTM) (bottom row).\n4.2.1\nQUALITATIVE PRIMING EXPERIMENTS\nWhen primed with an initial motif (Chopin’s Étude Op. 10, No. 5) shown in the top left corner of\nFigure 4, we see the models perform qualitatively differently. Transformer with relative attention\nelaborates the motif and creates phrases with clear contour which are repeated and varied. Baseline\nTransformer uses the motif in a more uniform fashion, while LSTM uses the motif initially but soon\ndrifts off to other material. Note that the generated samples are twice as long as the training sequences.\nRelative attention was able to generalize to lengths longer than trained but baseline Transformer\ndeteriorates beyond its training length. See Appendix C for visualizations of how the our Relative\nTransformer attends to past motifs.\n4.2.2\nHARMONIZATION: CONDITIONING ON MELODY\nTo explore the sequence-to-sequence setup of Transformers,\nwe experimented with a conditioned generation task where\nthe encoder takes in a given melody and the decoder has to\nrealize the entire performance, i.e. melody plus accompani-\nment. The melody is encoded as a sequence of tokens as in\nWaite (2016), quantized to a 100ms grid, while the decoder\nuses the performance encoding described in Section 3.1 (and\nfurther illustrated in A.2). We use relative attention on the de-\ncoder side and show in Table 4 it also improves performance.\nTable 4:\nValidation conditional\nNLL given groundtruth melody from\nPiano-e-Competition.\nModel variation\nNLL\nBaseline Transformer\n2.066\nRelative Transformer (ours)\n1.786\n4.2.3\nHUMAN EVALUATIONS\nTo compare the perceived sample quality of models trained on the Piano-e-Competition dataset,\nand their ability to generate a continuation for a priming sequence, we carried out a listening test\nstudy comparing the baseline Transformer, our Transformer with relative-attention, PerformanceRNN\n(LSTM), and the validation set. Participants were presented with two musical excerpts (from two\ndifferent models that were given the same priming sequence) and asked to rate which one is more\nmusical on a Likert scale. For each model, we generated 10 samples each with a different prime, and\ncompared them to three other models, resulting in 60 pairwise comparisons. Each pair was rated by 3\ndifferent participants, yielding a total of 180 comparisons.\nFigure 5 shows the number of comparisons in which an excerpt from each model was selected as\nmore musical. The improvement in sample quality from using relative attention over the baseline\nTransformer model was statistically signiﬁcant (see Appendix B for the analysis), both in aggregate\nand between the pair. Even though in aggregate LSTMs performed better in the study than the\nTransformer, despite having higher perplexity, but when compared against each other head to head,\nthe results were not statistically signiﬁcant (see Table 5 in Appendix B).\n8\n"
    },
    {
      "page_number": 9,
      "text": "(Ours)\nFigure 5: Number of wins for each model. Error bars show standard deviations of mean.\n5\nCONCLUSION\nIn this work we demonstrated that the Transformer equipped with relative attention is very well-suited\nfor generative modeling of symbolic music. The compelling long-term structure in the samples from\nour model leaves us enthusiastic about this direction of research. Moreover, the ability to expand\nupon a primer, in particular, suggests potential applications as creative tool.\nThe signiﬁcant improvement from relative attention highlights a shortcoming of the original Trans-\nformer that might also limit its performance in other domains. Improving the Transformer’s ability to\ncapture periodicity at various time scales, for instance, or relations between scalar features akin to\npitch could improve time-series models. Our memory-efﬁcient implementation enables the appli-\ncation of relative attention to much longer sequences such as long texts or even audio waveforms,\nwhich signiﬁcantly broadens the range of problems to which it could be applied.\n6\nACKNOWLEDGEMENT\nWe thank many colleagues from the Transformer (Vaswani et al., 2017) and Tensor2Tensor (Vaswani\net al., 2018) papers for helping us along the way: Lukasz Kaiser, Ryan Sepassi, Niki Parmar and Llion\nJones. Many thanks to Magenta and friends for their support throughout and for many insightful\ndiscussions: Jesse Engel, Adam Roberts, Fred Bertsch, Erich Elsen, Sander Dieleman, Sageev Oore,\nCarey Radebaugh, Natasha Jaques, Daphne Ippolito, Sherol Chan, Vida Vakilotojar, Dustin Tran,\nBen Poole and Tim Cooijmans.\nREFERENCES\nMoray Allan and Christopher KI Williams. Harmonising chorales by probabilistic inference. Advances\nin neural information processing systems, 17:25–32, 2005.\nNicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependen-\ncies in high-dimensional sequences: Application to polyphonic music generation and transcription.\nInternational Conference on Machine Learning, 2012.\nHao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. Musegan: Multi-track sequential\ngenerative adversarial networks for symbolic music generation and accompaniment. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, 2018.\nDouglas Eck and Juergen Schmidhuber. Finding temporal structure in music: Blues improvisation\nwith lstm recurrent networks. In Proceedings of the 12th IEEE Workshop on Neural Networks for\nSignal Processing, 2002.\nGaëtan Hadjeres, Jason Sakellariou, and François Pachet. Style imitation and chord invention in\npolyphonic music with exponential families. arXiv preprint arXiv:1609.05152, 2016.\nGaëtan Hadjeres, François Pachet, and Frank Nielsen. Deepbach: a steerable model for bach chorales\ngeneration. In International Conference on Machine Learning, pp. 1362–1371, 2017.\nGeoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural computation, 18(7):1527–1554, 2006.\n9\n"
    },
    {
      "page_number": 10,
      "text": "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, and Doug Eck. Coun-\nterpoint by convolution. In Proceedings of the International Conference on Music Information\nRetrieval, 2017.\nHugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In AISTATS,\nvolume 1, pp. 2, 2011.\nStefan Lattner, Maarten Grachten, and Gerhard Widmer. Imposing higher-level structure in poly-\nphonic music generation using convolutional restricted boltzmann machines and constraints.\nJournal of Creative Music Systems, 2(2), 2018.\nFeynman Liang. Bachbot: Automatic composition in the style of bach chorales. Masters thesis,\nUniversity of Cambridge, 2016.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating wikipedia by summarizing long sequences. In Proceedings of the International\nConference on Learning Representations, 2018.\nSageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and Karen Simonyan. This time with\nfeeling: Learning expressive musical performance. arXiv preprint arXiv:1808.03715, 2018.\nAnkur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel for natural language inference. In Proceedings of the Conference on Empirical Methods in\nNatural Language Processing, 2016.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, and Alexander Ku.\nImage transformer. In Proceedings of the International Conference on Machine Learning, 2018.\nDaniel Povey, Hossein Hadian, Pegah Ghahremani, Ke Li, and Sanjeev Khudanpur. A time-restricted\nself-attention layer for ASR. In Proceddings of the IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2018.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-\ntions. In Proceedings of the Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, volume 2, 2018.\nPaul Smolensky. Information processing in dynamical systems: Foundations of harmony theory.\nTechnical report, DTIC Document, 1986.\nBenigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In\nInternational Conference on Machine Learning, pp. 467–475, 2014.\nBenigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural\nautoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–\n7220, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems, 2017.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and\nJakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.\nElliot Waite.\nGenerating long-term structure in songs and stories.\nhttps://magenta.\ntensorflow.org/2016/07/15/lookback-rnn-attention-rnn, 2016.\n10\n"
    },
    {
      "page_number": 11,
      "text": "A\nDOMAIN-SPECIFIC REPRESENTATIONS\nAdapting sequence models for music requires making decisions on how to serialize a polyphonic\ntexture. The data type, whether score or performance, makes certain representations more natural for\nencoding all the information needed while still resulting in reasonable sequence lengths.\nA.1\nSERIALIZED INSTRUMENT/TIME GRID (J.S.BACH CHORALES)\nThe ﬁrst dataset, J.S. Bach Chorales, consists of four-part score-based choral music. The time\nresolution is sixteenth notes, making it possible to use a serialized grid-like representation. Figure 6\nshows how a pianoroll (left) can be represented as a grid (right), following (Huang et al., 2017). The\nrows show the MIDI pitch number of each of the four voices, from top to bottom being soprano (S),\nalto (A), tenor (T) and bass (B), while the columns is discretized time, advancing in sixteenth notes.\nHere longer notes such as quarter notes are broken down into multiple repetitions. To serialize the\ngrid into a sequence, we interleave the parts by ﬁrst iterating through all the voices at time step 1, and\nthen move to the next column, and then iterate again from top to bottom, and so on. The resulting\nsequence is S1A1T1B1S2A2T2B2..., where the subscript gives the time step. After serialization, the\nmost common sequence length is 1024. Each token is represented as onehot in pitch.\nS: 67, 67, 67, 67\nA: 62, 62, 62, 62\nT: 59, 59, 57, 57\nB: 43, 43, 45, 45\nFigure 6: The opening measure of BWV 428 is visualized as a pianoroll (left, where the x-axis is\ndiscretized time and y-axis is MIDI pitch number), and encoded in grid representation with sixteenth\nnote resolution (right). The soprano and alto voices have quarter notes at pitches G4 (67) and D4 (62),\nthe tenor has eighth notes at pitches B3 (59) and A3 (57), and the bass has eighth notes at pitches A2\n(45) and G2 (43).\nA.2\nMIDI-LIKE EVENT-BASED (PIANO-E-COMPETITION)\nThe second dataset, Piano-e-Competition, consists of polyphonic piano performances with expressive\ntiming and dynamics. The time resolution here is on the millisecond level, so a grid representation\nwould result in sequences that are too long. Instead, the polyphonic performance is serialized into a\nsequence of one hot encoded events as proposed in (Oore et al., 2018).\nFirst, the input MIDI ﬁles are preprocessed to extend note durations based on sustain pedal control\nevents. The sustain pedal is considered to be down whenever a sustain control change is encountered\nwith a value >= 64; the sustain pedal is then considered up after a control change with a value < 64.\nWithin a period where the sustain pedal is down, the duration of each note is extended to either the\nbeginning of the next note of the same pitch or the end of the sustain period, whichever happens ﬁrst.\nIf the original duration extends beyond the time when the sustain pedal is down, that original duration\nis used.\nNext, the MIDI note events are converted into a sequence from the following set of vocabulary: 128\nNOTE_ON events for starting a note of with one of the 128 MIDI pitches, 128 NOTE_OFF events\nfor ending a note with one of the 128 MIDI pitches, 100 TIME_SHIFT events representing forward\ntime shifts in 10ms increments from 10ms to 1s, and 32 SET_VELOCITY events representing the\nvelocity for future NOTE_ON events in the form of the 128 possible MIDI velocities quantized into\n32 bins. An example performance encoding is illustrated in Figure 7.\n11\n"
    },
    {
      "page_number": 12,
      "text": "SET_VELOCITY<80>, NOTE_ON<60>\nTIME_SHIFT<500>, NOTE_ON<64>\nTIME_SHIFT<500>, NOTE_ON<67>\nTIME_SHIFT<1000>, NOTE_OFF<60>, NOTE_OFF<64>,\nNOTE_OFF<67>\nTIME_SHIFT<500>, SET_VELOCITY<100>, NOTE_ON<65>\nTIME_SHIFT<500>, NOTE_OFF<65>\nFigure 7: A snippet of a piano performance visualized as a pianoroll (left) and encoded as performance\nevents (right, serialized from left to right and then down the rows). A C Major chord is arpeggiated\nwith the sustain pedal active. At the 2-second mark, the pedal is released, ending all of the notes. At\nthe 3-second mark, an F is played for .5 seconds. The C chord is played at velocity 80 and the F is\nplayed at velocity 100.\nB\nSUPPLEMENT OF LISTENING TEST\nB.1\nSTUDY PROCEDURE\nParticipants were presented with two musical excerpts that shared a common priming sequence. For\neach excerpt, the priming sequence was played, followed by 2.5 seconds of silence, followed by the\npriming sequence again and a continuation of that sequence. The continuations were either sampled\nfrom one of the models or extracted from our validation set. We evaluated all possible pairs in the\nspace of data and model samples, except from the same model. Each continuation had a length of 512\nevents using the encoding described in Section A.2. This corresponds to the length the models were\ntrained on to remove the deteriorating effect that happens with baseline Transformer when asked to\ngenerate beyond the length it was trained on. Participants were asked which excerpt they thought was\nmore musical on a Likert scale of 1 to 5. The pair is laid out left versus right, with 1 indicating the left\nis much more musical, 2 the left is slightly more musical, 3 being a tie, 4 being the right is slightly\nmore musical, and 5 the right is much more musical. For each model, we generated 10 samples\neach with a different prime, and compared them to three other models, resulting in 60 pairwise\ncomparisons. Each pair was rated by 3 different participants, yielding a total of 180 comparisons.\nB.2\nANALYSIS\nA Kruskal-Wallis H test of the ratings showed that there was a statistically signiﬁcant difference\nbetween the models: χ2(2) = 63.84, p = 8.86e-14< 0.01. Table 5 show a post-hoc analysis on the\ncomparisons within each pair, using the Wilcoxon signed-rank test for matched samples. Table 6\nshows a post-hoc analysis of how well each model performed when compared to all pairs, and\ncompares each model’s aggregate against each other, using the Mann–Whitney U test for independent\nsamples. We use a Bonferroni correction on both to correct for multiple comparisons. The win and\nloss counts bucket scores 4, 5 and scores 1, 2 respectively, while the tieing score is 3.\nBoth within pairs and between aggregates, participants rated samples from our relative Transformer\nas more musical than the baseline Transformer with p < 0.01/6.\nFor within pairs, we did not observe a consistent statistically signiﬁcant difference between the other\nmodel pairs, baseline transformer versus LSTM and LSTM versus relative Transformer.\nWhen comparing between aggregates, LSTM was overall perceived as more musical than baseline\nTransformer. Relative Transformer came a bit close to outperforming LSTM with p = 0.018. When\nwe listen to the samples from the two, they do sound qualitatively different. Relative Transformer\noften exhibits much more structure (as shown in Figure 4), but the effects were probably less\npronounced in the listening test because we used samples around 10s to 15s, which is half the length\nof those shown in Figure 4 to prevent the baseline Transformer from deteriorating. This weakens the\ncomparison on long-term structure.\nWhen compared to real music from the validation set, we see that in aggregates, real music was better\nthan LSTM and baseline Transformer. There was no statistical signiﬁcant difference between real\nmusic and relative Transformer. This is probably again due to the samples being too short as real\nmusic is deﬁnitely still better.\n12\n"
    },
    {
      "page_number": 13,
      "text": "Table 5: A post-hoc comparison of each pair on their pairwise comparisons with each other, using the\nWilcoxon signed-rank test for matched samples. p value less than 0.01/6=0.0016 yields a statistically\nsigniﬁcant difference and is marked by asterisk.\nPairs\nwins\nties\nlosses\np value\nOur relative transformer\nreal music\n11\n4\n15\n0.243\nOur relative transformer\nBaseline transformer\n23\n1\n6\n0.0006*\nOur relative transformer\nLSTM\n18\n1\n11\n0.204\nBaseline transformer\nLSTM\n5\n3\n22\n0.006\nBaseline transformer\nreal music\n6\n0\n24\n0.0004*\nLSTM\nreal music\n6\n2\n22\n0.0014\nTable 6: Comparing each pair on their aggregates (comparisons with all models) in (wins, ties, losses),\nusing the Mann–Whitney U test for independent samples.\nModel\nModel\np value\nOur relative transformer\n(52, 6, 32)\nreal music\n(61, 6, 23)\n0.020\nOur relative transformer\n(52, 6, 32)\nBaseline transformer\n(17, 4, 69)\n1.26e-9*\nOur relative transformer\n(52, 6, 32)\nLSTM\n(39, 6, 45)\n0.018\nBaseline transformer\n(17, 4, 69)\nLSTM\n(39, 6, 45)\n3.70e-5*\nBaseline transformer\n(17, 4, 69)\nreal music\n(61, 6, 23)\n6.73e-14*\nLSTM\n(39, 6, 45)\nreal music\n(61, 6, 23)\n4.06e-5*\nC\nVISUALIZING SOFTMAX ATTENTION\nOne advantage of attention-based models is that we can visualize its attention distribution 3. This\ngives us a glimpse of how the model might be building up recurring structures and how far it is\nattending back. The pianorolls in the visualizations below is a sample generated from Transformer\nwith relative attention. Each ﬁgure shows a query (the source of all the attention lines) and previous\nmemories being attended to (the notes that are receiving more softmax probabiliy is highlighted in).\nThe coloring of the attention lines correspond to different heads and the width to the weight of the\nsoftmax probability.\nFigure 8: This piece has a recurring triangular contour. The query is at one of the latter peaks and it\nattends to all of the previous high notes on the peak, all the way to beginning of the piece.\nFigure 9: The query a note in the left-hand, and it attends to its immediate past neighbors and mostly\nto the earlier left hand chords, with most attention lines distributed in the lower half of the pianoroll.\n13\n"
    },
    {
      "page_number": 14,
      "text": "D\nPREVIOUS FIGURES FOR THE “SKEWING” PROCEDURE\n0\n0\n.\n.\n.\n0\n0\n0\n.\n.\n.\n0\n0\n0\n0\n.\n.\n.\nSteps 1\nSteps 2,3:\nFigure 10: Relative global attention: Steps (from left to right) for “skewing” an absolute-by-relative\n(iq, r) indexed matrix into absolute-by-absolute (iq, jk). Grey indicates self-attention masks or entries\nintroduced by the skewing procedure. Positions with relative distance zero are marked. Entries\noutlined by purple are removed in step 3.\n(N+1, 2N-1)\n-1\n-N\n-2N+1\n-N\n(N, N)\n0\n0\n0\n.\n.\n.\n-1\n-N\n-2N+1\n-N\n-1\n-2N+1\n(N, 2N-1)\nPad N-1 after flatten \nSteps 1, 2\nSteps 3\nSteps 4\nFigure 11: Relative local attention: Steps (from left to right) for “skewing” an (iq, r) indexed matrix\nwith 2N −1 ranged relative indices r into (iq, jk indexed. Shapes are indicated above the boxes,\nwhile indices in the boxes give relative distances.\n14\n"
    }
  ]
}