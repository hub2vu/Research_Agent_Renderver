{
  "filename": "10.48550_arxiv.1706.03762",
  "original_filename": "1706.03762",
  "references_detected": 40,
  "titles_extracted": 38,
  "titles": [
    "Layer normalization",
    "Neural machine translation by jointly learning to align and translate",
    "Long short-term memory-networks for machine reading",
    "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "Xception: Deep learning with depthwise separable convolutions",
    "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "of NAACL",
    "Generating sequences with recurrent neural networks",
    "Deep residual learning for image recognition",
    "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
    "Long short-term memory",
    "Self-training PCFG grammars with latent annotations across languages",
    "Exploring the limits of language modeling",
    "Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS",
    "Neural GPUs learn algorithms",
    "Neural machine translation in linear time",
    "In International Conference on Learning Representations",
    "Adam: A method for stochastic optimization",
    "Factorization tricks for LSTM networks",
    "A structured self-attentive sentence embedding",
    "Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser",
    "Effective approaches to attentionbased neural machine translation",
    "Building a large annotated corpus of english: The penn treebank",
    "Effective self-training for parsing",
    "A decomposable attention model",
    "A deep reinforced model for abstractive summarization",
    "Learning accurate, compact, and interpretable tree annotation",
    "Using the output embedding to improve language models",
    "Neural machine translation of rare words with subword units",
    "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
    "Dropout: a simple way to prevent neural networks from overfitting",
    "End-to-end memory networks",
    "Sequence to sequence learning with neural networks",
    "Rethinking the inception architecture for computer vision",
    "Grammar as a foreign language",
    "Googleâ€™s neural machine translation system: Bridging the gap between human and machine translation",
    "Deep recurrent models with fast-forward connections for neural machine translation",
    "Fast and accurate shift-reduce constituent parsing"
  ],
  "diagnostics": {
    "has_references_header": true,
    "parsed_items_count": 40,
    "is_numbered": true,
    "id_normalized": true
  }
}