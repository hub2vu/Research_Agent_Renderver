{
  "filename": "120272_metaTextGrad_ Automatically optimizing language model optimizers.pdf",
  "total_pages": 32,
  "full_text": "metaTextGrad: Automatically optimizing language\nmodel optimizers\nGuowei Xu\nTsinghua University\nMert Yuksekgonul\nStanford University\nCarlos Guestrin\nStanford University\nJames Zou\nStanford University\nAbstract\nLarge language models (LLMs) are increasingly used in learning algorithms, eval-\nuations, and optimization tasks. Recent studies have shown that using LLM-based\noptimizers to automatically optimize model prompts, demonstrations, predictions\nthemselves, or other components can significantly enhance the performance of AI\nsystems, as demonstrated by frameworks such as DSPy and TextGrad. However,\noptimizers built on language models themselves are usually designed by humans\nwith manual design choices; optimizers themselves are not optimized. Moreover,\nthese optimizers are general purpose by design, to be useful to a broad audience,\nand are not tailored for specific tasks. To address these challenges, we propose\nmetaTextGrad, which focuses on designing a meta-optimizer to further enhance\nexisting optimizers and align them to be good optimizers for a given task. Our\napproach consists of two key components: a meta prompt optimizer and a meta\nstructure optimizer. The combination of these two significantly improves perfor-\nmance across multiple benchmarks, achieving an average absolute performance\nimprovement of up to 6% compared to the best baseline.\n1\nIntroduction\nLarge language models (LLMs) are increasingly used in learning algorithms, optimization, and\nevaluation tasks [1, 2, 3, 4, 5]. However, algorithms that incorporate LLMs often face significant\nchallenges. Firstly, many of these algorithms are still hand-crafted to a considerable extent, requiring\nsubstantial human expertise and effort to design and implement effectively. Second, LLMs are notably\nsensitive to the specific wording and structure of their instructions [6], making it tedious to improve\nthem effectively to be used in learning algorithms.\nMany studies have explored prompt optimization approaches to automatically design better prompts\nand enhance the performance of LLMs. For example, algorithms such as OPRO [3], MIPRO [7], and\nTextGrad [1] have introduced optimizers based on LLMs that support automatic prompt optimization.\nHowever, these optimizers are often fixed, applying the same optimization strategies with the\nsame optimizer prompts independent of the task, lacking a process to align with the specific task.\nImportantly, these optimizers are general purpose by design, to be useful to many different downstream\ntasks and be used by large user bases. Furthermore, different optimizers likely excel at different types\nof optimization tasks; thus, achieving the best of all optimizers introduces either a choice to be made\namong them or a strategy to ensemble them. There is no method to automatically design and improve\nthe optimizers to call based on the characteristics of a task.\nIn this work, we explore how to automatically optimize both the structure and the prompt of optimizers.\nSpecifically, we assume that all LLM calls are black-box calls, where the internal states of the LLM,\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nProgram\nTask\nOptimizers\nMeta Optimizer\nOptimized \nOptimizer\nCandidate \nOptimizer\nOptimized Program\nAnswer:  624\nQuest i on:  . . .  How many pages \ndoes he wr i t e a year ?\nAverage Accuracy:53%\nAverage Accuracy:45%\nAverage Accuracy:47%\n...\nFigure 1: Illustration of the meta-optimization process. A meta-optimizer optimizes LLM optimizers\nby aligning them with specific tasks through task interaction while leveraging the strengths of different\noptimizers to propose a more effective optimizer.\nas well as any information such as model gradients, are inaccessible, and only the inputs to and\noutputs of the LLM are observable. For the task to be optimized, we require only a small training\ndataset containing some input-output pairs and an evaluation metric the user wants to improve upon.\nWe propose using a meta-optimizer to automatically optimize the optimizer, with the objective of\nidentifying improved optimizers such that the program optimized by it achieves the best performance\non the given task. To this end, we introduce two types of meta-optimization strategies: the meta\nprompt optimizer and the meta structure optimizer. The meta prompt optimizer focuses on refining\nthe prompts of the LLM optimizers to enhance their effectiveness and better align them with specific\ntasks. Meanwhile, the meta structure optimizer is designed to automatically determine the optimal\ncombination and sequence of different optimizers or modules based on the characteristics of the task.\nBy combining these two types of meta-optimizers, we propose metaTextGrad, which integrates both\ncomponents into a unified framework. Specifically, given a set of input optimizers, metaTextGrad\nfirst performs prompt optimization for each optimizer independently. Then, it explores the combi-\nnation and sequencing of these optimizers to construct a more effective composite optimizer. We\nconducted experiments on multiple benchmarks, and the results demonstrate that our meta-optimized\noptimizers consistently outperform the existing ones.\nOverall, we summarize our contributions as follows. First, we introduce the concept of meta-\noptimization, highlighting that existing LLM optimizers often require further task alignment and\neffective combination through a meta-optimizer to maximize their potential. Second, we develop\ntwo types of meta-optimizers: the meta prompt optimizer and the meta structure optimizer. Building\non these, we propose metaTextGrad, which integrates both types of meta-optimizers into a unified\nframework. Lastly, experimental results on multiple benchmarks demonstrate that our method\nsignificantly outperforms baseline approaches in both performance and generalization.\n2\nProblem Statement\nHere, we will follow the notation from [7]. Consider an LLM program Φ that may contain multiple\nLLM calls forming a pipeline, with each call using a different prompt. The pipeline structure of a\nprogram and the prompt corresponding to each LLM call can be learnable and optimized by an LLM\noptimizer.\nThe task of the LLM optimizer is to find the optimal program Φ given a training dataset D (with pairs\nof inputs x and outputs y), and an evaluation metric µ.\nΦ∗= arg max\nΦ\n1\n|D|\nX\n(x,y)∈D\nµ(Φ(x), y)\n(1)\nExisting methods attempt to approximate solutions to this optimization problem from different\nperspectives. Optimizers such as MIPRO [7], OPRO [8], and TGD [9] aim to find better prompts,\nwhile optimizers like ADAS [10] focus on exploring structures. We unify the existing optimizer\n2\nalgorithms into a general framework, as outlined in Algorithm 1. Importantly, these optimizers are\nthe same type as of M, are designed by humans, and their structure and prompts are fixed.\nSpecifically, each optimizer should be capable of performing four operations:\n1. Initialize: Given the training dataset and the program to be optimized, prepare the training scheme.\n2. Propose: Suggest improvements to the program, which may involve modifications to the pipeline\nstructure, prompts, or other aspects.\n3.Update: Update the current optimal program based on the evaluation results of the improved\nprogram and determine the next proposal.\n4. ExtractOptimizedProgram: Return the best program found so far.\nAlgorithm 1 Inner Loop: Optimize Φ with Optimizer M\n1: Input: Optimizer M, Initial Program Φ, Max Iterations I\n2: Input: Training Data D, Validation Data Dval, Metric µ\n3: Output: Φ∗, i.e., the optimized version of Φ\n4: M.Initialize(D, Φ)\n5: for k ←1 to I do\n6:\nΦk ←M.Propose()\n7:\nσ ←\n1\n|Dval|\nP\n(x,y)∈Dval µ\n\u0000Φk(x), y\n\u0001\n8:\nM.Update(Φk, σ)\n9: end for\n10: (Φ∗, σ∗\u0001\n←M.ExtractOptimizedProgram()\n11: return\n\u0000Φ∗, σ∗\u0001\nThe problem we investigate is how to further enhance existing optimizers and align them to be\neffective optimizers for a given task, instead of relying on human-written optimizers. To this end,\nwe introduce the concept of a meta-optimizer. Let the optimized program obtained by an optimizer\nM be denoted as Φ∗= M.optimize(D, Φ). The optimization objective of the meta-optimizer is to\nfind improved optimizers such that the program optimized by this optimizer performs better on the\ncorresponding task. In particular, the optimization problem is formalized as:\nM ∗= arg max\nM\n1\n|D|\nX\n(x,y)∈D\nµ(M.optimize(D, Φ)(x), y).\n(2)\nIn optimization, good initializations often contribute to improved performance, in continuous or\ndiscrete optimization alike [11, 12]. Ideally, the meta-optimization framework should leverage the\nexisting, manually designed optimizers and achieve further improvements. Therefore, the input to the\nmeta-optimizer can include one or more existing optimizers as initialization, ultimately producing an\noptimized optimizer. The detailed process is illustrated in Algorithm 2.\nAlgorithm 2 Meta-Optimization of Optimizers\n1: Input: Meta-Optimizer c\nM\n2: Input: Max Meta-Iterations J, Max Inner-Iterations I\n3: Input: Initial optimizers {M (1), M (2), . . . , M (r)}\n4: Input: Training Data D, Validation Data Dval\n5: Input: Metric µ, Initial Program Φ\n6: Output: Optimized optimizer M ∗\n7: c\nM.Initialize(D, {M (i)}r\ni=1)\n8: for j ←1 to J do\n9:\nMj ←c\nM.Propose()\n10:\n(Φ∗\nj, σj) ←InnerLoop\n\u0000Mj, Φ, I, D, Dval, µ)\n11:\nc\nM.Update\n\u0000Mj, σj\n\u0001\n12: end for\n13: M ∗←c\nM.ExtractOptimizedOptimizer()\n14: return M ∗\nSpecifically, the meta-optimizer combines different input optimizers to propose new optimizers. The\nnewly proposed optimizers execute the process described in Algorithm 1 to obtain the inner loop\n3\noptimization results. Based on the performance of the newly designed optimizers, the meta-optimizer\nupdates the current best optimizer and determines the proposal for the next iteration (outer loop).\nHowever, similar to the problem of optimizing a program, finding an optimal optimizer using a meta-\noptimizer is generally an intractable optimization problem. Therefore, it is necessary to introduce\nappropriate parameterizations and simplifications to the problem. In our work, we primarily focus on\ntwo types of parameterizations for the meta-optimizer:\n1. automatically optimizing the prompts in the optimizer to align it with the given task (Meta Prompt\nOptimizer),\n2. automatically optimizing the combination of different optimizers based on the characteristics of\nthe task, forming a new composite optimizer (Meta Structure Optimizer).\nCollectively, we can meta-optimize using both parameterizations to obtain better optimizers.\n3\nmetaTextGrad: Automatically Optimizing Language Model Optimizers\nIn this section, we first introduce the theoretical insights behind the meta optimizer. Next, we\nfurther analyze and illustrate our motivation through concrete examples. Finally, we present the\nmetaTextGrad pipeline, which consists of the meta prompt optimizer and the meta structure opti-\nmizer.\n3.1\nTheoretical Insight\nWe present the theoretical motivation for meta optimization, highlighting the importance of aligning\nthe optimizer with the target task. In particular, it is shown that when both the training and test sets are\nsampled from the same underlying data distribution, an optimizer properly aligned via meta-learning\non the training set will, with high probability, produce programs on the test set whose accuracy\nclosely approaches that of the optimal optimizer. In contrast, an optimizer that has not been optimized\non the training set lacks such theoretical guarantee.\nWe begin by introducing the necessary notation. Let µ : X →[0, 1] denote the accuracy metric,\nwhere X is the space of natural language outputs. Let D be a distribution over natural language\ninputs x. Let Φθ be an optimizer parameterized by θ ∈Θ. Given input x and access to µ, Φθ makes\nT zeroth-order queries and returns an optimized output xT .\nDefine the loss of the optimizer on input x as L(θ, x) = 1 −µ(xT ) = 1 −µ(Φθ(x, T)). The\npopulation loss is given by R(θ) = Ex∼D[L(θ, x)]. Let S = {x1, . . . , xn} ∼Dn denote a dataset\nsampled from D, and define the empirical loss on S as RS(θ) = 1\nn\nPn\ni=1 L(θ, xi).\nLet bθS = arg minθ∈Θ RS(θ) be the optimizer parameters obtained via meta-learning on the training\nset S, and let θ∗= arg minθ∈Θ R(θ) be the globally optimal optimizer under distribution D. We can\nnow state the following theorem:\nTheorem 1. Let S1 and S2 be two datasets sampled independently from distribution D, with sizes n\nand m, respectively. Then, with probability at least 1 −δ, the optimizer bθS1 trained on S1 satisfies:\nRS2(bθ) ≤R(θ∗) +\nq\n2 log(6/δ)\nn\n+\nq\nlog(6/δ)\n2m\n.\n(3)\nThe proof of Theorem 1 is based on Hoeffding’s inequality, and the full derivation is provided in\nAppendix B. The theorem highlights the necessity of performing meta-optimization.\nIn contrast, an optimizer θ0 that hasn’t been optimized on the training dataset has no theoretical\nguarantee. We can still guarantee that RS2(θ0) is similar to R(θ0), but the guarantee says nothing\nabout how large R(θ0) is. If the optimizer is bad on average for this task, it will still be bad on S2\nwith high probability. This provides a theoretical foundation for the design of metaTextGrad.\n3.2\nMotivating Example\nExisting optimizers such as TextGrad and DSPy are manually designed by humans with the goal of\nperforming well across a broad distribution of tasks, and they indeed demonstrate strong average\nperformance.\n4\nInitial Optimizers\nYou are part of \nan optimization \nsystem that \nimproves text...                     \nMeta Optimizer\nInner Optimization Loop\nPropose \nnew \noptimizer\nUpdate \nbest \noptimizer\nProposed \noptimizer\nOptimized Optimizer\nInitial Program\nOptimized Program\nProposer\nUpdater\nYou are part of \nan optimization \nsystem whose \ngoal is to ensure \nthat the pipeline \ngenerates correct \n...\n1. Structure\n2.TGD\n3. ...\nFigure 2: Illustration of metaTextGrad. metaTextGrad combines a meta prompt optimizer and a\nmeta structure optimizer. Given a set of optimizers, metaTextGrad performs optimization in two\nsteps. First, it individually refines each optimizer by optimizing its prompts to better align with the\ntask. Then, it combines the different prompt-optimized optimizers to construct the final optimizer.\nFor example, the prompt used by the TextGrad TGD optimizer is as follows:\nTextGrad TGD optimizer Prompt\nYou are part of an optimization system that improves text (i.e., variable). You will be asked to\ncreatively and critically improve prompts, solutions to problems, code, or any other text-based\nvariable.\nAs can be seen, this prompt is indeed highly general, with phrasing such as ‘improve prompts,\nsolutions to problems, code, or any other text-based variable’.\nHowever, in some cases, such general-purpose prompt fails to effectively optimize model performance.\nWhile the optimizers in both TextGrad and DSPy can obtain learning signals from task-specific\nevaluators, these signals tend to be noisy and sparse.\nFirst, when feedback is provided solely in the form of scalar scores rather than textual guidance, it\nbecomes sparse, making optimization substantially more challenging. Second, although optimizers\nsuch as TextGrad’s TGD can accept textual gradient feedback, such feedback is often highly noisy,\nmaking it difficult for the optimizer to learn effectively. For example, the evaluator feedback received\nby the TGD optimizer may look like the following:\nFeedback Received by the TGD optimizer\nTo improve the prompt for the executer and enhance the objective function, consider the\nfollowing feedback: 1. **Explicit Criteria Definition**: The prompt should explicitly instruct\nthe executer to define the criteria for optical activity at the beginning of the response. This\ncan prevent ambiguity and ensure that the executer uses the correct scientific principles. For\nexample, the prompt could include a directive to \"List the criteria for optical activity before\nanalyzing each compound.\" 2. **Data Verification Directive**: Incorporate a step in the\nprompt that requires the executer to verify the input data against reliable sources. This could\nbe phrased as \"Cross-check the properties of each compound with a trusted chemical database\nbefore proceeding with the analysis.\" 3. **Structured Logical Reasoning**: Encourage a\nstructured approach to reasoning by breaking down the analysis into distinct steps. The\nprompt could suggest a format like \"For each compound, first identify chiral centers, then\nassess symmetry, and finally determine optical activity\".\n5\nFeedback Received by the TGD optimizer (continued)\n4. **Cross-Referencing Encouragement**: ... By incorporating these elements into the\nprompt, the executer can be guided to produce more accurate and reliable responses, thereby\nimproving alignment with the ground truth answer and enhancing the objective function.\nHere, the evaluator feedback includes suggestions such as ‘List the criteria for optical activity\nbefore analyzing each compound’, which are specific to a single problem instance. Such feedback is\nclearly noisy. Since a generic optimizer relies solely on feedback to guide its updates, it is likely to\nincorporate such suggestions into the optimized LLM program prompt, which can be detrimental to\nthe overall performance of the program.\nYet, in reality, we can choose to let the LLM optimizer adapt to a specific task distribution in order\nto achieve better performance. This is because if the distribution of programs generated by the\ntask-specific optimizer is more aligned with the task requirements, the difficulty of finding the optimal\nprogram will be significantly reduced, and the impact of noise in the evaluator’s signal will be greatly\nmitigated, even when the feedback is noisy.\nWe illustrate this using the BBH Dyck Languages task as an example, showing that aligning the LLM\noptimizer to a specific task distribution can lead to improved performance. For instance, consider the\nfollowing task-specific optimizer prompt:\nA Task-specific Optimizer Prompt\nYou are part of an optimization system specialized in improving prompts for bracket matching\nand sequence completion tasks. Your role is to enhance prompts that help solve Dyck\nlanguage problems, which involve proper nesting and closure of different types of brackets (,\n<>, ()). When improving prompts, focus on these critical aspects: (1) maintaining accurate\nbracket pair matching, (2) preserving the LIFO (Last In First Out) order of nested structures,\n(3) handling multiple bracket types simultaneously, and (4) ensuring complete closure of all\nopen brackets. You should critically analyze how the prompts can better guide the model to\ntrack open brackets, maintain proper nesting order, and systematically complete sequences.\nConsider incorporating pattern recognition strategies and explicit validation rules in the\nimproved prompts. Your improvements should lead to more reliable and accurate bracket\nsequence completions.\nIt can be observed that the task-specific optimizer leads to a shift in the distribution of LLM programs\nit tends to optimize, making it more likely to generate content that aligns with key task requirements,\nsuch as producing programs that satisfy the requirement of preserving the LIFO (Last In First Out)\norder, etc. As a result, even if the evaluator feedback is somewhat noisy or sparse, the LLM program\noptimized by the optimizer can still perform well, and is more likely to generate critical statements\nsuch as: Explicitly push each opening symbol onto the stack and pop it when a corresponding closing\nsymbol is encountered. After processing each symbol, describe the current state of the stack, focusing\non unmatched opening symbols. In contrast, if a generic optimizer is used, it becomes difficult to\ngenerate effective prompts under noisy or sparse feedback conditions.\nThis demonstrates that adapting to a new task distribution is meaningful. To avoid adapting to\neach new task distribution by hand, we meta-learn how to adapt, which is the core motivation of\nmetaTextGrad.\n3.3\nPipeline\n3.3.1\nMeta Prompt Optimizer\nLLM optimizers are usually designed by humans with manual design choices. As a result, the\noptimizers themselves are not optimized or aligned with a given task. We aim to optimize the prompts\nof LLM optimizers to further enhance their effectiveness and align them with tasks.\nBelow is a brief introduction to the implementation method of the meta prompt optimizer. The\ndetailed pseudocode can be found in Appendix A.1. To Initialize, the meta prompt optimizer runs\na round of optimization on the validation dataset to evaluate and record the initial performance of\n6\nthe optimizer. To Propose, the meta prompt optimizer randomly samples data examples from the\ntraining dataset and analyzes the general characteristics of the task type. Based on the current best\noptimizer prompt, it proposes an improved prompt that is more aligned with the task. To Update,\nthe optimized optimizer undergoes an inner loop optimization test on the validation dataset. If\nthe test results outperform those of the previously best optimizer, the optimizer is updated. To\nExtractOptimizedOptimizer, the meta prompt optimizer returns the best optimizer learned so far.\nAmong these steps, Propose is the core of the meta prompt optimizer and the only stage where LLM\ncalls are invoked. For detailed prompts, refer to Appendix D.1.\n3.3.2\nMeta Structure Optimizer\nA variety of LLM optimizers have been proposed to optimize program structures, prompts, and other\ncomponents. The meta structure optimizer is designed to automatically optimize the combination and\nordering of different optimizers based on the characteristics of the task.\nBelow, we briefly introduce the working principles of the meta structure optimizer. The detailed\npseudocode implementation can be found in Appendix A.2. To Initialize, the meta structure optimizer\nruns one round of optimization for each input optimizer on the validation dataset, selects the highest\nscore and the corresponding optimizer as the initial best value. To Propose, we provide the meta\nstructure optimizer with a set of reference optimizers. If previously optimized, better-performing\noptimizers exist, they are also included. Based on this, the meta structure optimizer integrates and\nproposes an improved optimizer. To Update, the meta structure optimizer evaluates whether the\nproposed optimizer shows improved performance on the validation dataset and updates the current\nbest optimizer and score. To ExtractOptimizedOptimizer, the meta structure optimizer returns\nthe best optimizer learned so far. The Propose stage is the only stage with LLM calls. For detailed\nprompts, please refer to Appendix D.2.\n3.3.3\nmetaTextGrad\nIn the previous two sections, we introduced the meta prompt optimizer and the meta structure\noptimizer. metaTextGrad is composed of these two components. Specifically, after receiving a set of\ninput optimizers, metaTextGrad first performs prompt optimization for each optimizer individually\nand then explores the combination of different optimizers to form a better composite optimizer.\n4\nExperiment\n4.1\nExperimental Setup\nIn this section, we use the existing optimizers from DSPy and TextGrad as baseline methods.\nWe evaluate our approach and the baselines on multiple benchmarks, including BBH [13, 14],\nMMLU [15], and GPQA [16]. To ensure reproducibility, we provide the prompts and structures of\nthe learned programs in Appendix E.\nBaselines. We used zero-shot CoT, few-shot CoT [17], self-consistency [18], best-of-N [19, 20],\nMIPROv2 [7], TextGrad TGD [1] and ADAS-TG [10] as baselines. Zero-shot CoT relies on direct\nchain-of-thought reasoning, whereas MIPRO and TextGrad’s TGD optimizer refine the program’s\nprompt. ADAS is an algorithm for automatically searching and optimizing the program’s structure.\nWe implemented this algorithm within TextGrad as a baseline, referred to as ADAS-TG.\nBenchmarks. We evaluate our method on four widely used, challenging, and diverse benchmarks:\nBBH Word Sorting, BBH Dyck Languages [13, 14], MMLU Abstract Algebra [15], and GPQA\nDiamond [16]. For detailed settings, please refer to Appendix C. Due to the non-determinism of\nLLM APIs [21], the test accuracy for each benchmark is averaged over five random seeds.\nIn our experiments, we consider three different levels of LLM calls: (1) LLM calls within the program\nitself, (2) LLM calls made by the optimizer while refining the program, and (3) LLM calls made by\nthe meta-optimizer when optimizing the optimizer. As shown in Section 4.3, the frequency of these\ncalls decreases significantly across these levels. This hierarchical structure allows for cost-effective\nresource allocation: the program should use a relatively economical model, the optimizer can leverage\na more capable model, and the meta-optimizer should employ the best available model. Consequently,\n7\nMethod\nWord Sorting Dyck Languages GPQA Diamond Abstract Algebra\nAverage\nVal\nTest\nVal\nTest\nVal\nTest\nVal\nTest\nVal\nTest\nVanilla prompting methods\nZero-shot CoT\n0.46\n0.55\n0.06\n0.05\n0.32\n0.34\n0.74\n0.70\n0.40 0.41\n8-shot CoT\n0.50\n0.52\n0.14\n0.19\n0.32\n0.35\n0.65\n0.71\n0.40 0.44\nSelf-consistency (8)\n0.47\n0.52\n0.10\n0.12\n0.40\n0.42\n0.76\n0.70\n0.43 0.44\nBest of N (8)\n0.48\n0.52\n0.14\n0.17\n0.37\n0.40\n0.77\n0.74\n0.44 0.46\nTextGrad optimizers\nTGD Optimizer\n0.54\n0.55\n0.10\n0.10\n0.34\n0.35\n0.76\n0.71\n0.44 0.43\nADAS-TG\n0.58\n0.58\n0.21\n0.16\n0.36\n0.37\n0.75\n0.70\n0.48 0.45\nDSPy optimizers\nZero-shot MIPROv2 0.57\n0.55\n0.19\n0.16\n0.43\n0.38\n0.76\n0.77\n0.49 0.47\n8-shot MIPROv2\n0.52\n0.57\n0.33\n0.26\n0.37\n0.34\n0.74\n0.65\n0.49 0.46\nMeta-optimized optimizers\nmetaTextGrad\n0.60\n0.65\n0.42\n0.37\n0.45\n0.40\n0.78\n0.71\n0.56 0.53\nTable 1: Accuracy (%) of GPT-4o-mini on benchmarks. Bold indicates the best result, and underlined\ntext represents the second-best.\nin our experiments, we use GPT-4o-mini for LLM calls within the program, GPT-4o for the MIPROv2\nand TGD optimizers, and the o1 model for the structure optimizer and the meta-optimizers.\n4.2\nMain Results\nAs shown in Table 1, we achieve up to an 11% absolute performance improvement across these\ndatasets, with an average performance significantly surpassing existing optimizers. Our method\noutperforms both TGD and ADAS-TG, which serve as the base candidates for metaTextGrad,\nacross all benchmarks and achieves the best performance on most of them. It’s worth noting that the\nprograms optimized by the meta-optimized optimizers also exhibit interesting properties.\n(1) The optimized optimizer is more aligned with specific tasks. For example, in the BBH Dyck\nLanguages task, the generated program includes components such as n type analyzer, and a stack\nvalidator, which closely match the nature of the task. In contrast, an unaligned optimizer tends to\npropose more generic and broadly applicable structures.\n(2) The optimized optimizer is more effective in handling finer details. For instance, in multi-step\nLLM calls, passing both the overall problem and subproblems to each LLM call helps maintain\na global understanding throughout the process. This behavior is more frequently observed in the\noptimized optimizer, whereas the initial optimizer tends to pass only the subproblems to each subpart.\n(3) The meta-optimized optimizer generally improves efficiency. Although we allocate six opti-\nmization steps per training epoch, we observe that meta-optimized optimizers, due to their stronger\ntask alignment, often achieve significant improvements within the first 1-2 steps. In contrast, other\noptimizers show more gradual improvements.\nPlease refer to Appendix E for the optimized programs and Appendix F for the optimized optimizers.\n4.3\nCost analysis\nIn this section, we examine the trade-off between effectiveness and computational cost.\nFirst, we analyze the token usage at different levels within a single epoch of meta optimization.\nThis analysis supports our design choice of using models with different capabilities at different\nlevels. As shown in Table 2, the token usage per optimization epoch on the MMLU Abstract Algebra\ndataset reveals that higher-level components require significantly fewer tokens than lower-level ones.\nThis justifies our hierarchical design. For comparison, a single round of zero-shot CoT requires\napproximately 140k tokens, indicating that the overall token consumption of our optimization pipeline\nremains within a reasonable and practical range.\n8\nLevel\nTokens\nProgram level\n∼400k\nOptimizer level\n∼100k\nMeta-optimizer level\n∼2.5k\nTable 2: Token analysis on Abstract Algebra.\nModel\nPerformance\nCost\n0-shot CoT (4o-mini)\n0.05\n0.14$\nOurs (4o-mini)\n0.37\n0.44$\n0-shot CoT (4o)\n0.18\n0.52$\nTable 3: Cost analysis on Dyck Languages.\nIn addition, we evaluate the cost and performance of the zero-shot CoT approach using both GPT-4o-\nmini and GPT-4o on the BBH Dyck Languages dataset, and compare them to our optimized approach\napplied to GPT-4o-mini. As shown in Table 3, our method achieves the best performance on BBH\nDyck Languages while incurring a lower cost than GPT-4o. This demonstrates that with effective\nprompt and structure optimization, a smaller model can outperform the zero-shot performance of a\nlarger model. These findings highlight the practical applicability and scalability of our approach.\n4.4\nTransferability of the optimized optimizer across models and datasets\nIn this section, we evaluate the transferability of our optimized optimizer across different language\nmodels and datasets. As shown in Table 4, our method trained on GPT-4o-mini achieves superior\nperformance compared to unoptimized baselines when evaluated on Claude 3 Haiku. Furthermore, as\nillustrated in Table 5, our optimizer trained on the GPQA diamond dataset also transfers effectively to\nthe Abstract Algebra dataset. These results demonstrate that our meta-optimized optimizer exhibits\nstrong transferability across models and datasets.\nMethod (Claude 3 Haiku)\nDyck Languages\nVal\nTest\nZero-shot CoT\n0.07\n0.10\nTextGrad optimizers\nTGD Optimizer\n0.10\n0.04\nADAS-TG\n0.35\n0.34\nOptimizers optimized on GPT-4o-mini\nmetaTextGrad\n0.32\n0.35\nTable 4: Transferability of the optimized opti-\nmizer across models.\nMethod\nAbstract Algebra\nVal\nTest\nZero-shot CoT\n0.74\n0.70\nTextGrad optimizers\nTGD Optimizer\n0.76\n0.71\nADAS-TG\n0.75\n0.70\nOptimizers optimized on GPQA diamond\nmetaTextGrad\n0.78\n0.77\nTable 5: Transferability of the optimized opti-\nmizer across datasets.\n4.5\nAnalysis of the effectiveness of each meta optimizer\nIn this section, we analyze the contributions of different components of the proposed meta optimizer.\nAs shown in Table 6, we find that all meta optimizers improve performance on the BBH Dyck\nLanguages benchmark. Optimizers optimized using either method outperform the original optimizer.\nAmong them, the meta prompt optimizer achieves the best improvement when applied to ADAS-TG.\nSplit\n0-shot CoT TGD ADAS-TG TGD (O) ADAS-TG (O) Struct (O) metaTextGrad\nVal\n0.06\n0.10\n0.21\n0.21\n0.42\n0.24\n0.42\nTest\n0.05\n0.10\n0.16\n0.24\n0.37\n0.16\n0.37\nTable 6: Analysis of the effectiveness of each meta optimizer on Dyck Languages. TGD (O), ADAS-\nTG (O), and Struct (O) respectively denote the TGD and ADAS-TG optimizers enhanced by the meta\nprompt optimizer, and the optimizers enhanced by the meta structure optimizer.\nHere, metaTextGrad produces the same results as the optimized ADAS-TG because the meta\noptimizer did not find a better option during the meta structure optimization. So, the optimized\nADAS-TG was selected as the best result. Notably, the best meta optimizer varies across benchmarks\ndue to task-specific differences. Despite this, all meta optimizers can effectively enhance the\nperformance of a given optimizer.\n9\n5\nRelated Work\n5.1\nPrompt optimization\nPrompt optimization has proven crucial for improving LLM performance. Initial strategies, such as\nfew-shot learning and in-context learning, demonstrated careful prompt design could significantly\nboost LLM performance [22]. Techniques like chain-of-thought reasoning [17] and ensemble\nmethods [23] also emerged as popular ways to structure prompts for effective problem-solving.\nHowever, hand-crafted approaches are limited in their utility. Efforts to automate prompt optimization\nhave led to the development of gradient-based approaches [24, 25]. These methods, however, require\naccess to model parameters, which limits their application to open-source models.\nTo address these constraints, alternative approaches have been proposed that leverage LLMs them-\nselves as prompt optimizers. APE [26] was among the first to introduce the concept of automatic\ninstruction generation and selection. [27] introduced the concept of meta prompt, demonstrating\nsystematically designing meta-prompts can improve prompt quality. DSPy [4] proposed a systematic\napproach for optimizing LLM programs, integrating structured optimization techniques.\nAnother line of research explores optimization methods based on textual feedback. ProTeGi [28] first\nintroduced the concept of textual gradients, highlighting that LLMs themselves can serve as a form\nof a loss function to guide the improvement of LLM programs. Building on this idea, TextGrad [1]\ndeveloped a structured textual gradient descent framework, demonstrating its applicability and\neffectiveness across multiple disciplines and domains. Concurrently, OPTO [29] proposed a related\napproach in which the optimizer receives an execution trace alongside feedback on the generated\noutput. Semantic gradient descent [30] refines textual gradient descent by enhancing feedback signals.\nHowever, the LLM optimizers proposed in these methods remain fixed during the optimization\nprocess, limiting their effectiveness and adaptability. Our approach addresses this limitation by\nleveraging a meta-optimizer to align LLM optimizers with the task.\n5.2\nClassical meta-learning\nGradient-based meta-learning algorithms such as Model-Agnostic Meta-Learning (MAML) [31],\nFirst-Order MAML, Reptile [32], and ANIL [33] cast learning to learn as finding a good initialization\nthat can be fine-tuned with only a handful of gradient steps. MAML jointly optimizes across tasks\nthrough a bi-level procedure, explicitly encouraging large improvements after one or two inner-loop\nupdates. Reptile shows that a simpler first-order update suffices, while ANIL’s ablation studies reveal\nthat the bulk of MAML’s gains stem from feature reuse rather than rapid weight adaptation, allowing\nthe inner loop to be removed for all but the task-specific head. Despite their successes, these methods\nutilize fixed optimizers. In contrast, we focus on optimizing LLM-based optimizers rather than\nclassical ones, and we meta-learn the optimizer so that the optimization strategy is aligned with each\nnew task, rather than merely learning a good initialization.\n6\nConclusion\nIn this paper, we propose metaTextGrad, a meta-optimization framework that enhances existing\nLLM optimizers by aligning them more effectively with tasks. Our method introduces two key\ncomponents: the meta prompt optimizer, which refines optimizer prompts for better task adaptation,\nand the meta structure optimizer, which determines the optimal combination of different optimizers.\nBy integrating these two components, metaTextGrad improves the efficiency of LLM optimizers,\nleading to better performance across a diverse range of benchmarks.\nLooking ahead, there are several promising directions for future research. First, even the meta\noptimizer we proposed can be optimized. In particular, our meta optimizer is designed by ourselves,\ninstead of learned by data, and there are techniques in the meta learning literature that could be\nadapted to allow this [34, 35]. Second, future work can explore different ways to parameterize\nthe optimizers. This study primarily focuses on refining optimizer prompts and their composition,\nbut meta-optimizers could also be leveraged to automatically enhance the optimization algorithms\nemployed by existing optimizers. We believe that optimizing LLM optimizers is a crucial step toward\nfurther improving the performance and task alignment capabilities of LLM-driven systems and has\nthe potential to provide valuable insights to the research community.\n10\nReferences\n[1] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and\nJames Zou. Textgrad: Automatic\" differentiation\" via text. arXiv preprint arXiv:2406.07496,\n2024.\n[2] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in Neural Information Processing Systems, 36, 2024.\n[3] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\nChen. Large language models as optimizers. In The Twelfth International Conference on\nLearning Representations, 2024.\n[4] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam,\nSri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather\nMiller, Matei Zaharia, and Christopher Potts. DSPy: Compiling declarative language model\ncalls into state-of-the-art pipelines. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[5] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information Processing Systems, 36, 2024.\n[6] Steffi Chern, Ethan Chern, Graham Neubig, and Pengfei Liu. Can large language models be\ntrusted for evaluation? scalable meta-evaluation of llms as evaluators via agent debate. arXiv\npreprint arXiv:2401.16788, 2024.\n[7] Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei\nZaharia, and Omar Khattab.\nOptimizing instructions and demonstrations for multi-stage\nlanguage model programs, 2024.\n[8] Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories\nwith recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing. Association for Computational Linguistics, December\n2022.\n[9] Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid\nPalangi, Ece Kamar, and Besmira Nushi. Attention satisfies: A constraint-satisfaction lens\non factual errors of language models. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[10] Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint\narXiv:2408.08435, 2024.\n[11] Qian Li, San-Yang Liu, and Xin-She Yang. Influence of initialization on the performance of\nmetaheuristic optimizers. Applied Soft Computing, 91:106193, 2020.\n[12] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of\ninitialization and momentum in deep learning. In International conference on machine learning,\npages 1139–1147. PMLR, 2013.\n[13] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging\nBIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, Toronto, Canada, July 2023. Association for\nComputational Linguistics.\n[14] Aarohi Srivastava, Abhinav Rastogi, and Abhishek Rao et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language models. Transactions on Machine\nLearning Research, 2023.\n11\n[15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In International\nConference on Learning Representations, 2021.\n[16] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a\nbenchmark. In First Conference on Language Modeling, 2024.\n[17] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems, 35:24824–24837, 2022.\n[18] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023.\n[19] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In\nProceedings of the 34th International Conference on Neural Information Processing Systems,\nNIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc.\n[20] Steve Webb. The physical basis of imrt and inverse planning. The British journal of radiology,\n76(910):678–689, 2003.\n[21] Sherman Chann. Non-determinism in gpt-4 is caused by sparse moe. https://152334h.\ngithub.io/blog/non-determinism-in-gpt-4/, 8 2023.\n[22] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas\nKing, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models\noutcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452,\n2023.\n[23] Jinliang Lu, Ziliang Pang, Min Xiao, Yaochen Zhu, Rui Xia, and Jiajun Zhang. Merge, ensemble,\nand cooperate! a survey on collaborative strategies in the era of large language models, 2024.\n[24] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Auto-\nPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4222–4235, Online, November 2020. Association for Computational\nLinguistics.\n[25] Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang,\nLuo Si, and Huajun Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic\noptimization for relation extraction. In Proceedings of the ACM Web conference 2022, pages\n2778–2788, 2022.\n[26] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\nand Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh\nInternational Conference on Learning Representations, 2023.\n[27] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt\nengineer. arXiv preprint arXiv:2311.05661, 2023.\n[28] Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic\nprompt optimization with “gradient descent” and beam search. In Houda Bouamor, Juan\nPino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 7957–7968, Singapore, December 2023. Association for\nComputational Linguistics.\n[29] Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the next autodiff: Generative\noptimization with rich feedback, execution traces, and llms. In A. Globerson, L. Mackey,\nD. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural\nInformation Processing Systems, volume 37, pages 71596–71642. Curran Associates, Inc.,\n2024.\n12\n[30] Wenyi Wang, Hisham A. Alyahya, Dylan R. Ashley, Oleg Serikov, Dmitrii Khizbullin, Francesco\nFaccio, and Jürgen Schmidhuber. How to correctly do semantic backpropagation on language-\nbased agentic systems, 2024.\n[31] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In Proceedings of the 34th International Conference on Machine\nLearning - Volume 70, ICML’17, page 1126–1135. JMLR.org, 2017.\n[32] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms,\n2018.\n[33] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature\nreuse? towards understanding the effectiveness of maml. In International Conference on\nLearning Representations, 2020.\n[34] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In Proceedings of the 34th International Conference on Machine\nLearning, 2017.\n[35] Jürgen Schmidhuber. Evolutionary Principles in Self-Referential Learning. PhD thesis, Techni-\ncal University of Munich, 1987.\n13\nA\nImplementation Details\nIn this section, we provide the implementation of the meta optimizers. The LLMCall component\ngenerates responses based on the meta optimizer’s prompt. The detailed prompts will be provided in\nthe next section of the appendix.\nA.1\nMeta Prompt Optimizer\nAlgorithm 3 Meta Prompt Optimizer\n1: function c\nM.Initialize(D, M)\n2:\nstore(D)\n3:\nM ∗←M\n4:\nσ∗←0\n5:\nreturn\n6: end function\n7: function c\nM.Propose()\n8:\nquestion, answer ∼D\n9:\nproposedOptimizer ←LLMCall(prompt, M ∗, question, answer)\n10:\nreturn proposedOptimizer\n11: end function\n12: function c\nM.Update(M, σ)\n13:\nif σ > σ∗then\n14:\nM ∗←M\n15:\nσ∗←σ\n16:\nend if\n17:\nreturn\n18: end function\n19: function c\nM.ExtractOptimizedOptimizer()\n20:\nreturn M ∗\n21: end function\nA.2\nMeta Structure Optimizer\nAlgorithm 4 Meta Structure Optimizer\n1: function c\nM.Initialize(D, {M (i)}r\ni=1)\n2:\nstore(D, {M (i)}r\ni=1)\n3:\nM ∗←None\n4:\nσ∗←0\n5:\nreturn\n6: end function\n7: function c\nM.Propose()\n8:\nproposedOptimizer ←LLMCall(prompt, M ∗, {M (i)}r\ni=1)\n9:\nreturn proposedOptimizer\n10: end function\n11: function c\nM.Update(M, σ)\n12:\nif σ > σ∗then\n13:\nM ∗←M\n14:\nσ∗←σ\n15:\nend if\n16:\nreturn\n17: end function\n18: function c\nM.ExtractOptimizedOptimizer()\n19:\nreturn M ∗\n20: end function\nA.3\nMeta Optimizer\nFor the meta optimizer, the execution process consists of two steps: (1) Each optimizer is individually\noptimized using the meta prompt optimizer. (2) The optimized optimizers are further refined using\nthe meta structure optimizer.\n14\nB\nProof of Theorem 1\nTheorem (Restatement of Theorem 1). Let S1 and S2 be two datasets sampled independently from\ndistribution D, with sizes n and m, respectively. Then, with probability at least 1 −δ, the optimizer\nbθS1 trained on S1 satisfies:\nRS2(bθ) ≤R(θ∗) +\nq\n2 log(6/δ)\nn\n+\nq\nlog(6/δ)\n2m\n.\nProof. According to Hoeffding’s inequality, for any 0 < δ < 1:\nPr\nS1∼Dn\nh\n|R(θ) −RS1(θ)| > εn(δ)\ni\n≤δ,\n(4)\nwhere:\nεn(δ) =\nq\nlog(2/δ)\n2n\n(5)\nUsing inequality 4, we can bound the population risk of the learned optimizer. With probability at\nleast 1 −2δ\n3 :\nR(bθ) ≤RS1(bθ) + εn(δ\n3) ≤RS1(θ∗) + εn(δ\n3) ≤R(θ∗) + 2 εn(δ\n3),\n(6)\nwhere θ∗= arg minθ R(θ).\nWith a union bound, with probability at least 1 −δ:\nRS2(bθ) ≤R(bθ) + εm(δ\n3) ≤R(θ∗) + 2 εn(δ\n3) + εm(δ\n3)\n(7)\nIn conclusion, with probability at least 1 −δ over the draws of S1 and S2:\nRS2(bθ) ≤R(θ∗) +\nq\n2 log(6/δ)\nn\n+\nq\nlog(6/δ)\n2m\n(8)\nC\nDetails on Experimental Benchmark Setup\nBBH.\nThe BBH task [13, 14] is a challenging benchmark that requires precise reasoning across\nvarious tasks. We select two subsets from the BBH benchmark: BBH Word Sorting and BBH Dyck\nLanguages. The BBH Word Sorting task requires the language model to sort a given set of words in\norder, while the BBH Dyck Languages task involves providing a string composed of various types of\nbrackets and asking the model to determine the characters needed to complete the bracket pairing.\nWe use the same train/validation/test splits as in TextGrad [1] (i.e., 50, 100, and 100 instances for\ntraining, validation, and testing) and follow the TextGrad approach by using GPT-4o to evaluate\nwhether the model’s output is correct, based on the predicted and ground truth answers.\nMMLU.\nThe MMLU benchmark [15] evaluates a model’s ability to answer questions across a\nwide range of scientific disciplines. We selected the MMLU Abstract Algebra dataset and created\ntraining, validation, and test sets consisting of 10, 50, and 40 questions, respectively. We assess model\naccuracy using exact matching, determining correctness by applying a regular expression to check\nwhether the model’s response contains \"Answer: [A-D]\" and matches the ground truth. However, we\nobserved that the DSPy baselines struggle to adhere to this output format. To accommodate this, we\nuse GPT-4o to evaluate the correctness of responses generated by DSPy-based methods.\nGPQA Diamond.\nThe GPQA Diamond benchmark [16] evaluates the model’s ability to solve\ngraduate-level, Google-proof questions. The benchmark consists of 198 questions, which we split\ninto training, validation, and test datasets containing 30, 100, and 68 questions. Similar to MMLU,\nwe use exact matching to determine response accuracy by applying a regular expression to check\nwhether the model’s answer follows the format \"Answer: [A-D]\" and matches the correct answer.\nFor the DSPy baselines, we use GPT-4o to evaluate the correctness of DSPy-generated responses.\n15\nD\nPrompts of Meta Optimizers\nIn this section, we provide the prompts used in the propose stage of the meta prompt optimizer and\nthe meta structure optimizer. The propose stage is the only part of the meta optimizer that involves an\nLLM call; all other functions consist of direct numerical updates or result retrieval, as explained in\npseudocode in Appendix A.\nD.1\nMeta Prompt Optimizer\nBelow, we present the prompt used for the meta prompt optimizer.\nMeta Prompt Optimizer\n# Task Requirement\nA TextGrad optimizer optimizes a TextGrad pipeline so that the pipeline can generate better\noutputs based on inputs.\nA TextGrad pipeline consists of several agents, each of which has a specific role in the\noptimization process, which is defined by different prompts.\nYou will be given a general task description of an optimizer and the specific task the pipeline\naims to solve.\nYour task is to propose an improved optimizer task description so that the optimizer can better\noptimize the pipeline for the given task.\n# Optimizer Code\nHere is the source code of the optimizer: (just for reference)\n{optimizer_source_code}\n# The task of the optimizer\nSpecifically,\nthe\npipeline\naims\nto\nsolve\nthis\nkind\nof\nquestion:\n{exam-\nple_set.get_question_type()}.\nAn example of the task is provided here:\nQuestion: {example_question}\nAnswer: {example_answer}\nThe LLM optimizer wants to improve a LLM pipeline to solve such kind of problems.\n# Current Task Description\nHere is the current task description of the optimizer, which you can improve: {opti-\nmizer_prompt}\n# Your task\nYou should identify what the optimizer should pay attention to in order to improve the\n{optimizer_type} of the pipeline for solving the given task.\nConduct a detailed analysis of the given example, and respond in the following format:\n“‘json\n\"improved_task_description\": \"...\", # Your improved task description for the optimizer\n“‘\nD.2\nMeta Structure Optimizer\nMeta Structure Optimizer\n# Task Requirement\nA TextGrad optimizer optimizes a TextGrad pipeline so that the pipeline can generate better\noutputs based on inputs.\nThe structure and prompts of current optimizers are fixed, and you will be given the source\ncode of some optimizers.\nYour task is to propose an improved optimizer code to better optimize the pipeline.\nThis can be achieved by integrating the reference codes of the given different optimizers and\nmerging them into a new optimizer.\n16\nMeta Structure Optimizer (Continued)\n# Task Details\nYou will be provided with implementations of several optimizers, each annotated with their\nrespective purposes.\nCarefully read through their functions and implementation details. Your task is to integrate\nthe different implementation approaches to provide a more optimized solution.\nIn this context, the step() function refers to the process of performing a single optimization\nstep.\nYou may notice that different optimizers are suitable for different purposes and should be\napplied in a specific order.\nFor example, if the pipeline aims to optimize the structure, this optimization will overwrite\nthe prompts of each previously optimized component, so the structure optimization should be\nexecuted first.\nYour implementation of the step() function should take the order into account, and each call\nto the step() function should only optimize a single aspect of the pipeline because the gradient\ncontext is not reusable.\nTherefore, you need to implement a logic so that the optimizer can use different optimizing\nstrategies in a sequential manner, with each strategy being applied self.epoch times before\nmoving to the next one.\n{optimizer_prompt}\n# Code Reminder\nYou should include the following necessary imports at the beginning of the code:\nimport inspect\nimport copy\nfrom typing import Type, List, Union\nfrom collections import defaultdict\nfrom textgrad.variable import Variable\nfrom textgrad import logger\nfrom textgrad.engine import EngineLM\nfrom textgrad.optimizer.optimizer import Optimizer, get_gradient_and_context_text\nfrom textgrad.model import Pipeline\nfrom textgrad.autograd import FormattedLLMCall\nfrom textgrad.config import validate_engine_or_get_default\nfrom\ntextgrad.optimizer.optimizer_prompts\nimport\nconstruct_tgd_prompt,\nOPTIMIZER_SYSTEM_PROMPT,\nPIPELINE_SYSTEM_PREFIX,\nPIPELINE_SYSTEM_SUFFIX\n# Note\nYou are required to build on one of the existing optimizers and improve it by integrating\nfeatures from the others, instead of starting from scratch.\nYou should only include the ImprovedOptimizer (inherited exactly from Optimizer) class in\nthe code, and no other classes or functions.\nYou should implement all the necessary functions and attributes in the ImprovedOptimizer\nclass to ensure that the code can be executed without errors.\nYou should not modify input signatures of the __init__ and step functions when implementing\nthe ImprovedOptimizer class.\nPlease output the complete improved Python code, and make sure to call the improved class\n’ImprovedOptimizer’; remember to also include all necessary attributes of the class so that\nthe code can be executed without errors.\nDo not include any additional comments or unnecessary text in the output.\n17\nE\nOptimized Programs\nTo facilitate the reproducibility of our work, we provide the program generated by the optimizer\noptimized with metaTextGrad.\nE.1\nBBH Word Sorting Benchmark\nBBH Word Sorting\nclass SimplePipeline(Pipeline):\ntask_description = (\n\"You will answer a reasoning question. Think step by step. \"\n\"The last line of your response should be of the following\nformat: Answer: \\$VALUE' \"\n\"where VALUE is the answer to the question.\"\n)\ndef __init__(self, engine: Union[EngineLM, str] = None, path=None):\nsuper().__init__(engine=engine, path=path)\nself.planner_prompt = Variable(\n\"Create a step by step plan for the question. Provide each\nstep on a new line.\",\nrequires_grad=True,\nrole_description=\"planner prompt\"\n)\nself.planner = tg.BlackboxLLM(self.engine, self.planner_prompt)\nself.subsolver_prompt = Variable(\n\"Solve this sub-step in detail\n, without giving the final answer.\",\nrequires_grad=True,\nrole_description=\"sub-step solver prompt\"\n)\nself.subsolver = tg.BlackboxLLM(self.engine,\nself.subsolver_prompt)\nself.final_prompt = Variable(\n\"\"\"Combine all reasoning into a coherent final response,\nending with the format: Answer: $VALUE\"\"\" ,\nrequires_grad=True,\nrole_description=\"final answer prompt\"\n)\nself.finalsolver = tg.BlackboxLLM(self.engine,\nself.final_prompt)\ndef _step_split_rule(self, text: str):\nreturn re.split(r\"\\n+\", text.strip())\ndef forward(self, question: Variable) -> Variable:\nplan = self.planner(question)\nsteps = Split()(self._step_split_rule, plan)\nsub_solutions = []\nfor step in steps:\nsub_solutions.append(self.subsolver(step))\naggregated_reasoning = Aggregate()(sub_solutions)\nfinal_answer = self.finalsolver(Aggregate()([question,\naggregated_reasoning]))\nreturn final_answer\n18\nE.2\nBBH Dyck Languages Benchmark\nBBH Dyck Languages\nclass SimplePipeline(Pipeline):\ntask_description = \"\"\"You will answer a reasoning question.\nThink step by step. The last line of your response\nshould be of the following format:\n'Answer: $VALUE' where VALUE is the answer to the question.\"\"\"\ndef __init__(self, engine: Union[EngineLM, str] = None, path=None):\nsuper().__init__(engine=engine, path=path)\nif not path:\nself.initial_planner = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Break down the Dyck sequence validation into\ndetailed steps. Format as numbered steps:\n1), 2), etc.\"\"\" ,\nrequires_grad=True,\nrole_description=\"creates detailed analysis plan\"\n)\n)\nself.type_analyzer = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Identify and categorize all bracket types.\nList each type and its corresponding closing\nbracket. Format: 'Types:\n[pairs]'\"\"\" ,\nrequires_grad=True,\nrole_description=\"analyzes bracket types and pairs\"\n)\n)\nself.stack_validator = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Simulate stack operations for bracket matching.\nShow stack state after each operation.\nEnd with 'Answer:\nValid/Invalid'\"\"\" ,\nrequires_grad=True,\nrole_description=\"performs stack-based validation\"\n)\n)\nself.nesting_analyzer = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Analyze nesting hierarchy. Check if inner\nbrackets close\nbefore outer brackets.\nEnd with 'Answer: Proper/Improper'\"\"\" ,\nrequires_grad=True,\nrole_description=\"validates nesting hierarchy\"\n)\n)\n19\nBBH Dyck Languages (Continued)\nself.depth_checker = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Calculate maximum nesting depth and verify\nbalanced structure. End with 'Answer:\nDepth=X,Balanced=Yes/No'\"\"\" ,\nrequires_grad=True,\nrole_description=\"checks nesting depth and balance\"\n)\n)\nself.sequence_validator = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Validate sequence completeness and correctness.\nEnd with 'Answer: Complete/Incomplete'\"\"\" ,\nrequires_grad=True,\nrole_description=\"validates sequence completeness\"\n)\n)\nself.final_evaluator = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Synthesize all analysis results and determine\nif this is a valid Dyck sequence.\nEnd with 'Answer: Yes/No'\"\"\" ,\nrequires_grad=True,\nrole_description=\"makes final validity\ndetermination\"\n)\n)\ndef forward(self, question: Variable) -> Variable:\nplan = self.initial_planner(question)\nanalysis_steps = Split()(lambda x: re.split(r'\\d\\)', x), plan)\ntype_analysis = self.type_analyzer(question)\nstack_validation = self.stack_validator(\nAggregate()([question, type_analysis])\n)\nnesting_analysis = self.nesting_analyzer(\nAggregate()([question, stack_validation])\n)\ndepth_analysis = self.depth_checker(\nAggregate()([question, nesting_analysis])\n)\nsequence_validation = self.sequence_validator(\nAggregate()([question, depth_analysis])\n)\n20\nBBH Dyck Languages (Continued)\nfinal_result = self.final_evaluator(\nAggregate()([\nquestion,\nstack_validation,\nnesting_analysis,\ndepth_analysis,\nsequence_validation\n])\n)\nreturn Extract()(r\"Answer: (.+)\", final_result)\nE.3\nGPQA Diamond Benchmark\nGPQA Diamond\nclass SimplePipeline(Pipeline):\ntask_description = \"\"\"You will answer a reasoning question.\nThink step by\nstep. The last line of your response should\nbe of the following format: 'Answer: $VALUE' where VALUE is\nthe answer to the question.\"\"\"\ndef __init__(self, engine: Union[EngineLM, str] = None, path=None):\nsuper().__init__(engine=engine, path=path)\nif not path:\nself.executer = textgrad.BlackboxLLM(\nself.engine,\ntextgrad.Variable(\n\"\"\"You will answer a multiple choice question.\nBegin by\nidentifying and listing all key\ndetails and constraints from the problem\nstatement. Use explicit reasoning steps to\noutline the solution, incorporating relevant\nscientific\nprinciples such as reaction\nmechanisms or stereochemistry. Verify your\ninitial conclusions by cross-checking each\nstep against the problem's requirements.\nConsider alternative answers and evaluate\nwhy they might be correct or incorrect.\nProvide a clear justification for your answer\nchoice, and rate your confidence in the final\nanswer on a scale from 1 to 10, explaining\nyour rationale. The last line of your response\nshould be of the following format:\n'Answer: $VALUE' where VALUE is one of ABCD.\"\"\" ,\nrequires_grad=True,\nrole_description=\"prompt for the executer,\nwhich aims to solve the task\"\n)\n)\ndef forward(self, question: Variable) -> Variable:\nreturn self.executer(question)\n21\nE.4\nMMLU Abstract Algebra Benchmark\nMMLU Abstract Algebra\nclass SimplePipeline(Pipeline):\ntask_description = \"\"\"You will answer a reasoning question.\nThink step by step. The last line of your response should\nbe of the following format:\n'Answer: $VALUE' where VALUE is the answer to the question.\"\"\"\ndef __init__(self, engine: Union[EngineLM, str] = None, path=None):\nsuper().__init__(engine=engine, path=path)\nif not path:\nself.executer = textgrad.BlackboxLLM(\nself.engine,\ntextgrad.Variable(\n\"\"\"You will answer a multiple choice question.\nAnalyze each\nstatement separately and provide\nyour reasoning. Use clear, logical steps,\nverifying each sub-component of the problem\nsystematically. Present each statement distinctly\nusing bullet points or numbers, ensuring the final\nconclusion is\nclearly separated from the statement\nevaluations. Include any assumptions or context\nnecessary for each conclusion. After evaluating\neach statement, reexamine your conclusions\nto confirm their correctness. Introduce a summary\nverification step before concluding. The last line\nof your response should be of the following format:\n'Answer: $VALUE' where VALUE is one of ABCD.\"\"\" ,\nrequires_grad=True,\nrole_description=\"prompt for the executer,\nwhich aims to solve the task\"\n)\n)\ndef forward(self, question: Variable) -> Variable:\nreturn self.executer(question)\nF\nOptimized Optimizers\nF.1\nOptimized TGD Optimizer Demo\nThe TGD optimizer is generic, so its optimized program offers only general guidance like “ensuring\nthat every opening bracket has a corresponding closing bracket,” without task-specific strategies. In\ncontrast, the optimized TGD aligns closely with the task, emphasizing ideas like “preserving the\nLIFO (Last In First Out) order of nested structures.” Since the optimized TGD optimizer is better\naligned with the task, its optimized program also effectively implements the LIFO principle, making\nit more effective. This is why a task-specific prompt optimizer is necessary.\nTGD Optimizer prompt\nYou are part of an optimization system that improves text (i.e., variable). You will be asked to\ncreatively and critically improve prompts, solutions to problems, code, or any other text-based\nvariable.\n22\nOptimized program prompt\nYou will answer a reasoning question. Think step by step, ensuring logical consistency\nand accuracy. Explicitly define the role and rules for each type of bracket in the sequence.\nVerify each step of your reasoning, ensuring that every opening bracket has a corresponding\nclosing bracket. Compare your predicted sequence with a known ground truth or expected\npattern, and explain any discrepancies. If errors are identified, re-evaluate and correct them.\nUse mental visualization techniques to aid in understanding sequences. Provide a clear\nand concise explanation. The last line of your response should be of the following format:\n’Answer: $VALUE’ where VALUE is the answer to the question.\nMeta-optimized TGD Optimizer prompt\nYou are part of an optimization system specialized in improving prompts for bracket matching\nand sequence completion tasks. Your role is to enhance prompts that help solve Dyck\nlanguage problems, which involve proper nesting and closure of different types of brackets\n({}, <>, ()). When improving prompts, focus on these critical aspects: (1) maintaining\naccurate bracket pair matching, (2) preserving the LIFO (Last In First Out) order of nested\nstructures, (3) handling multiple bracket types simultaneously, and (4) ensuring complete\nclosure of all open brackets. You should critically analyze how the prompts can better guide\nthe model to track open brackets, maintain proper nesting order, and systematically complete\nsequences. Consider incorporating pattern recognition strategies and explicit validation rules\nin the improved prompts. Your improvements should lead to more reliable and accurate\nbracket sequence completions.\nOptimized program prompt\nYou will answer a reasoning question. Focus on identifying and providing only the missing\nclosing symbols needed to complete the sequence. Use a stack-based approach to track\nopening and closing symbols, ensuring proper nesting and closure. Explicitly push each\nopening symbol onto the stack and pop it when a corresponding closing symbol is encountered.\nAfter processing each symbol, describe the current state of the stack, focusing on unmatched\nopening symbols. Ensure that the order of closing symbols matches the reverse order of\nunmatched opening symbols as they appear in the stack. Verify the sequence by checking\neach symbol’s nesting and closure order. Highlight critical decision points, such as when to\npop from the stack or add closing symbols. Use visual aids if necessary to represent the stack\nstate and sequence operations. After completing your analysis, perform a verification check\nby comparing the predicted closing symbols with the expected output. If discrepancies are\ndetected, revisit previous steps to self-correct. Conclude with a summary of the final state of\nthe stack and confirm that all brackets are matched. The last line of your response should be\nof the following format: ’Answer: $VALUE’ where VALUE is the answer to the question.\nF.2\nOptimized ADAS-TG Optimizer Demo\nThe original program contains generic components like a planner, reasoner, and synthesizer, lacking\ntask-specific focus. In contrast, the optimized version adds tailored modules such as a type analyzer,\nstack validator, and nesting analyzer, aligning closely with BBH Dyck Languages. This illustrates the\nnecessity of a task-specific prompt optimizer.\nADAS-TG Optimizer prompt\n(Some general instructions are omitted here.) Please produce only the code for the pipeline,\nwith a more systematic or hierarchical structure if possible.\n23\nMeta-optimized ADAS-TG Optimizer prompt\n(Some general instructions are omitted here.) Please produce only the code for the pipeline,\nwith the following structural improvements: 1) Implement a stack-based mechanism for\ntracking open brackets, 2) Create separate components for sequence parsing, bracket matching,\nand completion generation, 3) Include validation checks for proper nesting and bracket type\nmatching, 4) Ensure systematic handling of different bracket types ([], {}, (), <>), 5) Maintain\na hierarchical structure that clearly separates the parsing logic from the completion generation.\nThe pipeline should efficiently handle nested sequences while preserving the LIFO (Last In,\nFirst Out) order of brackets.\nG\nPerformance When Using an Open-source Model as the Optimizer\nWe further evaluate the generality of our framework by adopting fully open-source models. Specifi-\ncally, we employ the non-thinking variant of Qwen3-8B as the program model and Qwen3-235B-\nA22B as both the optimizer and meta-optimizer. The experiments are conducted on the BBH Dyck\nLanguages benchmark. As shown in Table 7, metaTextGrad continues to deliver strong performance.\nH\nPerformance When Applying to a Challenging Benchmark\nTo further assess the adaptability of our approach, we evaluate metaTextGrad on the ARC-AGI\nbenchmark, a task family known for requiring abstract reasoning and compositional generalization.\nFollowing common ADAS practice, we sample ARC-AGI instances with grid sizes ≤5 × 5,\ncomprising 20 training, 30 validation, and 30 test examples. The evaluation reports one-shot success\nrates, using Claude 3 Haiku as the program model and Claude 3.5 Sonnet as both optimizer and\nmeta-optimizer. As presented in Table 7, the results demonstrate that metaTextGrad remains highly\ncompetitive in this more demanding setting.\nMethod\nDyck Languages (Qwen models)\nARC-AGI (Challenging Benchmark)\nVal\nTest\nVal\nTest\nVanilla prompting methods\nZero-shot CoT\n0.27\n0.27\n0.27\n0.23\n8-shot CoT\n0.37\n0.40\n0.03\n0.00\nSelf-consistency (8)\n0.31\n0.32\n0.30\n0.23\nBest of N (8)\n0.39\n0.41\n0.27\n0.20\nTextGrad optimizers\nTGD Optimizer\n0.69\n0.68\n0.33\n0.33\nADAS-TG\n0.32\n0.34\n0.28\n0.26\nDSPy optimizers\nZero-shot MIPROv2\n0.59\n0.50\n0.30\n0.23\n8-shot MIPROv2\n0.57\n0.51\n0.33\n0.03\nMeta-optimized optimizers\nmetaTextGrad\n0.82\n0.77\n0.37\n0.40\nTable 7: Validation and test accuracy when using open-source models (Qwen models, Dyck Lan-\nguages) and evaluating on a challenging benchmark (ARC-AGI). Bold entries denote the best-\nperforming method within each benchmark.\nI\nPotential Limitations, Societal Consequences, and Broader Impacts\nLimitations. While metaTextGrad demonstrates stable performance across various models and\nbenchmarks, we acknowledge the following limitations:\n24\n(1) Although metaTextGrad improves performance on many benchmarks, it may not be effective in\nscenarios where the base model lacks sufficient task-relevant knowledge or reasoning capabilities.\nFor example, GPT-4o-mini struggles on math competition benchmarks such as AIME 2024, where\nmetaTextGrad alone cannot yield significant performance gains.\n(2) The meta-optimizer relies on strong instruction-following and problem analysis capabilities. As a\nresult, it currently requires advanced models such as o1 or Claude-3.5-Sonnet. Models like Gemini\n1.5 Pro do not yet perform adequately. However, given the rapid progress in model development, we\nexpect that more models will become suitable for the framework in the near future.\nSocietal Consequences. metaTextGrad can have meaningful societal consequences.\n• Acceleration of domain-specific AI applications. By making it easier to adapt LLMs to\nspecific tasks, metaTextGrad may accelerate the deployment of more reliable AI solutions\nin other domains.\n• Risk of automation bias or over-reliance. As optimizers become more autonomous, users\nmight rely on them without fully understanding their behavior or limitations.\n• Potential misuse for persuasive or manipulative systems. metaTextGrad could be\nexploited to generate more persuasive outputs.\nImpact Statement. This paper presents work whose goal is to advance the field of Machine Learning.\nThe paper is solely centered on the methodology itself. How it is applied and for what purpose is\nentirely up to the users. There are many potential societal consequences of our work, none which we\nfeel must be specifically highlighted here.\n25\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: We clearly state our claims in the abstract and the introduction section. They\naccurately reflect the methods in Section 3 and the experiments in Section 4.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We mentioned potential limitations in Appendix I.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\n26\nJustification: We provide the full set of assumptions in Section 3.1 and a complete proof in\nAppendix B.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We clearly describe our methods in Section 3 and Appendix A. The experi-\nmental settings are provided in Section 4 and Appendix C.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\n27\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: We provide open code access.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: The experimental settings are provided in Section 4 and Appendix C.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: Results shown in the experiment section are repeated for 5 times, using\ndifferent random seeds.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n28\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: The API is the only compute resource we used. We report the details of API\nusage in Section 4.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The paper conforms with the NeurIPS code of ethics.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We discuss the broader impacts in Appendix I. The paper is solely centered on\nthe methodology itself. How it is applied and for what purpose is entirely up to the users.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n29\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper poses no such risks.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: They are properly credited, mentioned and respected.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n30\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: Yes, code is the only new asset. It is well documented and the documentation\nis provided alongside the code.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n31\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [Yes]\nJustification: The LLM program, optimizer, and meta-optimizer are the core components of\nthe proposed method and are thoroughly discussed in the paper.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n32\n",
  "pages": [
    {
      "page_number": 1,
      "text": "metaTextGrad: Automatically optimizing language\nmodel optimizers\nGuowei Xu\nTsinghua University\nMert Yuksekgonul\nStanford University\nCarlos Guestrin\nStanford University\nJames Zou\nStanford University\nAbstract\nLarge language models (LLMs) are increasingly used in learning algorithms, eval-\nuations, and optimization tasks. Recent studies have shown that using LLM-based\noptimizers to automatically optimize model prompts, demonstrations, predictions\nthemselves, or other components can significantly enhance the performance of AI\nsystems, as demonstrated by frameworks such as DSPy and TextGrad. However,\noptimizers built on language models themselves are usually designed by humans\nwith manual design choices; optimizers themselves are not optimized. Moreover,\nthese optimizers are general purpose by design, to be useful to a broad audience,\nand are not tailored for specific tasks. To address these challenges, we propose\nmetaTextGrad, which focuses on designing a meta-optimizer to further enhance\nexisting optimizers and align them to be good optimizers for a given task. Our\napproach consists of two key components: a meta prompt optimizer and a meta\nstructure optimizer. The combination of these two significantly improves perfor-\nmance across multiple benchmarks, achieving an average absolute performance\nimprovement of up to 6% compared to the best baseline.\n1\nIntroduction\nLarge language models (LLMs) are increasingly used in learning algorithms, optimization, and\nevaluation tasks [1, 2, 3, 4, 5]. However, algorithms that incorporate LLMs often face significant\nchallenges. Firstly, many of these algorithms are still hand-crafted to a considerable extent, requiring\nsubstantial human expertise and effort to design and implement effectively. Second, LLMs are notably\nsensitive to the specific wording and structure of their instructions [6], making it tedious to improve\nthem effectively to be used in learning algorithms.\nMany studies have explored prompt optimization approaches to automatically design better prompts\nand enhance the performance of LLMs. For example, algorithms such as OPRO [3], MIPRO [7], and\nTextGrad [1] have introduced optimizers based on LLMs that support automatic prompt optimization.\nHowever, these optimizers are often fixed, applying the same optimization strategies with the\nsame optimizer prompts independent of the task, lacking a process to align with the specific task.\nImportantly, these optimizers are general purpose by design, to be useful to many different downstream\ntasks and be used by large user bases. Furthermore, different optimizers likely excel at different types\nof optimization tasks; thus, achieving the best of all optimizers introduces either a choice to be made\namong them or a strategy to ensemble them. There is no method to automatically design and improve\nthe optimizers to call based on the characteristics of a task.\nIn this work, we explore how to automatically optimize both the structure and the prompt of optimizers.\nSpecifically, we assume that all LLM calls are black-box calls, where the internal states of the LLM,\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n"
    },
    {
      "page_number": 2,
      "text": "Program\nTask\nOptimizers\nMeta Optimizer\nOptimized \nOptimizer\nCandidate \nOptimizer\nOptimized Program\nAnswer:  624\nQuest i on:  . . .  How many pages \ndoes he wr i t e a year ?\nAverage Accuracy:53%\nAverage Accuracy:45%\nAverage Accuracy:47%\n...\nFigure 1: Illustration of the meta-optimization process. A meta-optimizer optimizes LLM optimizers\nby aligning them with specific tasks through task interaction while leveraging the strengths of different\noptimizers to propose a more effective optimizer.\nas well as any information such as model gradients, are inaccessible, and only the inputs to and\noutputs of the LLM are observable. For the task to be optimized, we require only a small training\ndataset containing some input-output pairs and an evaluation metric the user wants to improve upon.\nWe propose using a meta-optimizer to automatically optimize the optimizer, with the objective of\nidentifying improved optimizers such that the program optimized by it achieves the best performance\non the given task. To this end, we introduce two types of meta-optimization strategies: the meta\nprompt optimizer and the meta structure optimizer. The meta prompt optimizer focuses on refining\nthe prompts of the LLM optimizers to enhance their effectiveness and better align them with specific\ntasks. Meanwhile, the meta structure optimizer is designed to automatically determine the optimal\ncombination and sequence of different optimizers or modules based on the characteristics of the task.\nBy combining these two types of meta-optimizers, we propose metaTextGrad, which integrates both\ncomponents into a unified framework. Specifically, given a set of input optimizers, metaTextGrad\nfirst performs prompt optimization for each optimizer independently. Then, it explores the combi-\nnation and sequencing of these optimizers to construct a more effective composite optimizer. We\nconducted experiments on multiple benchmarks, and the results demonstrate that our meta-optimized\noptimizers consistently outperform the existing ones.\nOverall, we summarize our contributions as follows. First, we introduce the concept of meta-\noptimization, highlighting that existing LLM optimizers often require further task alignment and\neffective combination through a meta-optimizer to maximize their potential. Second, we develop\ntwo types of meta-optimizers: the meta prompt optimizer and the meta structure optimizer. Building\non these, we propose metaTextGrad, which integrates both types of meta-optimizers into a unified\nframework. Lastly, experimental results on multiple benchmarks demonstrate that our method\nsignificantly outperforms baseline approaches in both performance and generalization.\n2\nProblem Statement\nHere, we will follow the notation from [7]. Consider an LLM program Φ that may contain multiple\nLLM calls forming a pipeline, with each call using a different prompt. The pipeline structure of a\nprogram and the prompt corresponding to each LLM call can be learnable and optimized by an LLM\noptimizer.\nThe task of the LLM optimizer is to find the optimal program Φ given a training dataset D (with pairs\nof inputs x and outputs y), and an evaluation metric µ.\nΦ∗= arg max\nΦ\n1\n|D|\nX\n(x,y)∈D\nµ(Φ(x), y)\n(1)\nExisting methods attempt to approximate solutions to this optimization problem from different\nperspectives. Optimizers such as MIPRO [7], OPRO [8], and TGD [9] aim to find better prompts,\nwhile optimizers like ADAS [10] focus on exploring structures. We unify the existing optimizer\n2\n"
    },
    {
      "page_number": 3,
      "text": "algorithms into a general framework, as outlined in Algorithm 1. Importantly, these optimizers are\nthe same type as of M, are designed by humans, and their structure and prompts are fixed.\nSpecifically, each optimizer should be capable of performing four operations:\n1. Initialize: Given the training dataset and the program to be optimized, prepare the training scheme.\n2. Propose: Suggest improvements to the program, which may involve modifications to the pipeline\nstructure, prompts, or other aspects.\n3.Update: Update the current optimal program based on the evaluation results of the improved\nprogram and determine the next proposal.\n4. ExtractOptimizedProgram: Return the best program found so far.\nAlgorithm 1 Inner Loop: Optimize Φ with Optimizer M\n1: Input: Optimizer M, Initial Program Φ, Max Iterations I\n2: Input: Training Data D, Validation Data Dval, Metric µ\n3: Output: Φ∗, i.e., the optimized version of Φ\n4: M.Initialize(D, Φ)\n5: for k ←1 to I do\n6:\nΦk ←M.Propose()\n7:\nσ ←\n1\n|Dval|\nP\n(x,y)∈Dval µ\n\u0000Φk(x), y\n\u0001\n8:\nM.Update(Φk, σ)\n9: end for\n10: (Φ∗, σ∗\u0001\n←M.ExtractOptimizedProgram()\n11: return\n\u0000Φ∗, σ∗\u0001\nThe problem we investigate is how to further enhance existing optimizers and align them to be\neffective optimizers for a given task, instead of relying on human-written optimizers. To this end,\nwe introduce the concept of a meta-optimizer. Let the optimized program obtained by an optimizer\nM be denoted as Φ∗= M.optimize(D, Φ). The optimization objective of the meta-optimizer is to\nfind improved optimizers such that the program optimized by this optimizer performs better on the\ncorresponding task. In particular, the optimization problem is formalized as:\nM ∗= arg max\nM\n1\n|D|\nX\n(x,y)∈D\nµ(M.optimize(D, Φ)(x), y).\n(2)\nIn optimization, good initializations often contribute to improved performance, in continuous or\ndiscrete optimization alike [11, 12]. Ideally, the meta-optimization framework should leverage the\nexisting, manually designed optimizers and achieve further improvements. Therefore, the input to the\nmeta-optimizer can include one or more existing optimizers as initialization, ultimately producing an\noptimized optimizer. The detailed process is illustrated in Algorithm 2.\nAlgorithm 2 Meta-Optimization of Optimizers\n1: Input: Meta-Optimizer c\nM\n2: Input: Max Meta-Iterations J, Max Inner-Iterations I\n3: Input: Initial optimizers {M (1), M (2), . . . , M (r)}\n4: Input: Training Data D, Validation Data Dval\n5: Input: Metric µ, Initial Program Φ\n6: Output: Optimized optimizer M ∗\n7: c\nM.Initialize(D, {M (i)}r\ni=1)\n8: for j ←1 to J do\n9:\nMj ←c\nM.Propose()\n10:\n(Φ∗\nj, σj) ←InnerLoop\n\u0000Mj, Φ, I, D, Dval, µ)\n11:\nc\nM.Update\n\u0000Mj, σj\n\u0001\n12: end for\n13: M ∗←c\nM.ExtractOptimizedOptimizer()\n14: return M ∗\nSpecifically, the meta-optimizer combines different input optimizers to propose new optimizers. The\nnewly proposed optimizers execute the process described in Algorithm 1 to obtain the inner loop\n3\n"
    },
    {
      "page_number": 4,
      "text": "optimization results. Based on the performance of the newly designed optimizers, the meta-optimizer\nupdates the current best optimizer and determines the proposal for the next iteration (outer loop).\nHowever, similar to the problem of optimizing a program, finding an optimal optimizer using a meta-\noptimizer is generally an intractable optimization problem. Therefore, it is necessary to introduce\nappropriate parameterizations and simplifications to the problem. In our work, we primarily focus on\ntwo types of parameterizations for the meta-optimizer:\n1. automatically optimizing the prompts in the optimizer to align it with the given task (Meta Prompt\nOptimizer),\n2. automatically optimizing the combination of different optimizers based on the characteristics of\nthe task, forming a new composite optimizer (Meta Structure Optimizer).\nCollectively, we can meta-optimize using both parameterizations to obtain better optimizers.\n3\nmetaTextGrad: Automatically Optimizing Language Model Optimizers\nIn this section, we first introduce the theoretical insights behind the meta optimizer. Next, we\nfurther analyze and illustrate our motivation through concrete examples. Finally, we present the\nmetaTextGrad pipeline, which consists of the meta prompt optimizer and the meta structure opti-\nmizer.\n3.1\nTheoretical Insight\nWe present the theoretical motivation for meta optimization, highlighting the importance of aligning\nthe optimizer with the target task. In particular, it is shown that when both the training and test sets are\nsampled from the same underlying data distribution, an optimizer properly aligned via meta-learning\non the training set will, with high probability, produce programs on the test set whose accuracy\nclosely approaches that of the optimal optimizer. In contrast, an optimizer that has not been optimized\non the training set lacks such theoretical guarantee.\nWe begin by introducing the necessary notation. Let µ : X →[0, 1] denote the accuracy metric,\nwhere X is the space of natural language outputs. Let D be a distribution over natural language\ninputs x. Let Φθ be an optimizer parameterized by θ ∈Θ. Given input x and access to µ, Φθ makes\nT zeroth-order queries and returns an optimized output xT .\nDefine the loss of the optimizer on input x as L(θ, x) = 1 −µ(xT ) = 1 −µ(Φθ(x, T)). The\npopulation loss is given by R(θ) = Ex∼D[L(θ, x)]. Let S = {x1, . . . , xn} ∼Dn denote a dataset\nsampled from D, and define the empirical loss on S as RS(θ) = 1\nn\nPn\ni=1 L(θ, xi).\nLet bθS = arg minθ∈Θ RS(θ) be the optimizer parameters obtained via meta-learning on the training\nset S, and let θ∗= arg minθ∈Θ R(θ) be the globally optimal optimizer under distribution D. We can\nnow state the following theorem:\nTheorem 1. Let S1 and S2 be two datasets sampled independently from distribution D, with sizes n\nand m, respectively. Then, with probability at least 1 −δ, the optimizer bθS1 trained on S1 satisfies:\nRS2(bθ) ≤R(θ∗) +\nq\n2 log(6/δ)\nn\n+\nq\nlog(6/δ)\n2m\n.\n(3)\nThe proof of Theorem 1 is based on Hoeffding’s inequality, and the full derivation is provided in\nAppendix B. The theorem highlights the necessity of performing meta-optimization.\nIn contrast, an optimizer θ0 that hasn’t been optimized on the training dataset has no theoretical\nguarantee. We can still guarantee that RS2(θ0) is similar to R(θ0), but the guarantee says nothing\nabout how large R(θ0) is. If the optimizer is bad on average for this task, it will still be bad on S2\nwith high probability. This provides a theoretical foundation for the design of metaTextGrad.\n3.2\nMotivating Example\nExisting optimizers such as TextGrad and DSPy are manually designed by humans with the goal of\nperforming well across a broad distribution of tasks, and they indeed demonstrate strong average\nperformance.\n4\n"
    },
    {
      "page_number": 5,
      "text": "Initial Optimizers\nYou are part of \nan optimization \nsystem that \nimproves text...                     \nMeta Optimizer\nInner Optimization Loop\nPropose \nnew \noptimizer\nUpdate \nbest \noptimizer\nProposed \noptimizer\nOptimized Optimizer\nInitial Program\nOptimized Program\nProposer\nUpdater\nYou are part of \nan optimization \nsystem whose \ngoal is to ensure \nthat the pipeline \ngenerates correct \n...\n1. Structure\n2.TGD\n3. ...\nFigure 2: Illustration of metaTextGrad. metaTextGrad combines a meta prompt optimizer and a\nmeta structure optimizer. Given a set of optimizers, metaTextGrad performs optimization in two\nsteps. First, it individually refines each optimizer by optimizing its prompts to better align with the\ntask. Then, it combines the different prompt-optimized optimizers to construct the final optimizer.\nFor example, the prompt used by the TextGrad TGD optimizer is as follows:\nTextGrad TGD optimizer Prompt\nYou are part of an optimization system that improves text (i.e., variable). You will be asked to\ncreatively and critically improve prompts, solutions to problems, code, or any other text-based\nvariable.\nAs can be seen, this prompt is indeed highly general, with phrasing such as ‘improve prompts,\nsolutions to problems, code, or any other text-based variable’.\nHowever, in some cases, such general-purpose prompt fails to effectively optimize model performance.\nWhile the optimizers in both TextGrad and DSPy can obtain learning signals from task-specific\nevaluators, these signals tend to be noisy and sparse.\nFirst, when feedback is provided solely in the form of scalar scores rather than textual guidance, it\nbecomes sparse, making optimization substantially more challenging. Second, although optimizers\nsuch as TextGrad’s TGD can accept textual gradient feedback, such feedback is often highly noisy,\nmaking it difficult for the optimizer to learn effectively. For example, the evaluator feedback received\nby the TGD optimizer may look like the following:\nFeedback Received by the TGD optimizer\nTo improve the prompt for the executer and enhance the objective function, consider the\nfollowing feedback: 1. **Explicit Criteria Definition**: The prompt should explicitly instruct\nthe executer to define the criteria for optical activity at the beginning of the response. This\ncan prevent ambiguity and ensure that the executer uses the correct scientific principles. For\nexample, the prompt could include a directive to \"List the criteria for optical activity before\nanalyzing each compound.\" 2. **Data Verification Directive**: Incorporate a step in the\nprompt that requires the executer to verify the input data against reliable sources. This could\nbe phrased as \"Cross-check the properties of each compound with a trusted chemical database\nbefore proceeding with the analysis.\" 3. **Structured Logical Reasoning**: Encourage a\nstructured approach to reasoning by breaking down the analysis into distinct steps. The\nprompt could suggest a format like \"For each compound, first identify chiral centers, then\nassess symmetry, and finally determine optical activity\".\n5\n"
    },
    {
      "page_number": 6,
      "text": "Feedback Received by the TGD optimizer (continued)\n4. **Cross-Referencing Encouragement**: ... By incorporating these elements into the\nprompt, the executer can be guided to produce more accurate and reliable responses, thereby\nimproving alignment with the ground truth answer and enhancing the objective function.\nHere, the evaluator feedback includes suggestions such as ‘List the criteria for optical activity\nbefore analyzing each compound’, which are specific to a single problem instance. Such feedback is\nclearly noisy. Since a generic optimizer relies solely on feedback to guide its updates, it is likely to\nincorporate such suggestions into the optimized LLM program prompt, which can be detrimental to\nthe overall performance of the program.\nYet, in reality, we can choose to let the LLM optimizer adapt to a specific task distribution in order\nto achieve better performance. This is because if the distribution of programs generated by the\ntask-specific optimizer is more aligned with the task requirements, the difficulty of finding the optimal\nprogram will be significantly reduced, and the impact of noise in the evaluator’s signal will be greatly\nmitigated, even when the feedback is noisy.\nWe illustrate this using the BBH Dyck Languages task as an example, showing that aligning the LLM\noptimizer to a specific task distribution can lead to improved performance. For instance, consider the\nfollowing task-specific optimizer prompt:\nA Task-specific Optimizer Prompt\nYou are part of an optimization system specialized in improving prompts for bracket matching\nand sequence completion tasks. Your role is to enhance prompts that help solve Dyck\nlanguage problems, which involve proper nesting and closure of different types of brackets (,\n<>, ()). When improving prompts, focus on these critical aspects: (1) maintaining accurate\nbracket pair matching, (2) preserving the LIFO (Last In First Out) order of nested structures,\n(3) handling multiple bracket types simultaneously, and (4) ensuring complete closure of all\nopen brackets. You should critically analyze how the prompts can better guide the model to\ntrack open brackets, maintain proper nesting order, and systematically complete sequences.\nConsider incorporating pattern recognition strategies and explicit validation rules in the\nimproved prompts. Your improvements should lead to more reliable and accurate bracket\nsequence completions.\nIt can be observed that the task-specific optimizer leads to a shift in the distribution of LLM programs\nit tends to optimize, making it more likely to generate content that aligns with key task requirements,\nsuch as producing programs that satisfy the requirement of preserving the LIFO (Last In First Out)\norder, etc. As a result, even if the evaluator feedback is somewhat noisy or sparse, the LLM program\noptimized by the optimizer can still perform well, and is more likely to generate critical statements\nsuch as: Explicitly push each opening symbol onto the stack and pop it when a corresponding closing\nsymbol is encountered. After processing each symbol, describe the current state of the stack, focusing\non unmatched opening symbols. In contrast, if a generic optimizer is used, it becomes difficult to\ngenerate effective prompts under noisy or sparse feedback conditions.\nThis demonstrates that adapting to a new task distribution is meaningful. To avoid adapting to\neach new task distribution by hand, we meta-learn how to adapt, which is the core motivation of\nmetaTextGrad.\n3.3\nPipeline\n3.3.1\nMeta Prompt Optimizer\nLLM optimizers are usually designed by humans with manual design choices. As a result, the\noptimizers themselves are not optimized or aligned with a given task. We aim to optimize the prompts\nof LLM optimizers to further enhance their effectiveness and align them with tasks.\nBelow is a brief introduction to the implementation method of the meta prompt optimizer. The\ndetailed pseudocode can be found in Appendix A.1. To Initialize, the meta prompt optimizer runs\na round of optimization on the validation dataset to evaluate and record the initial performance of\n6\n"
    },
    {
      "page_number": 7,
      "text": "the optimizer. To Propose, the meta prompt optimizer randomly samples data examples from the\ntraining dataset and analyzes the general characteristics of the task type. Based on the current best\noptimizer prompt, it proposes an improved prompt that is more aligned with the task. To Update,\nthe optimized optimizer undergoes an inner loop optimization test on the validation dataset. If\nthe test results outperform those of the previously best optimizer, the optimizer is updated. To\nExtractOptimizedOptimizer, the meta prompt optimizer returns the best optimizer learned so far.\nAmong these steps, Propose is the core of the meta prompt optimizer and the only stage where LLM\ncalls are invoked. For detailed prompts, refer to Appendix D.1.\n3.3.2\nMeta Structure Optimizer\nA variety of LLM optimizers have been proposed to optimize program structures, prompts, and other\ncomponents. The meta structure optimizer is designed to automatically optimize the combination and\nordering of different optimizers based on the characteristics of the task.\nBelow, we briefly introduce the working principles of the meta structure optimizer. The detailed\npseudocode implementation can be found in Appendix A.2. To Initialize, the meta structure optimizer\nruns one round of optimization for each input optimizer on the validation dataset, selects the highest\nscore and the corresponding optimizer as the initial best value. To Propose, we provide the meta\nstructure optimizer with a set of reference optimizers. If previously optimized, better-performing\noptimizers exist, they are also included. Based on this, the meta structure optimizer integrates and\nproposes an improved optimizer. To Update, the meta structure optimizer evaluates whether the\nproposed optimizer shows improved performance on the validation dataset and updates the current\nbest optimizer and score. To ExtractOptimizedOptimizer, the meta structure optimizer returns\nthe best optimizer learned so far. The Propose stage is the only stage with LLM calls. For detailed\nprompts, please refer to Appendix D.2.\n3.3.3\nmetaTextGrad\nIn the previous two sections, we introduced the meta prompt optimizer and the meta structure\noptimizer. metaTextGrad is composed of these two components. Specifically, after receiving a set of\ninput optimizers, metaTextGrad first performs prompt optimization for each optimizer individually\nand then explores the combination of different optimizers to form a better composite optimizer.\n4\nExperiment\n4.1\nExperimental Setup\nIn this section, we use the existing optimizers from DSPy and TextGrad as baseline methods.\nWe evaluate our approach and the baselines on multiple benchmarks, including BBH [13, 14],\nMMLU [15], and GPQA [16]. To ensure reproducibility, we provide the prompts and structures of\nthe learned programs in Appendix E.\nBaselines. We used zero-shot CoT, few-shot CoT [17], self-consistency [18], best-of-N [19, 20],\nMIPROv2 [7], TextGrad TGD [1] and ADAS-TG [10] as baselines. Zero-shot CoT relies on direct\nchain-of-thought reasoning, whereas MIPRO and TextGrad’s TGD optimizer refine the program’s\nprompt. ADAS is an algorithm for automatically searching and optimizing the program’s structure.\nWe implemented this algorithm within TextGrad as a baseline, referred to as ADAS-TG.\nBenchmarks. We evaluate our method on four widely used, challenging, and diverse benchmarks:\nBBH Word Sorting, BBH Dyck Languages [13, 14], MMLU Abstract Algebra [15], and GPQA\nDiamond [16]. For detailed settings, please refer to Appendix C. Due to the non-determinism of\nLLM APIs [21], the test accuracy for each benchmark is averaged over five random seeds.\nIn our experiments, we consider three different levels of LLM calls: (1) LLM calls within the program\nitself, (2) LLM calls made by the optimizer while refining the program, and (3) LLM calls made by\nthe meta-optimizer when optimizing the optimizer. As shown in Section 4.3, the frequency of these\ncalls decreases significantly across these levels. This hierarchical structure allows for cost-effective\nresource allocation: the program should use a relatively economical model, the optimizer can leverage\na more capable model, and the meta-optimizer should employ the best available model. Consequently,\n7\n"
    },
    {
      "page_number": 8,
      "text": "Method\nWord Sorting Dyck Languages GPQA Diamond Abstract Algebra\nAverage\nVal\nTest\nVal\nTest\nVal\nTest\nVal\nTest\nVal\nTest\nVanilla prompting methods\nZero-shot CoT\n0.46\n0.55\n0.06\n0.05\n0.32\n0.34\n0.74\n0.70\n0.40 0.41\n8-shot CoT\n0.50\n0.52\n0.14\n0.19\n0.32\n0.35\n0.65\n0.71\n0.40 0.44\nSelf-consistency (8)\n0.47\n0.52\n0.10\n0.12\n0.40\n0.42\n0.76\n0.70\n0.43 0.44\nBest of N (8)\n0.48\n0.52\n0.14\n0.17\n0.37\n0.40\n0.77\n0.74\n0.44 0.46\nTextGrad optimizers\nTGD Optimizer\n0.54\n0.55\n0.10\n0.10\n0.34\n0.35\n0.76\n0.71\n0.44 0.43\nADAS-TG\n0.58\n0.58\n0.21\n0.16\n0.36\n0.37\n0.75\n0.70\n0.48 0.45\nDSPy optimizers\nZero-shot MIPROv2 0.57\n0.55\n0.19\n0.16\n0.43\n0.38\n0.76\n0.77\n0.49 0.47\n8-shot MIPROv2\n0.52\n0.57\n0.33\n0.26\n0.37\n0.34\n0.74\n0.65\n0.49 0.46\nMeta-optimized optimizers\nmetaTextGrad\n0.60\n0.65\n0.42\n0.37\n0.45\n0.40\n0.78\n0.71\n0.56 0.53\nTable 1: Accuracy (%) of GPT-4o-mini on benchmarks. Bold indicates the best result, and underlined\ntext represents the second-best.\nin our experiments, we use GPT-4o-mini for LLM calls within the program, GPT-4o for the MIPROv2\nand TGD optimizers, and the o1 model for the structure optimizer and the meta-optimizers.\n4.2\nMain Results\nAs shown in Table 1, we achieve up to an 11% absolute performance improvement across these\ndatasets, with an average performance significantly surpassing existing optimizers. Our method\noutperforms both TGD and ADAS-TG, which serve as the base candidates for metaTextGrad,\nacross all benchmarks and achieves the best performance on most of them. It’s worth noting that the\nprograms optimized by the meta-optimized optimizers also exhibit interesting properties.\n(1) The optimized optimizer is more aligned with specific tasks. For example, in the BBH Dyck\nLanguages task, the generated program includes components such as n type analyzer, and a stack\nvalidator, which closely match the nature of the task. In contrast, an unaligned optimizer tends to\npropose more generic and broadly applicable structures.\n(2) The optimized optimizer is more effective in handling finer details. For instance, in multi-step\nLLM calls, passing both the overall problem and subproblems to each LLM call helps maintain\na global understanding throughout the process. This behavior is more frequently observed in the\noptimized optimizer, whereas the initial optimizer tends to pass only the subproblems to each subpart.\n(3) The meta-optimized optimizer generally improves efficiency. Although we allocate six opti-\nmization steps per training epoch, we observe that meta-optimized optimizers, due to their stronger\ntask alignment, often achieve significant improvements within the first 1-2 steps. In contrast, other\noptimizers show more gradual improvements.\nPlease refer to Appendix E for the optimized programs and Appendix F for the optimized optimizers.\n4.3\nCost analysis\nIn this section, we examine the trade-off between effectiveness and computational cost.\nFirst, we analyze the token usage at different levels within a single epoch of meta optimization.\nThis analysis supports our design choice of using models with different capabilities at different\nlevels. As shown in Table 2, the token usage per optimization epoch on the MMLU Abstract Algebra\ndataset reveals that higher-level components require significantly fewer tokens than lower-level ones.\nThis justifies our hierarchical design. For comparison, a single round of zero-shot CoT requires\napproximately 140k tokens, indicating that the overall token consumption of our optimization pipeline\nremains within a reasonable and practical range.\n8\n"
    },
    {
      "page_number": 9,
      "text": "Level\nTokens\nProgram level\n∼400k\nOptimizer level\n∼100k\nMeta-optimizer level\n∼2.5k\nTable 2: Token analysis on Abstract Algebra.\nModel\nPerformance\nCost\n0-shot CoT (4o-mini)\n0.05\n0.14$\nOurs (4o-mini)\n0.37\n0.44$\n0-shot CoT (4o)\n0.18\n0.52$\nTable 3: Cost analysis on Dyck Languages.\nIn addition, we evaluate the cost and performance of the zero-shot CoT approach using both GPT-4o-\nmini and GPT-4o on the BBH Dyck Languages dataset, and compare them to our optimized approach\napplied to GPT-4o-mini. As shown in Table 3, our method achieves the best performance on BBH\nDyck Languages while incurring a lower cost than GPT-4o. This demonstrates that with effective\nprompt and structure optimization, a smaller model can outperform the zero-shot performance of a\nlarger model. These findings highlight the practical applicability and scalability of our approach.\n4.4\nTransferability of the optimized optimizer across models and datasets\nIn this section, we evaluate the transferability of our optimized optimizer across different language\nmodels and datasets. As shown in Table 4, our method trained on GPT-4o-mini achieves superior\nperformance compared to unoptimized baselines when evaluated on Claude 3 Haiku. Furthermore, as\nillustrated in Table 5, our optimizer trained on the GPQA diamond dataset also transfers effectively to\nthe Abstract Algebra dataset. These results demonstrate that our meta-optimized optimizer exhibits\nstrong transferability across models and datasets.\nMethod (Claude 3 Haiku)\nDyck Languages\nVal\nTest\nZero-shot CoT\n0.07\n0.10\nTextGrad optimizers\nTGD Optimizer\n0.10\n0.04\nADAS-TG\n0.35\n0.34\nOptimizers optimized on GPT-4o-mini\nmetaTextGrad\n0.32\n0.35\nTable 4: Transferability of the optimized opti-\nmizer across models.\nMethod\nAbstract Algebra\nVal\nTest\nZero-shot CoT\n0.74\n0.70\nTextGrad optimizers\nTGD Optimizer\n0.76\n0.71\nADAS-TG\n0.75\n0.70\nOptimizers optimized on GPQA diamond\nmetaTextGrad\n0.78\n0.77\nTable 5: Transferability of the optimized opti-\nmizer across datasets.\n4.5\nAnalysis of the effectiveness of each meta optimizer\nIn this section, we analyze the contributions of different components of the proposed meta optimizer.\nAs shown in Table 6, we find that all meta optimizers improve performance on the BBH Dyck\nLanguages benchmark. Optimizers optimized using either method outperform the original optimizer.\nAmong them, the meta prompt optimizer achieves the best improvement when applied to ADAS-TG.\nSplit\n0-shot CoT TGD ADAS-TG TGD (O) ADAS-TG (O) Struct (O) metaTextGrad\nVal\n0.06\n0.10\n0.21\n0.21\n0.42\n0.24\n0.42\nTest\n0.05\n0.10\n0.16\n0.24\n0.37\n0.16\n0.37\nTable 6: Analysis of the effectiveness of each meta optimizer on Dyck Languages. TGD (O), ADAS-\nTG (O), and Struct (O) respectively denote the TGD and ADAS-TG optimizers enhanced by the meta\nprompt optimizer, and the optimizers enhanced by the meta structure optimizer.\nHere, metaTextGrad produces the same results as the optimized ADAS-TG because the meta\noptimizer did not find a better option during the meta structure optimization. So, the optimized\nADAS-TG was selected as the best result. Notably, the best meta optimizer varies across benchmarks\ndue to task-specific differences. Despite this, all meta optimizers can effectively enhance the\nperformance of a given optimizer.\n9\n"
    },
    {
      "page_number": 10,
      "text": "5\nRelated Work\n5.1\nPrompt optimization\nPrompt optimization has proven crucial for improving LLM performance. Initial strategies, such as\nfew-shot learning and in-context learning, demonstrated careful prompt design could significantly\nboost LLM performance [22]. Techniques like chain-of-thought reasoning [17] and ensemble\nmethods [23] also emerged as popular ways to structure prompts for effective problem-solving.\nHowever, hand-crafted approaches are limited in their utility. Efforts to automate prompt optimization\nhave led to the development of gradient-based approaches [24, 25]. These methods, however, require\naccess to model parameters, which limits their application to open-source models.\nTo address these constraints, alternative approaches have been proposed that leverage LLMs them-\nselves as prompt optimizers. APE [26] was among the first to introduce the concept of automatic\ninstruction generation and selection. [27] introduced the concept of meta prompt, demonstrating\nsystematically designing meta-prompts can improve prompt quality. DSPy [4] proposed a systematic\napproach for optimizing LLM programs, integrating structured optimization techniques.\nAnother line of research explores optimization methods based on textual feedback. ProTeGi [28] first\nintroduced the concept of textual gradients, highlighting that LLMs themselves can serve as a form\nof a loss function to guide the improvement of LLM programs. Building on this idea, TextGrad [1]\ndeveloped a structured textual gradient descent framework, demonstrating its applicability and\neffectiveness across multiple disciplines and domains. Concurrently, OPTO [29] proposed a related\napproach in which the optimizer receives an execution trace alongside feedback on the generated\noutput. Semantic gradient descent [30] refines textual gradient descent by enhancing feedback signals.\nHowever, the LLM optimizers proposed in these methods remain fixed during the optimization\nprocess, limiting their effectiveness and adaptability. Our approach addresses this limitation by\nleveraging a meta-optimizer to align LLM optimizers with the task.\n5.2\nClassical meta-learning\nGradient-based meta-learning algorithms such as Model-Agnostic Meta-Learning (MAML) [31],\nFirst-Order MAML, Reptile [32], and ANIL [33] cast learning to learn as finding a good initialization\nthat can be fine-tuned with only a handful of gradient steps. MAML jointly optimizes across tasks\nthrough a bi-level procedure, explicitly encouraging large improvements after one or two inner-loop\nupdates. Reptile shows that a simpler first-order update suffices, while ANIL’s ablation studies reveal\nthat the bulk of MAML’s gains stem from feature reuse rather than rapid weight adaptation, allowing\nthe inner loop to be removed for all but the task-specific head. Despite their successes, these methods\nutilize fixed optimizers. In contrast, we focus on optimizing LLM-based optimizers rather than\nclassical ones, and we meta-learn the optimizer so that the optimization strategy is aligned with each\nnew task, rather than merely learning a good initialization.\n6\nConclusion\nIn this paper, we propose metaTextGrad, a meta-optimization framework that enhances existing\nLLM optimizers by aligning them more effectively with tasks. Our method introduces two key\ncomponents: the meta prompt optimizer, which refines optimizer prompts for better task adaptation,\nand the meta structure optimizer, which determines the optimal combination of different optimizers.\nBy integrating these two components, metaTextGrad improves the efficiency of LLM optimizers,\nleading to better performance across a diverse range of benchmarks.\nLooking ahead, there are several promising directions for future research. First, even the meta\noptimizer we proposed can be optimized. In particular, our meta optimizer is designed by ourselves,\ninstead of learned by data, and there are techniques in the meta learning literature that could be\nadapted to allow this [34, 35]. Second, future work can explore different ways to parameterize\nthe optimizers. This study primarily focuses on refining optimizer prompts and their composition,\nbut meta-optimizers could also be leveraged to automatically enhance the optimization algorithms\nemployed by existing optimizers. We believe that optimizing LLM optimizers is a crucial step toward\nfurther improving the performance and task alignment capabilities of LLM-driven systems and has\nthe potential to provide valuable insights to the research community.\n10\n"
    },
    {
      "page_number": 11,
      "text": "References\n[1] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and\nJames Zou. Textgrad: Automatic\" differentiation\" via text. arXiv preprint arXiv:2406.07496,\n2024.\n[2] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in Neural Information Processing Systems, 36, 2024.\n[3] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\nChen. Large language models as optimizers. In The Twelfth International Conference on\nLearning Representations, 2024.\n[4] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam,\nSri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather\nMiller, Matei Zaharia, and Christopher Potts. DSPy: Compiling declarative language model\ncalls into state-of-the-art pipelines. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[5] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information Processing Systems, 36, 2024.\n[6] Steffi Chern, Ethan Chern, Graham Neubig, and Pengfei Liu. Can large language models be\ntrusted for evaluation? scalable meta-evaluation of llms as evaluators via agent debate. arXiv\npreprint arXiv:2401.16788, 2024.\n[7] Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei\nZaharia, and Omar Khattab.\nOptimizing instructions and demonstrations for multi-stage\nlanguage model programs, 2024.\n[8] Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories\nwith recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing. Association for Computational Linguistics, December\n2022.\n[9] Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid\nPalangi, Ece Kamar, and Besmira Nushi. Attention satisfies: A constraint-satisfaction lens\non factual errors of language models. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[10] Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint\narXiv:2408.08435, 2024.\n[11] Qian Li, San-Yang Liu, and Xin-She Yang. Influence of initialization on the performance of\nmetaheuristic optimizers. Applied Soft Computing, 91:106193, 2020.\n[12] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of\ninitialization and momentum in deep learning. In International conference on machine learning,\npages 1139–1147. PMLR, 2013.\n[13] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging\nBIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, Toronto, Canada, July 2023. Association for\nComputational Linguistics.\n[14] Aarohi Srivastava, Abhinav Rastogi, and Abhishek Rao et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language models. Transactions on Machine\nLearning Research, 2023.\n11\n"
    },
    {
      "page_number": 12,
      "text": "[15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In International\nConference on Learning Representations, 2021.\n[16] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a\nbenchmark. In First Conference on Language Modeling, 2024.\n[17] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems, 35:24824–24837, 2022.\n[18] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023.\n[19] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In\nProceedings of the 34th International Conference on Neural Information Processing Systems,\nNIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc.\n[20] Steve Webb. The physical basis of imrt and inverse planning. The British journal of radiology,\n76(910):678–689, 2003.\n[21] Sherman Chann. Non-determinism in gpt-4 is caused by sparse moe. https://152334h.\ngithub.io/blog/non-determinism-in-gpt-4/, 8 2023.\n[22] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas\nKing, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models\noutcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452,\n2023.\n[23] Jinliang Lu, Ziliang Pang, Min Xiao, Yaochen Zhu, Rui Xia, and Jiajun Zhang. Merge, ensemble,\nand cooperate! a survey on collaborative strategies in the era of large language models, 2024.\n[24] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Auto-\nPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4222–4235, Online, November 2020. Association for Computational\nLinguistics.\n[25] Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang,\nLuo Si, and Huajun Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic\noptimization for relation extraction. In Proceedings of the ACM Web conference 2022, pages\n2778–2788, 2022.\n[26] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\nand Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh\nInternational Conference on Learning Representations, 2023.\n[27] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt\nengineer. arXiv preprint arXiv:2311.05661, 2023.\n[28] Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic\nprompt optimization with “gradient descent” and beam search. In Houda Bouamor, Juan\nPino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 7957–7968, Singapore, December 2023. Association for\nComputational Linguistics.\n[29] Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the next autodiff: Generative\noptimization with rich feedback, execution traces, and llms. In A. Globerson, L. Mackey,\nD. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural\nInformation Processing Systems, volume 37, pages 71596–71642. Curran Associates, Inc.,\n2024.\n12\n"
    },
    {
      "page_number": 13,
      "text": "[30] Wenyi Wang, Hisham A. Alyahya, Dylan R. Ashley, Oleg Serikov, Dmitrii Khizbullin, Francesco\nFaccio, and Jürgen Schmidhuber. How to correctly do semantic backpropagation on language-\nbased agentic systems, 2024.\n[31] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In Proceedings of the 34th International Conference on Machine\nLearning - Volume 70, ICML’17, page 1126–1135. JMLR.org, 2017.\n[32] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms,\n2018.\n[33] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature\nreuse? towards understanding the effectiveness of maml. In International Conference on\nLearning Representations, 2020.\n[34] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In Proceedings of the 34th International Conference on Machine\nLearning, 2017.\n[35] Jürgen Schmidhuber. Evolutionary Principles in Self-Referential Learning. PhD thesis, Techni-\ncal University of Munich, 1987.\n13\n"
    },
    {
      "page_number": 14,
      "text": "A\nImplementation Details\nIn this section, we provide the implementation of the meta optimizers. The LLMCall component\ngenerates responses based on the meta optimizer’s prompt. The detailed prompts will be provided in\nthe next section of the appendix.\nA.1\nMeta Prompt Optimizer\nAlgorithm 3 Meta Prompt Optimizer\n1: function c\nM.Initialize(D, M)\n2:\nstore(D)\n3:\nM ∗←M\n4:\nσ∗←0\n5:\nreturn\n6: end function\n7: function c\nM.Propose()\n8:\nquestion, answer ∼D\n9:\nproposedOptimizer ←LLMCall(prompt, M ∗, question, answer)\n10:\nreturn proposedOptimizer\n11: end function\n12: function c\nM.Update(M, σ)\n13:\nif σ > σ∗then\n14:\nM ∗←M\n15:\nσ∗←σ\n16:\nend if\n17:\nreturn\n18: end function\n19: function c\nM.ExtractOptimizedOptimizer()\n20:\nreturn M ∗\n21: end function\nA.2\nMeta Structure Optimizer\nAlgorithm 4 Meta Structure Optimizer\n1: function c\nM.Initialize(D, {M (i)}r\ni=1)\n2:\nstore(D, {M (i)}r\ni=1)\n3:\nM ∗←None\n4:\nσ∗←0\n5:\nreturn\n6: end function\n7: function c\nM.Propose()\n8:\nproposedOptimizer ←LLMCall(prompt, M ∗, {M (i)}r\ni=1)\n9:\nreturn proposedOptimizer\n10: end function\n11: function c\nM.Update(M, σ)\n12:\nif σ > σ∗then\n13:\nM ∗←M\n14:\nσ∗←σ\n15:\nend if\n16:\nreturn\n17: end function\n18: function c\nM.ExtractOptimizedOptimizer()\n19:\nreturn M ∗\n20: end function\nA.3\nMeta Optimizer\nFor the meta optimizer, the execution process consists of two steps: (1) Each optimizer is individually\noptimized using the meta prompt optimizer. (2) The optimized optimizers are further refined using\nthe meta structure optimizer.\n14\n"
    },
    {
      "page_number": 15,
      "text": "B\nProof of Theorem 1\nTheorem (Restatement of Theorem 1). Let S1 and S2 be two datasets sampled independently from\ndistribution D, with sizes n and m, respectively. Then, with probability at least 1 −δ, the optimizer\nbθS1 trained on S1 satisfies:\nRS2(bθ) ≤R(θ∗) +\nq\n2 log(6/δ)\nn\n+\nq\nlog(6/δ)\n2m\n.\nProof. According to Hoeffding’s inequality, for any 0 < δ < 1:\nPr\nS1∼Dn\nh\n|R(θ) −RS1(θ)| > εn(δ)\ni\n≤δ,\n(4)\nwhere:\nεn(δ) =\nq\nlog(2/δ)\n2n\n(5)\nUsing inequality 4, we can bound the population risk of the learned optimizer. With probability at\nleast 1 −2δ\n3 :\nR(bθ) ≤RS1(bθ) + εn(δ\n3) ≤RS1(θ∗) + εn(δ\n3) ≤R(θ∗) + 2 εn(δ\n3),\n(6)\nwhere θ∗= arg minθ R(θ).\nWith a union bound, with probability at least 1 −δ:\nRS2(bθ) ≤R(bθ) + εm(δ\n3) ≤R(θ∗) + 2 εn(δ\n3) + εm(δ\n3)\n(7)\nIn conclusion, with probability at least 1 −δ over the draws of S1 and S2:\nRS2(bθ) ≤R(θ∗) +\nq\n2 log(6/δ)\nn\n+\nq\nlog(6/δ)\n2m\n(8)\nC\nDetails on Experimental Benchmark Setup\nBBH.\nThe BBH task [13, 14] is a challenging benchmark that requires precise reasoning across\nvarious tasks. We select two subsets from the BBH benchmark: BBH Word Sorting and BBH Dyck\nLanguages. The BBH Word Sorting task requires the language model to sort a given set of words in\norder, while the BBH Dyck Languages task involves providing a string composed of various types of\nbrackets and asking the model to determine the characters needed to complete the bracket pairing.\nWe use the same train/validation/test splits as in TextGrad [1] (i.e., 50, 100, and 100 instances for\ntraining, validation, and testing) and follow the TextGrad approach by using GPT-4o to evaluate\nwhether the model’s output is correct, based on the predicted and ground truth answers.\nMMLU.\nThe MMLU benchmark [15] evaluates a model’s ability to answer questions across a\nwide range of scientific disciplines. We selected the MMLU Abstract Algebra dataset and created\ntraining, validation, and test sets consisting of 10, 50, and 40 questions, respectively. We assess model\naccuracy using exact matching, determining correctness by applying a regular expression to check\nwhether the model’s response contains \"Answer: [A-D]\" and matches the ground truth. However, we\nobserved that the DSPy baselines struggle to adhere to this output format. To accommodate this, we\nuse GPT-4o to evaluate the correctness of responses generated by DSPy-based methods.\nGPQA Diamond.\nThe GPQA Diamond benchmark [16] evaluates the model’s ability to solve\ngraduate-level, Google-proof questions. The benchmark consists of 198 questions, which we split\ninto training, validation, and test datasets containing 30, 100, and 68 questions. Similar to MMLU,\nwe use exact matching to determine response accuracy by applying a regular expression to check\nwhether the model’s answer follows the format \"Answer: [A-D]\" and matches the correct answer.\nFor the DSPy baselines, we use GPT-4o to evaluate the correctness of DSPy-generated responses.\n15\n"
    },
    {
      "page_number": 16,
      "text": "D\nPrompts of Meta Optimizers\nIn this section, we provide the prompts used in the propose stage of the meta prompt optimizer and\nthe meta structure optimizer. The propose stage is the only part of the meta optimizer that involves an\nLLM call; all other functions consist of direct numerical updates or result retrieval, as explained in\npseudocode in Appendix A.\nD.1\nMeta Prompt Optimizer\nBelow, we present the prompt used for the meta prompt optimizer.\nMeta Prompt Optimizer\n# Task Requirement\nA TextGrad optimizer optimizes a TextGrad pipeline so that the pipeline can generate better\noutputs based on inputs.\nA TextGrad pipeline consists of several agents, each of which has a specific role in the\noptimization process, which is defined by different prompts.\nYou will be given a general task description of an optimizer and the specific task the pipeline\naims to solve.\nYour task is to propose an improved optimizer task description so that the optimizer can better\noptimize the pipeline for the given task.\n# Optimizer Code\nHere is the source code of the optimizer: (just for reference)\n{optimizer_source_code}\n# The task of the optimizer\nSpecifically,\nthe\npipeline\naims\nto\nsolve\nthis\nkind\nof\nquestion:\n{exam-\nple_set.get_question_type()}.\nAn example of the task is provided here:\nQuestion: {example_question}\nAnswer: {example_answer}\nThe LLM optimizer wants to improve a LLM pipeline to solve such kind of problems.\n# Current Task Description\nHere is the current task description of the optimizer, which you can improve: {opti-\nmizer_prompt}\n# Your task\nYou should identify what the optimizer should pay attention to in order to improve the\n{optimizer_type} of the pipeline for solving the given task.\nConduct a detailed analysis of the given example, and respond in the following format:\n“‘json\n\"improved_task_description\": \"...\", # Your improved task description for the optimizer\n“‘\nD.2\nMeta Structure Optimizer\nMeta Structure Optimizer\n# Task Requirement\nA TextGrad optimizer optimizes a TextGrad pipeline so that the pipeline can generate better\noutputs based on inputs.\nThe structure and prompts of current optimizers are fixed, and you will be given the source\ncode of some optimizers.\nYour task is to propose an improved optimizer code to better optimize the pipeline.\nThis can be achieved by integrating the reference codes of the given different optimizers and\nmerging them into a new optimizer.\n16\n"
    },
    {
      "page_number": 17,
      "text": "Meta Structure Optimizer (Continued)\n# Task Details\nYou will be provided with implementations of several optimizers, each annotated with their\nrespective purposes.\nCarefully read through their functions and implementation details. Your task is to integrate\nthe different implementation approaches to provide a more optimized solution.\nIn this context, the step() function refers to the process of performing a single optimization\nstep.\nYou may notice that different optimizers are suitable for different purposes and should be\napplied in a specific order.\nFor example, if the pipeline aims to optimize the structure, this optimization will overwrite\nthe prompts of each previously optimized component, so the structure optimization should be\nexecuted first.\nYour implementation of the step() function should take the order into account, and each call\nto the step() function should only optimize a single aspect of the pipeline because the gradient\ncontext is not reusable.\nTherefore, you need to implement a logic so that the optimizer can use different optimizing\nstrategies in a sequential manner, with each strategy being applied self.epoch times before\nmoving to the next one.\n{optimizer_prompt}\n# Code Reminder\nYou should include the following necessary imports at the beginning of the code:\nimport inspect\nimport copy\nfrom typing import Type, List, Union\nfrom collections import defaultdict\nfrom textgrad.variable import Variable\nfrom textgrad import logger\nfrom textgrad.engine import EngineLM\nfrom textgrad.optimizer.optimizer import Optimizer, get_gradient_and_context_text\nfrom textgrad.model import Pipeline\nfrom textgrad.autograd import FormattedLLMCall\nfrom textgrad.config import validate_engine_or_get_default\nfrom\ntextgrad.optimizer.optimizer_prompts\nimport\nconstruct_tgd_prompt,\nOPTIMIZER_SYSTEM_PROMPT,\nPIPELINE_SYSTEM_PREFIX,\nPIPELINE_SYSTEM_SUFFIX\n# Note\nYou are required to build on one of the existing optimizers and improve it by integrating\nfeatures from the others, instead of starting from scratch.\nYou should only include the ImprovedOptimizer (inherited exactly from Optimizer) class in\nthe code, and no other classes or functions.\nYou should implement all the necessary functions and attributes in the ImprovedOptimizer\nclass to ensure that the code can be executed without errors.\nYou should not modify input signatures of the __init__ and step functions when implementing\nthe ImprovedOptimizer class.\nPlease output the complete improved Python code, and make sure to call the improved class\n’ImprovedOptimizer’; remember to also include all necessary attributes of the class so that\nthe code can be executed without errors.\nDo not include any additional comments or unnecessary text in the output.\n17\n"
    },
    {
      "page_number": 18,
      "text": "E\nOptimized Programs\nTo facilitate the reproducibility of our work, we provide the program generated by the optimizer\noptimized with metaTextGrad.\nE.1\nBBH Word Sorting Benchmark\nBBH Word Sorting\nclass SimplePipeline(Pipeline):\ntask_description = (\n\"You will answer a reasoning question. Think step by step. \"\n\"The last line of your response should be of the following\nformat: Answer: \\$VALUE' \"\n\"where VALUE is the answer to the question.\"\n)\ndef __init__(self, engine: Union[EngineLM, str] = None, path=None):\nsuper().__init__(engine=engine, path=path)\nself.planner_prompt = Variable(\n\"Create a step by step plan for the question. Provide each\nstep on a new line.\",\nrequires_grad=True,\nrole_description=\"planner prompt\"\n)\nself.planner = tg.BlackboxLLM(self.engine, self.planner_prompt)\nself.subsolver_prompt = Variable(\n\"Solve this sub-step in detail\n, without giving the final answer.\",\nrequires_grad=True,\nrole_description=\"sub-step solver prompt\"\n)\nself.subsolver = tg.BlackboxLLM(self.engine,\nself.subsolver_prompt)\nself.final_prompt = Variable(\n\"\"\"Combine all reasoning into a coherent final response,\nending with the format: Answer: $VALUE\"\"\" ,\nrequires_grad=True,\nrole_description=\"final answer prompt\"\n)\nself.finalsolver = tg.BlackboxLLM(self.engine,\nself.final_prompt)\ndef _step_split_rule(self, text: str):\nreturn re.split(r\"\\n+\", text.strip())\ndef forward(self, question: Variable) -> Variable:\nplan = self.planner(question)\nsteps = Split()(self._step_split_rule, plan)\nsub_solutions = []\nfor step in steps:\nsub_solutions.append(self.subsolver(step))\naggregated_reasoning = Aggregate()(sub_solutions)\nfinal_answer = self.finalsolver(Aggregate()([question,\naggregated_reasoning]))\nreturn final_answer\n18\n"
    },
    {
      "page_number": 19,
      "text": "E.2\nBBH Dyck Languages Benchmark\nBBH Dyck Languages\nclass SimplePipeline(Pipeline):\ntask_description = \"\"\"You will answer a reasoning question.\nThink step by step. The last line of your response\nshould be of the following format:\n'Answer: $VALUE' where VALUE is the answer to the question.\"\"\"\ndef __init__(self, engine: Union[EngineLM, str] = None, path=None):\nsuper().__init__(engine=engine, path=path)\nif not path:\nself.initial_planner = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Break down the Dyck sequence validation into\ndetailed steps. Format as numbered steps:\n1), 2), etc.\"\"\" ,\nrequires_grad=True,\nrole_description=\"creates detailed analysis plan\"\n)\n)\nself.type_analyzer = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Identify and categorize all bracket types.\nList each type and its corresponding closing\nbracket. Format: 'Types:\n[pairs]'\"\"\" ,\nrequires_grad=True,\nrole_description=\"analyzes bracket types and pairs\"\n)\n)\nself.stack_validator = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Simulate stack operations for bracket matching.\nShow stack state after each operation.\nEnd with 'Answer:\nValid/Invalid'\"\"\" ,\nrequires_grad=True,\nrole_description=\"performs stack-based validation\"\n)\n)\nself.nesting_analyzer = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Analyze nesting hierarchy. Check if inner\nbrackets close\nbefore outer brackets.\nEnd with 'Answer: Proper/Improper'\"\"\" ,\nrequires_grad=True,\nrole_description=\"validates nesting hierarchy\"\n)\n)\n19\n"
    },
    {
      "page_number": 20,
      "text": "BBH Dyck Languages (Continued)\nself.depth_checker = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Calculate maximum nesting depth and verify\nbalanced structure. End with 'Answer:\nDepth=X,Balanced=Yes/No'\"\"\" ,\nrequires_grad=True,\nrole_description=\"checks nesting depth and balance\"\n)\n)\nself.sequence_validator = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Validate sequence completeness and correctness.\nEnd with 'Answer: Complete/Incomplete'\"\"\" ,\nrequires_grad=True,\nrole_description=\"validates sequence completeness\"\n)\n)\nself.final_evaluator = tg.BlackboxLLM(\nself.engine,\nVariable(\n\"\"\"Synthesize all analysis results and determine\nif this is a valid Dyck sequence.\nEnd with 'Answer: Yes/No'\"\"\" ,\nrequires_grad=True,\nrole_description=\"makes final validity\ndetermination\"\n)\n)\ndef forward(self, question: Variable) -> Variable:\nplan = self.initial_planner(question)\nanalysis_steps = Split()(lambda x: re.split(r'\\d\\)', x), plan)\ntype_analysis = self.type_analyzer(question)\nstack_validation = self.stack_validator(\nAggregate()([question, type_analysis])\n)\nnesting_analysis = self.nesting_analyzer(\nAggregate()([question, stack_validation])\n)\ndepth_analysis = self.depth_checker(\nAggregate()([question, nesting_analysis])\n)\nsequence_validation = self.sequence_validator(\nAggregate()([question, depth_analysis])\n)\n20\n"
    },
    {
      "page_number": 21,
      "text": "BBH Dyck Languages (Continued)\nfinal_result = self.final_evaluator(\nAggregate()([\nquestion,\nstack_validation,\nnesting_analysis,\ndepth_analysis,\nsequence_validation\n])\n)\nreturn Extract()(r\"Answer: (.+)\", final_result)\nE.3\nGPQA Diamond Benchmark\nGPQA Diamond\nclass SimplePipeline(Pipeline):\ntask_description = \"\"\"You will answer a reasoning question.\nThink step by\nstep. The last line of your response should\nbe of the following format: 'Answer: $VALUE' where VALUE is\nthe answer to the question.\"\"\"\ndef __init__(self, engine: Union[EngineLM, str] = None, path=None):\nsuper().__init__(engine=engine, path=path)\nif not path:\nself.executer = textgrad.BlackboxLLM(\nself.engine,\ntextgrad.Variable(\n\"\"\"You will answer a multiple choice question.\nBegin by\nidentifying and listing all key\ndetails and constraints from the problem\nstatement. Use explicit reasoning steps to\noutline the solution, incorporating relevant\nscientific\nprinciples such as reaction\nmechanisms or stereochemistry. Verify your\ninitial conclusions by cross-checking each\nstep against the problem's requirements.\nConsider alternative answers and evaluate\nwhy they might be correct or incorrect.\nProvide a clear justification for your answer\nchoice, and rate your confidence in the final\nanswer on a scale from 1 to 10, explaining\nyour rationale. The last line of your response\nshould be of the following format:\n'Answer: $VALUE' where VALUE is one of ABCD.\"\"\" ,\nrequires_grad=True,\nrole_description=\"prompt for the executer,\nwhich aims to solve the task\"\n)\n)\ndef forward(self, question: Variable) -> Variable:\nreturn self.executer(question)\n21\n"
    },
    {
      "page_number": 22,
      "text": "E.4\nMMLU Abstract Algebra Benchmark\nMMLU Abstract Algebra\nclass SimplePipeline(Pipeline):\ntask_description = \"\"\"You will answer a reasoning question.\nThink step by step. The last line of your response should\nbe of the following format:\n'Answer: $VALUE' where VALUE is the answer to the question.\"\"\"\ndef __init__(self, engine: Union[EngineLM, str] = None, path=None):\nsuper().__init__(engine=engine, path=path)\nif not path:\nself.executer = textgrad.BlackboxLLM(\nself.engine,\ntextgrad.Variable(\n\"\"\"You will answer a multiple choice question.\nAnalyze each\nstatement separately and provide\nyour reasoning. Use clear, logical steps,\nverifying each sub-component of the problem\nsystematically. Present each statement distinctly\nusing bullet points or numbers, ensuring the final\nconclusion is\nclearly separated from the statement\nevaluations. Include any assumptions or context\nnecessary for each conclusion. After evaluating\neach statement, reexamine your conclusions\nto confirm their correctness. Introduce a summary\nverification step before concluding. The last line\nof your response should be of the following format:\n'Answer: $VALUE' where VALUE is one of ABCD.\"\"\" ,\nrequires_grad=True,\nrole_description=\"prompt for the executer,\nwhich aims to solve the task\"\n)\n)\ndef forward(self, question: Variable) -> Variable:\nreturn self.executer(question)\nF\nOptimized Optimizers\nF.1\nOptimized TGD Optimizer Demo\nThe TGD optimizer is generic, so its optimized program offers only general guidance like “ensuring\nthat every opening bracket has a corresponding closing bracket,” without task-specific strategies. In\ncontrast, the optimized TGD aligns closely with the task, emphasizing ideas like “preserving the\nLIFO (Last In First Out) order of nested structures.” Since the optimized TGD optimizer is better\naligned with the task, its optimized program also effectively implements the LIFO principle, making\nit more effective. This is why a task-specific prompt optimizer is necessary.\nTGD Optimizer prompt\nYou are part of an optimization system that improves text (i.e., variable). You will be asked to\ncreatively and critically improve prompts, solutions to problems, code, or any other text-based\nvariable.\n22\n"
    },
    {
      "page_number": 23,
      "text": "Optimized program prompt\nYou will answer a reasoning question. Think step by step, ensuring logical consistency\nand accuracy. Explicitly define the role and rules for each type of bracket in the sequence.\nVerify each step of your reasoning, ensuring that every opening bracket has a corresponding\nclosing bracket. Compare your predicted sequence with a known ground truth or expected\npattern, and explain any discrepancies. If errors are identified, re-evaluate and correct them.\nUse mental visualization techniques to aid in understanding sequences. Provide a clear\nand concise explanation. The last line of your response should be of the following format:\n’Answer: $VALUE’ where VALUE is the answer to the question.\nMeta-optimized TGD Optimizer prompt\nYou are part of an optimization system specialized in improving prompts for bracket matching\nand sequence completion tasks. Your role is to enhance prompts that help solve Dyck\nlanguage problems, which involve proper nesting and closure of different types of brackets\n({}, <>, ()). When improving prompts, focus on these critical aspects: (1) maintaining\naccurate bracket pair matching, (2) preserving the LIFO (Last In First Out) order of nested\nstructures, (3) handling multiple bracket types simultaneously, and (4) ensuring complete\nclosure of all open brackets. You should critically analyze how the prompts can better guide\nthe model to track open brackets, maintain proper nesting order, and systematically complete\nsequences. Consider incorporating pattern recognition strategies and explicit validation rules\nin the improved prompts. Your improvements should lead to more reliable and accurate\nbracket sequence completions.\nOptimized program prompt\nYou will answer a reasoning question. Focus on identifying and providing only the missing\nclosing symbols needed to complete the sequence. Use a stack-based approach to track\nopening and closing symbols, ensuring proper nesting and closure. Explicitly push each\nopening symbol onto the stack and pop it when a corresponding closing symbol is encountered.\nAfter processing each symbol, describe the current state of the stack, focusing on unmatched\nopening symbols. Ensure that the order of closing symbols matches the reverse order of\nunmatched opening symbols as they appear in the stack. Verify the sequence by checking\neach symbol’s nesting and closure order. Highlight critical decision points, such as when to\npop from the stack or add closing symbols. Use visual aids if necessary to represent the stack\nstate and sequence operations. After completing your analysis, perform a verification check\nby comparing the predicted closing symbols with the expected output. If discrepancies are\ndetected, revisit previous steps to self-correct. Conclude with a summary of the final state of\nthe stack and confirm that all brackets are matched. The last line of your response should be\nof the following format: ’Answer: $VALUE’ where VALUE is the answer to the question.\nF.2\nOptimized ADAS-TG Optimizer Demo\nThe original program contains generic components like a planner, reasoner, and synthesizer, lacking\ntask-specific focus. In contrast, the optimized version adds tailored modules such as a type analyzer,\nstack validator, and nesting analyzer, aligning closely with BBH Dyck Languages. This illustrates the\nnecessity of a task-specific prompt optimizer.\nADAS-TG Optimizer prompt\n(Some general instructions are omitted here.) Please produce only the code for the pipeline,\nwith a more systematic or hierarchical structure if possible.\n23\n"
    },
    {
      "page_number": 24,
      "text": "Meta-optimized ADAS-TG Optimizer prompt\n(Some general instructions are omitted here.) Please produce only the code for the pipeline,\nwith the following structural improvements: 1) Implement a stack-based mechanism for\ntracking open brackets, 2) Create separate components for sequence parsing, bracket matching,\nand completion generation, 3) Include validation checks for proper nesting and bracket type\nmatching, 4) Ensure systematic handling of different bracket types ([], {}, (), <>), 5) Maintain\na hierarchical structure that clearly separates the parsing logic from the completion generation.\nThe pipeline should efficiently handle nested sequences while preserving the LIFO (Last In,\nFirst Out) order of brackets.\nG\nPerformance When Using an Open-source Model as the Optimizer\nWe further evaluate the generality of our framework by adopting fully open-source models. Specifi-\ncally, we employ the non-thinking variant of Qwen3-8B as the program model and Qwen3-235B-\nA22B as both the optimizer and meta-optimizer. The experiments are conducted on the BBH Dyck\nLanguages benchmark. As shown in Table 7, metaTextGrad continues to deliver strong performance.\nH\nPerformance When Applying to a Challenging Benchmark\nTo further assess the adaptability of our approach, we evaluate metaTextGrad on the ARC-AGI\nbenchmark, a task family known for requiring abstract reasoning and compositional generalization.\nFollowing common ADAS practice, we sample ARC-AGI instances with grid sizes ≤5 × 5,\ncomprising 20 training, 30 validation, and 30 test examples. The evaluation reports one-shot success\nrates, using Claude 3 Haiku as the program model and Claude 3.5 Sonnet as both optimizer and\nmeta-optimizer. As presented in Table 7, the results demonstrate that metaTextGrad remains highly\ncompetitive in this more demanding setting.\nMethod\nDyck Languages (Qwen models)\nARC-AGI (Challenging Benchmark)\nVal\nTest\nVal\nTest\nVanilla prompting methods\nZero-shot CoT\n0.27\n0.27\n0.27\n0.23\n8-shot CoT\n0.37\n0.40\n0.03\n0.00\nSelf-consistency (8)\n0.31\n0.32\n0.30\n0.23\nBest of N (8)\n0.39\n0.41\n0.27\n0.20\nTextGrad optimizers\nTGD Optimizer\n0.69\n0.68\n0.33\n0.33\nADAS-TG\n0.32\n0.34\n0.28\n0.26\nDSPy optimizers\nZero-shot MIPROv2\n0.59\n0.50\n0.30\n0.23\n8-shot MIPROv2\n0.57\n0.51\n0.33\n0.03\nMeta-optimized optimizers\nmetaTextGrad\n0.82\n0.77\n0.37\n0.40\nTable 7: Validation and test accuracy when using open-source models (Qwen models, Dyck Lan-\nguages) and evaluating on a challenging benchmark (ARC-AGI). Bold entries denote the best-\nperforming method within each benchmark.\nI\nPotential Limitations, Societal Consequences, and Broader Impacts\nLimitations. While metaTextGrad demonstrates stable performance across various models and\nbenchmarks, we acknowledge the following limitations:\n24\n"
    },
    {
      "page_number": 25,
      "text": "(1) Although metaTextGrad improves performance on many benchmarks, it may not be effective in\nscenarios where the base model lacks sufficient task-relevant knowledge or reasoning capabilities.\nFor example, GPT-4o-mini struggles on math competition benchmarks such as AIME 2024, where\nmetaTextGrad alone cannot yield significant performance gains.\n(2) The meta-optimizer relies on strong instruction-following and problem analysis capabilities. As a\nresult, it currently requires advanced models such as o1 or Claude-3.5-Sonnet. Models like Gemini\n1.5 Pro do not yet perform adequately. However, given the rapid progress in model development, we\nexpect that more models will become suitable for the framework in the near future.\nSocietal Consequences. metaTextGrad can have meaningful societal consequences.\n• Acceleration of domain-specific AI applications. By making it easier to adapt LLMs to\nspecific tasks, metaTextGrad may accelerate the deployment of more reliable AI solutions\nin other domains.\n• Risk of automation bias or over-reliance. As optimizers become more autonomous, users\nmight rely on them without fully understanding their behavior or limitations.\n• Potential misuse for persuasive or manipulative systems. metaTextGrad could be\nexploited to generate more persuasive outputs.\nImpact Statement. This paper presents work whose goal is to advance the field of Machine Learning.\nThe paper is solely centered on the methodology itself. How it is applied and for what purpose is\nentirely up to the users. There are many potential societal consequences of our work, none which we\nfeel must be specifically highlighted here.\n25\n"
    },
    {
      "page_number": 26,
      "text": "NeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: We clearly state our claims in the abstract and the introduction section. They\naccurately reflect the methods in Section 3 and the experiments in Section 4.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We mentioned potential limitations in Appendix I.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\n26\n"
    },
    {
      "page_number": 27,
      "text": "Justification: We provide the full set of assumptions in Section 3.1 and a complete proof in\nAppendix B.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We clearly describe our methods in Section 3 and Appendix A. The experi-\nmental settings are provided in Section 4 and Appendix C.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\n27\n"
    },
    {
      "page_number": 28,
      "text": "Question: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: We provide open code access.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: The experimental settings are provided in Section 4 and Appendix C.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: Results shown in the experiment section are repeated for 5 times, using\ndifferent random seeds.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n28\n"
    },
    {
      "page_number": 29,
      "text": "• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: The API is the only compute resource we used. We report the details of API\nusage in Section 4.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The paper conforms with the NeurIPS code of ethics.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We discuss the broader impacts in Appendix I. The paper is solely centered on\nthe methodology itself. How it is applied and for what purpose is entirely up to the users.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n29\n"
    },
    {
      "page_number": 30,
      "text": "• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper poses no such risks.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: They are properly credited, mentioned and respected.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n30\n"
    },
    {
      "page_number": 31,
      "text": "• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: Yes, code is the only new asset. It is well documented and the documentation\nis provided alongside the code.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n31\n"
    },
    {
      "page_number": 32,
      "text": "• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [Yes]\nJustification: The LLM program, optimizer, and meta-optimizer are the core components of\nthe proposed method and are thoroughly discussed in the paper.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n32\n"
    }
  ]
}