{
  "filename": "118575_Inverse Methods for Missing Data Imputation.pdf",
  "total_pages": 26,
  "full_text": "Inverse Methods for Missing Data Imputation\nHao Wang1âˆ—Zhengnan Li1âˆ—\nZhichao Chen2\nXu Chen3â€ \nShuting He4\nGuangyi Liu5\nHaoxuan Li6â€ \nZhouchen Lin2,7,8,â€ \n1Xiaohongshu Inc.\n2State Key Lab of General AI, School of Intelligence Science and Technology, Peking University\n3Gaoling School of Artificial Intelligence, Renmin University of China\n4School of Computing and Artificial Intelligence, Shanghai University of Finance and Economics\n5Department of Control Science and Engineering, Zhejiang University\n6Center for Data Science, Peking University\n7Institute for Artificial Intelligence, Peking University\n8Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China\nAbstract\nIterative imputation is a prevalent method for completing missing data, which\ninvolves iteratively imputing each feature by treating it as a target variable and\npredicting its missing values using the remaining features. However, existing itera-\ntive imputation methods exhibit two critical defects: (1) model misspecification,\nwhere a uniform parametric form of model is applied across different features,\nconflicting with heterogeneous data generation processes; (2) underuse of oracle\nfeatures, where all features are treated as potentially missing, neglecting the valu-\nable information in fully observed features. In this work, we propose kernel point\nimputation (KPI), a bi-level optimization framework designed to address these\nissues. The inner-level optimization optimizes the model form for each feature\nin a reproducing kernel Hilbert space, mitigating model misspecification. The\nouter-level optimization leverages oracle features as supervision signals to refine\nimputations. Extensive experiments on real-world datasets demonstrate that KPI\nconsistently outperforms state-of-the-art imputation methods. Code is available at\nhttps://github.com/FMLYD/kpi.git.\n1\nIntroduction\nMissing data is a ubiquitous challenge in real-world data collection and analytics [24, 45]. For\nexample, in manufacturing, temperature sensors may fail due to overheating or electrical disruptions,\ncompromising data integrity and impeding analytical workflows [1]. Similarly, equipment-monitoring\nsystems can experience lost connectivity in electrical sensors, impeding fault detection and intro-\nducing security risks [36]. These issues highlight the importance of missing data imputation (MDI)\ntechniques, which aim to recover missing data using observed ones, thereby enhancing the integrity\nof collected datasets and the reliability of data-driven applications.\nExisting MDI methods can be broadly categorized as discriminative or generative [5]. On the one\nhand, discriminative methods, such as statistical imputation (e.g., mean and median imputation [44])\nand iterative imputation (which iteratively predicts missing values using univariate models [26, 29]),\nhave been well developed. On the other hand, generative methods have recently attracted attention\nfor their capacity to model complex data structures [1]. However, they often encounter training\nchallenges [18, 25] or rely on strong data assumptions [42, 27]. Empirically, generative methods may\nâˆ—This work was done in the internship at Xiaohongshu Inc. Both authors have equal contribution.\nâ€ Corresponding author.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nfrequently be outperformed by discriminative methods [45, 23]. Therefore, discriminative methods\nremain the preferred choice for MDI in practice [39].\nAmong discriminative methods, iterative imputation is widely adopted due to its straightforward\nimplementation and strong empirical performance. It specifies univariate models for each feature\nconditioned on the rest and iteratively imputes missing values until convergence [26]. However, this\napproach has two limitations. First, it assumes that all features contain missing values, neglecting the\nutility of fully observed features, known as oracle features, which can provide strong supervisory\nsignals for imputation. Second, it applies a fixed-form parametric model to all features, which risks\nmodel misspecification, as different features often exhibit heterogeneous dependencies that cannot be\nadequately captured by a fixed-form parametric model [5].\nTo counteract the two limitations, we reformulate iterative imputation as a bi-level optimization prob-\nlem. The inner-level optimization adaptively selects functional forms from reproducing kernel Hilbert\nspaces (RKHS) for each feature, reducing model misspecification. The outer-level optimization aligns\nthe imputed values with oracle features, leveraging them as direct supervision signals. Subsequently,\nwe propose kernel point imputation (KPI), which expresses the optimal model as a linear combination\nof kernel functions, enabling efficient solution via stochastic gradient descent. Furthermore, we\ndesign an adaptive kernel ensemble strategy to dynamically combine kernels, thereby enhancing\nmodel expressiveness and alleviating hyperparameter selection challenge amidst incomplete data.\nContributions. The key contributions of this study are summarized as follows:\nâ€¢ We introduce a bi-level optimization framework for MDI which optimizes model form for each\nfeature within a RKHS to address model misspecification and exploits oracle features as supervision\nto refine imputation results.\nâ€¢ We develop the KPI algorithm, which solves the bi-level optimization problem via stochastic\ngradient descent. Additionally, we develop a kernel ensemble method to counteract the difficulty of\nkernel parameter selection amidst missing data.\nâ€¢ We conduct extensive experiments to demonstrate the superiority of KPI over existing MDI methods\nmethods and to highlight the utility of oracle features in enhancing imputation accuracy.\n2\nPreliminaries\nAs a preliminary note, this study aims to impute missing values as an end goalâ€”specifically, to\nestimate their most probable values. We are not considering imputation as a means to obtain input\nfor some downstream tasks [5], such as training regression models for label prediction [16, 14] or\npseudo-labeling for unbiased learning [10]. Methods in these scenarios often require joint training to\noptimize specific objectives [16]. In this work, we concentrate on the MDI problem.\nSuppose X(id) âˆˆRNÃ—D is the ideally complete data matrix with N samples and D features. The\npresence of missing entries in X(id) is indicated by a binary matrix M âˆˆ{0, 1}NÃ—D, where each\nentry Mn,d is set to 1 if the corresponding entry X(id)\nn,d is missing, and 0 otherwise. Consequently,\nthe observed dataset X(obs) can be derived using the Hadamard product:\nX(obs) := X(id) âŠ™(1 âˆ’M) + nan âŠ™M.\n(1)\nThe goal of MDI is to recover the missing entries by constructing an imputation matrix X(imp) âˆˆ\nRNÃ—D that closely approximates X(id). Different imputation methods vary in how they generate\nX(imp) from X(obs) and M.\nOne prevalent approach is the iterative imputation method, which iteratively imputes each feature by\ntreating it as a target variable and predicting its missing values using the remaining features as inputs.\nSpecifically, in the training stage, let the d-th feature as the target feature, denoted as Y(obs)\nd\n= X(obs)\nÂ·,d\n,\nthe method fits an imputation model fÎ¸ with parameters Î¸ that learns the relationship between the\ntarget feature and the remaining features:\nmin\nÎ¸\n\r\r\rY(obs)\nd\nâˆ’fÎ¸(X(obs)\nÂ·,âˆ’d )\n\r\r\r\n2\n2 ,\n(2)\nwhere X(obs)\nÂ·,âˆ’d denotes the matrix X(obs) with the d-th column removed. The method cycles the target\nfeature for D times, training univariate imputation models for all features. In the inference stage,\n2\n0.0\n2.5\n5.0\n7.5\n10.0\nInput\n0\n2\n4\nOutput\n0.0\n2.5\n5.0\n7.5\n10.0\nInput\n0\n2\n4\nOutput\n(a) The implication of model misspecification.\nâœ“\n6\n8.2\n6\n4\n2.2\nâœ˜\n3\n4.2\n3\n2.2\n-\nâœ˜\n2.8\n6\n9\n5.8\n-\nâœ˜\n3.2\n4\n2.8\n1.8\n-\nâœ“\n5.6\n8\n6.4\n4.2\n2\nâœ“\n3\n4.2\n2.8\n2\n1\nâœ˜\n3\n4\n3.2\n2\n-\nâœ“\n3\n5.8\n9\n6\n3\nX4\nX3\nX2\nX1\nY\nâœ“\n6\n8.2\n6\n4\n2.2\nâœ˜\n3\n4.2\n3\n2.2\n-\nâœ˜\n2.8\n6\n9\n5.8\n-\nâœ˜\n3.2\n4\n2.8\n1.8\n-\nâœ“\n5.6\n8\n6.4\n4.2\n2\nâœ˜\n3\n4.2\n2.8\n2\n-\nâœ˜\n3\n4\n3.2\n2\n-\nâœ˜\n3\n5.8\n9\n6\n-\nX4\nX3\nX2\nX1\nY\n(b) The implication of overlooking oracle features.\nFigure 1: Case study illustrating the limitations of iterative imputation. In panel (a), circular and\ncross markers indicate observed and missing values, respectively, while lines represent imputation\nmodel outputs. In panel (b), â€œâœ“â€ denotes whether a sample can be used for training the imputation\nmodel for Y ; dark areas indicate missing indices in Y at missing ratios of 50% (left) and 75% (right).\nimputation proceeds iteratively: for each target feature, its missing values are estimated using the\ncorresponding univariate model, incorporating previously imputed values of other features. The\nimputed columns are concatenated to construct the imputation matrix X(imp).\n3\nMethodology\n3.1\nMotivation\nIterative imputation approaches MDI to a canonical regression problem, estimating each feature\nusing the remaining features. In this section, we demonstrate that this approach leads to model\nmisspecification and underuse of oracle features, thereby degrading imputation performance.\nThe first limitation is the risk of model misspecification. Iterative imputation methods typically\nemploy a single predefined parametric form for all features, such as linear models [26] or decision\ntrees [29]. However, real-world data often exhibit heterogeneous dependencies among features that\ncannot be effectively captured by a single parametric model [5]. For instance, temperature sensor\ndata in a manufacturing process may have a linear relationship with pressure, while vibration data\nmay display a nonlinear relationship with pressure. Imposing a uniform parametric form thus fails to\naccommodate these diverse dependencies, leading to suboptimal imputation performance.\nThe second limitation is the underuse of oracle features. Iterative methods, by treating all features\nas equally prone to missingness, suffer from limited training data under high missing ratios. Oracle\nfeatures, which have minimal missing values, can provide critical supervision for imputing other\nvariables. For instance, in health records, demographic data often serves as reliable oracle features,\nwhile in industrial settings, catastrophic data can fulfill this role. However, iterative approaches\nneglect these reliable features, thereby limiting overall imputation quality.\nCase study. To illustrate the above limitations of the existing iterative imputation method, a case study\nis conducted. Fig. 1 (a) demonstrates how a fixed model form can lead to model misspecification. In\nthe left panel, a linear model accurately captures the linear feature (red) but fails to fit the nonlinear\nfeatures (blue and green). Conversely, the right panel shows that a nonlinear model fits the sine\nfeature well but overfits the other features. Therefore, a fixed parametric form risks misspecification\nand thereby hampers imputation performance. Fig. 1 (b) illustrates the impact of overlooking oracle\nfeatures. To impute missing values in the target column Y , the iterative method constructs a univariate\nmodel using X1, . . . , X4 as inputs. The model is trained using solely samples with non-missing Y\nvalues. With high missing ratios, only a few samples are usable for training (two in the right panel),\nwhich is insufficient to learn a robust model. In contrast, the four fully observed oracle features\n(X1,..., X4) are overlooked, forfeiting an opportunity to enhance imputation accuracy.\nThese limitations underscore the need for an improved iterative approach that effectively leverages\noracle features and mitigates model misspecification for improved imputation performance. In\nparticular, there are three key questions to be explored: (1) How to adaptively elect different model\nforms to each feature to reduce misspecification? (2) How to incorporate oracle features in imputation?\n(3) Do model-form adaptation and oracle features indeed boost imputation accuracy?\n3\n3.2\nA bi-level optimization framework for iterative imputation\nWe propose a novel bi-level optimization formulation to overcome the limitations of iterative methods.\nThis framework customizes model forms for each feature within a reproducing kernel Hilbert space\n(RKHS) and integrates oracle features as supervision signals.\nBased on the iterative imputation in (2), to mitigate model misspecification, we replace the single\nparametric form of the standard iterative approach, expressed as the fÎ¸ in (2), with a flexible form in\nRKHS. Given the d-th feature as the target: Y(obs) = X(obs)\nÂ·,d\n, we reformulate the imputation task as:\nf âˆ—= arg min\nfâˆˆH\n\r\r\rY(obs) âˆ’f(X(obs)\nÂ·,âˆ’d )\n\r\r\r\n2\n2 + Î»âˆ¥fâˆ¥2\nH,\n(3)\nwhere f âˆ—is the optimal model for that feature. The capacity of RKHS ensures that f âˆ—can capture\neffectively heterogeneous feature relationships, reducing the risk of misspecification.\nTo exploit oracle features, a natural approach is to incorporate them as supervision signals that guide\nimputation. Suppose Y(obs) is an oracle feature and f âˆ—is the associated optimum estimator; we\nupdate the imputed values as:\nmin\nX(miss)\n\r\r\rY(obs) âˆ’f âˆ—(X(miss)\nÂ·,âˆ’d , X(obs)\nÂ·,âˆ’d )\n\r\r\r\n2\n2 .\n(4)\nThis approach is based on a perhaps surprising point-of-view: if X(miss) is well-imputed, applying\nf âˆ—should yield outputs consistent with the ground truth. Otherwise, the imputations of X(miss)\ndeviate from the underlying relationship captured by f âˆ—. Thus, by adjusting X(miss) to minimize this\ndiscrepancy, the imputation process self-corrects, effectively using oracle features as a supervision\nmechanism.\nCombining (3) and (4) yields our bi-level optimization framework for MDI. Given the d-th feature as\nthe target, i.e., Y(obs) = X(obs)\nd\n, X = (X(miss)\nÂ·,âˆ’d , X(obs)\nÂ·,âˆ’d ), the optimization problem is formulated as:\nmin\nX(miss) min\nfâˆˆH\n\r\r\rY(obs) âˆ’f(X)\n\r\r\r\n2\n2 + Î»âˆ¥fâˆ¥2\nH.\n(5)\nSimilar to the standard iterative method, each featureâ€”both oracle features and those with missing\nvalues-is iteratively treated as the target feature Y(obs). In the inner optimization, the optimal\nfunction f is selected within the RKHS, thereby mitigating model misspecification. In the outer\noptimization, the imputed values are refined, effectively incorporating oracle features as supervision\nsignals. Therefore, this bi-level optimization framework provides a principled approach to MDI,\naddressing the limitations of iterative methods.\n3.3\nKernel function, universal property and learning objective\nTo solve the inner loop in (5), we approximate the optimum function f âˆ—by leveraging Gaussian\nkernels in the RKHS. We start by clarifying key kernel properties in Definition 3.1 and 3.2.\nDefinition 3.1 (Kernel function). Let X be a non-empty set. A function K : X Ã— X â†’R is a\nkernel function if there exists a Hilbert space H and a feature map Ïˆ : X â†’H such that âˆ€x, xâ€² âˆˆX,\nK(x, xâ€²) := âŸ¨Ïˆ(x), Ïˆ(xâ€²)âŸ©H .\nDefinition 3.2 (Universal kernel). For X compact Hausdorff, A universal kernel ensures that any\ncontinuous function e : X â†’R can be approximated arbitrarily well within RKHS H. Specifically,\nfor any Ïµ > 0, there exists f âˆˆH such that: supxâˆˆX |f(x) âˆ’e(x)| â‰¤Ïµ.\nGaussian kernel is a typical kernel function formulated as:\nK(x, xâ€²) = exp\n \nâˆ’âˆ¥x âˆ’xâ€²âˆ¥2\n2Ïƒ2\n!\n,\nwhich satisfies the universal property in Definition 3.2 [28]. It implies that by using the Gaussian\nkernel, the associated RKHS H = span{K(Â·, x) | x âˆˆX} admits uniform approximation of any\ncontinuous function.\n4\n2\n2\n3.2\n1\n4\n4\n6\n2\n6\n6\n9\n3\n4\n4.2\n6\n1.8\n2\n2\n3\n0.8\n2\n2\n3.2\n1\n4\n4\n6\n2\n4\n4.2\n6\n1.8\n2\n2.2\n3\n0.8\nğ—!, ğ˜!\nğ—\", ğ˜\"\nğ—($%&)\nForward pass\nBackward pass\nMissing indices\nâˆ‡ğ˜!\"ğ’«\nâˆ‡ğ—!#\n\" ğ’«\nâˆ‡ğ—!$\n\" ğ’«\nâˆ‡ğ—!!\n\" ğ’«\nâˆ‡ğ˜$\"ğ’«\nâˆ‡ğ—$#\n\" ğ’«\nâˆ‡ğ—$$\n\" ğ’«\nâˆ‡ğ—!!\n\" ğ’«\nâˆ‡ğ˜!%ğ’«\nâˆ‡ğ—!#\n% ğ’«\nâˆ‡ğ—!$\n% ğ’«\nâˆ‡ğ—!!\n% ğ’«\nâˆ‡ğ˜$%ğ’«\nâˆ‡ğ—$#\n% ğ’«\nâˆ‡ğ—$$\n% ğ’«\nâˆ‡ğ—!!\n% ğ’«\n0\nâˆ‡ğ—!#\n% ğ’«\n0\nâˆ‡ğ—!!\n% ğ’«\n0\n0\n0\nâˆ‡ğ—!!\n% ğ’«\n0\n0\nâˆ‡ğ—!$\n\" ğ’«\n0\n0\n0\n0\n0\nOracle features\nSample\nUpdate imputation\n2\nCalculate ğ’«\n0.1\n0.4\n0.1\n0.2\n0.2\nğš«\nâˆ‡ğš«ğ’«\nGradient \nmasking\nFigure 2: Visualization of the workflow of KPI, where the dataset contains 5 samples and 4 features.\nThe sampling batch size is set to 2. The last column is the oracle feature without missing values.\nLemma 3.3 (Representer theorem). Suppose h(âˆ¥fâˆ¥) : R+ â†’R is a non-decreasing function.\nThe minimizer of an empirical risk functional regularized by h(âˆ¥fâˆ¥) admits the form: f âˆ—(Â·) =\nPn\ni=1 Î±iK(Â·, xi) where Î± = (Î±1, . . . , Î±n)âŠ¤and K is the associated kernel function.\nLemma 3.4. Let Ys, Yt âˆˆRBÃ—1 be the target feature and Xs, Xt be the corresponding input\nfeatures; Suppose f âˆ—is the optimal model minimizing the empirical risk in the inner optimization of\n(5), its output on Xt is given by f âˆ—(Xt) = KXtXs Â· Î±, where Î± = (K + Î»I)âˆ’1Ys; KXtXs is the\nkernel matrix computed with Xt and Xs.\nSince H is potentially infinite-dimensional, directly identifying f âˆ—is infeasible. Nevertheless, the\nrepresenter theorem in Lemma 3.3 provides a finite approximation to f âˆ—. Accordingly, by sampling\ntwo batches of dataâ€”target features (Ys, Yt) and input features (Xs, Xt)â€”Lemma 3.4 yields that\nthe output of f âˆ—at Xt can be represented as:\nf âˆ—(Xt) = KXtXs(KXsXs + Î»I)âˆ’1Ys,\n(6)\nwhich analytically expresses the output of the optimum model f âˆ—as a linear combination of kernel\nfunctions. It enables adaptively selecting the optimum model for each feature, and simplifies the\nbi-level optimization problem in (5) to a differentiable loss function:\nmin\nXs,Xt\n\r\rYt âˆ’KXtXs(KXsXs + Î»I)âˆ’1Ys)\n\r\r2\n2 ,\n(7)\nFurthermore, selecting kernel hyperparameters (e.g., Gaussian kernel variance) can be challenging\namidst missing data. To alliviate this problem, we introduce multiple kernels with distinct parameters\nand learn to ensemble them adaptively. Suppose K1, K2, ..., KE are E kernel matrices, each with a\ndifferent configuration. We define a learnable simplex vector âˆ†âˆˆRK, and construct the ensembled\nkernel as Kâˆ†= K1âˆ†1 + ... + KEâˆ†E. Putting together, the final objective becomes:\nP =\n\r\rYt âˆ’Kâˆ†\nXtXs(Kâˆ†\nXsXs + Î»I)âˆ’1Ys)\n\r\r2\n2 .\n(8)\n3.4\nOverall workflow\nWhile the learning objective is well defined, its role in actual imputation remains unclear. To this end,\nwe propose the kernel point imputation (KPI) method, which iteratively minimizes the objective (8)\nto refine missing value imputations. The core procedure is shown in Fig. 2 and detailed as follows.\nInitialization. Given the incomplete dataset X(obs), we initialize missing entries using the mean\nof observed steps, obtaining an initial imputation matrix Ximp. The imputed values are treated as\nlearnable parameters, and their gradients are tracked throughout training.\nForward Pass. Two batches are sampled from the imputation matrix. In each iteration, a column\nis randomly chosen as the target feature (Ys, Yt âˆˆRBÃ—1), with the remaining columns as input\nfeatures (Xs, Xt âˆˆRBÃ—(Dâˆ’1)), where B represents batch size, s and t differentiates different batches.\nThe objective P is computed following (8).\n5\nTable 1: Imputation performance in terms of MSE and MAE on 7 datasets.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMean\n0.742\n0.452\n0.837\n0.789\n0.829\n1.165\n0.754\n4.145\n0.740\n2.841\n0.589\n4.682\n0.764\n1.121\nMode\n0.948\n0.770\n0.935\n1.159\n1.026\n1.749\n0.925\n7.741\n1.254\n7.832\n0.593\n6.240\n0.823\n1.372\nMedian\n0.706\n0.469\n0.811\n0.884\n0.820\n1.165\n0.713\n4.356\n0.698\n3.029\n0.500\n5.066\n0.756\n1.123\nMICE\n0.580\n0.127\n0.745\n0.474\n0.856\n1.021\n0.733\n4.539\n0.417\n1.312\n0.536\n3.415\n0.824\n0.971\nMiss.F\n0.560\n0.241\n0.732\n0.650\n0.764\n0.994\n0.593\n3.277\n0.526\n1.497\n0.436\n3.202\n0.686\n0.898\nSinkhorn\n0.835\n0.466\n0.906\n0.796\n0.898\n1.225\n0.848\n4.945\n0.827\n3.233\n0.775\n6.114\n0.857\n1.170\nTDM\n0.730\n0.487\n0.819\n0.769\n0.799\n1.113\n0.726\n3.965\n0.722\n2.792\n0.570\n4.756\n0.752\n1.098\nCSDI-T\n0.726\n1.870\n0.849\n2.683\n0.821\n3.802\n0.761\n15.493\n0.731\n12.291\n0.575\n19.919\n0.780\n4.084\nMissDiff\n0.719\n1.332\n0.840\n1.699\n0.816\n3.523\n0.749\n13.432\n0.728\n14.462\n0.564\n23.320\n0.758\n5.184\nGAIN\n0.730\n0.396\n0.777\n0.688\n0.729\n0.942\n0.572\n3.318\n0.448\n1.413\n0.476\n4.669\n0.754\n1.095\nMIRACLE\n0.795\n0.674\n0.487\n0.305\n0.831\n1.154\n3.208\n45.816\n3.518\n36.784\n0.521\n3.975\n0.555\n0.685\nMIWAE\n0.582\n0.266\n0.746\n0.630\n0.807\n1.071\n0.636\n4.118\n0.525\n1.804\n0.475\n4.977\n0.657\n0.844\nRemasker\n0.439\n0.131\n0.767\n0.750\n0.528\n0.522\n0.599\n3.584\n0.447\n1.268\n0.401\n2.811\n0.546\n0.636\nNewImp\n0.465\n0.177\n0.412\n0.292\n0.405\n0.401\n0.431\n2.495\n0.320\n0.8575\n0.332\n2.992\n0.497\n0.692\nkpi(Ours)\n0.397\n0.121\n0.347\n0.284\n0.402\n0.394\n0.400\n2.387\n0.319\n0.747\n0.264\n2.131\n0.491\n0.685\nNote: Each entry represents the average results at four missing ratios: 0.1, 0.2, 0.3, and 0.4. The best and\nsecond-best results are bolded and underlined, respectively.\nBackward Pass. The gradients of P with respect to Xs, Xt and âˆ†are calculated using automatic\ndifferentiation. The imputed values in Xs and Xt as well as âˆ†are then updated using gradient\ndescent with an update rate Î·:\nXs â†Xs âˆ’Î·âˆ‡XsP âŠ™Ms,\nXt â†Xt âˆ’Î·âˆ‡XtP âŠ™Mt,\nâˆ†â†âˆ†âˆ’Î·âˆ‡âˆ†P,\n(9)\nwhere only the missing values (with M = 1) are updated, while the observed values (with M = 0)\nremain unchanged during this process. Moreover, the gradient of the matrix inverse term is stopped\nfor numerical stability. KPI iteratively executes the forward and backward passes sampling different\nbatches until hitting the early-stopping criteria on the validation dataset. In this process, each feature\nis iteratively treated as the target feature while the remaining features are treated as the input features.\nThis ensures that all featuresâ€”including oracle featuresâ€”are fully exploited as supervision signals.\n4\nEmpirical Investigation\n4.1\nExperimental setup\nâ€¢ Datasets: The empirical study is performed on public tabular datasets from [1], incuding Blood\nTransfusion(BT) Concrete Compression (CC) Connectionist Bench Vowel (CBV) Ionosphere (IS)\nParkinsons (PS) Qsar Biodegradation (QB) Wine Quality White (QWQ). To simulate missing data\nscenarios, we employ a mask matrix generated by a Bernoulli random variable with a preset mean.\nâ€¢ Baselines: The performance of KPI is compared against various imputation methods, including\niterative imputers (MICE [26], Miss.F. [29]), and generative models (GAIN [42], MIWAE [19],\nMiss.D [24], CSDI-T [31], ReMasker [2], and NewImp [1]). We also assess methods that do not\nconform to these categories, such as MIRACLE [7], Sinkhorn [23], and TDM [45].\nâ€¢ Implementation details: To ensure convergence, we cap the number of iterations at 500 and adopt\nan early stopping criterion based on validation performance, with a patience of 10 epochs. The\nAdam optimizer is used for training [6]. Key hyperparameters, namely Î· and B, are determined\nby allocating 5% of the training data for validation and finetuning over [0.0001, 0.01] for Î· and\n6\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMAE\nMISS.F\nKIP\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\n0.2\nWASS\nMISS.F\nKIP\n(a) The results on the CC dataset.\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMAE\nMISS.F\nKIP\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\nWASS\nMISS.F\nKIP\n(b) The results on the CBV dataset.\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.4\n0.5\n0.6\nMAE\nMISS.F\nKIP\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.3\n0.4\n0.5\n0.6\nWASS\nMISS.F\nKIP\n(c) The results on the IS dataset.\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.2\n0.3\n0.4\nMAE\nMISS.F\nKIP\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\n0.2\n0.3\nWASS\nMISS.F\nKIP\n(d) The results on the QB dataset.\nFigure 3: The performance of Miss.F and KPI given varying ratios of oracle features.\n[64, 512] for B. Performance is assessed using modified mean absolute error (MAE) and mean\nsquared error (MSE), focusing on the imputed values at missing entries, following [45, 5]. In\naddition, we report the distribution discrepancy (WASS), measured as the Wasserstein distance [5].\nThe experiments are performed on a platform with two Intel(R) Xeon(R) Platinum 8383C CPUs @\n2.70GHz and a NVIDIA GeForce RTX 4090 GPU.\n4.2\nOverall performance\nTab. 1 presents the average imputation results of KPI and baseline methods under missing ratios\npmiss = 0.1, 0.2, 0.3, and 0.4. Key observations are summarized as follows:\nâ€¢ The iterative imputers exhibits promising performance in most cases. For instance, MICE outper-\nforms simple imputers by large margin over most datasets. MissForest employs random forest as\nthe base model, excelling in handling tabular data, which further improves imputation quality.\nâ€¢ The canonical generative imputers [31, 24], originally tailored for time-series data, often falling\nbehind iterative methods. This can be attributed to the implicit maximization of imputation\nentropy in diffusion models, which negatively impacts accuracy [1]. By contrast, recent generative\napproaches such as NewImp and Remasker handle this issue and achieve strong results, obtaining\nthe best results among baseline methods.\nâ€¢ KPI improves the iterative imputers by adaptively selecting the optimal imputer for each feature and\ninvolving oracle features as supervision signals. This strategy consistently improves performance,\nas evidenced by KPI outperforming all baselines across all 7 datasetsâ€”often by a substantial\nmargin, particularly on the CC and QB datasetsâ€”demonstrating strong practical effectiveness.\n4.3\nImpact of oracle features\nIn this section, we assess the impact of using oracle features as supervision signal on imputation\nperformance. Specifically, we simulate based on complete datasets to generate varying ratios of oracle\nfeatures and evaluate the imputation performance. Two models are considered: KPI and another\ncanonical iterative imputer: Miss.F.\nThe results are presented in Fig. 3. As the ratio of oracle features increases, KPI consistently exhibits\nlower imputation error, showcasing the utility of oracle features. In contrast, Miss.F shows little\nimprovement as oracle feature ratio increases. This difference arises because Miss.F only uses oracle\nfeatures as inputs, whereas KPI exploits them as supervision signals to refine the imputation results.\n7\nTable 2: Varying kernel number results.\nCC\nE\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\n1\n0.082\n-\n0.058\n-\n0.155\n-\n3\n0.070\n14.6%â†“\n0.055\n5.2%â†“\n0.116\n25.2%â†“\n5\n0.069\n15.9%â†“\n0.046\n20.7%â†“\n0.108\n30.3%â†“\n7\n0.065\n20.7%â†“\n0.039\n32.8%â†“\n0.091\n41.3%â†“\nCBV\nE\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\n1\n0.128\n-\n0.095\n-\n0.233\n-\n3\n0.110\n14.1%â†“\n0.085\n10.5%â†“\n0.226\n3.0%â†“\n5\n0.098\n23.4%â†“\n0.075\n21.1%â†“\n0.216\n7.3%â†“\n7\n0.087\n32.0%â†“\n0.066\n30.5%â†“\n0.205\n12.0%â†“\nBT\nDistances\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\n1\n0.334\n-\n0.109\n-\n0.363\n-\n3\n0.318\n4.8%â†“\n0.101\n7.3%â†“\n0.352\n3.0%â†“\n5\n0.305\n8.7%â†“\n0.096\n11.9%â†“\n0.343\n5.5%â†“\n7\n0.302\n9.6%â†“\n0.089\n18.3%â†“\n0.338\n6.9%â†“\nTable 3: Varying kernel function results.\nCC\nKernel\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\nLinear\n0.099\n-\n0.051\n-\n0.203\n-\nPoly\n0.065\n34.3%â†“\n0.039\n23.5%â†“\n0.091\n55.2%â†“\nLaplacian\n0.068\n31.3%â†“\n0.042\n17.6%â†“\n0.093\n54.2%â†“\nGaussian\n0.076\n23.2%â†“\n0.035\n31.4%â†“\n0.082\n59.6%â†“\nCBV\nKernel\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\nLinear\n0.089\n-\n0.087\n-\n0.219\n-\nPoly\n0.087\n2.2%â†“\n0.088\n1.1% â†‘\n0.214\n2.3%â†“\nLaplacian\n0.083\n6.7%â†“\n0.080\n8.1%â†“\n0.211\n3.7%â†“\nGaussian\n0.087\n2.2%â†“\n0.066\n24.1%â†“\n0.205\n6.4%â†“\nBT\nDistances\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\nLinear\n0.326\n-\n0.101\n-\n0.359\n-\nPoly\n0.305\n6.4%â†“\n0.090\n10.9%â†“\n0.342\n4.7%â†“\nLaplacian\n0.316\n3.1%â†“\n0.091\n9.9%â†“\n0.346\n3.6%â†“\nGaussian\n0.302\n7.4%â†“\n0.089\n11.9%â†“\n0.338\n5.8%â†“\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\nMAE\n0.1\n0.2\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.0\nWASS\n0.1\n0.2\n(a) Varying learning rate results on CC\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\nMAE\n0.1\n0.2\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.0\n0.5\nWASS\n0.1\n0.2\n(b) Varying learning rate results on CBV\n64\n128\n256\n512\n1024\nB\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMAE\n0.1\n0.2\n64\n128\n256\n512\n1024\nB\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nWASS\n0.1\n0.2\n(c) Varying batch size results on CC\n64\n128\n256\n512\n1024\nB\n0.00\n0.25\n0.50\nMAE\n0.1\n0.2\n64\n128\n256\n512\n1024\nB\n0.00\n0.25\nWASS\n0.1\n0.2\n(d) Varying batch size results on CBV\nFigure 4: Varying learning rate and batch size results with missing ratios 0.1 and 0.2.\n4.4\nImpact of kernel strategy\nIn this section, we analyze the impact of kernel function and kernel amount (E) on imputation\nperformance. The key observations are summarized as follows.\nâ€¢ The multiple kernel ensembling mechanism has a substantial impact. As shown in Tab. 2, increasing\nE from 1 to 7 consistently reduces MSE from 0.082 to 0.065, indicating a relative reduction of\n20.7%. This gain is attributed to the increased flexibility in adaptively selecting kernel parameters,\nallowing KPI to better represent the optimal imputation model for each feature.\nâ€¢ The performance of different kernel functions showcases the importance of kernel universality. The\nlinear kernel, which is not universal and has limited RKHS capacity, yields the worst performance.\nThe polynomial kernel, with a larger RKHS, performs better. The Gaussian kernel exhibits the\nbest overall performance. The superiority is attributed to its universality, i.e., the associated RKHS\nadmits uniform approximation of any continuous function. Such extensive RKHS capacity enables\nKPI to optimize the imputation model for each feature, thereby enhancing imputation performance.\n4.5\nParameter sensitivity analysis\nIn this section, we examine the influence of critical hyperparameters on the performance of KPI\nin Fig. 4. Below are the key observations:\n8\nâ€¢ The update rate (Î·) plays a pivotal role in controlling the volume of updates to the imputation\nmatrix each epoch. As Î· is reduced from 0.02 to approximately 0.01, both MAE and RMSE\ndecrease, indicating that a smaller Î· enhances update stability. However, further reduction of Î·\nto 0.001 results in increased MAE and RMSE, where the meaningful update direction becomes\novershadowed by noise, preventing model convergence within the allocated epochs.\nâ€¢ The batch size (B) affects the scale of the problem in calculating discrepancies, with sizes ranging\nfrom 64 to 1024 examined. There is a weak yet consistent decrease in MAE and RMSE as the\nbatch size increases to B = 512, enhancing the reliability of estimations. Increasing the batch size\nbeyond this point yields diminishing returns and may lead to unnecessary computational overhead.\n5\nRelated works\nThe pervasive presence of missing data undermines the integrity of collected datasets and the reliability\nof data-driven applications, underscoring the necessity for effective missing data imputation (MDI).\nTo achieve accurate MDI, existing approaches can be broadly categorized into two paradigms:\ndiscriminative and generative, each with distinct advantages and limitations [5, 21].\nThe iterative method [29, 26, 43, 15] is one of the most popular methods in discriminative imputation,\ninitiated from imputation by chained equations (ICE) [26], which employs specific models to estimate\nmissing values for each feature based on the remaining observable features. On the basis of ICE, a\nline of work advocates for employing modern parametric models, such as neural networks [19, 7],\nBayesian models [26] and random forest [29], which enhances the capacity of imputation models\nand thereby accommodating complex missing patterns. In a different line of work, various training\ntechniques are investigated within the paradigm, such as multiple imputation [26], ensemble learning\n[29], and multitask learning [19], which enhances the utility to accommodate diverse contexts.\nWhile this paradigm offers enhanced flexibility and accuracy, it fails to utilize the oracle features\neffectively and risks model misspecification, which can lead to suboptimal imputation results in noisy\nenvironments. Our research advances this methodology by handling the two limitations.\nApart from the iterative methods, there are other notable approaches in the discriminative paradigm.\nThe simple direct paradigm employs elementary statistical measures like mean, median, and mode to\nreplace missing values, offering quick and straightforward solutions. However, this approach lacks\nthe capacity to accommodate complex relationships [17, 20], often producing trivial and inadequate\nimputation results that fail to meet the expectation in practice. Another notable approach is matrix\nfactorization, which decomposes the data matrix into two low-rank matrices, capturing the latent\nstructure of the data for imputation [8, 4]. This method is particularly effective in collaborative\nfiltering and recommendation systems [12, 37]. Recent advances explore a novel methodology based\non distribution discrepancy minimization[45, 23]. This approach builds on the assumption that, under\nthe independent and identically distributed (i.i.d.) condition, any two data batches should share\nthe same underlying distribution, thereby exhibiting minimal discrepancy. Subsequent studies have\nextended this idea by refining discrepancy measures to accommodate different data characteristics\nsuch as neighboring effects [40, 35], noisy observations [36], and temporal dependencies [39].\nThe generative paradigm restates imputation as a conditional generation problem, using advanced\nneural architectures and generative training strategies, such as generative adversarial networks\n[42, 30, 13] and diffusions [31, 41, 1], to approximate data distributions and perform imputation. This\nstrategy incorporates the strengths of generative models, capturing and utilizing complex relationships,\nwhich potentially enhances the imputation quality when ample data is available. However, it also\nbears the defects with generative models, such as the instability associated with adversarial training\nand the operational complexity of diffusions [18, 25], hampering their use in practice.\n6\nConclusion\nIterative imputation methods are widely used for handling missing data, yet existing approaches\nare often limited by model misspecification and underuse of oracle features. To overcome these\nchallenges, we introduce KPI, a bi-level optimization framework which optimizes model form within\nRKHS for each feature, reducing model misspecification, and exploits oracle features as effective\nsupervision. Extensive experiments on real-world datasets demonstrate that KPI achieves superior\nimputation performance and effectively leverages oracle features.\n9\nLimitations & future work. In this work, we do not accommodate potential noise in datasets,\nwhich is a prevalent challenge in industrial settings [4, 3]. Future research could incorporate robust\noptimization techniques and truncate outliers in the kernel matrix which has potential to improve\nnoise robustness. Additionally, this work mitigates the difficulty of concise kernel parameter selection\nvia adaptive ensembling, which is an heuristic approach. Subsequent work may explore meta-learning\nstrategies with theoretical guarantees for accurate kernel parameter selection.\nAcknowledgments\nZ. Lin was supported by the NSF China (No. 62276004) and the State Key Laboratory of General\nArtificial Intelligence. H. Li was supported by National Natural Science Foundation of China\n(623B2002).\nReferences\n[1] Zhichao Chen, Haoxuan Li, Fangyikang Wang, Haotian Zhang, Hu Xu, Xiaoyu Jiang, Zhihuan Song, and\nHao Wang. Rethinking the diffusion models for missing data imputation: A gradient flow perspective. In\nProc. Adv. Neural Inf. Process. Syst., 2024.\n[2] Tianyu Du, Luca Melis Melis, and Ting Wang. Remasker: Imputing tabular data with masked autoencoding.\nIn Proc. Int. Conf. Learn. Represent., 2024.\n[3] Xinxin Feng, Haitao Zhang, Can Wang, and Haifeng Zheng. Traffic data recovery from corrupted and\nincomplete observations via spatial-temporal trpca. IEEE Trans. Intell. Transp. Syst., 23(10):17835â€“17848,\n2022.\n[4] Liyang Hu, Yuheng Jia, Weijie Chen, Longhui Wen, and Zhirui Ye. A flexible and robust tensor completion\napproach for traffic data recovery with low-rankness. IEEE Trans. Intell. Transp. Syst., 25(3):2558â€“2572,\n2023.\n[5] Daniel Jarrett, Bogdan Cebere, Tennison Liu, Alicia Curth, and Mihaela van der Schaar. Hyperimpute:\nGeneralized iterative imputation with automatic model selection. In Proc. Int. Conf. Mach. Learn., volume\n162, pages 9916â€“9937, 2022.\n[6] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Int. Conf. Learn.\nRepresent., pages 1â€“9, 2015.\n[7] Trent Kyono, Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. MIRACLE: causally-aware imputation\nvia learning missing data mechanisms. In Proc. Adv. Neural Inf. Process. Syst., pages 23806â€“23817, 2021.\n[8] Daniel Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. Proc. Adv. Neural\nInf. Process. Syst., 13, 2000.\n[9] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. Propensity matters: Measuring\nand enhancing balancing for recommendation. In Proc. Int. Conf. Mach. Learn., volume 202, pages\n20182â€“20194. PMLR, 2023.\n[10] Haoxuan Li, Chunyuan Zheng, Shuyi Wang, Kunhan Wu, Eric Wang, Peng Wu, Zhi Geng, Xu Chen, and\nXiao-Hua Zhou. Relaxing the accurate imputation assumption in doubly robust learning for debiased\ncollaborative filtering. In Proc. Int. Conf. Mach. Learn., volume 235, pages 29448â€“29460, 2024.\n[11] Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, and Xiao-Hua Zhou. Debiased\nrecommendation with noisy feedback. In Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining,\npage 1576â€“1586, 2024.\n[12] Haoxuan Li, Chunyuan Zheng, Yanghao Xiao, Peng Wu, Zhi Geng, Xu Chen, and Peng Cui. Debiased\ncollaborative filtering with kernel-based causal balancing. In Proc. Int. Conf. Learn. Represent., pages 1â€“9,\n2024.\n[13] Haozhe Li, Yilin Liao, Zijian Tian, Zhaoran Liu, Jiaqi Liu, and Xinggao Liu. Bidirectional stackable\nrecurrent generative adversarial imputation network for specific emitter missing data imputation. IEEE\nTrans. Inf. Forensics Security, 19:2967â€“2980, 2024.\n[14] Steven Cheng-Xian Li, Bo Jiang, and Benjamin M. Marlin. Misgan: Learning from incomplete data with\ngenerative adversarial networks. In Proc. Int. Conf. Learn. Represent., 2019.\n10\n[15] Jingchen Liu, Andrew Gelman, Jennifer Hill, Yu-Sung Su, and Jonathan Kropko. On the stationary\ndistribution of iterative imputations. Biometrika, 101(1):155â€“173, 2014.\n[16] Qianli Ma, Sen Li, and Garrison W Cottrell. Adversarial joint-learning recurrent neural network for\nincomplete time series classification. IEEE Trans. Pattern Anal. Mach. Intell., 44(4):1765â€“1776, 2020.\n[17] R Malarvizhi and Antony Selvadoss Thanamani. K-nearest neighbor in missing data imputation. Int. J.\nEng. Res. Dev, 5(1):5â€“7, 2012.\n[18] Pierre-Alexandre Mattei and Jes Frellsen. Leveraging the exact likelihood of deep latent variable models.\nIn Proc. Adv. Neural Inf. Process. Syst., pages 3859â€“3870, 2018.\n[19] Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: deep generative modelling and imputation of incomplete\ndata sets. In Proc. Int. Conf. Mach. Learn., volume 97, pages 4413â€“4423, 2019.\n[20] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning\nlarge incomplete matrices. J. Mach. Learn. Res., 11:2287â€“2322, 2010.\n[21] Xiaoye Miao, Yangyang Wu, Lu Chen, Yunjun Gao, and Jianwei Yin. An experimental survey of missing\ndata imputation algorithms. IEEE Trans. Knowl. Data Eng., 35(7):6630â€“6650, 2022.\n[22] Mehryar Mohri. Foundations of machine learning, 2018.\n[23] Boris Muzellec, Julie Josse, Claire Boyer, and Marco Cuturi. Missing data imputation using optimal\ntransport. In Proc. Int. Conf. Mach. Learn., volume 119, pages 7130â€“7140, 2020.\n[24] Yidong Ouyang, Liyan Xie, Chongxuan Li, and Guang Cheng. Missdiff: Training diffusion models on\ntabular data with missing values. arXiv preprint arXiv:2307.00467, 2023.\n[25] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-\nmate inference in deep generative models. In Proc. Int. Conf. Mach. Learn., volume 32, pages 1278â€“1286,\n2014.\n[26] Patrick Royston and Ian R White. Multiple imputation by chained equations (mice): implementation in\nstata. J. Statist. Softw., 45:1â€“20, 2011.\n[27] Indro Spinelli, Simone Scardapane, and Aurelio Uncini. Missing data imputation with adversarially-trained\ngraph convolutional networks. Neural Netw., 129:249â€“260, 2020.\n[28] Bharath K. Sriperumbudur, Kenji Fukumizu, and Gert R. G. Lanckriet. On the relation between universality,\ncharacteristic kernels and RKHS embedding of measures. In Proc. Int. Conf. Artif. Intell. Statist., volume 9\nof JMLR Proceedings, pages 773â€“780. JMLR.org, 2010.\n[29] Daniel J Stekhoven and Peter BÃ¼hlmann. Missforestâ€”non-parametric missing value imputation for\nmixed-type data. Bioinformatics, 28(1):112â€“118, 2012.\n[30] Ziyue Sun, Haozhe Li, Wenhai Wang, Jiaqi Liu, and Xinggao Liu. DTIN: dual transformer-based\nimputation nets for multivariate time series emitter missing data. Knowl. Based Syst., 284:111270, 2024.\n[31] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion\nmodels for probabilistic time series imputation. Proc. Adv. Neural Inf. Process. Syst., 34:24804â€“24816,\n2021.\n[32] Hao Wang, Zhichao Chen, Jiajun Fan, Haoxuan Li, Tianqiao Liu, Weiming Liu, Quanyu Dai, Yichao Wang,\nZhenhua Dong, and Ruiming Tang. Optimal transport for treatment effect estimation. In Proc. Adv. Neural\nInf. Process. Syst., volume 36, pages 5404â€“5418, 2023.\n[33] Hao Wang, Zhichao Chen, Zhaoran Liu, Xu Chen, Haoxuan Li, and Zhouchen Lin. Proximity matters:\nLocal proximity enhanced balancing for treatment effect estimation. Proc. ACM SIGKDD Int. Conf. Knowl.\nDiscovery Data Mining, 2025.\n[34] Hao Wang, Zhichao Chen, Zhaoran Liu, Haozhe Li, Degui Yang, Xinggao Liu, and Haoxuan Li. Entire\nspace counterfactual learning for reliable content recommendations. IEEE Trans. Inf. Forensics Security,\n20:1755â€“1764, 2025.\n[35] Hao Wang, Zhichao Chen, Zhaoran Liu, Licheng Pan, Hu Xu, Yilin Liao, Haozhe Li, and Xinggao Liu.\nSpot-i: Similarity preserved optimal transport for industrial iot data imputation. IEEE Trans. Ind. Informat.,\n20(12):14421â€“14429, 2024.\n11\n[36] Hao Wang, Zhichao Chen, Yuan Shen, Hui Zheng, Degui Yang, Dangjun Zhao, and Buge Liang. Unbiased\nrecommender learning from implicit feedback via weakly supervised learning. In IEEE Trans. Neural\nNetw. Learn. Syst., 2025.\n[37] Hao Wang, Zhichao Chen, Haotian Wang, Yanchao Tan, Licheng Pan, Tianqiao Liu, Xu Chen, Haoxuan\nLi, and Zhouchen Lin. Unbiased recommender learning from implicit feedback via weakly supervised\nlearning. In Proc. Int. Conf. Mach. Learn., 2025.\n[38] Hao Wang, Zhichao Chen, Honglei Zhang, Zhengnan Li, Licheng Pan, Haoxuan Li, and Mingming Gong.\nDebiased recommendation via wasserstein causal balancing. ACM T. Inform. Syst., 2025.\n[39] Hao Wang, Zhengnan Li, Haoxuan Li, Xu Chen, Mingming Gong, Bin Chen, and Zhichao Chen. Optimal\ntransport for time series imputation. In Proc. Int. Conf. Learn. Represent., pages 1â€“9, 2025.\n[40] Hao Wang, Xinggao Liu, Zhaoran Liu, Haozhe Li, Yilin Liao, Yuxin Huang, and Zhichao Chen. Lspt-d:\nLocal similarity preserved transport for direct industrial data imputation. IEEE Trans. Autom. Sci. Eng.,\n22:9438â€“9448, 2025.\n[41] Hu Xu, Zhaoran Liu, Hao Wang, Changdi Li, Yunlong Niu, Wenhai Wang, and Xinggao Liu. Denoising\ndiffusion straightforward models for energy conversion monitoring data imputation. IEEE Trans. Ind.\nInformat., 20(10):11987â€“11997, 2024.\n[42] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: missing data imputation using generative\nadversarial nets. In Proc. Int. Conf. Mach. Learn., volume 80, pages 5675â€“5684, 2018.\n[43] Aoqian Zhang, Shaoxu Song, Yu Sun, and Jianmin Wang. Learning individual models for imputation. In\nProc. IEEE Int. Conf. Data Eng., pages 160â€“171. IEEE, 2019.\n[44] Zhongheng Zhang. Missing data imputation: focusing on single imputation. Ann. Transl. Med, 4(1):9,\n2016.\n[45] He Zhao, Ke Sun, Amir Dezfouli, and Edwin V. Bonilla. Transformed distribution matching for missing\nvalue imputation. In Proc. Int. Conf. Mach. Learn., volume 202, pages 42159â€“42186, 2023.\n12\nA\nTheoretical justification\nBuilding upon the foundational theorems established earlier, we delve deeper into the theoretical as-\npects of our kernel ridge regression-based imputation framework. By leveraging advanced properties\nof kernel functionsâ€”such as universality, injective mappings, and the reproducing propertyâ€”we\nfurther substantiate the advantages and robustness of our method. This section introduces additional\ntheorems and proofs that highlight these properties and their implications for the imputation problem.\nLemma A.1 (Representer theorem). Suppose h(âˆ¥fâˆ¥) : R+ â†’R is a non-decreasing function.\nThe minimizer of an empirical risk functional regularized by h(âˆ¥fâˆ¥) admits the form: f âˆ—(Â·) =\nPn\ni=1 Î±iK(Â·, xi) where Î± = (Î±1, . . . , Î±n)âŠ¤and K is the associated kernel function.\nProof. The proof can be found in Theorem 6.11 of Mohri et al. [22].\nTheorem A.2 (Lemma 3.4 in the main text). Let Ys, Yt âˆˆRBÃ—1 be the target feature and Xs,\nXt be the corresponding input features; Suppose f âˆ—is the optimal model minimizing the empirical\nrisk in the inner optimization of (5), its output on Xt is given by f âˆ—(Xt) = KXtXs Â· Î±, where\nÎ± = (K + Î»I)âˆ’1y; KXtXs is the kernel matrix computed with Xt and Xs.\nProof. Consider the samples Xs and Ys where Ys is the observed target, and Xs comprises the\ninput features. The empirical risk minimization objective with â„“2 regularization to select the optimal\nfunctional form is\nmin\nfâˆˆH âˆ¥Ys âˆ’f(Xs)âˆ¥2\n2 + Î»âˆ¥fâˆ¥2\nH,\n(10)\nwhich corresponds precisely to the inner loop of (5). According to Lemma A.1, when h is an identity\nfunction (in (10)) and H is a RKHS associated with kernel K, the minimizer f âˆ—must admit the\nexplicit form\nf âˆ—(x) =\nB\nX\ni=1\nÎ±iK(x, xs\ni),\n(11)\nfor some coefficients Î±1, . . . , Î±B.\nSubstituting this form into the empirical risk (10), the optimization problem becomes\nmin\nÎ±âˆˆRB âˆ¥Ys âˆ’KXsXsÎ±âˆ¥2\n2 + Î»Î±âŠ¤KXsXsÎ±,\n(12)\nwhere KXsXs is the B Ã— B Gram matrix, with (i, j)-th entry K(xs\ni, xs\nj), Ys is the length-B target\nvector, and Î± is the vector of coefficients.\nExpanding the loss function in matrix notation yields\n(Ys âˆ’KXsXsÎ±)âŠ¤(Ys âˆ’KXsXsÎ±) + Î»Î±âŠ¤KXsXsÎ±.\n(13)\nDue to symmetry of KXsXs, this simplifies to\nYsâŠ¤Ys âˆ’2YsâŠ¤KXsXsÎ± + Î±âŠ¤(K2\nXsXs + Î»KXsXs)Î±.\n(14)\nAccording to the first-order condition, setting the derivative with respect to Î± to zero and solving for\nÎ± gives\nâˆ’2KâŠ¤\nXsXsYs + 2(K2\nXsXs + Î»KXsXs)Î± = 0,\n(15)\nwhich is equivalent to:\nKXsXs(KXsXs + Î»I)Î± = KXsXsYs.\n(16)\nAssuming KXsXs is invertible, we have:\n(KXsXs + Î»I)Î± = Ys.\n(17)\nwhich immediately follows from multiplying both sides by Kâˆ’1\nXsXs. Solving for Î± gives:\nÎ± = (KXsXs + Î»I)âˆ’1Ys.\n(18)\n13\nSubstituting (18) into (11) leads to\nf âˆ—(x) =\nB\nX\ni=1\nÎ±iK(x, xs\ni) = K(x)(KXsXs + Î»I)âˆ’1Ys,\n(19)\nwhere K(x) is the 1 Ã— B vector [K(x, xs\n1), Â· Â· Â· , K(x, xs\nB)]. For a (possibly distinct) batch of inputs\nXt, evaluating f âˆ—at each xt\nj gives\nf âˆ—(xt\n1) =\nB\nX\ni=1\nÎ±iK(xt\n1, xs\ni) =\n\u0002\nK(xt\n1, xs\n1), K(xt\n1, xs\n2), ..., K(xt\n1, xs\nB)\n\u0003\n(KXsXs + Î»I)âˆ’1Ys,\nf âˆ—(xt\n2) =\nB\nX\ni=1\nÎ±iK(xt\n2, xs\ni) =\n\u0002\nK(xt\n2, xs\n1), K(xt\n2, xs\n2), ..., K(xt\n2, xs\nB)\n\u0003\n(KXsXs + Î»I)âˆ’1Ys,\n...\nf âˆ—(xt\nB) =\nB\nX\ni=1\nÎ±iK(xt\nB, xs\ni) =\n\u0002\nK(xt\nB, xs\n1), K(xt\nB, xs\n2), ..., K(xt\nB, xs\nB)\n\u0003\n(KXsXs + Î»I)âˆ’1Ys,\n(20)\nwhich may be stacked to give the vector-valued expression\nf âˆ—(Xt) = KXtXs(KXsXs + Î»I)âˆ’1Ys,\n(21)\nwhere KXtXs is the matrix with entries [KXtXs]ij = K(xt\ni, xs\nj). The proof is completed.\nDefinition A.3 (Kernel Functions). Let x, xâ€² âˆˆRD be two vectors in the input feature space. A\nkernel function K : RD Ã— RD â†’R is a symmetric, positive semi-definite function that quantifies the\nsimilarity between x and xâ€². Commonly used kernel functions include:\n1. Linear Kernel: Klinear(x, xâ€²) = xâŠ¤xâ€², which computes the inner product between two vectors\nand corresponds to the case where no explicit feature transformation is applied.\n2. Polynomial Kernel: Kpoly(x, xâ€²) =\n\u0000xâŠ¤xâ€² + c\n\u0001d, where c â‰¥0 is a constant coefficient trading\noff the influence of higher-order versus lower-order terms, and d âˆˆN is the degree of the\npolynomial. It enables learning non-linear relationships by implicitly mapping the input features\ninto a higher-dimensional polynomial feature space.\n3. Gaussian Kernel: Kgauss(x, xâ€²) = exp\n\u0010\nâˆ’âˆ¥xâˆ’xâ€²âˆ¥2\n2Ïƒ2\n\u0011\n, where âˆ¥x âˆ’xâ€²âˆ¥2 denotes the squared\nEuclidean distance between x and xâ€², and Ïƒ > 0 is a scale parameter controlling the width of\nthe kernel. The Gaussian kernel is widely used due to its ability to model localized and highly\nnon-linear interactions.\nB\nImplementation details\nB.1\nDataset description and process strategy\nIn this paper, we use datasets from the UCI repository for model validation, in alignment with the a\nrecent NeurIPS-24 publication [1]. Detailed statistics for all selected datasets are provided in Tab. 4.\nTo simulate missing data, we first construct a binary mask matrix M. The observed data matrix,\ndenoted as X(obs), is derived by element-wise application of the complement mask 1 âˆ’M to the\nfully observed data matrix X(id). Specifically, each entry x(obs)\nnd\nin X(obs) is given by x(obs)\nnd\n= x(id)\nnd\nif mnd = 0; otherwise, x(obs)\nnd\nis assigned the value Null. On the generation of M, we consider three\ncanonical missing data mechanisms:\nâ€¢ Missing Completely at Random (MCAR): The probability of entry-wise missingness is indepen-\ndent of both observed and unobserved data. To simulate MCAR, each entry of M is independently\nset to 1 (missing) with probability pmiss, and to 0 (observed) with probability 1 âˆ’pmiss.\n14\nTable 4: The statistics of involved datasets.\nAbbreviation\nDataset Name\nNumber (N)\nDimension (D)\nBT\nBlood Transfusion\n748\n4\nCC\nConcrete Compression\n1030\n7\nCBV\nConnectionist Bench Vowel\n990\n10\nIS\nIonosphere\n351\n34\nPK\nParkinsons\n195\n23\nQB\nQSAR Biodegradation\n1055\n41\nWQW\nWine Quality White\n4898\n11\nNote. The column â€˜Dimensionâ€™ and â€˜Numberâ€™ denotes the number of variables and samples in each\ndataset, respectively.\nâ€¢ Missing at Random (MAR): The missingness of a variable depends only on values of observed\nvariables [33, 32, 9]. To generate MAR scenarios, we randomly select a subset of features to be\nalways observed. The missingness in the remaining features is simulated using a logistic regression\nmodel, where the observed features act as predictors. The model parameters are randomly initialized,\nand the intercept (bias) is calibrated to yield the desired missingness rate.\nâ€¢ Missing Not at Random (MNAR): The probability that a value is missing depends on the\nunobserved (missing) values themselves [34, 38, 11]. For MNAR simulation, we adopt the\nprocedure in [1, 5]: the logistic model used for MAR is repurposed, but its inputs are themselves\nmasked by an independent MCAR mechanism, making the missingness dependent on both observed\nand unobserved features.\nB.2\nTraining protocols\nTo ensure reliable convergence, we set a maximum of 500 training iterations and adopt early stopping\nbased on validation performance, using a patience parameter of 10 epochs. Optimization throughout\nis conducted using the Adam optimizer [6]. The kernel function is specified as the Gaussian kernel.\nThe main hyperparameters, specifically the update rate Î·, batch size B, kernel number E and variance\nÏƒ are determined by allocating 5% of the training data as a validation set and tuning over the intervals\nÎ· âˆˆ[0.0001, 0.01], B âˆˆ[64, 512], E âˆˆ[1, 7] and Ïƒ âˆˆ[0.01, 10]. All experiments are conducted on\na hardware platform comprising two Intel(R) Xeon(R) Platinum 8383C CPUs (2.70GHz) and an\nNVIDIA GeForce RTX 4090 GPU.\nOn the implementation of baseline methods, we closely follow the implementation details in\nNewImp [1]. Hyperparameter reproducibility was confirmed in our environment, and we adopted the\nprovided settings to run the baseline scripts and report the corresponding results of NewImp. The\nreproduction of other models also follows NewImp. Specifically, the batch size for ReMasker is set\nto 64, whereas for all other baseline models it is fixed at 512. The MIWAE model is configured\nwith a latent dimension of 16 and 32 hidden units. The TDM model is implemented with two layers,\neach containing 16 hidden units. For the MIRACLE model, the number of hidden units is set to\n32. ReMasker is implemented with an embedding dimension of 32, a depth of 6, a mask ratio of\n0.5, encoder and decoder depths of 6 and 4 respectively, and uses 4 attention heads. Both MissDiff\nand CSDI-T are set with a channel size of 16, an embedding dimension of 128, and two layers. The\ndiffusion step parameter is set to 100 for these models, and the number of particles is set to 50.\nB.3\nEvaluation metrics\nThe imputed data matrix X(imp) is evaluated to assess imputation quality. Following the protocol\nin [45], we primarily employ the modified mean absolute error (MAE) and root mean squared error\n(RMSE) for evaluation:\nMAE :=\n1\nPN\nn=1\nPD\nd=1 Â¯mnd\nN\nX\nn=1\nD\nX\nd=1\n\f\f\fx(imp)\nnd\nâˆ’x(obs)\nnd\n\f\f\f Â¯mnd,\n(22)\nRMSE :=\nv\nu\nu\nt\n1\nPN\nn=1\nPD\nd=1 Â¯mnd\nN\nX\nn=1\nD\nX\nd=1\n\r\r\rx(imp)\nnd\nâˆ’x(obs)\nnd\n\r\r\r\n2\n2 Â¯mnd,\n(23)\n15\n32\n64\n256\n512\n1024\nBatch size (B)\n0.7\n1.4\n2.1\n2.8\n3.5\nTime (ms)\ngaussian\nlinear\npoly\n32\n64\n256\n512\n1024\nBatch size (B)\n2.8\n3.5\nTime (ms)\ngaussian\nlinear\npoly\n(a) Computation time with different batch sizes.\n16\n32\n64\n128\n256\nFeature amount (D)\n1.2\n1.6\n2.0\nTime (ms)\ngaussian\nlinear\npoly\n16\n32\n64\n128\n256\nFeature amount (D)\n2.5\n3.0\n3.5\n4.0\n4.5\nTime (ms)\ngaussian\nlinear\npoly\n(b) Computation time with different feature amounts.\nFigure 5: Running time of the forward pass (left panels) and backward pass (right panels) given\nvarying settings. Different colors indicate different kernel functions. The colored lines and the\nshadowed areas indicate the mean values and the 99.9% confidence intervals.\nwhere Â¯mnd âˆˆÂ¯M indicates positions of imputed (originally missing) values with Â¯mnd = 1 âˆ’mnd,\nand x(obs)\nnd\nâˆˆX(obs) is the ground-truth value from the fully observed data. As only the originally\nmissing entries are imputed, we restrict the calculation of error metrics to the indices where Â¯mnd = 1.\nIn addition to the point-wise error metrics above, we also consider the squared Wasserstein distance\n(abbreviated as WASS) [1], which quantifies the discrepancy between the distributions of the imputed\nvalues and the corresponding ground-truth values at the missing positions (M = 1).\nC\nAdditional experimental results\nC.1\nAn empirical analysis on complexity\nIn this section, we examine the practical computational complexity of KPI. While the overall\nconvergence was analyzed in Theorem ??, the computational cost per iteration, which includes\nboth the forward and backward passes, has not been thoroughly discussed. To address this gap, we\nconducted experiments using IntelÂ® XeonÂ® Gold 6140 CPUs and Nvidia RTX 4090 GPUs, with\neach experiment repeated 100 times to ensure reliability.\nThe results are presented in Fig. 5. The running time per iteration remains limited (within 4 ms)\nacross a diverse range of hyperparameters, demonstrating the feasibility of KPI for real-world\napplications. Other key observations are summarized as follows:\nâ€¢ To explore the impact of batch size (B), we vary B within a wide range from 32 to 1024 while\nkeeping the feature amount (D) to 8. The running cost of the forward pass increased with the batch\nsize, as expected. This is attributed to the larger matrix inversion in (8), which cannot be efficiently\naccelerated by GPUs. In contrast, the backward pass cost was weakly correlated with B, since\ngradient computations after constructing the computation graph can be parallelized.\nâ€¢ To investigate the impact of feature amount (D), we maintain a constant batch size of 64. A weak\ncorrelation between D and the running time is observed. This is because varying D primarily\naffects the complexity of each kernel matrix computation, which can be effectively mitigated by\nGPU acceleration. This highlights a practical advantage of KPI: its efficiency in handling datasets\nwith a large number of features.\nâ€¢ We observe that the type of kernel function also affects the running cost. The gaussian kernel\nexhibits the largest running time compared to other kernel functions, in terms of both forward pass\nand the backward pass.\nC.2\nAdditional overall performance results given different missing ratios\nTab. 5-8 detail the imputation performance of KPI and baselines, with results for different missing\nratios: 0.1, 0.2, 0.3, and 0.4 listed separately. The results demonstrate that KPI consistently outper-\nforms the baselines in all settings, achieving superior performance in terms of both MAE and WASS.\nThis consistent superiority across varying missing ratios underscores the effectiveness and robustness\nof KPI for missing data imputation.\n16\nTable 5: Imputation performance comparison with missing ratio of 0.1.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.118\n0.027\n0.155\n0.075\n0.196\n0.16\n0.175\n0.406\n0.097\n0.133\n0.122\n0.271\n0.192\n0.151\nMiss.F\n0.123\n0.037\n0.172\n0.111\n0.185\n0.149\n0.141\n0.295\n0.128\n0.163\n0.102\n0.284\n0.164\n0.129\nSinkhorn\n0.840\n0.434\n0.903\n0.614\n0.896\n0.837\n0.850\n2.047\n0.841\n1.513\n0.784\n2.622\n0.856\n0.755\nTDM\n0.724\n0.415\n0.815\n0.545\n0.787\n0.690\n0.720\n1.592\n0.731\n1.295\n0.565\n1.977\n0.745\n0.650\nCSDI-T\n0.727\n1.914\n0.850\n2.680\n0.815\n3.753\n0.766\n16.714\n0.743\n12.939\n0.578\n20.407\n0.775\n4.022\nMissDiff\n0.718\n1.446\n0.847\n1.803\n0.812\n4.101\n0.750\n13.640\n0.744\n16.209\n0.566\n25.062\n0.755\n6.037\nGAIN\n0.739\n0.355\n0.759\n0.479\n0.690\n0.541\n0.532\n1.137\n0.399\n0.460\n0.409\n1.192\n0.736\n0.621\nMIRACLE\n0.528\n0.174\n0.382\n0.161\n0.778\n0.682\n3.723\n26.666\n3.777\n18.544\n0.461\n1.103\n0.485\n0.364\nMIWAE\n0.539\n0.226\n0.698\n0.436\n0.782\n0.668\n0.603\n1.638\n0.526\n0.861\n0.450\n2.044\n0.626\n0.507\nRemasker\n0.365\n0.099\n1.041\n0.830\n0.448\n0.249\n0.715\n1.775\n0.500\n0.739\n0.489\n1.737\n0.503\n0.364\nNewImp\n0.383\n0.091\n0.273\n0.110\n0.231\n0.101\n0.423\n1.013\n0.251\n0.281\n0.305\n1.067\n1.045\n0.834\nKPI(Ours)\n0.084\n0.022\n0.023\n0.01\n0.051\n0.016\n0.093\n0.217\n0.071\n0.066\n0.05\n0.219\n0.082\n0.058\nTable 6: Imputation performance comparison with missing ratio of 0.2.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.145\n0.027\n0.171\n0.097\n0.208\n0.218\n0.186\n0.915\n0.102\n0.243\n0.13\n0.589\n0.2\n0.211\nMiss.F\n0.141\n0.058\n0.173\n0.131\n0.187\n0.206\n0.144\n0.589\n0.133\n0.316\n0.106\n0.579\n0.168\n0.181\nSinkhorn\n0.834\n0.428\n0.907\n0.711\n0.902\n1.079\n0.842\n3.908\n0.819\n2.572\n0.773\n5.036\n0.854\n1.030\nTDM\n0.725\n0.431\n0.812\n0.659\n0.800\n0.939\n0.720\n3.097\n0.710\n2.167\n0.567\n3.855\n0.750\n0.927\nCSDI-T\n0.724\n1.808\n0.847\n2.674\n0.823\n3.760\n0.759\n15.642\n0.724\n12.409\n0.574\n19.999\n0.777\n4.057\nMissDiff\n0.714\n1.282\n0.835\n1.707\n0.818\n3.658\n0.746\n13.473\n0.718\n14.872\n0.562\n23.777\n0.757\n5.526\nGAIN\n0.727\n0.350\n0.759\n0.585\n0.701\n0.739\n0.526\n2.231\n0.409\n0.830\n0.407\n2.292\n0.724\n0.853\nMIRACLE\n0.637\n0.271\n0.443\n0.234\n0.878\n1.102\n3.361\n43.583\n3.612\n31.612\n0.487\n2.558\n0.533\n0.556\nMIWAE\n0.569\n0.223\n0.730\n0.535\n0.801\n0.904\n0.620\n3.198\n0.511\n1.398\n0.465\n4.102\n0.653\n0.728\nRemasker\n0.403\n0.108\n0.412\n1.223\n0.488\n0.392\n0.613\n2.959\n0.450\n1.077\n0.397\n2.482\n0.523\n0.531\nNewImp\n0.441\n0.141\n0.360\n0.201\n0.310\n0.221\n0.411\n1.937\n0.283\n0.592\n0.329\n2.273\n0.468\n0.265\nKPI(Ours)\n0.095\n0.032\n0.077\n0.051\n0.085\n0.064\n0.098\n0.436\n0.075\n0.128\n0.062\n0.322\n0.103\n0.11\nTable 7: Imputation performance comparison with missing ratio of 0.3.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nmice\n0.156\n0.043\n0.195\n0.133\n0.219\n0.289\n0.186\n1.393\n0.107\n0.387\n0.134\n1.064\n0.21\n0.28\nmissforest\n0.14\n0.064\n0.189\n0.182\n0.2\n0.292\n0.154\n1.049\n0.131\n0.423\n0.113\n1.085\n0.176\n0.262\nsink\n0.828\n0.475\n0.911\n0.853\n0.904\n1.368\n0.851\n6.014\n0.828\n3.898\n0.774\n7.291\n0.859\n1.313\ntdm\n0.733\n0.506\n0.825\n0.834\n0.809\n1.260\n0.730\n4.796\n0.723\n3.337\n0.571\n5.629\n0.754\n1.240\nCSDI-T\n0.717\n1.905\n0.851\n2.684\n0.826\n3.816\n0.761\n14.942\n0.729\n12.044\n0.574\n19.732\n0.782\n4.093\nMissDiff\n0.718\n1.317\n0.842\n1.656\n0.822\n3.313\n0.751\n13.341\n0.725\n13.806\n0.563\n22.714\n0.759\n4.894\ngain\n0.742\n0.413\n0.780\n0.729\n0.736\n1.041\n0.566\n3.702\n0.460\n1.656\n0.434\n3.637\n0.730\n1.140\nmiracle\n0.951\n0.850\n0.535\n0.371\n0.841\n1.302\n3.036\n54.592\n3.432\n43.764\n0.542\n4.814\n0.582\n0.792\nmiwae\n0.593\n0.273\n0.769\n0.692\n0.818\n1.210\n0.650\n4.974\n0.527\n2.113\n0.480\n5.810\n0.667\n0.955\nremasker\n0.459\n0.134\n0.552\n0.371\n0.541\n0.586\n0.534\n3.949\n0.415\n1.365\n0.349\n2.928\n0.557\n0.724\nNewImp\n0.481\n0.181\n0.472\n0.341\n0.423\n0.443\n0.442\n3.066\n0.321\n1.015\n0.350\n3.666\n0.558\n0.379\nKPI(Ours)\n0.104\n0.028\n0.116\n0.088\n0.122\n0.122\n0.102\n0.712\n0.083\n0.217\n0.069\n0.562\n0.148\n0.215\n17\nTable 8: Imputation performance comparison with missing ratio of 0.4.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.162\n0.03\n0.223\n0.168\n0.233\n0.354\n0.186\n1.824\n0.111\n0.549\n0.15\n1.491\n0.222\n0.329\nMISS.F\n0.156\n0.083\n0.198\n0.226\n0.192\n0.346\n0.154\n1.346\n0.133\n0.595\n0.114\n1.254\n0.177\n0.326\nSinkhorn\n0.837\n0.530\n0.904\n1.008\n0.889\n1.616\n0.847\n7.810\n0.821\n4.948\n0.768\n9.508\n0.860\n1.582\nTDM\n0.737\n0.596\n0.824\n1.037\n0.802\n1.561\n0.733\n6.376\n0.724\n4.367\n0.578\n7.565\n0.758\n1.574\nCSDI-T\n0.735\n1.851\n0.847\n2.694\n0.817\n3.877\n0.757\n14.671\n0.727\n11.771\n0.577\n19.537\n0.786\n4.164\nMissDiff\n0.726\n1.284\n0.837\n1.628\n0.812\n3.019\n0.750\n13.272\n0.723\n12.960\n0.566\n21.728\n0.761\n4.278\nGAIN\n0.712\n0.464\n0.812\n0.960\n0.791\n1.448\n0.665\n6.203\n0.522\n2.708\n0.653\n11.553\n0.826\n1.767\nMiracle\n1.066\n1.401\n0.602\n0.483\n0.826\n1.529\n2.714\n58.421\n3.250\n53.215\n0.595\n7.425\n0.620\n1.027\nMIWAE\n0.629\n0.344\n0.786\n0.856\n0.828\n1.502\n0.670\n6.663\n0.535\n2.842\n0.504\n7.952\n0.683\n1.185\nRemasker\n0.528\n0.182\n1.022\n1.541\n0.636\n0.860\n0.534\n5.653\n0.424\n1.891\n0.368\n4.096\n0.601\n0.926\nNewImp\n0.563\n0.301\n0.553\n0.520\n0.542\n0.743\n0.451\n4.035\n0.351\n1.563\n0.378\n4.989\n1.022\n1.542\nKPI(Ours)\n0.113\n0.039\n0.132\n0.134\n0.144\n0.191\n0.107\n1.022\n0.09\n0.337\n0.084\n1.028\n0.159\n0.303\nC.3\nAdditional overall performance results given different missing mechanisms\nTab. 9 and 10 provide a detailed evaluation of the imputation performance of KPI and various\nbaselines under MAR and MNAR mechanisms, respectively. These missing mechanisms are more\ncomplex and challenging compared to the MCAR setting reported in Tab. 1, but they are also more\nrepresentative of real-world scenarios.\nThe results demonstrate that KPI consistently outperforms the baselines in all settings, achieving\nsuperior performance in terms of both metrics. This consistent superiority across different missing\nmechanisms highlights the effectiveness and robustness of KPI for missing data imputation, making\nit a reliable choice for diverse real-world applications.\nC.4\nAdditional hyperparameter sensitivity results\nFig. 6 presents an extended analysis of hyperparameter sensitivity under higher missing ratios of 0.3\nand 0.4, building upon the scenarios explored in Fig. 4 with missing ratios of 0.1 and 0.2. These\nadditional experiments provide insights into the modelâ€™s behavior under more challenging conditions.\nOverall, the model exhibits greater sensitivity to hyperparameter choices at higher missing ratios. This\nindicates that hyperparameter tuning becomes increasingly important as the missing ratio increases.\nHowever, despite the heightened sensitivity, the trends observed across different missing ratios remain\nconsistent. For instance, the optimal update rate is found to be 0.01 for both CC and CBV across all\nfour missing ratios. This consistency reduces the complexity of tuning the model for each specific\nmissing ratio and implies that the selected hyperparameters for KPI are reliable and robust across a\nrange of missing data scenarios.\n18\nTable 9: Imputation performance comparison under MAR missing mechanism.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.481\n0.109\n0.626\n0.349\n0.839\n0.652\n0.677\n1.113\n0.492\n0.946\n0.604\n2.42\n0.824\n0.775\nMISS.F\n0.718\n0.727\n0.632\n0.404\n0.783\n0.572\n0.58\n1.261\n0.797\n1.517\n0.58\n2.671\n0.709\n0.63\nCSDI-T\n1.094\n5.465\n0.894\n3.212\n0.826\n4.286\n0.707\n15.194\n1.262\n19.116\n0.782\n23.176\n0.815\n4.919\nMissDiff\n1.019\n2.835\n0.888\n2.189\n0.852\n6.008\n0.704\n13.233\n1.219\n22.773\n0.762\n34.125\n0.805\n6.919\ngain\n1.082\n1.187\n0.782\n0.570\n0.700\n0.503\n0.456\n0.609\n0.709\n1.383\n0.552\n1.716\n0.729\n0.672\nMIRACLE\n0.699\n0.510\n0.356\n0.141\n0.710\n0.530\n3.837\n19.874\n4.518\n23.842\n0.605\n1.636\n0.494\n0.385\nMIWAE\n0.747\n0.784\n0.735\n0.517\n0.788\n0.617\n0.474\n0.811\n0.758\n1.674\n0.660\n2.892\n0.629\n0.575\nRemasker\n0.598\n0.689\n1.023\n0.854\n0.467\n0.235\n0.691\n1.149\n0.668\n1.062\n0.557\n1.563\n0.489\n0.397\nSinkhorn\n1.106\n1.319\n0.958\n0.718\n0.923\n0.804\n0.829\n1.350\n1.257\n3.392\n0.904\n3.032\n0.905\n0.912\nTDM\n1.015\n1.313\n0.855\n0.614\n0.823\n0.653\n0.691\n0.995\n1.194\n3.311\n0.752\n2.739\n0.794\n0.776\nNewImp\n0.401\n0.171\n0.232\n0.111\n0.221\n0.070\n0.331\n0.504\n0.462\n0.745\n0.560\n3.330\n0.372\n0.293\nmultikip\n0.367\n0.107\n0.199\n0.121\n0.225\n0.115\n0.345\n0.864\n0.457\n0.68\n0.339\n1.965\n0.362\n0.311\nTable 10: Imputation performance comparison under MNAR missing mechanism.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.696\n0.411\n0.66\n0.384\n0.832\n0.734\n0.708\n1.831\n0.499\n0.98\n0.589\n2.377\n0.807\n0.724\nMISS.F\n0.731\n0.579\n0.697\n0.483\n0.748\n0.631\n0.587\n1.944\n0.726\n1.513\n0.5\n2.291\n0.667\n0.567\nCSDI-T\n0.885\n3.105\n0.885\n2.923\n0.838\n3.922\n0.759\n16.833\n1.016\n14.173\n0.683\n20.330\n0.795\n4.275\nGAIN\n0.846\n0.595\n0.782\n0.545\n0.698\n0.570\n0.529\n1.151\n0.571\n1.305\n0.474\n1.943\n0.730\n0.684\nMIRACLE\n0.655\n0.319\n0.371\n0.163\n0.842\n0.802\n3.725\n27.093\n4.196\n25.052\n0.576\n2.249\n0.518\n0.437\nMIWAE\n0.658\n0.424\n0.735\n0.508\n0.808\n0.731\n0.559\n1.535\n0.628\n1.400\n0.561\n3.318\n0.641\n0.577\nRemasker\n0.481\n0.297\n1.028\n0.886\n0.487\n0.296\n0.660\n1.701\n0.586\n1.059\n0.516\n2.128\n0.519\n0.434\nSinkhorn\n0.967\n0.752\n0.940\n0.698\n0.925\n0.911\n0.854\n2.121\n1.049\n2.906\n0.844\n3.755\n0.880\n0.859\nTDM\n0.858\n0.732\n0.849\n0.620\n0.821\n0.756\n0.724\n1.640\n0.969\n2.713\n0.661\n3.202\n0.772\n0.744\nNewImp\n0.645\n0.461\n0.585\n0.593\n0.562\n0.837\n0.442\n3.945\n0.434\n2.328\n0.441\n7.161\n0.601\n1.102\nmultikip\n0.573\n0.257\n0.271\n0.22\n0.357\n0.352\n0.391\n1.344\n0.423\n0.769\n0.286\n1.967\n0.458\n0.563\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\nMAE\n0.3\n0.4\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\n1.0\nWASS\n0.3\n0.4\n(a) Varying learning rate results on CC\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\n1.0\nMAE\n0.3\n0.4\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\n1.0\n1.5\n2.0\n2.5\nWASS\n0.3\n0.4\n(b) Varying learning rate results on CBV\n64\n128\n256\n512\n1024\nB\n0.3\n0.4\n0.5\n0.6\n0.7\nMAE\n0.3\n0.4\n64\n128\n256\n512\n1024\nB\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nWASS\n0.3\n0.4\n(c) Varying batch size results on CC\n64\n128\n256\n512\n1024\nB\n0.50\n0.75\nMAE\n0.3\n0.4\n64\n128\n256\n512\n1024\nB\n0.50\n0.75\n1.00\nWASS\n0.3\n0.4\n(d) Varying batch size results on CBV\nFigure 6: Varying learning rate and batch size results with missing ratios 0.3 and 0.4.\n19\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaperâ€™s contributions and scope?\nAnswer: [Yes]\nJustification: The claims in the abstract and introduction correctly summarize the theoretical\nand empirical contributions of the paper. They are well-aligned with the scope, methods,\nand results presented in the main text.\nGuidelines:\nâ€¢ The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\nâ€¢ The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\nâ€¢ The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\nâ€¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: There is a separate \"Limitations\" section.\nGuidelines:\nâ€¢ The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\nâ€¢ The authors are encouraged to create a separate \"Limitations\" section in their paper.\nâ€¢ The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\nâ€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\nâ€¢ The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\nâ€¢ The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\nâ€¢ If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\nâ€¢ While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that arenâ€™t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n20\nAnswer: [Yes]\nJustification: In appendix.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include theoretical results.\nâ€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\nâ€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.\nâ€¢ The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\nâ€¢ Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\nâ€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: Yes, it is already provided. We will release our code soon.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include experiments.\nâ€¢ If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\nâ€¢ If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\nâ€¢ Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\nâ€¢ While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\n21\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [No]\nJustification: The code is not yet publicly released at submission time. We plan to make the\ncodebase and data processing scripts publicly available soon.\nGuidelines:\nâ€¢ The answer NA means that paper does not include experiments requiring code.\nâ€¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\nâ€¢ While we encourage the release of code and data, we understand that this might not be\npossible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\nâ€¢ The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\nâ€¢ The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\nâ€¢ The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\nâ€¢ At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\nâ€¢ Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: The experimental setting is detailed. Additional training configurations are\nprovided in Appendix.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include experiments.\nâ€¢ The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\nâ€¢ The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: Yes.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include experiments.\nâ€¢ The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\nâ€¢ The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n22\nâ€¢ The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\nâ€¢ The assumptions made should be given (e.g., Normally distributed errors).\nâ€¢ It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\nâ€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\nâ€¢ For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\nâ€¢ If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: The paper provides sufficient details on computational resources in Appendix.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include experiments.\nâ€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\nâ€¢ The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\nâ€¢ The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidnâ€™t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: This work complies fully with the NeurIPS Code of Ethics. It uses only public\ndatasets and poses no foreseeable ethical risks.\nGuidelines:\nâ€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\nâ€¢ If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\nâ€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: There is no societal impact of the work performed.\nGuidelines:\nâ€¢ The answer NA means that there is no societal impact of the work performed.\nâ€¢ If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n23\nâ€¢ Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\nâ€¢ The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\nâ€¢ The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\nâ€¢ If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This work poses no such risks.\nGuidelines:\nâ€¢ The answer NA means that the paper poses no such risks.\nâ€¢ Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\nâ€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\nâ€¢ We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: This work does not use existing assets.\nGuidelines:\nâ€¢ The answer NA means that the paper does not use existing assets.\nâ€¢ The authors should cite the original paper that produced the code package or dataset.\nâ€¢ The authors should state which version of the asset is used and, if possible, include a\nURL.\nâ€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.\nâ€¢ For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\nâ€¢ If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n24\nâ€¢ For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\nâ€¢ If this information is not available online, the authors are encouraged to reach out to\nthe assetâ€™s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: This work does not release any new asset.\nGuidelines:\nâ€¢ The answer NA means that the paper does not release new assets.\nâ€¢ Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\nâ€¢ The paper should discuss whether and how consent was obtained from people whose\nasset is used.\nâ€¢ At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This study does not involve any human participants or crowdsourcing tasks.\nGuidelines:\nâ€¢ The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\nâ€¢ Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\nâ€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: No human subjects or crowdsourced data were involved in this study; all\nexperiments used public datasets. IRB approval is thus not applicable.\nGuidelines:\nâ€¢ The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\nâ€¢ Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\nâ€¢ We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n25\nâ€¢ For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: This work does not involve any LLMs in its core algorithmic design or\nempirical methodology.\nGuidelines:\nâ€¢ The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\nâ€¢ Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n26\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Inverse Methods for Missing Data Imputation\nHao Wang1âˆ—Zhengnan Li1âˆ—\nZhichao Chen2\nXu Chen3â€ \nShuting He4\nGuangyi Liu5\nHaoxuan Li6â€ \nZhouchen Lin2,7,8,â€ \n1Xiaohongshu Inc.\n2State Key Lab of General AI, School of Intelligence Science and Technology, Peking University\n3Gaoling School of Artificial Intelligence, Renmin University of China\n4School of Computing and Artificial Intelligence, Shanghai University of Finance and Economics\n5Department of Control Science and Engineering, Zhejiang University\n6Center for Data Science, Peking University\n7Institute for Artificial Intelligence, Peking University\n8Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China\nAbstract\nIterative imputation is a prevalent method for completing missing data, which\ninvolves iteratively imputing each feature by treating it as a target variable and\npredicting its missing values using the remaining features. However, existing itera-\ntive imputation methods exhibit two critical defects: (1) model misspecification,\nwhere a uniform parametric form of model is applied across different features,\nconflicting with heterogeneous data generation processes; (2) underuse of oracle\nfeatures, where all features are treated as potentially missing, neglecting the valu-\nable information in fully observed features. In this work, we propose kernel point\nimputation (KPI), a bi-level optimization framework designed to address these\nissues. The inner-level optimization optimizes the model form for each feature\nin a reproducing kernel Hilbert space, mitigating model misspecification. The\nouter-level optimization leverages oracle features as supervision signals to refine\nimputations. Extensive experiments on real-world datasets demonstrate that KPI\nconsistently outperforms state-of-the-art imputation methods. Code is available at\nhttps://github.com/FMLYD/kpi.git.\n1\nIntroduction\nMissing data is a ubiquitous challenge in real-world data collection and analytics [24, 45]. For\nexample, in manufacturing, temperature sensors may fail due to overheating or electrical disruptions,\ncompromising data integrity and impeding analytical workflows [1]. Similarly, equipment-monitoring\nsystems can experience lost connectivity in electrical sensors, impeding fault detection and intro-\nducing security risks [36]. These issues highlight the importance of missing data imputation (MDI)\ntechniques, which aim to recover missing data using observed ones, thereby enhancing the integrity\nof collected datasets and the reliability of data-driven applications.\nExisting MDI methods can be broadly categorized as discriminative or generative [5]. On the one\nhand, discriminative methods, such as statistical imputation (e.g., mean and median imputation [44])\nand iterative imputation (which iteratively predicts missing values using univariate models [26, 29]),\nhave been well developed. On the other hand, generative methods have recently attracted attention\nfor their capacity to model complex data structures [1]. However, they often encounter training\nchallenges [18, 25] or rely on strong data assumptions [42, 27]. Empirically, generative methods may\nâˆ—This work was done in the internship at Xiaohongshu Inc. Both authors have equal contribution.\nâ€ Corresponding author.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n"
    },
    {
      "page_number": 2,
      "text": "frequently be outperformed by discriminative methods [45, 23]. Therefore, discriminative methods\nremain the preferred choice for MDI in practice [39].\nAmong discriminative methods, iterative imputation is widely adopted due to its straightforward\nimplementation and strong empirical performance. It specifies univariate models for each feature\nconditioned on the rest and iteratively imputes missing values until convergence [26]. However, this\napproach has two limitations. First, it assumes that all features contain missing values, neglecting the\nutility of fully observed features, known as oracle features, which can provide strong supervisory\nsignals for imputation. Second, it applies a fixed-form parametric model to all features, which risks\nmodel misspecification, as different features often exhibit heterogeneous dependencies that cannot be\nadequately captured by a fixed-form parametric model [5].\nTo counteract the two limitations, we reformulate iterative imputation as a bi-level optimization prob-\nlem. The inner-level optimization adaptively selects functional forms from reproducing kernel Hilbert\nspaces (RKHS) for each feature, reducing model misspecification. The outer-level optimization aligns\nthe imputed values with oracle features, leveraging them as direct supervision signals. Subsequently,\nwe propose kernel point imputation (KPI), which expresses the optimal model as a linear combination\nof kernel functions, enabling efficient solution via stochastic gradient descent. Furthermore, we\ndesign an adaptive kernel ensemble strategy to dynamically combine kernels, thereby enhancing\nmodel expressiveness and alleviating hyperparameter selection challenge amidst incomplete data.\nContributions. The key contributions of this study are summarized as follows:\nâ€¢ We introduce a bi-level optimization framework for MDI which optimizes model form for each\nfeature within a RKHS to address model misspecification and exploits oracle features as supervision\nto refine imputation results.\nâ€¢ We develop the KPI algorithm, which solves the bi-level optimization problem via stochastic\ngradient descent. Additionally, we develop a kernel ensemble method to counteract the difficulty of\nkernel parameter selection amidst missing data.\nâ€¢ We conduct extensive experiments to demonstrate the superiority of KPI over existing MDI methods\nmethods and to highlight the utility of oracle features in enhancing imputation accuracy.\n2\nPreliminaries\nAs a preliminary note, this study aims to impute missing values as an end goalâ€”specifically, to\nestimate their most probable values. We are not considering imputation as a means to obtain input\nfor some downstream tasks [5], such as training regression models for label prediction [16, 14] or\npseudo-labeling for unbiased learning [10]. Methods in these scenarios often require joint training to\noptimize specific objectives [16]. In this work, we concentrate on the MDI problem.\nSuppose X(id) âˆˆRNÃ—D is the ideally complete data matrix with N samples and D features. The\npresence of missing entries in X(id) is indicated by a binary matrix M âˆˆ{0, 1}NÃ—D, where each\nentry Mn,d is set to 1 if the corresponding entry X(id)\nn,d is missing, and 0 otherwise. Consequently,\nthe observed dataset X(obs) can be derived using the Hadamard product:\nX(obs) := X(id) âŠ™(1 âˆ’M) + nan âŠ™M.\n(1)\nThe goal of MDI is to recover the missing entries by constructing an imputation matrix X(imp) âˆˆ\nRNÃ—D that closely approximates X(id). Different imputation methods vary in how they generate\nX(imp) from X(obs) and M.\nOne prevalent approach is the iterative imputation method, which iteratively imputes each feature by\ntreating it as a target variable and predicting its missing values using the remaining features as inputs.\nSpecifically, in the training stage, let the d-th feature as the target feature, denoted as Y(obs)\nd\n= X(obs)\nÂ·,d\n,\nthe method fits an imputation model fÎ¸ with parameters Î¸ that learns the relationship between the\ntarget feature and the remaining features:\nmin\nÎ¸\n\r\r\rY(obs)\nd\nâˆ’fÎ¸(X(obs)\nÂ·,âˆ’d )\n\r\r\r\n2\n2 ,\n(2)\nwhere X(obs)\nÂ·,âˆ’d denotes the matrix X(obs) with the d-th column removed. The method cycles the target\nfeature for D times, training univariate imputation models for all features. In the inference stage,\n2\n"
    },
    {
      "page_number": 3,
      "text": "0.0\n2.5\n5.0\n7.5\n10.0\nInput\n0\n2\n4\nOutput\n0.0\n2.5\n5.0\n7.5\n10.0\nInput\n0\n2\n4\nOutput\n(a) The implication of model misspecification.\nâœ“\n6\n8.2\n6\n4\n2.2\nâœ˜\n3\n4.2\n3\n2.2\n-\nâœ˜\n2.8\n6\n9\n5.8\n-\nâœ˜\n3.2\n4\n2.8\n1.8\n-\nâœ“\n5.6\n8\n6.4\n4.2\n2\nâœ“\n3\n4.2\n2.8\n2\n1\nâœ˜\n3\n4\n3.2\n2\n-\nâœ“\n3\n5.8\n9\n6\n3\nX4\nX3\nX2\nX1\nY\nâœ“\n6\n8.2\n6\n4\n2.2\nâœ˜\n3\n4.2\n3\n2.2\n-\nâœ˜\n2.8\n6\n9\n5.8\n-\nâœ˜\n3.2\n4\n2.8\n1.8\n-\nâœ“\n5.6\n8\n6.4\n4.2\n2\nâœ˜\n3\n4.2\n2.8\n2\n-\nâœ˜\n3\n4\n3.2\n2\n-\nâœ˜\n3\n5.8\n9\n6\n-\nX4\nX3\nX2\nX1\nY\n(b) The implication of overlooking oracle features.\nFigure 1: Case study illustrating the limitations of iterative imputation. In panel (a), circular and\ncross markers indicate observed and missing values, respectively, while lines represent imputation\nmodel outputs. In panel (b), â€œâœ“â€ denotes whether a sample can be used for training the imputation\nmodel for Y ; dark areas indicate missing indices in Y at missing ratios of 50% (left) and 75% (right).\nimputation proceeds iteratively: for each target feature, its missing values are estimated using the\ncorresponding univariate model, incorporating previously imputed values of other features. The\nimputed columns are concatenated to construct the imputation matrix X(imp).\n3\nMethodology\n3.1\nMotivation\nIterative imputation approaches MDI to a canonical regression problem, estimating each feature\nusing the remaining features. In this section, we demonstrate that this approach leads to model\nmisspecification and underuse of oracle features, thereby degrading imputation performance.\nThe first limitation is the risk of model misspecification. Iterative imputation methods typically\nemploy a single predefined parametric form for all features, such as linear models [26] or decision\ntrees [29]. However, real-world data often exhibit heterogeneous dependencies among features that\ncannot be effectively captured by a single parametric model [5]. For instance, temperature sensor\ndata in a manufacturing process may have a linear relationship with pressure, while vibration data\nmay display a nonlinear relationship with pressure. Imposing a uniform parametric form thus fails to\naccommodate these diverse dependencies, leading to suboptimal imputation performance.\nThe second limitation is the underuse of oracle features. Iterative methods, by treating all features\nas equally prone to missingness, suffer from limited training data under high missing ratios. Oracle\nfeatures, which have minimal missing values, can provide critical supervision for imputing other\nvariables. For instance, in health records, demographic data often serves as reliable oracle features,\nwhile in industrial settings, catastrophic data can fulfill this role. However, iterative approaches\nneglect these reliable features, thereby limiting overall imputation quality.\nCase study. To illustrate the above limitations of the existing iterative imputation method, a case study\nis conducted. Fig. 1 (a) demonstrates how a fixed model form can lead to model misspecification. In\nthe left panel, a linear model accurately captures the linear feature (red) but fails to fit the nonlinear\nfeatures (blue and green). Conversely, the right panel shows that a nonlinear model fits the sine\nfeature well but overfits the other features. Therefore, a fixed parametric form risks misspecification\nand thereby hampers imputation performance. Fig. 1 (b) illustrates the impact of overlooking oracle\nfeatures. To impute missing values in the target column Y , the iterative method constructs a univariate\nmodel using X1, . . . , X4 as inputs. The model is trained using solely samples with non-missing Y\nvalues. With high missing ratios, only a few samples are usable for training (two in the right panel),\nwhich is insufficient to learn a robust model. In contrast, the four fully observed oracle features\n(X1,..., X4) are overlooked, forfeiting an opportunity to enhance imputation accuracy.\nThese limitations underscore the need for an improved iterative approach that effectively leverages\noracle features and mitigates model misspecification for improved imputation performance. In\nparticular, there are three key questions to be explored: (1) How to adaptively elect different model\nforms to each feature to reduce misspecification? (2) How to incorporate oracle features in imputation?\n(3) Do model-form adaptation and oracle features indeed boost imputation accuracy?\n3\n"
    },
    {
      "page_number": 4,
      "text": "3.2\nA bi-level optimization framework for iterative imputation\nWe propose a novel bi-level optimization formulation to overcome the limitations of iterative methods.\nThis framework customizes model forms for each feature within a reproducing kernel Hilbert space\n(RKHS) and integrates oracle features as supervision signals.\nBased on the iterative imputation in (2), to mitigate model misspecification, we replace the single\nparametric form of the standard iterative approach, expressed as the fÎ¸ in (2), with a flexible form in\nRKHS. Given the d-th feature as the target: Y(obs) = X(obs)\nÂ·,d\n, we reformulate the imputation task as:\nf âˆ—= arg min\nfâˆˆH\n\r\r\rY(obs) âˆ’f(X(obs)\nÂ·,âˆ’d )\n\r\r\r\n2\n2 + Î»âˆ¥fâˆ¥2\nH,\n(3)\nwhere f âˆ—is the optimal model for that feature. The capacity of RKHS ensures that f âˆ—can capture\neffectively heterogeneous feature relationships, reducing the risk of misspecification.\nTo exploit oracle features, a natural approach is to incorporate them as supervision signals that guide\nimputation. Suppose Y(obs) is an oracle feature and f âˆ—is the associated optimum estimator; we\nupdate the imputed values as:\nmin\nX(miss)\n\r\r\rY(obs) âˆ’f âˆ—(X(miss)\nÂ·,âˆ’d , X(obs)\nÂ·,âˆ’d )\n\r\r\r\n2\n2 .\n(4)\nThis approach is based on a perhaps surprising point-of-view: if X(miss) is well-imputed, applying\nf âˆ—should yield outputs consistent with the ground truth. Otherwise, the imputations of X(miss)\ndeviate from the underlying relationship captured by f âˆ—. Thus, by adjusting X(miss) to minimize this\ndiscrepancy, the imputation process self-corrects, effectively using oracle features as a supervision\nmechanism.\nCombining (3) and (4) yields our bi-level optimization framework for MDI. Given the d-th feature as\nthe target, i.e., Y(obs) = X(obs)\nd\n, X = (X(miss)\nÂ·,âˆ’d , X(obs)\nÂ·,âˆ’d ), the optimization problem is formulated as:\nmin\nX(miss) min\nfâˆˆH\n\r\r\rY(obs) âˆ’f(X)\n\r\r\r\n2\n2 + Î»âˆ¥fâˆ¥2\nH.\n(5)\nSimilar to the standard iterative method, each featureâ€”both oracle features and those with missing\nvalues-is iteratively treated as the target feature Y(obs). In the inner optimization, the optimal\nfunction f is selected within the RKHS, thereby mitigating model misspecification. In the outer\noptimization, the imputed values are refined, effectively incorporating oracle features as supervision\nsignals. Therefore, this bi-level optimization framework provides a principled approach to MDI,\naddressing the limitations of iterative methods.\n3.3\nKernel function, universal property and learning objective\nTo solve the inner loop in (5), we approximate the optimum function f âˆ—by leveraging Gaussian\nkernels in the RKHS. We start by clarifying key kernel properties in Definition 3.1 and 3.2.\nDefinition 3.1 (Kernel function). Let X be a non-empty set. A function K : X Ã— X â†’R is a\nkernel function if there exists a Hilbert space H and a feature map Ïˆ : X â†’H such that âˆ€x, xâ€² âˆˆX,\nK(x, xâ€²) := âŸ¨Ïˆ(x), Ïˆ(xâ€²)âŸ©H .\nDefinition 3.2 (Universal kernel). For X compact Hausdorff, A universal kernel ensures that any\ncontinuous function e : X â†’R can be approximated arbitrarily well within RKHS H. Specifically,\nfor any Ïµ > 0, there exists f âˆˆH such that: supxâˆˆX |f(x) âˆ’e(x)| â‰¤Ïµ.\nGaussian kernel is a typical kernel function formulated as:\nK(x, xâ€²) = exp\n \nâˆ’âˆ¥x âˆ’xâ€²âˆ¥2\n2Ïƒ2\n!\n,\nwhich satisfies the universal property in Definition 3.2 [28]. It implies that by using the Gaussian\nkernel, the associated RKHS H = span{K(Â·, x) | x âˆˆX} admits uniform approximation of any\ncontinuous function.\n4\n"
    },
    {
      "page_number": 5,
      "text": "2\n2\n3.2\n1\n4\n4\n6\n2\n6\n6\n9\n3\n4\n4.2\n6\n1.8\n2\n2\n3\n0.8\n2\n2\n3.2\n1\n4\n4\n6\n2\n4\n4.2\n6\n1.8\n2\n2.2\n3\n0.8\nğ—!, ğ˜!\nğ—\", ğ˜\"\nğ—($%&)\nForward pass\nBackward pass\nMissing indices\nâˆ‡ğ˜!\"ğ’«\nâˆ‡ğ—!#\n\" ğ’«\nâˆ‡ğ—!$\n\" ğ’«\nâˆ‡ğ—!!\n\" ğ’«\nâˆ‡ğ˜$\"ğ’«\nâˆ‡ğ—$#\n\" ğ’«\nâˆ‡ğ—$$\n\" ğ’«\nâˆ‡ğ—!!\n\" ğ’«\nâˆ‡ğ˜!%ğ’«\nâˆ‡ğ—!#\n% ğ’«\nâˆ‡ğ—!$\n% ğ’«\nâˆ‡ğ—!!\n% ğ’«\nâˆ‡ğ˜$%ğ’«\nâˆ‡ğ—$#\n% ğ’«\nâˆ‡ğ—$$\n% ğ’«\nâˆ‡ğ—!!\n% ğ’«\n0\nâˆ‡ğ—!#\n% ğ’«\n0\nâˆ‡ğ—!!\n% ğ’«\n0\n0\n0\nâˆ‡ğ—!!\n% ğ’«\n0\n0\nâˆ‡ğ—!$\n\" ğ’«\n0\n0\n0\n0\n0\nOracle features\nSample\nUpdate imputation\n2\nCalculate ğ’«\n0.1\n0.4\n0.1\n0.2\n0.2\nğš«\nâˆ‡ğš«ğ’«\nGradient \nmasking\nFigure 2: Visualization of the workflow of KPI, where the dataset contains 5 samples and 4 features.\nThe sampling batch size is set to 2. The last column is the oracle feature without missing values.\nLemma 3.3 (Representer theorem). Suppose h(âˆ¥fâˆ¥) : R+ â†’R is a non-decreasing function.\nThe minimizer of an empirical risk functional regularized by h(âˆ¥fâˆ¥) admits the form: f âˆ—(Â·) =\nPn\ni=1 Î±iK(Â·, xi) where Î± = (Î±1, . . . , Î±n)âŠ¤and K is the associated kernel function.\nLemma 3.4. Let Ys, Yt âˆˆRBÃ—1 be the target feature and Xs, Xt be the corresponding input\nfeatures; Suppose f âˆ—is the optimal model minimizing the empirical risk in the inner optimization of\n(5), its output on Xt is given by f âˆ—(Xt) = KXtXs Â· Î±, where Î± = (K + Î»I)âˆ’1Ys; KXtXs is the\nkernel matrix computed with Xt and Xs.\nSince H is potentially infinite-dimensional, directly identifying f âˆ—is infeasible. Nevertheless, the\nrepresenter theorem in Lemma 3.3 provides a finite approximation to f âˆ—. Accordingly, by sampling\ntwo batches of dataâ€”target features (Ys, Yt) and input features (Xs, Xt)â€”Lemma 3.4 yields that\nthe output of f âˆ—at Xt can be represented as:\nf âˆ—(Xt) = KXtXs(KXsXs + Î»I)âˆ’1Ys,\n(6)\nwhich analytically expresses the output of the optimum model f âˆ—as a linear combination of kernel\nfunctions. It enables adaptively selecting the optimum model for each feature, and simplifies the\nbi-level optimization problem in (5) to a differentiable loss function:\nmin\nXs,Xt\n\r\rYt âˆ’KXtXs(KXsXs + Î»I)âˆ’1Ys)\n\r\r2\n2 ,\n(7)\nFurthermore, selecting kernel hyperparameters (e.g., Gaussian kernel variance) can be challenging\namidst missing data. To alliviate this problem, we introduce multiple kernels with distinct parameters\nand learn to ensemble them adaptively. Suppose K1, K2, ..., KE are E kernel matrices, each with a\ndifferent configuration. We define a learnable simplex vector âˆ†âˆˆRK, and construct the ensembled\nkernel as Kâˆ†= K1âˆ†1 + ... + KEâˆ†E. Putting together, the final objective becomes:\nP =\n\r\rYt âˆ’Kâˆ†\nXtXs(Kâˆ†\nXsXs + Î»I)âˆ’1Ys)\n\r\r2\n2 .\n(8)\n3.4\nOverall workflow\nWhile the learning objective is well defined, its role in actual imputation remains unclear. To this end,\nwe propose the kernel point imputation (KPI) method, which iteratively minimizes the objective (8)\nto refine missing value imputations. The core procedure is shown in Fig. 2 and detailed as follows.\nInitialization. Given the incomplete dataset X(obs), we initialize missing entries using the mean\nof observed steps, obtaining an initial imputation matrix Ximp. The imputed values are treated as\nlearnable parameters, and their gradients are tracked throughout training.\nForward Pass. Two batches are sampled from the imputation matrix. In each iteration, a column\nis randomly chosen as the target feature (Ys, Yt âˆˆRBÃ—1), with the remaining columns as input\nfeatures (Xs, Xt âˆˆRBÃ—(Dâˆ’1)), where B represents batch size, s and t differentiates different batches.\nThe objective P is computed following (8).\n5\n"
    },
    {
      "page_number": 6,
      "text": "Table 1: Imputation performance in terms of MSE and MAE on 7 datasets.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMean\n0.742\n0.452\n0.837\n0.789\n0.829\n1.165\n0.754\n4.145\n0.740\n2.841\n0.589\n4.682\n0.764\n1.121\nMode\n0.948\n0.770\n0.935\n1.159\n1.026\n1.749\n0.925\n7.741\n1.254\n7.832\n0.593\n6.240\n0.823\n1.372\nMedian\n0.706\n0.469\n0.811\n0.884\n0.820\n1.165\n0.713\n4.356\n0.698\n3.029\n0.500\n5.066\n0.756\n1.123\nMICE\n0.580\n0.127\n0.745\n0.474\n0.856\n1.021\n0.733\n4.539\n0.417\n1.312\n0.536\n3.415\n0.824\n0.971\nMiss.F\n0.560\n0.241\n0.732\n0.650\n0.764\n0.994\n0.593\n3.277\n0.526\n1.497\n0.436\n3.202\n0.686\n0.898\nSinkhorn\n0.835\n0.466\n0.906\n0.796\n0.898\n1.225\n0.848\n4.945\n0.827\n3.233\n0.775\n6.114\n0.857\n1.170\nTDM\n0.730\n0.487\n0.819\n0.769\n0.799\n1.113\n0.726\n3.965\n0.722\n2.792\n0.570\n4.756\n0.752\n1.098\nCSDI-T\n0.726\n1.870\n0.849\n2.683\n0.821\n3.802\n0.761\n15.493\n0.731\n12.291\n0.575\n19.919\n0.780\n4.084\nMissDiff\n0.719\n1.332\n0.840\n1.699\n0.816\n3.523\n0.749\n13.432\n0.728\n14.462\n0.564\n23.320\n0.758\n5.184\nGAIN\n0.730\n0.396\n0.777\n0.688\n0.729\n0.942\n0.572\n3.318\n0.448\n1.413\n0.476\n4.669\n0.754\n1.095\nMIRACLE\n0.795\n0.674\n0.487\n0.305\n0.831\n1.154\n3.208\n45.816\n3.518\n36.784\n0.521\n3.975\n0.555\n0.685\nMIWAE\n0.582\n0.266\n0.746\n0.630\n0.807\n1.071\n0.636\n4.118\n0.525\n1.804\n0.475\n4.977\n0.657\n0.844\nRemasker\n0.439\n0.131\n0.767\n0.750\n0.528\n0.522\n0.599\n3.584\n0.447\n1.268\n0.401\n2.811\n0.546\n0.636\nNewImp\n0.465\n0.177\n0.412\n0.292\n0.405\n0.401\n0.431\n2.495\n0.320\n0.8575\n0.332\n2.992\n0.497\n0.692\nkpi(Ours)\n0.397\n0.121\n0.347\n0.284\n0.402\n0.394\n0.400\n2.387\n0.319\n0.747\n0.264\n2.131\n0.491\n0.685\nNote: Each entry represents the average results at four missing ratios: 0.1, 0.2, 0.3, and 0.4. The best and\nsecond-best results are bolded and underlined, respectively.\nBackward Pass. The gradients of P with respect to Xs, Xt and âˆ†are calculated using automatic\ndifferentiation. The imputed values in Xs and Xt as well as âˆ†are then updated using gradient\ndescent with an update rate Î·:\nXs â†Xs âˆ’Î·âˆ‡XsP âŠ™Ms,\nXt â†Xt âˆ’Î·âˆ‡XtP âŠ™Mt,\nâˆ†â†âˆ†âˆ’Î·âˆ‡âˆ†P,\n(9)\nwhere only the missing values (with M = 1) are updated, while the observed values (with M = 0)\nremain unchanged during this process. Moreover, the gradient of the matrix inverse term is stopped\nfor numerical stability. KPI iteratively executes the forward and backward passes sampling different\nbatches until hitting the early-stopping criteria on the validation dataset. In this process, each feature\nis iteratively treated as the target feature while the remaining features are treated as the input features.\nThis ensures that all featuresâ€”including oracle featuresâ€”are fully exploited as supervision signals.\n4\nEmpirical Investigation\n4.1\nExperimental setup\nâ€¢ Datasets: The empirical study is performed on public tabular datasets from [1], incuding Blood\nTransfusion(BT) Concrete Compression (CC) Connectionist Bench Vowel (CBV) Ionosphere (IS)\nParkinsons (PS) Qsar Biodegradation (QB) Wine Quality White (QWQ). To simulate missing data\nscenarios, we employ a mask matrix generated by a Bernoulli random variable with a preset mean.\nâ€¢ Baselines: The performance of KPI is compared against various imputation methods, including\niterative imputers (MICE [26], Miss.F. [29]), and generative models (GAIN [42], MIWAE [19],\nMiss.D [24], CSDI-T [31], ReMasker [2], and NewImp [1]). We also assess methods that do not\nconform to these categories, such as MIRACLE [7], Sinkhorn [23], and TDM [45].\nâ€¢ Implementation details: To ensure convergence, we cap the number of iterations at 500 and adopt\nan early stopping criterion based on validation performance, with a patience of 10 epochs. The\nAdam optimizer is used for training [6]. Key hyperparameters, namely Î· and B, are determined\nby allocating 5% of the training data for validation and finetuning over [0.0001, 0.01] for Î· and\n6\n"
    },
    {
      "page_number": 7,
      "text": "0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMAE\nMISS.F\nKIP\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\n0.2\nWASS\nMISS.F\nKIP\n(a) The results on the CC dataset.\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMAE\nMISS.F\nKIP\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\nWASS\nMISS.F\nKIP\n(b) The results on the CBV dataset.\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.4\n0.5\n0.6\nMAE\nMISS.F\nKIP\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.3\n0.4\n0.5\n0.6\nWASS\nMISS.F\nKIP\n(c) The results on the IS dataset.\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.2\n0.3\n0.4\nMAE\nMISS.F\nKIP\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of oracle features\n0.1\n0.2\n0.3\nWASS\nMISS.F\nKIP\n(d) The results on the QB dataset.\nFigure 3: The performance of Miss.F and KPI given varying ratios of oracle features.\n[64, 512] for B. Performance is assessed using modified mean absolute error (MAE) and mean\nsquared error (MSE), focusing on the imputed values at missing entries, following [45, 5]. In\naddition, we report the distribution discrepancy (WASS), measured as the Wasserstein distance [5].\nThe experiments are performed on a platform with two Intel(R) Xeon(R) Platinum 8383C CPUs @\n2.70GHz and a NVIDIA GeForce RTX 4090 GPU.\n4.2\nOverall performance\nTab. 1 presents the average imputation results of KPI and baseline methods under missing ratios\npmiss = 0.1, 0.2, 0.3, and 0.4. Key observations are summarized as follows:\nâ€¢ The iterative imputers exhibits promising performance in most cases. For instance, MICE outper-\nforms simple imputers by large margin over most datasets. MissForest employs random forest as\nthe base model, excelling in handling tabular data, which further improves imputation quality.\nâ€¢ The canonical generative imputers [31, 24], originally tailored for time-series data, often falling\nbehind iterative methods. This can be attributed to the implicit maximization of imputation\nentropy in diffusion models, which negatively impacts accuracy [1]. By contrast, recent generative\napproaches such as NewImp and Remasker handle this issue and achieve strong results, obtaining\nthe best results among baseline methods.\nâ€¢ KPI improves the iterative imputers by adaptively selecting the optimal imputer for each feature and\ninvolving oracle features as supervision signals. This strategy consistently improves performance,\nas evidenced by KPI outperforming all baselines across all 7 datasetsâ€”often by a substantial\nmargin, particularly on the CC and QB datasetsâ€”demonstrating strong practical effectiveness.\n4.3\nImpact of oracle features\nIn this section, we assess the impact of using oracle features as supervision signal on imputation\nperformance. Specifically, we simulate based on complete datasets to generate varying ratios of oracle\nfeatures and evaluate the imputation performance. Two models are considered: KPI and another\ncanonical iterative imputer: Miss.F.\nThe results are presented in Fig. 3. As the ratio of oracle features increases, KPI consistently exhibits\nlower imputation error, showcasing the utility of oracle features. In contrast, Miss.F shows little\nimprovement as oracle feature ratio increases. This difference arises because Miss.F only uses oracle\nfeatures as inputs, whereas KPI exploits them as supervision signals to refine the imputation results.\n7\n"
    },
    {
      "page_number": 8,
      "text": "Table 2: Varying kernel number results.\nCC\nE\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\n1\n0.082\n-\n0.058\n-\n0.155\n-\n3\n0.070\n14.6%â†“\n0.055\n5.2%â†“\n0.116\n25.2%â†“\n5\n0.069\n15.9%â†“\n0.046\n20.7%â†“\n0.108\n30.3%â†“\n7\n0.065\n20.7%â†“\n0.039\n32.8%â†“\n0.091\n41.3%â†“\nCBV\nE\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\n1\n0.128\n-\n0.095\n-\n0.233\n-\n3\n0.110\n14.1%â†“\n0.085\n10.5%â†“\n0.226\n3.0%â†“\n5\n0.098\n23.4%â†“\n0.075\n21.1%â†“\n0.216\n7.3%â†“\n7\n0.087\n32.0%â†“\n0.066\n30.5%â†“\n0.205\n12.0%â†“\nBT\nDistances\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\n1\n0.334\n-\n0.109\n-\n0.363\n-\n3\n0.318\n4.8%â†“\n0.101\n7.3%â†“\n0.352\n3.0%â†“\n5\n0.305\n8.7%â†“\n0.096\n11.9%â†“\n0.343\n5.5%â†“\n7\n0.302\n9.6%â†“\n0.089\n18.3%â†“\n0.338\n6.9%â†“\nTable 3: Varying kernel function results.\nCC\nKernel\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\nLinear\n0.099\n-\n0.051\n-\n0.203\n-\nPoly\n0.065\n34.3%â†“\n0.039\n23.5%â†“\n0.091\n55.2%â†“\nLaplacian\n0.068\n31.3%â†“\n0.042\n17.6%â†“\n0.093\n54.2%â†“\nGaussian\n0.076\n23.2%â†“\n0.035\n31.4%â†“\n0.082\n59.6%â†“\nCBV\nKernel\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\nLinear\n0.089\n-\n0.087\n-\n0.219\n-\nPoly\n0.087\n2.2%â†“\n0.088\n1.1% â†‘\n0.214\n2.3%â†“\nLaplacian\n0.083\n6.7%â†“\n0.080\n8.1%â†“\n0.211\n3.7%â†“\nGaussian\n0.087\n2.2%â†“\n0.066\n24.1%â†“\n0.205\n6.4%â†“\nBT\nDistances\nMSE\nâˆ†MSE\nWASS\nâˆ†WASS\nMAE\nâˆ†MAE\nLinear\n0.326\n-\n0.101\n-\n0.359\n-\nPoly\n0.305\n6.4%â†“\n0.090\n10.9%â†“\n0.342\n4.7%â†“\nLaplacian\n0.316\n3.1%â†“\n0.091\n9.9%â†“\n0.346\n3.6%â†“\nGaussian\n0.302\n7.4%â†“\n0.089\n11.9%â†“\n0.338\n5.8%â†“\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\nMAE\n0.1\n0.2\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.0\nWASS\n0.1\n0.2\n(a) Varying learning rate results on CC\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\nMAE\n0.1\n0.2\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.0\n0.5\nWASS\n0.1\n0.2\n(b) Varying learning rate results on CBV\n64\n128\n256\n512\n1024\nB\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMAE\n0.1\n0.2\n64\n128\n256\n512\n1024\nB\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nWASS\n0.1\n0.2\n(c) Varying batch size results on CC\n64\n128\n256\n512\n1024\nB\n0.00\n0.25\n0.50\nMAE\n0.1\n0.2\n64\n128\n256\n512\n1024\nB\n0.00\n0.25\nWASS\n0.1\n0.2\n(d) Varying batch size results on CBV\nFigure 4: Varying learning rate and batch size results with missing ratios 0.1 and 0.2.\n4.4\nImpact of kernel strategy\nIn this section, we analyze the impact of kernel function and kernel amount (E) on imputation\nperformance. The key observations are summarized as follows.\nâ€¢ The multiple kernel ensembling mechanism has a substantial impact. As shown in Tab. 2, increasing\nE from 1 to 7 consistently reduces MSE from 0.082 to 0.065, indicating a relative reduction of\n20.7%. This gain is attributed to the increased flexibility in adaptively selecting kernel parameters,\nallowing KPI to better represent the optimal imputation model for each feature.\nâ€¢ The performance of different kernel functions showcases the importance of kernel universality. The\nlinear kernel, which is not universal and has limited RKHS capacity, yields the worst performance.\nThe polynomial kernel, with a larger RKHS, performs better. The Gaussian kernel exhibits the\nbest overall performance. The superiority is attributed to its universality, i.e., the associated RKHS\nadmits uniform approximation of any continuous function. Such extensive RKHS capacity enables\nKPI to optimize the imputation model for each feature, thereby enhancing imputation performance.\n4.5\nParameter sensitivity analysis\nIn this section, we examine the influence of critical hyperparameters on the performance of KPI\nin Fig. 4. Below are the key observations:\n8\n"
    },
    {
      "page_number": 9,
      "text": "â€¢ The update rate (Î·) plays a pivotal role in controlling the volume of updates to the imputation\nmatrix each epoch. As Î· is reduced from 0.02 to approximately 0.01, both MAE and RMSE\ndecrease, indicating that a smaller Î· enhances update stability. However, further reduction of Î·\nto 0.001 results in increased MAE and RMSE, where the meaningful update direction becomes\novershadowed by noise, preventing model convergence within the allocated epochs.\nâ€¢ The batch size (B) affects the scale of the problem in calculating discrepancies, with sizes ranging\nfrom 64 to 1024 examined. There is a weak yet consistent decrease in MAE and RMSE as the\nbatch size increases to B = 512, enhancing the reliability of estimations. Increasing the batch size\nbeyond this point yields diminishing returns and may lead to unnecessary computational overhead.\n5\nRelated works\nThe pervasive presence of missing data undermines the integrity of collected datasets and the reliability\nof data-driven applications, underscoring the necessity for effective missing data imputation (MDI).\nTo achieve accurate MDI, existing approaches can be broadly categorized into two paradigms:\ndiscriminative and generative, each with distinct advantages and limitations [5, 21].\nThe iterative method [29, 26, 43, 15] is one of the most popular methods in discriminative imputation,\ninitiated from imputation by chained equations (ICE) [26], which employs specific models to estimate\nmissing values for each feature based on the remaining observable features. On the basis of ICE, a\nline of work advocates for employing modern parametric models, such as neural networks [19, 7],\nBayesian models [26] and random forest [29], which enhances the capacity of imputation models\nand thereby accommodating complex missing patterns. In a different line of work, various training\ntechniques are investigated within the paradigm, such as multiple imputation [26], ensemble learning\n[29], and multitask learning [19], which enhances the utility to accommodate diverse contexts.\nWhile this paradigm offers enhanced flexibility and accuracy, it fails to utilize the oracle features\neffectively and risks model misspecification, which can lead to suboptimal imputation results in noisy\nenvironments. Our research advances this methodology by handling the two limitations.\nApart from the iterative methods, there are other notable approaches in the discriminative paradigm.\nThe simple direct paradigm employs elementary statistical measures like mean, median, and mode to\nreplace missing values, offering quick and straightforward solutions. However, this approach lacks\nthe capacity to accommodate complex relationships [17, 20], often producing trivial and inadequate\nimputation results that fail to meet the expectation in practice. Another notable approach is matrix\nfactorization, which decomposes the data matrix into two low-rank matrices, capturing the latent\nstructure of the data for imputation [8, 4]. This method is particularly effective in collaborative\nfiltering and recommendation systems [12, 37]. Recent advances explore a novel methodology based\non distribution discrepancy minimization[45, 23]. This approach builds on the assumption that, under\nthe independent and identically distributed (i.i.d.) condition, any two data batches should share\nthe same underlying distribution, thereby exhibiting minimal discrepancy. Subsequent studies have\nextended this idea by refining discrepancy measures to accommodate different data characteristics\nsuch as neighboring effects [40, 35], noisy observations [36], and temporal dependencies [39].\nThe generative paradigm restates imputation as a conditional generation problem, using advanced\nneural architectures and generative training strategies, such as generative adversarial networks\n[42, 30, 13] and diffusions [31, 41, 1], to approximate data distributions and perform imputation. This\nstrategy incorporates the strengths of generative models, capturing and utilizing complex relationships,\nwhich potentially enhances the imputation quality when ample data is available. However, it also\nbears the defects with generative models, such as the instability associated with adversarial training\nand the operational complexity of diffusions [18, 25], hampering their use in practice.\n6\nConclusion\nIterative imputation methods are widely used for handling missing data, yet existing approaches\nare often limited by model misspecification and underuse of oracle features. To overcome these\nchallenges, we introduce KPI, a bi-level optimization framework which optimizes model form within\nRKHS for each feature, reducing model misspecification, and exploits oracle features as effective\nsupervision. Extensive experiments on real-world datasets demonstrate that KPI achieves superior\nimputation performance and effectively leverages oracle features.\n9\n"
    },
    {
      "page_number": 10,
      "text": "Limitations & future work. In this work, we do not accommodate potential noise in datasets,\nwhich is a prevalent challenge in industrial settings [4, 3]. Future research could incorporate robust\noptimization techniques and truncate outliers in the kernel matrix which has potential to improve\nnoise robustness. Additionally, this work mitigates the difficulty of concise kernel parameter selection\nvia adaptive ensembling, which is an heuristic approach. Subsequent work may explore meta-learning\nstrategies with theoretical guarantees for accurate kernel parameter selection.\nAcknowledgments\nZ. Lin was supported by the NSF China (No. 62276004) and the State Key Laboratory of General\nArtificial Intelligence. H. Li was supported by National Natural Science Foundation of China\n(623B2002).\nReferences\n[1] Zhichao Chen, Haoxuan Li, Fangyikang Wang, Haotian Zhang, Hu Xu, Xiaoyu Jiang, Zhihuan Song, and\nHao Wang. Rethinking the diffusion models for missing data imputation: A gradient flow perspective. In\nProc. Adv. Neural Inf. Process. Syst., 2024.\n[2] Tianyu Du, Luca Melis Melis, and Ting Wang. Remasker: Imputing tabular data with masked autoencoding.\nIn Proc. Int. Conf. Learn. Represent., 2024.\n[3] Xinxin Feng, Haitao Zhang, Can Wang, and Haifeng Zheng. Traffic data recovery from corrupted and\nincomplete observations via spatial-temporal trpca. IEEE Trans. Intell. Transp. Syst., 23(10):17835â€“17848,\n2022.\n[4] Liyang Hu, Yuheng Jia, Weijie Chen, Longhui Wen, and Zhirui Ye. A flexible and robust tensor completion\napproach for traffic data recovery with low-rankness. IEEE Trans. Intell. Transp. Syst., 25(3):2558â€“2572,\n2023.\n[5] Daniel Jarrett, Bogdan Cebere, Tennison Liu, Alicia Curth, and Mihaela van der Schaar. Hyperimpute:\nGeneralized iterative imputation with automatic model selection. In Proc. Int. Conf. Mach. Learn., volume\n162, pages 9916â€“9937, 2022.\n[6] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Int. Conf. Learn.\nRepresent., pages 1â€“9, 2015.\n[7] Trent Kyono, Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. MIRACLE: causally-aware imputation\nvia learning missing data mechanisms. In Proc. Adv. Neural Inf. Process. Syst., pages 23806â€“23817, 2021.\n[8] Daniel Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. Proc. Adv. Neural\nInf. Process. Syst., 13, 2000.\n[9] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. Propensity matters: Measuring\nand enhancing balancing for recommendation. In Proc. Int. Conf. Mach. Learn., volume 202, pages\n20182â€“20194. PMLR, 2023.\n[10] Haoxuan Li, Chunyuan Zheng, Shuyi Wang, Kunhan Wu, Eric Wang, Peng Wu, Zhi Geng, Xu Chen, and\nXiao-Hua Zhou. Relaxing the accurate imputation assumption in doubly robust learning for debiased\ncollaborative filtering. In Proc. Int. Conf. Mach. Learn., volume 235, pages 29448â€“29460, 2024.\n[11] Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, and Xiao-Hua Zhou. Debiased\nrecommendation with noisy feedback. In Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining,\npage 1576â€“1586, 2024.\n[12] Haoxuan Li, Chunyuan Zheng, Yanghao Xiao, Peng Wu, Zhi Geng, Xu Chen, and Peng Cui. Debiased\ncollaborative filtering with kernel-based causal balancing. In Proc. Int. Conf. Learn. Represent., pages 1â€“9,\n2024.\n[13] Haozhe Li, Yilin Liao, Zijian Tian, Zhaoran Liu, Jiaqi Liu, and Xinggao Liu. Bidirectional stackable\nrecurrent generative adversarial imputation network for specific emitter missing data imputation. IEEE\nTrans. Inf. Forensics Security, 19:2967â€“2980, 2024.\n[14] Steven Cheng-Xian Li, Bo Jiang, and Benjamin M. Marlin. Misgan: Learning from incomplete data with\ngenerative adversarial networks. In Proc. Int. Conf. Learn. Represent., 2019.\n10\n"
    },
    {
      "page_number": 11,
      "text": "[15] Jingchen Liu, Andrew Gelman, Jennifer Hill, Yu-Sung Su, and Jonathan Kropko. On the stationary\ndistribution of iterative imputations. Biometrika, 101(1):155â€“173, 2014.\n[16] Qianli Ma, Sen Li, and Garrison W Cottrell. Adversarial joint-learning recurrent neural network for\nincomplete time series classification. IEEE Trans. Pattern Anal. Mach. Intell., 44(4):1765â€“1776, 2020.\n[17] R Malarvizhi and Antony Selvadoss Thanamani. K-nearest neighbor in missing data imputation. Int. J.\nEng. Res. Dev, 5(1):5â€“7, 2012.\n[18] Pierre-Alexandre Mattei and Jes Frellsen. Leveraging the exact likelihood of deep latent variable models.\nIn Proc. Adv. Neural Inf. Process. Syst., pages 3859â€“3870, 2018.\n[19] Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: deep generative modelling and imputation of incomplete\ndata sets. In Proc. Int. Conf. Mach. Learn., volume 97, pages 4413â€“4423, 2019.\n[20] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning\nlarge incomplete matrices. J. Mach. Learn. Res., 11:2287â€“2322, 2010.\n[21] Xiaoye Miao, Yangyang Wu, Lu Chen, Yunjun Gao, and Jianwei Yin. An experimental survey of missing\ndata imputation algorithms. IEEE Trans. Knowl. Data Eng., 35(7):6630â€“6650, 2022.\n[22] Mehryar Mohri. Foundations of machine learning, 2018.\n[23] Boris Muzellec, Julie Josse, Claire Boyer, and Marco Cuturi. Missing data imputation using optimal\ntransport. In Proc. Int. Conf. Mach. Learn., volume 119, pages 7130â€“7140, 2020.\n[24] Yidong Ouyang, Liyan Xie, Chongxuan Li, and Guang Cheng. Missdiff: Training diffusion models on\ntabular data with missing values. arXiv preprint arXiv:2307.00467, 2023.\n[25] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-\nmate inference in deep generative models. In Proc. Int. Conf. Mach. Learn., volume 32, pages 1278â€“1286,\n2014.\n[26] Patrick Royston and Ian R White. Multiple imputation by chained equations (mice): implementation in\nstata. J. Statist. Softw., 45:1â€“20, 2011.\n[27] Indro Spinelli, Simone Scardapane, and Aurelio Uncini. Missing data imputation with adversarially-trained\ngraph convolutional networks. Neural Netw., 129:249â€“260, 2020.\n[28] Bharath K. Sriperumbudur, Kenji Fukumizu, and Gert R. G. Lanckriet. On the relation between universality,\ncharacteristic kernels and RKHS embedding of measures. In Proc. Int. Conf. Artif. Intell. Statist., volume 9\nof JMLR Proceedings, pages 773â€“780. JMLR.org, 2010.\n[29] Daniel J Stekhoven and Peter BÃ¼hlmann. Missforestâ€”non-parametric missing value imputation for\nmixed-type data. Bioinformatics, 28(1):112â€“118, 2012.\n[30] Ziyue Sun, Haozhe Li, Wenhai Wang, Jiaqi Liu, and Xinggao Liu. DTIN: dual transformer-based\nimputation nets for multivariate time series emitter missing data. Knowl. Based Syst., 284:111270, 2024.\n[31] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion\nmodels for probabilistic time series imputation. Proc. Adv. Neural Inf. Process. Syst., 34:24804â€“24816,\n2021.\n[32] Hao Wang, Zhichao Chen, Jiajun Fan, Haoxuan Li, Tianqiao Liu, Weiming Liu, Quanyu Dai, Yichao Wang,\nZhenhua Dong, and Ruiming Tang. Optimal transport for treatment effect estimation. In Proc. Adv. Neural\nInf. Process. Syst., volume 36, pages 5404â€“5418, 2023.\n[33] Hao Wang, Zhichao Chen, Zhaoran Liu, Xu Chen, Haoxuan Li, and Zhouchen Lin. Proximity matters:\nLocal proximity enhanced balancing for treatment effect estimation. Proc. ACM SIGKDD Int. Conf. Knowl.\nDiscovery Data Mining, 2025.\n[34] Hao Wang, Zhichao Chen, Zhaoran Liu, Haozhe Li, Degui Yang, Xinggao Liu, and Haoxuan Li. Entire\nspace counterfactual learning for reliable content recommendations. IEEE Trans. Inf. Forensics Security,\n20:1755â€“1764, 2025.\n[35] Hao Wang, Zhichao Chen, Zhaoran Liu, Licheng Pan, Hu Xu, Yilin Liao, Haozhe Li, and Xinggao Liu.\nSpot-i: Similarity preserved optimal transport for industrial iot data imputation. IEEE Trans. Ind. Informat.,\n20(12):14421â€“14429, 2024.\n11\n"
    },
    {
      "page_number": 12,
      "text": "[36] Hao Wang, Zhichao Chen, Yuan Shen, Hui Zheng, Degui Yang, Dangjun Zhao, and Buge Liang. Unbiased\nrecommender learning from implicit feedback via weakly supervised learning. In IEEE Trans. Neural\nNetw. Learn. Syst., 2025.\n[37] Hao Wang, Zhichao Chen, Haotian Wang, Yanchao Tan, Licheng Pan, Tianqiao Liu, Xu Chen, Haoxuan\nLi, and Zhouchen Lin. Unbiased recommender learning from implicit feedback via weakly supervised\nlearning. In Proc. Int. Conf. Mach. Learn., 2025.\n[38] Hao Wang, Zhichao Chen, Honglei Zhang, Zhengnan Li, Licheng Pan, Haoxuan Li, and Mingming Gong.\nDebiased recommendation via wasserstein causal balancing. ACM T. Inform. Syst., 2025.\n[39] Hao Wang, Zhengnan Li, Haoxuan Li, Xu Chen, Mingming Gong, Bin Chen, and Zhichao Chen. Optimal\ntransport for time series imputation. In Proc. Int. Conf. Learn. Represent., pages 1â€“9, 2025.\n[40] Hao Wang, Xinggao Liu, Zhaoran Liu, Haozhe Li, Yilin Liao, Yuxin Huang, and Zhichao Chen. Lspt-d:\nLocal similarity preserved transport for direct industrial data imputation. IEEE Trans. Autom. Sci. Eng.,\n22:9438â€“9448, 2025.\n[41] Hu Xu, Zhaoran Liu, Hao Wang, Changdi Li, Yunlong Niu, Wenhai Wang, and Xinggao Liu. Denoising\ndiffusion straightforward models for energy conversion monitoring data imputation. IEEE Trans. Ind.\nInformat., 20(10):11987â€“11997, 2024.\n[42] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: missing data imputation using generative\nadversarial nets. In Proc. Int. Conf. Mach. Learn., volume 80, pages 5675â€“5684, 2018.\n[43] Aoqian Zhang, Shaoxu Song, Yu Sun, and Jianmin Wang. Learning individual models for imputation. In\nProc. IEEE Int. Conf. Data Eng., pages 160â€“171. IEEE, 2019.\n[44] Zhongheng Zhang. Missing data imputation: focusing on single imputation. Ann. Transl. Med, 4(1):9,\n2016.\n[45] He Zhao, Ke Sun, Amir Dezfouli, and Edwin V. Bonilla. Transformed distribution matching for missing\nvalue imputation. In Proc. Int. Conf. Mach. Learn., volume 202, pages 42159â€“42186, 2023.\n12\n"
    },
    {
      "page_number": 13,
      "text": "A\nTheoretical justification\nBuilding upon the foundational theorems established earlier, we delve deeper into the theoretical as-\npects of our kernel ridge regression-based imputation framework. By leveraging advanced properties\nof kernel functionsâ€”such as universality, injective mappings, and the reproducing propertyâ€”we\nfurther substantiate the advantages and robustness of our method. This section introduces additional\ntheorems and proofs that highlight these properties and their implications for the imputation problem.\nLemma A.1 (Representer theorem). Suppose h(âˆ¥fâˆ¥) : R+ â†’R is a non-decreasing function.\nThe minimizer of an empirical risk functional regularized by h(âˆ¥fâˆ¥) admits the form: f âˆ—(Â·) =\nPn\ni=1 Î±iK(Â·, xi) where Î± = (Î±1, . . . , Î±n)âŠ¤and K is the associated kernel function.\nProof. The proof can be found in Theorem 6.11 of Mohri et al. [22].\nTheorem A.2 (Lemma 3.4 in the main text). Let Ys, Yt âˆˆRBÃ—1 be the target feature and Xs,\nXt be the corresponding input features; Suppose f âˆ—is the optimal model minimizing the empirical\nrisk in the inner optimization of (5), its output on Xt is given by f âˆ—(Xt) = KXtXs Â· Î±, where\nÎ± = (K + Î»I)âˆ’1y; KXtXs is the kernel matrix computed with Xt and Xs.\nProof. Consider the samples Xs and Ys where Ys is the observed target, and Xs comprises the\ninput features. The empirical risk minimization objective with â„“2 regularization to select the optimal\nfunctional form is\nmin\nfâˆˆH âˆ¥Ys âˆ’f(Xs)âˆ¥2\n2 + Î»âˆ¥fâˆ¥2\nH,\n(10)\nwhich corresponds precisely to the inner loop of (5). According to Lemma A.1, when h is an identity\nfunction (in (10)) and H is a RKHS associated with kernel K, the minimizer f âˆ—must admit the\nexplicit form\nf âˆ—(x) =\nB\nX\ni=1\nÎ±iK(x, xs\ni),\n(11)\nfor some coefficients Î±1, . . . , Î±B.\nSubstituting this form into the empirical risk (10), the optimization problem becomes\nmin\nÎ±âˆˆRB âˆ¥Ys âˆ’KXsXsÎ±âˆ¥2\n2 + Î»Î±âŠ¤KXsXsÎ±,\n(12)\nwhere KXsXs is the B Ã— B Gram matrix, with (i, j)-th entry K(xs\ni, xs\nj), Ys is the length-B target\nvector, and Î± is the vector of coefficients.\nExpanding the loss function in matrix notation yields\n(Ys âˆ’KXsXsÎ±)âŠ¤(Ys âˆ’KXsXsÎ±) + Î»Î±âŠ¤KXsXsÎ±.\n(13)\nDue to symmetry of KXsXs, this simplifies to\nYsâŠ¤Ys âˆ’2YsâŠ¤KXsXsÎ± + Î±âŠ¤(K2\nXsXs + Î»KXsXs)Î±.\n(14)\nAccording to the first-order condition, setting the derivative with respect to Î± to zero and solving for\nÎ± gives\nâˆ’2KâŠ¤\nXsXsYs + 2(K2\nXsXs + Î»KXsXs)Î± = 0,\n(15)\nwhich is equivalent to:\nKXsXs(KXsXs + Î»I)Î± = KXsXsYs.\n(16)\nAssuming KXsXs is invertible, we have:\n(KXsXs + Î»I)Î± = Ys.\n(17)\nwhich immediately follows from multiplying both sides by Kâˆ’1\nXsXs. Solving for Î± gives:\nÎ± = (KXsXs + Î»I)âˆ’1Ys.\n(18)\n13\n"
    },
    {
      "page_number": 14,
      "text": "Substituting (18) into (11) leads to\nf âˆ—(x) =\nB\nX\ni=1\nÎ±iK(x, xs\ni) = K(x)(KXsXs + Î»I)âˆ’1Ys,\n(19)\nwhere K(x) is the 1 Ã— B vector [K(x, xs\n1), Â· Â· Â· , K(x, xs\nB)]. For a (possibly distinct) batch of inputs\nXt, evaluating f âˆ—at each xt\nj gives\nf âˆ—(xt\n1) =\nB\nX\ni=1\nÎ±iK(xt\n1, xs\ni) =\n\u0002\nK(xt\n1, xs\n1), K(xt\n1, xs\n2), ..., K(xt\n1, xs\nB)\n\u0003\n(KXsXs + Î»I)âˆ’1Ys,\nf âˆ—(xt\n2) =\nB\nX\ni=1\nÎ±iK(xt\n2, xs\ni) =\n\u0002\nK(xt\n2, xs\n1), K(xt\n2, xs\n2), ..., K(xt\n2, xs\nB)\n\u0003\n(KXsXs + Î»I)âˆ’1Ys,\n...\nf âˆ—(xt\nB) =\nB\nX\ni=1\nÎ±iK(xt\nB, xs\ni) =\n\u0002\nK(xt\nB, xs\n1), K(xt\nB, xs\n2), ..., K(xt\nB, xs\nB)\n\u0003\n(KXsXs + Î»I)âˆ’1Ys,\n(20)\nwhich may be stacked to give the vector-valued expression\nf âˆ—(Xt) = KXtXs(KXsXs + Î»I)âˆ’1Ys,\n(21)\nwhere KXtXs is the matrix with entries [KXtXs]ij = K(xt\ni, xs\nj). The proof is completed.\nDefinition A.3 (Kernel Functions). Let x, xâ€² âˆˆRD be two vectors in the input feature space. A\nkernel function K : RD Ã— RD â†’R is a symmetric, positive semi-definite function that quantifies the\nsimilarity between x and xâ€². Commonly used kernel functions include:\n1. Linear Kernel: Klinear(x, xâ€²) = xâŠ¤xâ€², which computes the inner product between two vectors\nand corresponds to the case where no explicit feature transformation is applied.\n2. Polynomial Kernel: Kpoly(x, xâ€²) =\n\u0000xâŠ¤xâ€² + c\n\u0001d, where c â‰¥0 is a constant coefficient trading\noff the influence of higher-order versus lower-order terms, and d âˆˆN is the degree of the\npolynomial. It enables learning non-linear relationships by implicitly mapping the input features\ninto a higher-dimensional polynomial feature space.\n3. Gaussian Kernel: Kgauss(x, xâ€²) = exp\n\u0010\nâˆ’âˆ¥xâˆ’xâ€²âˆ¥2\n2Ïƒ2\n\u0011\n, where âˆ¥x âˆ’xâ€²âˆ¥2 denotes the squared\nEuclidean distance between x and xâ€², and Ïƒ > 0 is a scale parameter controlling the width of\nthe kernel. The Gaussian kernel is widely used due to its ability to model localized and highly\nnon-linear interactions.\nB\nImplementation details\nB.1\nDataset description and process strategy\nIn this paper, we use datasets from the UCI repository for model validation, in alignment with the a\nrecent NeurIPS-24 publication [1]. Detailed statistics for all selected datasets are provided in Tab. 4.\nTo simulate missing data, we first construct a binary mask matrix M. The observed data matrix,\ndenoted as X(obs), is derived by element-wise application of the complement mask 1 âˆ’M to the\nfully observed data matrix X(id). Specifically, each entry x(obs)\nnd\nin X(obs) is given by x(obs)\nnd\n= x(id)\nnd\nif mnd = 0; otherwise, x(obs)\nnd\nis assigned the value Null. On the generation of M, we consider three\ncanonical missing data mechanisms:\nâ€¢ Missing Completely at Random (MCAR): The probability of entry-wise missingness is indepen-\ndent of both observed and unobserved data. To simulate MCAR, each entry of M is independently\nset to 1 (missing) with probability pmiss, and to 0 (observed) with probability 1 âˆ’pmiss.\n14\n"
    },
    {
      "page_number": 15,
      "text": "Table 4: The statistics of involved datasets.\nAbbreviation\nDataset Name\nNumber (N)\nDimension (D)\nBT\nBlood Transfusion\n748\n4\nCC\nConcrete Compression\n1030\n7\nCBV\nConnectionist Bench Vowel\n990\n10\nIS\nIonosphere\n351\n34\nPK\nParkinsons\n195\n23\nQB\nQSAR Biodegradation\n1055\n41\nWQW\nWine Quality White\n4898\n11\nNote. The column â€˜Dimensionâ€™ and â€˜Numberâ€™ denotes the number of variables and samples in each\ndataset, respectively.\nâ€¢ Missing at Random (MAR): The missingness of a variable depends only on values of observed\nvariables [33, 32, 9]. To generate MAR scenarios, we randomly select a subset of features to be\nalways observed. The missingness in the remaining features is simulated using a logistic regression\nmodel, where the observed features act as predictors. The model parameters are randomly initialized,\nand the intercept (bias) is calibrated to yield the desired missingness rate.\nâ€¢ Missing Not at Random (MNAR): The probability that a value is missing depends on the\nunobserved (missing) values themselves [34, 38, 11]. For MNAR simulation, we adopt the\nprocedure in [1, 5]: the logistic model used for MAR is repurposed, but its inputs are themselves\nmasked by an independent MCAR mechanism, making the missingness dependent on both observed\nand unobserved features.\nB.2\nTraining protocols\nTo ensure reliable convergence, we set a maximum of 500 training iterations and adopt early stopping\nbased on validation performance, using a patience parameter of 10 epochs. Optimization throughout\nis conducted using the Adam optimizer [6]. The kernel function is specified as the Gaussian kernel.\nThe main hyperparameters, specifically the update rate Î·, batch size B, kernel number E and variance\nÏƒ are determined by allocating 5% of the training data as a validation set and tuning over the intervals\nÎ· âˆˆ[0.0001, 0.01], B âˆˆ[64, 512], E âˆˆ[1, 7] and Ïƒ âˆˆ[0.01, 10]. All experiments are conducted on\na hardware platform comprising two Intel(R) Xeon(R) Platinum 8383C CPUs (2.70GHz) and an\nNVIDIA GeForce RTX 4090 GPU.\nOn the implementation of baseline methods, we closely follow the implementation details in\nNewImp [1]. Hyperparameter reproducibility was confirmed in our environment, and we adopted the\nprovided settings to run the baseline scripts and report the corresponding results of NewImp. The\nreproduction of other models also follows NewImp. Specifically, the batch size for ReMasker is set\nto 64, whereas for all other baseline models it is fixed at 512. The MIWAE model is configured\nwith a latent dimension of 16 and 32 hidden units. The TDM model is implemented with two layers,\neach containing 16 hidden units. For the MIRACLE model, the number of hidden units is set to\n32. ReMasker is implemented with an embedding dimension of 32, a depth of 6, a mask ratio of\n0.5, encoder and decoder depths of 6 and 4 respectively, and uses 4 attention heads. Both MissDiff\nand CSDI-T are set with a channel size of 16, an embedding dimension of 128, and two layers. The\ndiffusion step parameter is set to 100 for these models, and the number of particles is set to 50.\nB.3\nEvaluation metrics\nThe imputed data matrix X(imp) is evaluated to assess imputation quality. Following the protocol\nin [45], we primarily employ the modified mean absolute error (MAE) and root mean squared error\n(RMSE) for evaluation:\nMAE :=\n1\nPN\nn=1\nPD\nd=1 Â¯mnd\nN\nX\nn=1\nD\nX\nd=1\n\f\f\fx(imp)\nnd\nâˆ’x(obs)\nnd\n\f\f\f Â¯mnd,\n(22)\nRMSE :=\nv\nu\nu\nt\n1\nPN\nn=1\nPD\nd=1 Â¯mnd\nN\nX\nn=1\nD\nX\nd=1\n\r\r\rx(imp)\nnd\nâˆ’x(obs)\nnd\n\r\r\r\n2\n2 Â¯mnd,\n(23)\n15\n"
    },
    {
      "page_number": 16,
      "text": "32\n64\n256\n512\n1024\nBatch size (B)\n0.7\n1.4\n2.1\n2.8\n3.5\nTime (ms)\ngaussian\nlinear\npoly\n32\n64\n256\n512\n1024\nBatch size (B)\n2.8\n3.5\nTime (ms)\ngaussian\nlinear\npoly\n(a) Computation time with different batch sizes.\n16\n32\n64\n128\n256\nFeature amount (D)\n1.2\n1.6\n2.0\nTime (ms)\ngaussian\nlinear\npoly\n16\n32\n64\n128\n256\nFeature amount (D)\n2.5\n3.0\n3.5\n4.0\n4.5\nTime (ms)\ngaussian\nlinear\npoly\n(b) Computation time with different feature amounts.\nFigure 5: Running time of the forward pass (left panels) and backward pass (right panels) given\nvarying settings. Different colors indicate different kernel functions. The colored lines and the\nshadowed areas indicate the mean values and the 99.9% confidence intervals.\nwhere Â¯mnd âˆˆÂ¯M indicates positions of imputed (originally missing) values with Â¯mnd = 1 âˆ’mnd,\nand x(obs)\nnd\nâˆˆX(obs) is the ground-truth value from the fully observed data. As only the originally\nmissing entries are imputed, we restrict the calculation of error metrics to the indices where Â¯mnd = 1.\nIn addition to the point-wise error metrics above, we also consider the squared Wasserstein distance\n(abbreviated as WASS) [1], which quantifies the discrepancy between the distributions of the imputed\nvalues and the corresponding ground-truth values at the missing positions (M = 1).\nC\nAdditional experimental results\nC.1\nAn empirical analysis on complexity\nIn this section, we examine the practical computational complexity of KPI. While the overall\nconvergence was analyzed in Theorem ??, the computational cost per iteration, which includes\nboth the forward and backward passes, has not been thoroughly discussed. To address this gap, we\nconducted experiments using IntelÂ® XeonÂ® Gold 6140 CPUs and Nvidia RTX 4090 GPUs, with\neach experiment repeated 100 times to ensure reliability.\nThe results are presented in Fig. 5. The running time per iteration remains limited (within 4 ms)\nacross a diverse range of hyperparameters, demonstrating the feasibility of KPI for real-world\napplications. Other key observations are summarized as follows:\nâ€¢ To explore the impact of batch size (B), we vary B within a wide range from 32 to 1024 while\nkeeping the feature amount (D) to 8. The running cost of the forward pass increased with the batch\nsize, as expected. This is attributed to the larger matrix inversion in (8), which cannot be efficiently\naccelerated by GPUs. In contrast, the backward pass cost was weakly correlated with B, since\ngradient computations after constructing the computation graph can be parallelized.\nâ€¢ To investigate the impact of feature amount (D), we maintain a constant batch size of 64. A weak\ncorrelation between D and the running time is observed. This is because varying D primarily\naffects the complexity of each kernel matrix computation, which can be effectively mitigated by\nGPU acceleration. This highlights a practical advantage of KPI: its efficiency in handling datasets\nwith a large number of features.\nâ€¢ We observe that the type of kernel function also affects the running cost. The gaussian kernel\nexhibits the largest running time compared to other kernel functions, in terms of both forward pass\nand the backward pass.\nC.2\nAdditional overall performance results given different missing ratios\nTab. 5-8 detail the imputation performance of KPI and baselines, with results for different missing\nratios: 0.1, 0.2, 0.3, and 0.4 listed separately. The results demonstrate that KPI consistently outper-\nforms the baselines in all settings, achieving superior performance in terms of both MAE and WASS.\nThis consistent superiority across varying missing ratios underscores the effectiveness and robustness\nof KPI for missing data imputation.\n16\n"
    },
    {
      "page_number": 17,
      "text": "Table 5: Imputation performance comparison with missing ratio of 0.1.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.118\n0.027\n0.155\n0.075\n0.196\n0.16\n0.175\n0.406\n0.097\n0.133\n0.122\n0.271\n0.192\n0.151\nMiss.F\n0.123\n0.037\n0.172\n0.111\n0.185\n0.149\n0.141\n0.295\n0.128\n0.163\n0.102\n0.284\n0.164\n0.129\nSinkhorn\n0.840\n0.434\n0.903\n0.614\n0.896\n0.837\n0.850\n2.047\n0.841\n1.513\n0.784\n2.622\n0.856\n0.755\nTDM\n0.724\n0.415\n0.815\n0.545\n0.787\n0.690\n0.720\n1.592\n0.731\n1.295\n0.565\n1.977\n0.745\n0.650\nCSDI-T\n0.727\n1.914\n0.850\n2.680\n0.815\n3.753\n0.766\n16.714\n0.743\n12.939\n0.578\n20.407\n0.775\n4.022\nMissDiff\n0.718\n1.446\n0.847\n1.803\n0.812\n4.101\n0.750\n13.640\n0.744\n16.209\n0.566\n25.062\n0.755\n6.037\nGAIN\n0.739\n0.355\n0.759\n0.479\n0.690\n0.541\n0.532\n1.137\n0.399\n0.460\n0.409\n1.192\n0.736\n0.621\nMIRACLE\n0.528\n0.174\n0.382\n0.161\n0.778\n0.682\n3.723\n26.666\n3.777\n18.544\n0.461\n1.103\n0.485\n0.364\nMIWAE\n0.539\n0.226\n0.698\n0.436\n0.782\n0.668\n0.603\n1.638\n0.526\n0.861\n0.450\n2.044\n0.626\n0.507\nRemasker\n0.365\n0.099\n1.041\n0.830\n0.448\n0.249\n0.715\n1.775\n0.500\n0.739\n0.489\n1.737\n0.503\n0.364\nNewImp\n0.383\n0.091\n0.273\n0.110\n0.231\n0.101\n0.423\n1.013\n0.251\n0.281\n0.305\n1.067\n1.045\n0.834\nKPI(Ours)\n0.084\n0.022\n0.023\n0.01\n0.051\n0.016\n0.093\n0.217\n0.071\n0.066\n0.05\n0.219\n0.082\n0.058\nTable 6: Imputation performance comparison with missing ratio of 0.2.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.145\n0.027\n0.171\n0.097\n0.208\n0.218\n0.186\n0.915\n0.102\n0.243\n0.13\n0.589\n0.2\n0.211\nMiss.F\n0.141\n0.058\n0.173\n0.131\n0.187\n0.206\n0.144\n0.589\n0.133\n0.316\n0.106\n0.579\n0.168\n0.181\nSinkhorn\n0.834\n0.428\n0.907\n0.711\n0.902\n1.079\n0.842\n3.908\n0.819\n2.572\n0.773\n5.036\n0.854\n1.030\nTDM\n0.725\n0.431\n0.812\n0.659\n0.800\n0.939\n0.720\n3.097\n0.710\n2.167\n0.567\n3.855\n0.750\n0.927\nCSDI-T\n0.724\n1.808\n0.847\n2.674\n0.823\n3.760\n0.759\n15.642\n0.724\n12.409\n0.574\n19.999\n0.777\n4.057\nMissDiff\n0.714\n1.282\n0.835\n1.707\n0.818\n3.658\n0.746\n13.473\n0.718\n14.872\n0.562\n23.777\n0.757\n5.526\nGAIN\n0.727\n0.350\n0.759\n0.585\n0.701\n0.739\n0.526\n2.231\n0.409\n0.830\n0.407\n2.292\n0.724\n0.853\nMIRACLE\n0.637\n0.271\n0.443\n0.234\n0.878\n1.102\n3.361\n43.583\n3.612\n31.612\n0.487\n2.558\n0.533\n0.556\nMIWAE\n0.569\n0.223\n0.730\n0.535\n0.801\n0.904\n0.620\n3.198\n0.511\n1.398\n0.465\n4.102\n0.653\n0.728\nRemasker\n0.403\n0.108\n0.412\n1.223\n0.488\n0.392\n0.613\n2.959\n0.450\n1.077\n0.397\n2.482\n0.523\n0.531\nNewImp\n0.441\n0.141\n0.360\n0.201\n0.310\n0.221\n0.411\n1.937\n0.283\n0.592\n0.329\n2.273\n0.468\n0.265\nKPI(Ours)\n0.095\n0.032\n0.077\n0.051\n0.085\n0.064\n0.098\n0.436\n0.075\n0.128\n0.062\n0.322\n0.103\n0.11\nTable 7: Imputation performance comparison with missing ratio of 0.3.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nmice\n0.156\n0.043\n0.195\n0.133\n0.219\n0.289\n0.186\n1.393\n0.107\n0.387\n0.134\n1.064\n0.21\n0.28\nmissforest\n0.14\n0.064\n0.189\n0.182\n0.2\n0.292\n0.154\n1.049\n0.131\n0.423\n0.113\n1.085\n0.176\n0.262\nsink\n0.828\n0.475\n0.911\n0.853\n0.904\n1.368\n0.851\n6.014\n0.828\n3.898\n0.774\n7.291\n0.859\n1.313\ntdm\n0.733\n0.506\n0.825\n0.834\n0.809\n1.260\n0.730\n4.796\n0.723\n3.337\n0.571\n5.629\n0.754\n1.240\nCSDI-T\n0.717\n1.905\n0.851\n2.684\n0.826\n3.816\n0.761\n14.942\n0.729\n12.044\n0.574\n19.732\n0.782\n4.093\nMissDiff\n0.718\n1.317\n0.842\n1.656\n0.822\n3.313\n0.751\n13.341\n0.725\n13.806\n0.563\n22.714\n0.759\n4.894\ngain\n0.742\n0.413\n0.780\n0.729\n0.736\n1.041\n0.566\n3.702\n0.460\n1.656\n0.434\n3.637\n0.730\n1.140\nmiracle\n0.951\n0.850\n0.535\n0.371\n0.841\n1.302\n3.036\n54.592\n3.432\n43.764\n0.542\n4.814\n0.582\n0.792\nmiwae\n0.593\n0.273\n0.769\n0.692\n0.818\n1.210\n0.650\n4.974\n0.527\n2.113\n0.480\n5.810\n0.667\n0.955\nremasker\n0.459\n0.134\n0.552\n0.371\n0.541\n0.586\n0.534\n3.949\n0.415\n1.365\n0.349\n2.928\n0.557\n0.724\nNewImp\n0.481\n0.181\n0.472\n0.341\n0.423\n0.443\n0.442\n3.066\n0.321\n1.015\n0.350\n3.666\n0.558\n0.379\nKPI(Ours)\n0.104\n0.028\n0.116\n0.088\n0.122\n0.122\n0.102\n0.712\n0.083\n0.217\n0.069\n0.562\n0.148\n0.215\n17\n"
    },
    {
      "page_number": 18,
      "text": "Table 8: Imputation performance comparison with missing ratio of 0.4.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.162\n0.03\n0.223\n0.168\n0.233\n0.354\n0.186\n1.824\n0.111\n0.549\n0.15\n1.491\n0.222\n0.329\nMISS.F\n0.156\n0.083\n0.198\n0.226\n0.192\n0.346\n0.154\n1.346\n0.133\n0.595\n0.114\n1.254\n0.177\n0.326\nSinkhorn\n0.837\n0.530\n0.904\n1.008\n0.889\n1.616\n0.847\n7.810\n0.821\n4.948\n0.768\n9.508\n0.860\n1.582\nTDM\n0.737\n0.596\n0.824\n1.037\n0.802\n1.561\n0.733\n6.376\n0.724\n4.367\n0.578\n7.565\n0.758\n1.574\nCSDI-T\n0.735\n1.851\n0.847\n2.694\n0.817\n3.877\n0.757\n14.671\n0.727\n11.771\n0.577\n19.537\n0.786\n4.164\nMissDiff\n0.726\n1.284\n0.837\n1.628\n0.812\n3.019\n0.750\n13.272\n0.723\n12.960\n0.566\n21.728\n0.761\n4.278\nGAIN\n0.712\n0.464\n0.812\n0.960\n0.791\n1.448\n0.665\n6.203\n0.522\n2.708\n0.653\n11.553\n0.826\n1.767\nMiracle\n1.066\n1.401\n0.602\n0.483\n0.826\n1.529\n2.714\n58.421\n3.250\n53.215\n0.595\n7.425\n0.620\n1.027\nMIWAE\n0.629\n0.344\n0.786\n0.856\n0.828\n1.502\n0.670\n6.663\n0.535\n2.842\n0.504\n7.952\n0.683\n1.185\nRemasker\n0.528\n0.182\n1.022\n1.541\n0.636\n0.860\n0.534\n5.653\n0.424\n1.891\n0.368\n4.096\n0.601\n0.926\nNewImp\n0.563\n0.301\n0.553\n0.520\n0.542\n0.743\n0.451\n4.035\n0.351\n1.563\n0.378\n4.989\n1.022\n1.542\nKPI(Ours)\n0.113\n0.039\n0.132\n0.134\n0.144\n0.191\n0.107\n1.022\n0.09\n0.337\n0.084\n1.028\n0.159\n0.303\nC.3\nAdditional overall performance results given different missing mechanisms\nTab. 9 and 10 provide a detailed evaluation of the imputation performance of KPI and various\nbaselines under MAR and MNAR mechanisms, respectively. These missing mechanisms are more\ncomplex and challenging compared to the MCAR setting reported in Tab. 1, but they are also more\nrepresentative of real-world scenarios.\nThe results demonstrate that KPI consistently outperforms the baselines in all settings, achieving\nsuperior performance in terms of both metrics. This consistent superiority across different missing\nmechanisms highlights the effectiveness and robustness of KPI for missing data imputation, making\nit a reliable choice for diverse real-world applications.\nC.4\nAdditional hyperparameter sensitivity results\nFig. 6 presents an extended analysis of hyperparameter sensitivity under higher missing ratios of 0.3\nand 0.4, building upon the scenarios explored in Fig. 4 with missing ratios of 0.1 and 0.2. These\nadditional experiments provide insights into the modelâ€™s behavior under more challenging conditions.\nOverall, the model exhibits greater sensitivity to hyperparameter choices at higher missing ratios. This\nindicates that hyperparameter tuning becomes increasingly important as the missing ratio increases.\nHowever, despite the heightened sensitivity, the trends observed across different missing ratios remain\nconsistent. For instance, the optimal update rate is found to be 0.01 for both CC and CBV across all\nfour missing ratios. This consistency reduces the complexity of tuning the model for each specific\nmissing ratio and implies that the selected hyperparameters for KPI are reliable and robust across a\nrange of missing data scenarios.\n18\n"
    },
    {
      "page_number": 19,
      "text": "Table 9: Imputation performance comparison under MAR missing mechanism.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.481\n0.109\n0.626\n0.349\n0.839\n0.652\n0.677\n1.113\n0.492\n0.946\n0.604\n2.42\n0.824\n0.775\nMISS.F\n0.718\n0.727\n0.632\n0.404\n0.783\n0.572\n0.58\n1.261\n0.797\n1.517\n0.58\n2.671\n0.709\n0.63\nCSDI-T\n1.094\n5.465\n0.894\n3.212\n0.826\n4.286\n0.707\n15.194\n1.262\n19.116\n0.782\n23.176\n0.815\n4.919\nMissDiff\n1.019\n2.835\n0.888\n2.189\n0.852\n6.008\n0.704\n13.233\n1.219\n22.773\n0.762\n34.125\n0.805\n6.919\ngain\n1.082\n1.187\n0.782\n0.570\n0.700\n0.503\n0.456\n0.609\n0.709\n1.383\n0.552\n1.716\n0.729\n0.672\nMIRACLE\n0.699\n0.510\n0.356\n0.141\n0.710\n0.530\n3.837\n19.874\n4.518\n23.842\n0.605\n1.636\n0.494\n0.385\nMIWAE\n0.747\n0.784\n0.735\n0.517\n0.788\n0.617\n0.474\n0.811\n0.758\n1.674\n0.660\n2.892\n0.629\n0.575\nRemasker\n0.598\n0.689\n1.023\n0.854\n0.467\n0.235\n0.691\n1.149\n0.668\n1.062\n0.557\n1.563\n0.489\n0.397\nSinkhorn\n1.106\n1.319\n0.958\n0.718\n0.923\n0.804\n0.829\n1.350\n1.257\n3.392\n0.904\n3.032\n0.905\n0.912\nTDM\n1.015\n1.313\n0.855\n0.614\n0.823\n0.653\n0.691\n0.995\n1.194\n3.311\n0.752\n2.739\n0.794\n0.776\nNewImp\n0.401\n0.171\n0.232\n0.111\n0.221\n0.070\n0.331\n0.504\n0.462\n0.745\n0.560\n3.330\n0.372\n0.293\nmultikip\n0.367\n0.107\n0.199\n0.121\n0.225\n0.115\n0.345\n0.864\n0.457\n0.68\n0.339\n1.965\n0.362\n0.311\nTable 10: Imputation performance comparison under MNAR missing mechanism.\nDatasets\nBT\nCC\nCBV\nIS\nPK\nQB\nWQW\nMetrics\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMAE\nWASS\nMICE\n0.696\n0.411\n0.66\n0.384\n0.832\n0.734\n0.708\n1.831\n0.499\n0.98\n0.589\n2.377\n0.807\n0.724\nMISS.F\n0.731\n0.579\n0.697\n0.483\n0.748\n0.631\n0.587\n1.944\n0.726\n1.513\n0.5\n2.291\n0.667\n0.567\nCSDI-T\n0.885\n3.105\n0.885\n2.923\n0.838\n3.922\n0.759\n16.833\n1.016\n14.173\n0.683\n20.330\n0.795\n4.275\nGAIN\n0.846\n0.595\n0.782\n0.545\n0.698\n0.570\n0.529\n1.151\n0.571\n1.305\n0.474\n1.943\n0.730\n0.684\nMIRACLE\n0.655\n0.319\n0.371\n0.163\n0.842\n0.802\n3.725\n27.093\n4.196\n25.052\n0.576\n2.249\n0.518\n0.437\nMIWAE\n0.658\n0.424\n0.735\n0.508\n0.808\n0.731\n0.559\n1.535\n0.628\n1.400\n0.561\n3.318\n0.641\n0.577\nRemasker\n0.481\n0.297\n1.028\n0.886\n0.487\n0.296\n0.660\n1.701\n0.586\n1.059\n0.516\n2.128\n0.519\n0.434\nSinkhorn\n0.967\n0.752\n0.940\n0.698\n0.925\n0.911\n0.854\n2.121\n1.049\n2.906\n0.844\n3.755\n0.880\n0.859\nTDM\n0.858\n0.732\n0.849\n0.620\n0.821\n0.756\n0.724\n1.640\n0.969\n2.713\n0.661\n3.202\n0.772\n0.744\nNewImp\n0.645\n0.461\n0.585\n0.593\n0.562\n0.837\n0.442\n3.945\n0.434\n2.328\n0.441\n7.161\n0.601\n1.102\nmultikip\n0.573\n0.257\n0.271\n0.22\n0.357\n0.352\n0.391\n1.344\n0.423\n0.769\n0.286\n1.967\n0.458\n0.563\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\nMAE\n0.3\n0.4\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\n1.0\nWASS\n0.3\n0.4\n(a) Varying learning rate results on CC\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\n1.0\nMAE\n0.3\n0.4\n0.0005\n0.001\n0.005\n0.01\n0.02\n0.5\n1.0\n1.5\n2.0\n2.5\nWASS\n0.3\n0.4\n(b) Varying learning rate results on CBV\n64\n128\n256\n512\n1024\nB\n0.3\n0.4\n0.5\n0.6\n0.7\nMAE\n0.3\n0.4\n64\n128\n256\n512\n1024\nB\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nWASS\n0.3\n0.4\n(c) Varying batch size results on CC\n64\n128\n256\n512\n1024\nB\n0.50\n0.75\nMAE\n0.3\n0.4\n64\n128\n256\n512\n1024\nB\n0.50\n0.75\n1.00\nWASS\n0.3\n0.4\n(d) Varying batch size results on CBV\nFigure 6: Varying learning rate and batch size results with missing ratios 0.3 and 0.4.\n19\n"
    },
    {
      "page_number": 20,
      "text": "NeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaperâ€™s contributions and scope?\nAnswer: [Yes]\nJustification: The claims in the abstract and introduction correctly summarize the theoretical\nand empirical contributions of the paper. They are well-aligned with the scope, methods,\nand results presented in the main text.\nGuidelines:\nâ€¢ The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\nâ€¢ The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\nâ€¢ The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\nâ€¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: There is a separate \"Limitations\" section.\nGuidelines:\nâ€¢ The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\nâ€¢ The authors are encouraged to create a separate \"Limitations\" section in their paper.\nâ€¢ The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\nâ€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\nâ€¢ The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\nâ€¢ The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\nâ€¢ If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\nâ€¢ While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that arenâ€™t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n20\n"
    },
    {
      "page_number": 21,
      "text": "Answer: [Yes]\nJustification: In appendix.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include theoretical results.\nâ€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\nâ€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.\nâ€¢ The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\nâ€¢ Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\nâ€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: Yes, it is already provided. We will release our code soon.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include experiments.\nâ€¢ If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\nâ€¢ If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\nâ€¢ Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\nâ€¢ While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\n21\n"
    },
    {
      "page_number": 22,
      "text": "Question: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [No]\nJustification: The code is not yet publicly released at submission time. We plan to make the\ncodebase and data processing scripts publicly available soon.\nGuidelines:\nâ€¢ The answer NA means that paper does not include experiments requiring code.\nâ€¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\nâ€¢ While we encourage the release of code and data, we understand that this might not be\npossible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\nâ€¢ The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\nâ€¢ The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\nâ€¢ The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\nâ€¢ At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\nâ€¢ Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: The experimental setting is detailed. Additional training configurations are\nprovided in Appendix.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include experiments.\nâ€¢ The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\nâ€¢ The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: Yes.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include experiments.\nâ€¢ The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\nâ€¢ The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n22\n"
    },
    {
      "page_number": 23,
      "text": "â€¢ The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\nâ€¢ The assumptions made should be given (e.g., Normally distributed errors).\nâ€¢ It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\nâ€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\nâ€¢ For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\nâ€¢ If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: The paper provides sufficient details on computational resources in Appendix.\nGuidelines:\nâ€¢ The answer NA means that the paper does not include experiments.\nâ€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\nâ€¢ The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\nâ€¢ The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidnâ€™t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: This work complies fully with the NeurIPS Code of Ethics. It uses only public\ndatasets and poses no foreseeable ethical risks.\nGuidelines:\nâ€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\nâ€¢ If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\nâ€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: There is no societal impact of the work performed.\nGuidelines:\nâ€¢ The answer NA means that there is no societal impact of the work performed.\nâ€¢ If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n23\n"
    },
    {
      "page_number": 24,
      "text": "â€¢ Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\nâ€¢ The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\nâ€¢ The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\nâ€¢ If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This work poses no such risks.\nGuidelines:\nâ€¢ The answer NA means that the paper poses no such risks.\nâ€¢ Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\nâ€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\nâ€¢ We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: This work does not use existing assets.\nGuidelines:\nâ€¢ The answer NA means that the paper does not use existing assets.\nâ€¢ The authors should cite the original paper that produced the code package or dataset.\nâ€¢ The authors should state which version of the asset is used and, if possible, include a\nURL.\nâ€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.\nâ€¢ For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\nâ€¢ If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n24\n"
    },
    {
      "page_number": 25,
      "text": "â€¢ For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\nâ€¢ If this information is not available online, the authors are encouraged to reach out to\nthe assetâ€™s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: This work does not release any new asset.\nGuidelines:\nâ€¢ The answer NA means that the paper does not release new assets.\nâ€¢ Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\nâ€¢ The paper should discuss whether and how consent was obtained from people whose\nasset is used.\nâ€¢ At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This study does not involve any human participants or crowdsourcing tasks.\nGuidelines:\nâ€¢ The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\nâ€¢ Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\nâ€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: No human subjects or crowdsourced data were involved in this study; all\nexperiments used public datasets. IRB approval is thus not applicable.\nGuidelines:\nâ€¢ The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\nâ€¢ Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\nâ€¢ We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n25\n"
    },
    {
      "page_number": 26,
      "text": "â€¢ For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: This work does not involve any LLMs in its core algorithmic design or\nempirical methodology.\nGuidelines:\nâ€¢ The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\nâ€¢ Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n26\n"
    }
  ]
}