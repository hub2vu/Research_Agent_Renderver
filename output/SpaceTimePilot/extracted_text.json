{
  "filename": "SpaceTimePilot.pdf",
  "total_pages": 17,
  "full_text": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across\nSpace and Time\nZhening Huang1,2\nHyeonho Jeong2\nXuelin Chen2\nYulia Gryaditskaya2\nTuanfeng Y. Wang2\nJoan Lasenby1\nChun-Hao Huang2\n1University of Cambridge\n2Adobe Research\nhttps://zheninghuang.github.io/Space-Time-Pilot/\nt=80\nt=80\nreverse\nmotion\nt=10\nt=0\nt=20\nt=40\nt=60\nSource Video\nSynthesized Video\nt=70\nt=80\nt=60\nt=40\nt=20\nt=0\nbullet-time\nat t=60\nSource Video\nSynthesized Video\nt=10\nt=0\nt=20\nt=40\nt=60\nt=60\nt=60\nt=60\nt=60\nt=60\nt=60\nslow motion\nx0.5\nSource Video\nSynthesized Video\nt=10\nt=0\nt=20\nt=40\nt=60\nt=5\nt=0\nt=10\nt=20\nt=30\nt=40\nFrame Number\n80\n你的段落⽂字\nTwo ballet dancers in blue and white costumes performing a graceful routine on a dark stage with a red velvet curtain and dramatic spotlights.\nA couple in 18th-century Baroque attire dancing on a theater stage in front of a live symphony orchestra; warm theatrical lighting.\nA couple performing a retro swing dance on a Parisian bridge; woman in a red polka-dot dress, classic Haussmann buildings in the background.\nCamera Trajactory Control\nAnimation Time Control\nSpaceTimePilot\nSpaceTimePilot\nSpaceTimePilot\n0\nt=80\nFigure 1. SpaceTimePilot enables unified control over both camera and time within a single diffusion model, producing continuous and\ncoherent videos along arbitrary space–time trajectories. Given a source video (odd rows), our model synthesizes new videos (even rows)\nwith retimed motion sequences, including slow motion, reverse motion, and bullet time, while precisely controlling camera movement\naccording to a given camera trajectory.\nAbstract\nWe present SpaceTimePilot, a video diffusion model that\ndisentangles space and time for controllable generative ren-\ndering.\nGiven a monocular video, SpaceTimePilot can\nindependently alter the camera viewpoint and the motion\nsequence within the generative process, re-rendering the\nscene for continuous and arbitrary exploration across space\nand time. To achieve this, we introduce an effective an-\nimation time-embedding mechanism in the diffusion pro-\ncess, allowing explicit control of the output video’s motion\nsequence with respect to that of the source video. As no\ndatasets provide paired videos of the same dynamic scene\nwith continuous temporal variations, we propose a simple\n1\narXiv:2512.25075v1  [cs.CV]  31 Dec 2025\nyet effective temporal-warping training scheme that repur-\nposes existing multi-view datasets to mimic temporal dif-\nferences. This strategy effectively supervises the model to\nlearn temporal control and achieve robust space–time dis-\nentanglement.\nTo further enhance the precision of dual\ncontrol, we introduce two additional components: an im-\nproved camera-conditioning mechanism that allows alter-\ning the camera from the first frame, and Cam×Time, the first\nsynthetic Space and Time full-coverage rendering dataset\nthat provides fully free space–time video trajectories within\na scene. Joint training on the temporal-warping scheme\nand the Cam×Time dataset yields more precise temporal\ncontrol. We evaluate SpaceTimePilot on both real-world\nand synthetic data, demonstrating clear space–time disen-\ntanglement and strong results compared to prior work.\n1. Introduction\nVideos are 2D projections of an evolving 3D world, where\nthe underlying generative factors consist of spatial variation\n(camera viewpoint) and temporal evolution (dynamic scene\nmotion). Learning to understand and disentangle these fac-\ntors from observed videos is fundamental for tasks such\nas scene understanding, 4D reconstruction, video editing,\nand generative rendering, to name a few.\nIn this work,\nwe approach this challenge from the perspective of gen-\nerative rendering. Given a single observed video of a dy-\nnamic scene, our goal is to synthesize novel views (re-\nframe/reangle) and/or at different moments in time (retime),\nwhile remaining faithful to the underlying scene dynamic.\nA common strategy is to first reconstruct dynamic 3D\ncontent from 2D observations, i.e., perform 4D reconstruc-\ntion, and then re-render the scene. These methods model\nboth spatial and temporal variations using representations\nsuch as NeRFs [22, 25] or Dynamic Gaussian Splatting\n[15, 42], often aided by cues like geometry [27, 28], op-\ntical flow [19, 20], depth [6, 47], or long-term 2D tracks\n[17, 37].\nHowever, even full 4D reconstructions typi-\ncally show artifacts under novel viewpoints. More recent\nwork [21, 43] uses multi-view video diffusion to gener-\nate sparse, time-conditioned views and refines them via\nGaussian-splatting optimization, but rendering quality re-\nmains limited.\nAdvances in video diffusion models [3–\n5, 12, 16, 30, 39, 46, 51] further enable camera re-posing\nwith more lightweight point cloud representations, reduc-\ning the need for heavy 4D reconstruction. While effective in\npreserving identity, their reliance on per-frame depth and re-\nprojection limits robustness under large viewpoint changes.\nTo mitigate this, newer approaches condition generation\nsolely on camera parameters, achieving strong novel-view\nsynthesis on both static [14] and dynamic scenes [2, 9, 33].\nAutoregressive models like Genie-3 [29] even enable inter-\nactive scene exploration from a single image, showing that\nCamera\nCamera\nCamera\nCamera Control Models\n4D Multi-View Models\nSpaceTimePilot\nTime\nobserved input\nsynthesized views or starting frames of videos\nframe progression\nTime\nTime\nFigure 2. Space–time controllability across methods. Blue cells\ndenote the input video/views, while arrows and dots indicate gen-\nerated continuous videos or sparse frames. Camera-control V2V\nmodels [2, 33] modify only the camera trajectory while keeping\ntime strictly monotonic. 4D multi-view models [21, 43] synthe-\nsize discrete sparse views conditioned on space and time, but do\nnot generate continuous video sequences. SpaceTimePilot enables\nfree movement along both the camera and time axes with full con-\ntrol over direction and speed, supporting bullet-time, slow-motion,\nreverse playback, and mixed space–time trajectories.\ndiffusion models can encode implicit 4D priors. Nonethe-\nless, despite progress in spatial viewpoint control, current\nmethods still lack full 4D exploration, i.e., the ability to\nnavigate scenes freely across both space and time.\nIn this work, we introduce SpaceTimePilot, the first\nvideo diffusion model that enables joint spatial and tem-\nporal control. SpaceTimePilot introduces a new notion of\n“animation time” to capture the temporal status of scene\ndynamics in the source video. As such, it naturally disen-\ntangles temporal control and camera control by expressing\nthem as two independent signals. A high-level comparison\nbetween our approach and prior methods is illustrated in\nFig. 2. Unlike previous methods, SpaceTimePilot enables\nfree navigation along both the camera and time axes. Train-\ning such a model requires dynamic videos that exhibit mul-\ntiple forms of temporal playback while simultaneously be-\ning captured under multiple camera motions, which is only\nfeasible in a controlled studio setups. Although temporal di-\nversity can be increased by combining multiple real datasets\ne.g. [23, 53], as done in [41, 43], this approach remains sub-\noptimal, as the coverage of temporal variation is still insuf-\nficient to learn the underlying meaning of temporal control.\nExisting synthetic datasets [1, 2] also do not exhibit such\nproperties.\nTo address this limitation, we introduce a simple yet\neffective temporal-warping training scheme that augments\nexisting multi-view video datasets [1, 2] to simulate di-\nverse conditioning types while preserving continuous video\nstructure. By warping input sequences in time, the model\nis exposed to varied temporal behaviors without requiring\nadditional data collection.\nThis simple yet crucial strat-\negy allows the model to learn temporal control signals, en-\nabling it to directly exhibit space–time disentanglement ef-\nfects during generation. We further ablate various temporal-\nconditioning schemes and introduce a convolution-based\n2\ntemporal-control mechanism that enables finer-grained ma-\nnipulation of temporal behavior and supports effects such\nas bullet-time at any timestep within the video. While tem-\nporal warping increases temporal diversity, it can still en-\ntangle camera and scene dynamics – for example, tempo-\nral manipulation may inadvertently affect camera behavior.\nTo further strengthen disentanglement, we introduce a new\ndataset that spans the full grid of camera–time combinations\nalong a trajectory. Our synthetic Cam×Time dataset con-\ntains 180k videos rendered from 500 animations across 100\nscenes and three camera paths. Each path provides full-\nmotion sequences for every camera pose, yielding dense\nmulti-view and full-temporal coverage. This rich supervi-\nsion enables effective disentanglement of spatial and tem-\nporal control.\nExperimental results show that SpaceTimePilot success-\nfully disentangles space and time in generative rendering\nfrom single videos, outperforming adapted state-of-the-art\nbaselines by a significant margin. Our main contributions\nare summarized as follows:\n• We introduce SpaceTimePilot, the first video diffusion\nmodel that disentangles spatial and temporal factors to en-\nable continuous and controllable novel view synthesis as\nwell as temporal control from a single video.\n• We propose the temporal-warping strategy that repur-\nposes multi-view datasets to simulate diverse temporal\nvariations. By training on these warped sequences, the\nmodel effectively learns temporal control without the\nneed for explicitly constructed video pairs captured un-\nder different temporal settings.\n• We propose a more precise camera–time conditioning\nmechanism, illustrating how viewpoint and temporal em-\nbeddings can be jointly integrated into diffusion models\nto achieve fine-grained spatiotemporal control.\n• We construct the Cam×Time Dataset, providing dense\nspatiotemporal sampling of dynamic scenes across cam-\nera trajectories and motion sequences. This dataset sup-\nplies the necessary supervision for learning disentangled\n4D representations and supports precise camera–time\ncontrol in generative rendering.\n2. Related work\nWe aim to re-render a video from new viewpoints with tem-\nporal control, a task closely related to Novel View Synthesis\n(NVS) from monocular video inputs.\nVideo-based NVS. Prior video-based NVS methods can\nbe broadly characterized along two axes: (i) whether they\ntarget static or dynamic scenes, and (ii) whether they incor-\nporate explicit 3D geometry in the generation pipeline.\nFor static scenes, geometry-based methods reconstruct\nscene geometry from the input frames and use diffusion\nmodels to complete or hallucinate regions that are unseen\nunder new viewpoints [13, 31, 44, 49]. Although these ap-\nproaches achieve high rendering quality, they rely on heavy\n3D preprocessing. Geometry-free approaches [2, 33, 52]\nbypass explicit geometry and directly condition the diffu-\nsion process on observed views and camera poses to syn-\nthesize new viewpoints.\nFor dynamic scenes, inpainting-based methods such as\nTrajectoryCrafter [48], ReCapture [50], and Reangle [13]\nalso adopt warp-and-inpaint pipelines, while GEN3C [31]\nextends this with an evolving 3D cache and EPiC [40] im-\nproves efficiency via a lightweight ControlNet framework.\nGeometry-free dynamic models [1, 2, 33, 35, 36] instead\nlearn camera-conditioned generation from multi-view or 4D\ndatasets (e.g., Kubric-4D [7]), enabling smoother and more\nstable NVS with minimal 3D inductive bias. Proprietary\nsystems like Genie 3 [29] further demonstrate real-time,\ncontinuous camera control in dynamic scenes, underscor-\ning the potential of video diffusion models for interactive\nviewpoint manipulation.\nDisentangling Space and Time. Despite great progress in\ncamera controllability (space), the methods discussed above\ndo not address temporal control (time). Meanwhile, disen-\ntangling spatial and temporal factors has become a central\nfocus in 4D scene generation, recently advanced through\ndiffusion-based models. 4DiM [41] introduces a Masked\nFiLM mechanism that defaults to identity transformations\nwhen conditioning signals (e.g., camera pose or time) are\nabsent, enabling unified representations across both static\nand dynamic data through multi-modal supervision. Simi-\nlarly, CAT4D [43] leverages multi-view images to conduct\n4D dynamic reconstruction to achieve space–time disen-\ntanglement but remains constrained by its reliance on ex-\nplicit 4D reconstruction pipelines, which limits scalability\nand controllability. In contrast, our approach builds upon\ntext-to-video diffusion models and introduces a new tempo-\nral embeddings module and refined camera conditioning to\nachieve fully controllable 4D generative reconstruction.\n3. Method\nWe introduce SpaceTimePilot, a method that takes a source\nvideo Vsrc ∈RF ×C×H×W as input and synthesizes a tar-\nget video Vtrg ∈RF ×C×H×W , following an input cam-\nera trajectory ctrg ∈RF ×3×4 and temporal control signal\nttrg ∈RF . Here, F denotes the number of frames, C the\nnumber of color channels, and H and W are the frame\nheight and width, respectively. Each cf\ntrg ∈R3×4 represents\nthe camera extrinsic parameters (rotation and translation) at\nframe f, with respect to the 1st frame of Vsrc. The target\nvideo Vtrg preserves the scene’s underlying dynamics, ge-\nometry, and appearance in Vsrc, while adhering to the cam-\nera motion and temporal progression specified by ctrg and\nttrg. A key feature of our method is the disentanglement\n3\nof spatial and temporal factors in the generative process,\nenabling effects such as bullet-time and retimed playback\nfrom novel viewpoints (see Fig. 1).\n3.1. Preliminaries\nOur framework builds upon recent advances in large-scale\ntext-to-video diffusion models and camera-conditioned\nvideo generation. We adopt a latent video diffusion back-\nbone similar to modern text-to-video foundation models\n[34], consisting of a 3D Variational Auto-Encoder (VAE)\nfor latent compression and a Transformer-based denoising\nmodel (DiT) operating over multi-modal tokens.\nAdditionally,\nour\ndesign\ndraws\ninspiration\nfrom\nReCamMaster [2],\nwhich introduces explicit camera\nconditioning for video synthesis. Given an input camera\ntrajectory c ∈RF ×3×4, spatial conditioning is achieved by\nfirst projecting the camera sequence to the space of video\ntokens and adding it to the features:\nx′ = x + Ecam (c) ,\n(1)\nwhere x is the output of the patchifying module and x′ is\nthe input to self-attention layers. The camera encoder Ecam\nmaps each flattened 3 × 4 camera matrix (12-dimensional)\ninto the target feature space, while also transforming the\ntemporal dimension from F to F ′.\n3.2. Disentangling Space and Time\nWe achieve spatial and temporal disentanglement through\na two-fold approach: a dedicated time representation and\nspecialized datasets.\n3.2.1. Time representation\nRecent video diffusion models include position embeddings\nfor latent frame index f ′, such as RoPE(f ′). However, we\nfound using RoPE(f ′) for temporal control to be ineffective,\nas it interferes with camera signals: RoPE(f ′) often con-\nstrains both temporal and camera motion simultaneously.\nTo address space and time disentanglement, we introduce a\ndedicated time control parameter t ∈RF . By manipulating\nttrg, we can control the temporal progression of the synthe-\nsized video Vtrg. For example, setting ttrg to a constant locks\nVtrg to a specific timestamp in Vsrc, while reversing the frame\nindices produces a playback of Vsrc in reverse.\n(Top) For multi-view dynamic scene datasets, a set of\ntemporal warping operations, including reverse, playback,\nzigzag motion, slow motion, and freeze are apppplied with\nteh source video as standford. This gives explicit supervi-\nsion for temporal control, without constructing additional\ntemporally varied training data.\n(Bottom) Existing camera-control and joint dataset train-\ning rely on monotonic time progression and static scene\nvideos, making it difficult for models to understand tem-\nporal variation. The introduced temporal mappings from\n1\n2\n3\n4\n5\n6\n7\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n1\n2\n3\n4\n5\n5\n5\n5\n6\n7\n8\n5\n5\n5\n5\n5\n5\n5\n5\n1\n2\n3\n4\n5\n6\n5\n4\n3\n2\n1\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n(f)  Freeze \n(a) Standard Forward\n(b) Reverse Playback\n(e) ZigZag Motion\n(d) Global Slow Motion x 0.5\n(c) Motion-to-Freeze\nPivot P, Where [N/2] < P < N\nFreezing P for N/2 steps, Where 0 < P < N\nPivot P\nRepeat P for whole video, Where 0 < P < N\nSource Video\nTarget Video\nStandard Forward\nframe index\n(a) Camera Control TI2V \n(c) Temporal Wrapping (ours)\ndynamic vidoes\n(2) Comparison of different training scheme for time control\n(b) Joint Dataset Training\nstatic scene videos\ndynamic vidoes\nframe index\nframe index\nanimation time t w.r.t. source \nanimation time t w.r.t. source \nanimation time t w.r.t. source \n(1) Temporal Warping Augmentation\nFigure 3. Temporal Wrapping for Spatiotemporal Disentan-\nglement. (Top) For multi-view dynamic scene datasets [2], a set\nof temporal warping operations (e.g.\nreverse playback, zigzag\nmotion, slow motion, and freeze) are applied to the target video,\nwith the source video kept as the standard forward reference, pro-\nviding explicit supervision for temporal control . (Bottom) Com-\npared with existing camera-control [2, 33] and joint-dataset train-\ning strategies [41, 43], which rely on monotonic time progression\nand static-scene videos to demonstrate temporal differences, Tem-\nporal Wrapping provide much more diverse and explicit signals of\ntemporal variation, leading to disentanglement of space and time.\nmulti-view video data, which provide diverse and clear sig-\nnal on tempral variation, and directly lead to disentangle-\nment of space and time.\nTime Embedding. To inject temporal control into the dif-\nfusion model, we analyze several approaches. First, we can\nencode time similar to a frame index using RoPE embed-\nding.\nHowever, we find it less suitable for time control\n(visual evaluations are provided in Supp. Mat.). Instead,\nwe adopt sinusoidal time embeddings applied at the latent\nframe f ′ level, which provide a stable and continuous rep-\nresentation of each frame’s temporal position and offer a\nfavorable trade-off between precision and stability. We fur-\nther observe that each latent frame corresponds to a con-\ntinuous temporal chunk, and propose using embeddings of\noriginal frame indices f to support finer granularity of time\ncontrol. To accomplish this, we introduce a time encod-\ning approach Eani(t), where t ∈RF . We first compute\nthe sinusoidal time embeddings to represent the temporal\nsequence, esrc = SinPE(tsrc), etrg = SinPE(ttrg), where\ntsrc, ttrg ∈RF . Next, we apply two 1D convolution lay-\ners to progressively project these embeddings into the latent\nframe space, e = Conv1D2(Conv1D1(e)). Finally, we add\nthese time features to the camera features and video tokens\n4\nembeddings, updating Eq. (1) as follows:\nx′ = x + Ecam (c) + Eani (t) .\n(2)\nIn Sec. 4.2, we compare our approach with alternative\nconditioning strategies, such as using sinusoidal embed-\ndings where tsrc, ttrg are directly defined in RF ′, and em-\nploying an MLP instead of a 1D convolution for compres-\nsion. We demonstrate both qualitatively and quantitatively\nthe advantages of our proposed method.\n3.2.2. Datasets\nTo enable temporal manipulation in our approach, we re-\nquire paired training data that includes examples of time\nremapping.\nAchieving spatial-temporal disentanglement\nfurther requires data containing examples of both camera\nand temporal controls. To the best of our knowledge, no\npublicly available datasets satisfy these requirements. Only\na few prior works, such as 4DiM [41] and CAT4D [43], have\nattempted to address spatial-temporal disentanglement. A\ncommon strategy is to jointly train on static-scene datasets\nand multi-view video datasets [23, 53]. The limited con-\ntrol variability in these datasets leads to confusion between\ntemporal evolution and spatial movement, resulting in en-\ntangled or unstable behaviors [41, 43]. We address this lim-\nitation by augmenting existing multi-view video data with\ntemporal warping and by proposing a new synthetic dataset.\nTemporal Warping Augmentation. We introduce simple\naugmentations that add controllable temporal variations to\nmulti-view video datasets. During training, given a source\nvideo Vsrc = {If\nsrc}F\nf=1 and a target video Vtrg = {If\ntrg}F\nf=1,\nwe apply a temporal warping function τ : [1, F] →[1, F]\nto the target sequence, producing a warped video V ′\ntrg =\n{Iτ(f)\ntrg\n}F\nf=1.\nThe source animation timestamps are uni-\nformly sampled, tsrc = 1 : F.\nWarped timestamps,\nttrg = τ(tsrc), introduce non-linear temporal effects (see\nFig. 3 top b–e): (i) reversal, (ii) acceleration, (iii) freez-\ning, (iv) segmental slow motion, and (v) zigzag motion, in\nwhich the animation repeatedly reverses direction. After\nthese augmentations, the paired video sequences (Vsrc, V ′\ntrg)\ndiffer in both camera trajectories and temporal dynamics,\nproviding the model with a clear signal for learning disen-\ntangled spatiotemporal representations.\nSynthetic Cam×Time Dataset for Precise Spatiotempo-\nral Control.\nWhile our temporal warping augmentations\nencourage strong disentanglement between spatial and tem-\nporal factors, achieving fine-grained and continuous con-\ntrol — that is, smooth and precise adjustment of tempo-\nral dynamics — benefits from a dataset that systemati-\ncally covers both dimensions. To this end, we construct\nCam×Time, a new synthetic spatiotemporal dataset ren-\ndered in Blender. Given a camera trajectory and an ani-\nmated subject, Cam×Time exhaustively samples the cam-\nera–time grid, capturing each dynamic scene across diverse\nTable 1. Comparison of existing multi-view datasets for cam-\nera and temporal control against Cam×Time. Cam×Time pro-\nvides full-grid rendering (Figure 4), enabling target videos to sam-\nple arbitrary temporal variations over the full range from 0 to 120.\nDataset\nDynamic scenes\nSrc. Time: tsrc\nTgt. Time: ttrg\nCamera\nRE10k [53]\n✘\n1\n1\nMoving\nDL3DV10k [23]\n✘\n1\n1\nMoving\nMannequinChallenge [32]\n✘\n1\n1\nMoving\nKubric-4D [33]\n✔\n1:60\n1:60\nMoving\nReCamMaster [2]\n✔\n1:80\n1:80\nMoving\nSynCamMaster [1]\n✔\n1:80\n1:80\nFixed\nCam×Time (ours)\n✔\n1:120\n{1, 2, . . . , 120}120\nMoving\ncontinuous camera traj.\nanimation time\nfull space-time grid rendering \n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nsource \nvideo\n...\n...\n...\n...\nFigure 4. Cam×Time dataset visualization. (Top) A space-time\ngrid defined by a camera trajectory c = [c1, ..., cF ] and animation\nstatus t = [t1, ..., tF ]. Cam×Time renders images for all (c, t)\npairs, covering the full grid for learning disentangled spatial and\ntemporal control. Any two sampled sequences of F frames from\nthe grid can form a source-target pair. (Bottom) One typical choice\nof source videos is taking the diagonal cells in green.\ncombinations of camera viewpoints and temporal states\n(c, t), as illustrated in Fig. 4. The source video is obtained\nby sampling the diagonal frames of the dense grid (Fig. 4\n(bottom)), while the target videos are obtained by more\nfree-form sampling of continuous sequences.\nWe com-\npare Cam×Time against existing datasets in Tab. 1. While\n[23, 32, 53] are real videos with complex camera path anno-\ntations, they either do not provide time-synchronized video\npairs [32] or only provide pairs of static scenes [23, 53].\nSynthetic multi-view video datasets [1, 2, 33] provide pairs\nof dynamic videos but do not allow training for time control.\nIn contrast, Cam×Time enables fine-grained manipulation\nof both camera motion and temporal dynamics, enabling\nbullet-time effects, motion stabilization, and flexible combi-\nnations of the controls. We designate part of Cam×Time as\na test set, aiming for it to serve as a benchmark for control-\nlable video generation. We will release it to support future\nresearch on fine-grained spatiotemporal modeling.\n5\nTable 2. Quantitative comparison across temporal controls (Direction (forward, backward motion), Speed (slow modes), Bullet Time). We\nreport PSNR↑, SSIM↑, and LPIPS↓. Best results are in bold. SpaceTimeMethod showcase best performance for temporal control overall.\nMethod\nPSNR↑\nSSIM↑\nLPIPS↓\nDir.\nSpeed\nBullet\nAvg\nDir.\nSpeed\nBullet\nAvg\nDir.\nSpeed\nBullet\nAvg\nReCamM+preshuffled†\n17.13\n14.84\n14.61\n15.52\n0.6623\n0.6050\n0.5965\n0.6213\n0.3930\n0.4793\n0.4863\n0.4529\nReCamM+jointdata\n18.32\n17.57\n17.69\n17.86\n0.7322\n0.7220\n0.7209\n0.7250\n0.2972\n0.3158\n0.3089\n0.3073\nSpaceTimePilot (Ours)\n21.75\n20.87\n20.85\n21.16\n0.7725\n0.7645\n0.7653\n0.7674\n0.1697\n0.1917\n0.1677\n0.1764\n† Uses simple frame-rearrangement operators (reversal, repetition, freezing) applied prior to inference to emulate temporal manipulation.\n3.3. Precise Camera Conditioning\nWe aim for full camera trajectory control in the target video.\nIn contrast, the previous novel-view synthesis approach [2]\nassumes that the first frame is identical in source and target\nvideos and that the target camera trajectory is defined rel-\native to it. This stems from the two limitations. First, the\nexisting approach ignores the source video trajectory, yield-\ning suboptimal source features computed using the target\ntrajectory for consistency:\nx′\nsrc = xsrc + Ecam (ctrg) ,\nx′\ntrg = xtrg + Ecam (ctrg) .\nSecond, it is trained on datasets where the first frame is al-\nways identical across the source and target videos. This\nlatter limitation is addressed in our training datasets design.\nTo overcome the former, we devise a source-aware cam-\nera conditioning. We estimate camera poses for both the\nsource and target videos using a pretrained pose estimator,\nand inject them jointly into the diffusion model to provide\nexplicit geometric context. Eq. 2 is therefore extended into:\nx′\nsrc = xsrc + Ecam (csrc) + Eani (tsrc) ,\n(3)\nx′\ntrg = xtrg + Ecam (ctrg) + Eani (ttrg) ,\nx′ = [x′\ntrg, x′\nsrc]frame-dim,\nwhere x′ denotes the input of the DiT model, which is the\nconcatenation of target and source tokens along the frame\ndimension. This formulation provides the model with both\nsource and target camera context, enabling spatially consis-\ntent generation and precise control over camera trajectories.\n3.4. Support for Longer Video Segments\nFinally, to showcase the full potential of our camera and\ntemporal control, we adopt a simple autoregressive video\ngeneration strategy, generating each new segment Vtrg con-\nditioned on the previously generated segment Vprv and a\nsource video Vsrc to produce longer videos.\nTo enable this capability during inference, we need to\nextend our training scenario to support conditioning on two\nvideos, where one serves as Vsrc and the other as Vprv.\nThe source video Vsrc is taken directly from the multi-view\ndatasets or from our synthetic dataset, as was described pre-\nviously. Vprv is constructed in a similar way to Vtrg — either\nusing temporal warping augmentations or by sampling from\nthe dense space-time grid of our synthetic dataset. When\ntemporal warping is applied, Vprv and Vtrg may originate\nfrom the same or different multi-view sequences represent-\ning the same time interval. To maintain full flexibility of\ncontrol, we do not enforce any other explicit correlations\nbetween Vprv and Vtrg, apart from specifying camera param-\neters relative to the selected source video frame.\nNote that not constraining the source and target videos\nto share the same first frame (as discussed in Sec. 3.3) is\ncrucial for achieving flexible camera control in longer se-\nquences. For instance, this design enables extended bullet-\ntime effects: we can first generate a rotation around a se-\nlected point up to 45◦(Vtrg,1), and then continue from 45◦\nto 90◦(Vtrg,2). Conditioning on two consecutive source seg-\nments allows the model to leverage information from newly\ngenerated viewpoints. In the bullet-time example, condi-\ntioning on the previously generated video enables the model\nto incorporate information from all newly synthesized view-\npoints, rather than relying solely on the viewpoint of the\ncorresponding moment in the source video.\n4. Experiments\nImplementation details.\nWe adopt the Wan-2.1 T2V-\n1.3B model [34], which produces F ′=21 latent frames\nand decodes them into F=81 RGB frames using a 3D-\nVAE. The network is conditioned on camera and animation-\ntime controls as defined in Eq. 3. Unless otherwise spec-\nified, SpaceTimePilot is trained with ReCamMaster and\nSynCamMaster datasets with the temporal warping aug-\nmentation described in Sec. 3.2.2, along with Cam×Time.\nPlease refer to Supp. Mat. for complete network architec-\nture and additional training details.\n4.1. Comparison with State-of-the-Art Baselines\n4.1.1. Time-Control Evaluation.\nWe first evaluate the retiming capability of our model. To\nfactor out the error induced by camera control, we condition\nSpaceTimePilot on a fixed camera pose while varying only\nthe temporal control signal.\nExperiments are performed\non the withheld Cam×Time test split, which contains 50\nscenes rendered with dense full-grid trajectories that can\n6\nbullet-time\nat t=40\nbullet-time\nat t=40\nslow motion\n40-80\nreverse\nmotion\nslow motion\n40-80\nreverse\nmotion\nt=40\nt=0\nt=80\nt=20\nt=40\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nFrame Number\n80\n0\nFrame Number\n80\n0\nFigure 5. Qualitative results of SpaceTimePilot. Our model enables fully disentangled control over camera motion and temporal dy-\nnamics. Each row shows a different combination of camera trajectory (left icons) and temporal warping (right icons). SpaceTimePilot\nproduces coherent videos under diverse controls, including normal playback, reverse playback, bullet-time, slow-motion, replay motion,\nand complex camera paths (pan, tilt, zoom, and vertical motion).\nbe retimed into arbitrary temporal sequences. For each test\ncase, we take a moving-camera source video but set the tar-\nget camera trajectory to the first-frame pose. We then ap-\nply a range of temporal control signals, including reverse,\nbullet-time, zigzag, slow motion, and normal playback, to\nsynthesize the corresponding retimed outputs.\nSince we\nhave ground-truth frames for all temporal configurations,\nwe report perceptual losses: PSNR, SSIM, and LPIPS.\nWe consider two baselines: (1) ReCamM+preshuffled:\noriginal ReCamMaster combined with input re-shuffling;\nand (2) ReCamM+jointdata: following [41, 43], we train\nReCamMaster with additional static-scene datasets [18, 53]\nwhich provide only one single temporal pattern.\nWhile frame shuffling may succeed in simple scenar-\nios, it fails to disentangle camera and temporal control. As\nshown in Table 2, this approach exhibits the weakest tem-\nporal controllability. Although incorporating static-scene\ndatasets improves performance, particularly in the bullet-\ntime category, relying on a single temporal control pattern\nremains insufficient for achieving robust temporal consis-\ntency. In contrast, SpaceTimePilot consistently outperforms\nall baselines across all temporal configurations.\nTable 3. VBench visual-quality evaluation across six dimensions.\nHigher is better for all metrics.\nMethod\nImgQ↑BGCons↑Motion↑SubjCons↑Flicker↑Aesthetic↑\nTraj-Crafter [48] 0.6389\n0.9376\n0.9888\n0.9463\n0.9816\n0.5172\nReCamM [2]\n0.6302\n0.9114\n0.9945\n0.9181\n0.9825\n0.5332\nReCamM+Aug\n0.6315\n0.9165\n0.9946\n0.9313\n0.9788\n0.5385\nSTPilot (Ours)\n0.6486\n0.9199\n0.9947\n0.9325\n0.9781\n0.5315\n4.1.2. Visual Quality Evaluation.\nNext, we evaluate the perceptual realism of our 1800 gen-\nerated videos using VBench [10]. We report all standard\nvisual quality metrics to provide a comprehensive assess-\nment of generative fidelity. Table 3 shows that our model\nachieves visual quality comparable to the baselines.\n4.1.3. Camera-Control Evaluation.\nFinlay, we evaluate the effectiveness of our camera control\nmechanism detailed in Sec. 3.3. Unlike the retiming evalu-\nation above, which relies on synthetic ground-truth videos,\nhere we construct a real-world 90-video evaluation set from\nOpenVideoHD [26], encompassing diverse dynamic human\nand object motions. Each method is evaluated across 20\ncamera trajectories: 10 starting from the same initial pose\nas the source video and 10 from different initial poses,\n7\nTable 4. Camera accuracy and first-frame estimation. For camera control,\nthe enhanced camera control mechanism enables the generated video to start\nfrom an arbitrary camera angle while maintaining good camera accuracy.\nMethod\nRelRot↓RelTrans↓AbsRot↓AbsTrans↓Rot† ↓RTA15† ↑RTA30† ↑\nTraj-Crafter [48]\n5.94\n0.50\n6.93\n0.52\n9.76\n22.96%\n25.93%\nReCamM [2]\n4.26\n0.32\n10.08\n0.34\n7.49\n7.61%\n10.20%\nReCamM+Aug\n3.66\n0.43\n11.74\n0.46\n13.88\n3.89%\n5.93%\nSpaceTimePilot (ours)\n2.71\n0.33\n5.63\n0.34\n4.09\n35.19%\n54.44%\nTable 5. Time-embedding compressor ablation. The pro-\nposed time-embedding method, trained with temporal warp-\ning on the proposed dataset, yields sharper results overall.\nTime Embedding\nPSNR↑SSIM↑LPIPS↓\nUniform Sampling\n14.10 0.5981 0.5039\n1D-Conv\n14.75 0.6134 0.4878\n1D-Conv + Joint Data\n15.41 0.6252 0.4830\n1D-Conv +Cam×Time 21.16 0.7674 0.1764\n† Evaluation based on first-frame camera accuracy.\nt=0\nt=20\nt=40\nt=80\nt=80\nt=60\nt=40\nt=0\nt=80\nt=60\nt=40\nt=0\nt=80\nt=60\nt=40\nt=0\nSrc Video\nOurs\nReCamMaster\nTrajectory\nCrafter\nframe 0\nframe 20\nframe 40\nframe 80\nFigure 6. Qualitative comparison of disentangled camera-time\ncontrol. In this example, we apply reverse playback (time) and\na pan-right camera motion starting from the first-frame pose to a\nsource video (top), whose original camera motion is dolly-in (red\nto blue). SpaceTimePilot, by explicitly disentangling space and\ntime, achieves correct camera control (red boxes) together with\naccurate temporal control (green boxes). For TrajectoryCrafter, it\nfirst reverses the frames and then apply their method for viewpoint\ncontrol, resulting in incorrect camera motion. ReCamMaster (with\njoint-dataset training) is unable to perform temporal control, lead-\ning to failure cases.\nresulting in a total of 1800 generated videos. We apply\nSpatialTracker-v2 [45] to recover camera poses from the\ngenerated videos and compare them with the corresponding\ninput camera poses. To ensure consistent scale, we align the\nmagnitude of the first two camera locations. Trajectory ac-\ncuracy is quantified using RotErr and TransErr follow-\ning [8], under two protocols: (1) evaluating the raw trajecto-\nries defined w.r.t. the first frame (relative protocol, RelRot,\nRelTrans) and (2) evaluating after aligning to the estimated\npose of the first frame (absolute protocol, AbsRot, Ab-\nsTrans). Specifically, we transform the recovered raw tra-\njectories by multiplying the relative pose between the gener-\nated and source first frames, estimated by DUSt3R [38]. We\nalso compare this DUSt3R pose with the target trajectory’s\ninitial pose, and report RotErr, RTA@15 and RTA@30, as\ntranslation magnitude is scale-ambiguous.\nTo measure only the impact of source camera condition-\ning, we consider the original ReCamMaster [2] (ReCamM)\nand two variants. Since ReCamMaster is originally trained\nCamera: pan left.  Time: freeze at  t = 40.\nCamera: tilt-down.   Time: freeze at  t = 40.\nMLP Compressor 1D-Conv Compressor\nUniform Resampling\nframe 0\nframe 40\nframe 80\nMLP Compressor 1D-Conv Compressor\nUniform Resampling\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nFigure 7. Temporal compression ablation. Comparing uniform\nresampling, MLP, and 1D-Conv compressors under tilt-down and\npan-right bullet-time controls, ttrg = [40, . . . , 40].\non datasets where the first frame of the source and target\nvideos are identical, the model always copies the first frame\nregardless of the input camera pose. For fairness, we re-\ntrain ReCamMaster with more data augmentations to in-\nclude non-identical first frames, denoted as ReCamM+Aug.\nNext, we condition the model additionally with source cam-\neras csrc following Eq. 3, denoted as ReCamM+Aug+csrc.\nFinally we also report the results of TrajectoryCrafter [48].\nIn Table 4, we observe that the absolute protocol pro-\nduces consistently higher errors, as trajectories must not\nonly match the overall shape (relative protocol) but also\nalign correctly in position and orientation.\nInterestingly,\nReCamM+Aug yields higher errors than the original Re-\nCamM, whereas incorporating source cameras csrc results\nin the best overall performance. This suggests that, with-\nout explicit reference to csrc, exposure to more augmented\nvideos with differing initial frames can instead confuse the\nmodel. The newly introduced conditioning signal on the\nsource video’s trajectory csrc achieves substantially better\ncamera-control accuracy across all metrics, more reliable\nfirst-frame alignment, and more faithful adherence to the\nfull trajectory than all baselines.\n4.1.4. Qualitative results.\nBesides the quantitative evaluation, we also demonstrate the\nstrength of SpaceTimePilot with visual examples. In Fig. 6,\nwe show that only our method correctly synthesizes both\nthe camera motion (red boxes) and the animation-time state\n(green boxes). While ReCamMaster handles camera control\nwell, it cannot modify the temporal state, such as enabling\n8\nreverse playback. TrajectoryCrafter, in contrast, is confused\nby the reverse frame shuffle, causing the camera pose of the\nlast source frame (blue boxes) to incorrectly appear in the\nfirst frame of the generated video. More visual results can\nbe found in Fig. 5.\n4.2. Ablation Study\nTo validate the effectiveness of the proposed Time embed-\nding module, in Table 5, we follow the time-control eval-\nuation set up in Sec. 4.1.1 and compare our 1D convo-\nlutional time embedding against several variants and al-\nternatives discussed in Sec. 3.2.1: (1) Uniform-Sampling:\nsampling the 81-frame embedding uniformly to a 21-frame\nsequence, which is equivalent to adopting sinusoidal em-\nbeddings at the latent frame f ′ level; (2) 1D-Conv: us-\ning 1D convolution layers to compress from t ∈RF to\nt ∈RF ′, trained with ReCamMaster and SynCamMaster\ndatasets. (3) 1D-Conv+jointdata: row 2 but including addi-\ntionally static-scene datasets [18, 53]. (4) 1D-Conv (ours):\nrow 2 but instead including the proposed Cam×Time. We\nobserve that applying a 1D convolution to learn a compact\nrepresentation by compressing the fine-grained F-dim em-\nbeddings into a F ′-dim space performs notably better than\ndirectly constructing sinusoidal embeddings at the coarse\nf ′ level.\nIncorporating static-scene datasets yields only\nlimited improvements, likely due to their restricted tem-\nporal control patterns.\nBy contrast, using the proposed\nCam×Time consistently delivers the largest gains across\nall three metrics, confirming the effectiveness of our newly\nintroduced datasets. Furthermore, as shown in Fig. 7, we\npresent a visual comparison of bullet-time results using uni-\nform sampling and an MLP instead of the 1D convolu-\ntion for compressing the temporal control signal. Uniform\nsampling produces noticeable artifacts, and the MLP com-\npressor causes abrupt camera motion, whereas the 1D con-\nvolution effectively locks the animation time and enables\nsmooth camera movement.\n5. Conclusion\nWe present SpaceTimePilot, the first video diffusion model\nto provide fully disentangled spatial and temporal control,\nenabling 4D space-time exploration from a single monoc-\nular video.\nOur method introduces a new “animation\ntime” representation together with a source-aware camera-\ncontrol mechanism that leverages both source and target\nposes. This is supported by the synthetic Cam×Time and\na temporal-warping training scheme, which supply dense\nspatiotemporal supervision. These components allow pre-\ncise camera and time manipulation, arbitrary initial poses,\nand flexible multi-round generation. Across extensive ex-\nperiments, SpaceTimePilot consistently surpasses state-of-\nthe-art baselines, offering significantly improved camera-\ncontrol accuracy and reliable execution of complex retiming\neffects such as reverse playback, slow motion, and bullet-\ntime.\n6. Acknowledgement\nWe would like to extend our gratitude to Duygu Ceylan,\nPaul Guerrero, and Zifan Shi for insightful discussions and\nvaluable feedback on the manuscript. We also thank Rudi\nWu for helpful discussions on implementation details of\nCAT4D.\nReferences\n[1] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan,\nXiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and\nDi Zhang.\nSynCamMaster: Synchronizing multi-camera\nvideo generation from diverse viewpoints.\narXiv preprint\narXiv:2412.07760, 2024. 2, 3, 5, 14, 16, 17\n[2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui\nMu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei\nWan, and Di Zhang. ReCamMaster: Camera-controlled gen-\nerative rendering from a single video. In ICCV, 2025. 2, 3,\n4, 5, 6, 7, 8, 14, 16, 17\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her-\nrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur,\nGuanghui Liu, Amit Raj, et al. Lumiere: A space-time diffu-\nsion model for video generation. In SIGGRAPH Asia 2024\nConference Papers, pages 1–11, 2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh.\nVideo generation models as world simulators.\n2024. 2\n[6] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nICCV, pages 5712–5721, 2021. 2\n[7] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,\nYilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-\ngasam, Florian Golemo, Charles Herrmann, Thomas Kipf,\nAbhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-\nTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek\nNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-\nwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,\nMatan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,\nSuhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,\nFangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-\nable dataset generator. In CVPR, 2022. 3\n[8] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo\nDai, Hongsheng Li, and Ceyuan Yang.\nCameraCtrl: En-\nabling camera control for text-to-video generation. In ICLR,\n2025. 8\n[9] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng\nWei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and\n9\nHongsheng Li. CameraCtrl II: Dynamic scene exploration\nvia camera-controlled video diffusion models. arXiv preprint\narXiv:2503.10592, 2025. 2\n[10] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si,\nYuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin,\nNattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin\nWang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Com-\nprehensive benchmark suite for video generative models. In\nCVPR, 2024. 7\n[11] Adobe Systems Inc. Mixamo, 2018. Accessed: 2025-03-07.\n12\n[12] Hyeonho Jeong, Chun-Hao Paul Huang, Jong Chul Ye, Niloy\nMitra, and Duygu Ceylan. Track4Gen: Teaching video dif-\nfusion models to track points improves video generation. In\nCVPR, 2025. 2\n[13] Hyeonho Jeong, Suhyeon Lee, and Jong Chul Ye. Reangle-\nA-Video: 4d video generation as video-to-video translation.\nIn ICCV, 2025. 3\n[14] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi,\nTianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang\nXu. LVSM: A large view synthesis model with minimal 3d\ninductive bias. In ICLR, 2025. 2\n[15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¨uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Trans. Graph., 42(4):139–1,\n2023. 2\n[16] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024.\n2\n[17] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and\nKostas Daniilidis. MoSca: Dynamic gaussian fusion from\ncasual videos via 4d motion scaffolds. In CVPR, 2025. 2\n[18] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker,\nNoah Snavely, Ce Liu, and William T Freeman. Learning\nthe depths of moving people by watching frozen people. In\nCVPR, 2019. 7, 9, 14, 16\n[19] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In CVPR, pages 6498–6508, 2021. 2\n[20] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\nrendering. In CVPR, 2023. 2\n[21] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang,\nZhangyang Wang, Konstantinos N Plataniotis, Yao Zhao,\nand Yunchao Wei. Diffusion4D: Fast spatial-temporal con-\nsistent 4d generation via video diffusion models. In NeurIPS,\n2024. 2\n[22] Jinwei Lin.\nDynamic NeRF: A review.\narXiv preprint\narXiv:2405.08609, 2024. 2\n[23] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin,\nKun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu,\net al.\nDL3DV-10K: A large-scale scene dataset for deep\nlearning-based 3d vision.\nIn CVPR, pages 22160–22169,\n2024. 2, 5\n[24] Jiaxin Lu, Chun-Hao Paul Huang, Uttaran Bhattacharya,\nQixing Huang, and Yi Zhou. Humoto: A 4d dataset of mocap\nhuman object interactions. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n10886–10897, 2025. 12\n[25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, pages 99–106. ACM New York, NY, USA,\n2021. 2\n[26] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhen-\nheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai.\nOpenVid-1M: A large-scale high-quality dataset for text-to-\nvideo generation. arXiv preprint arXiv:2407.02371, 2024.\n7\n[27] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn ICCV, pages 5865–5874, 2021. 2\n[28] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B. Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHyperNeRF: A higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Transactions on Graphics (TOG), 40\n(6):238:1–238:12, 2021. 2\n[29] Jack Parker-Holder and Shlomi Fruchter. Genie 3: A new\nfrontier for world models. Google DeepMind Blog, 2025.\nAccessed: ¡insert date you retrieved¿. 2, 3\n[30] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary,\nDingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan\nMisra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng\nLi, Luxin Zhang, Mannat Singh, Mary Williamson, Matt\nLe, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Pe-\nter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly,\nSai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak\nDatta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy,\nShelly Sheynin, Siddharth Bhattacharya, Simran Motwani,\nTao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xi-\naoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu,\nYi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian\nHe, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun\nMallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh\nWood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schon-\nfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff\nLiang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik\nSivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos\nGeorgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li,\nSimone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petro-\nvic, and Yuming Du. Movie gen: A cast of media foundation\nmodels, 2024. 2\n[31] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling,\nYifan Lu, Merlin Nimier-David, Thomas M¨uller, Alexander\nKeller, Sanja Fidler, and Jun Gao.\nGEN3C: 3d-informed\nworld-consistent video generation with precise camera con-\ntrol. In CVPR, 2025. 3\n10\n[32] Chris Rockwell, Joseph Tung, Tsung-Yi Lin, Ming-Yu Liu,\nDavid F. Fouhey, and Chen-Hsuan Lin.\nDynamic camera\nposes and where to find them. In CVPR, 2025. 5\n[33] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sar-\ngent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi\nZheng, and Carl Vondrick. Generative camera dolly: Ex-\ntreme monocular dynamic novel view synthesis.\nECCV,\n2024. 2, 3, 4, 5\n[34] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao,\nChen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianx-\niao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jin-\ngren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao,\nKeyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang,\nPandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei\nZhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui,\nTingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang,\nWenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu,\nXianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu\nLv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yi-\ntong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun\nZheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi\nJiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open\nand advanced large-scale video generative models.\narXiv\npreprint arXiv:2503.20314, 2025. 4, 6\n[35] Chaoyang Wang,\nAshkan Mirzaei,\nVidit Goel,\nWilli\nMenapace, Aliaksandr Siarohin, Avalon Vinella, Michael\nVasilkovsky, Ivan Skorokhodov, Vladislav Shakhrai, Sergey\nKorolev, Sergey Tulyakov, and Peter Wonka. 4real-video-\nv2: Fused view-time attention and feedforward reconstruc-\ntion for 4d scene generation. In Adv. Neural Inform. Process.\nSyst., 2025. 3\n[36] Chaoyang Wang, Peiye Zhuang, Tuan Duc Ngo, Willi Mena-\npace, Aliaksandr Siarohin, Michael Vasilkovsky, Ivan Sko-\nrokhodov, Sergey Tulyakov, Peter Wonka, and Hsin-Ying\nLee. 4real-video: Learning generalizable photo-realistic 4d\nvideo diffusion. In CVPR, 2025. 3\n[37] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi\nLi, and Angjoo Kanazawa. Shape of motion: 4d reconstruc-\ntion from a single video. In ICCV, 2025. 2\n[38] Shuzhe Wang,\nVincent Leroy,\nYohann Cabon,\nBoris\nChidlovskii, and Jerome Revaud. DUSt3R: Geometric 3d\nvision made easy. In CVPR, 2024. 8\n[39] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu,\nPeiqing Yang, et al. Lavie: High-quality video generation\nwith cascaded latent diffusion models. IJCV, pages 1–20,\n2024. 2\n[40] Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon,\nYue Zhang, and Mohit Bansal. EPiC: Efficient Video Camera\nControl Learning with Precise Anchor-Video. arXiv preprint\narXiv:2505.21876, 2025. 3\n[41] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasac-\nchi, and David J. Fleet.\nControlling space and time with\ndiffusion models. In ICLR, 2025. 2, 3, 4, 5, 7\n[42] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 20310–20320, 2024. 2\n[43] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi\nZheng, Jonathan T Barron, and Aleksander Holynski.\nCat4D: Create anything in 4d with multi-view video diffu-\nsion models. In CVPR, 2024. 2, 3, 4, 5, 7\n[44] Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu,\nDahua Lin, and Gordon Wetzstein. Video world models with\nlong-term spatial memory. arXiv preprint arXiv:2506.05284,\n2025. 3\n[45] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Iurii\nMakarov, Bingyi Kang, Xin Zhu, Hujun Bao, Yujun Shen,\nand Xiaowei Zhou.\nSpatialTrackerV2: 3d point tracking\nmade easy. In ICCV, 2025. 8\n[46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 2\n[47] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park,\nand Jan Kautz.\nNovel view synthesis of dynamic scenes\nwith globally coherent depths from a monocular camera. In\nCVPR, pages 5339–5348, 2020. 2\n[48] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan.\nTra-\njectoryCrafter: Redirecting camera trajectory for monocular\nvideos via diffusion models. In ICCV, 2025. 3, 7, 8\n[49] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li,\nZhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan,\nand Yonghong Tian. Viewcrafter: Taming video diffusion\nmodels for high-fidelity novel view synthesis. arXiv preprint\narXiv:2409.02048, 2024. 3\n[50] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Kar-\nnad, David E Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng\nShou, Neal Wadhwa, and Nataniel Ruiz. ReCapture: Gen-\nerative video camera controls for user-provided videos using\nmasked video fine-tuning. In CVPR, 2024. 3\n[51] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation. IJCV, pages 1–15, 2024. 2\n[52] Jensen (Jinghao) Zhou, Hang Gao, Vikram Voleti, Aaryaman\nVasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian\nRupprecht, and Varun Jampani. Stable Virtual Camera: Gen-\nerative view synthesis with diffusion models. arXiv preprint,\n2025. 3\n[53] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magnification: Learning view syn-\nthesis using multiplane images. In SIGGRAPH, 2018. 2, 5,\n7, 9, 14, 16\n11\nA. Network Architecture\nThe network architecture of SpaceTimePilot is depicted in\nFig. 8. The newly introduced animation-time embedder Eani\nencodes the source and target animation times, tsrc and ttrg,\ninto tensors matching the shapes of xsrc and xtrg, which are\nthen added to them respectively. During training, we train\nonly the camera embedder Ecam, the animation-time embed-\nder Eani, the self-attention (full-3D attention), and the pro-\njector layers before the cross-attention.\nfff\nff\nZsrc\nZtrg\nAdd noise\nPatchify\nSpace-Time DiT\nZ’trg\n3D-VAE\nDecoder\n3D-VAE\nencoder\nVt’\n /t\n/t\nCamera\nembedding\nTime\nEmbedding\nSelf-Attention\nProjector\nCross-Attention\nFeed-forward NN\ncam\ncam\ncam\ncam\ncam\ncam\ncam\ncam\ntime\ntime\ntime\ntime\ntime\ntime\ntime\ntime\n(patchified Ztrg)\n(patchified Zsrc)\nFigure 8. Architecture of SpaceTimePilot. Our model jointly\nconditions on camera trajectories and temporal control signals via\nspace–time attention, enabling non-monotonic motion generation\nsuch as reversals, repeats, accelerations, and zigzag time.\nB. Longer Space-Time Exploration Video with\nDisentangled Controls\nOne of the central advantages of SpaceTimePilot is its abil-\nity to freely navigate both spatial and temporal dimen-\nsions, with arbitrary starting points in each dimension and\nfully customizable trajectories through them.\nAlthough\neach individual generation is limited to an 81-frame win-\ndow, we show that SpaceTimePilot can effectively extend\nthis window indefinitely through a multi-turn autoregres-\nsive inference scheme, enabling continuous and control-\nlable space–time exploration from a single input video. The\noverall pipeline is illustrated in Fig. 9.\nThe core idea is to generate the final video in autore-\ngressive segments that connect seamlessly. For example,\ngiven a source video of 81 frames, we may first generate\na 0.5× slow-motion sequence covering frames 0–40 with\na new camera trajectory. Then, continuing both the visual\ncontext and the generated camera trajectory, we can pro-\nduce the next segment starting from the final camera pose\nof the previous output, while temporally traversing the re-\nmaining frames 40–81. This yields an autoregressive chain\nof viewpoint-controlled video segments that together create\na continuous long-range space–time trajectory.\nA key property that enables this behavior is that our\nmodel can generate video segments whose camera poses do\nnot need to start at the first frame. This allows precise con-\ntrol over the starting point, both in time and viewpoint, for\nevery generated chunk, ensuring smooth, consistent motion\nover extended sequences.\nTo maintain contextual coherence across iterations, we\nintroduce a lightweight memory mechanism. During train-\ning, the model is conditioned on a pair of source videos,\nwhich enables consistent chaining during inference. Specif-\nically:\n• At iteration i = 1, the model is conditioned only on the\noriginal source video.\n• At iteration i = 2, it is conditioned on both the source\nvideo and the previously generated 81-frame segment.\n• This process repeats, with each iteration conditioning on\nthe source video as well as the most recent generated seg-\nment.\nThis simple yet effective strategy allows SpaceTimePilot\nto generate arbitrarily long, smoothly connected sequences\nwith continuous and precise control over both temporal ma-\nnipulation and camera motion.\nHere, we showcase how this can be used to conduct large\nviewpoint changes, as demonstrated in Fig. 10.\nC.\nAdditional\nDetails\non\nthe\nProposed\nCam×Time Dataset.\nThe Cam×Time dataset is built using high-quality, com-\nmercially licensed 3D environments that include both real-\nistic indoor and outdoor scenes. For each environment, we\npopulate the space with multiple animated human charac-\nters. The character assets are sourced from Mixamo [11]\nand HUMOTO [24], and each character is manually tex-\ntured and refined to ensure realistic geometry, appearance,\nand material quality. The animations span a diverse range\nof human motions, including locomotion, gestures, and\nhuman-object interactions. Examples of scenes are shown\nin Fig. 11. Please refer to the complementary website for\nthe video examples.\nTo capture rich spatial coverage, we generate four dis-\ntinct camera trajectories for every scene.\nCamera paths\ninclude rotational orbits, linear tracking motions, and\nsmoothly curved arcs. A dedicated validity module ensures\nthat each trajectory: (1) begins at a collision-free location\nwith clear visibility of the main character, (2) maintains\nnon-intersecting movement with the environment through-\nout the path, and (3) preserves full subject visibility across\nall viewpoints.\n12\nSpaceTimePilot\ndual src video\nSource video1 (81 frames)\ncamera traj 1\n(wrt src video 1)\ntemporal traj 1 \n(wrt src video 1)\nSpaceTimePilot\ndual src video\nOutput video 1\n(81 frames)\nOutput video 2\n(81 frames)\nVideo Stitching\nFinal Output.   (81 * n frames)\ncamera traj 2\n(wrt src video 1)\ntemporal traj 2 \n(wrt src video 1)\nSpaceTimePilot\ndual src video\ncamera traj 2\n(wrt src video 1)\ntemporal traj 2 \n(wrt src video 1)\nCopy of Source video1\n...\nFigure 9. Overview of the multi-turn autoregressive inference scheme. The model first generates an 81-frame segment conditioned\non the source video and a chosen space–time trajectory. The resulting output is then reused as a secondary source video for subsequent\niterations, each with its own camera and temporal trajectory. By chaining these iterations and stitching the outputs, SpaceTimePilot\nproduces a long, coherent video that follows an arbitrary space–time path.\nFigure 10.\nMulti-turn autoregressive generation with SpaceTimePilot. Top row: source video frames. Rows 2–4: Turn-1, Turn-2,\nand Turn-3 generations. At each turn, SpaceTimePilot jointly conditions on (1) the original source video and (2) the previously generated\nchunk, ensuring temporal continuity, stable motion progression, and consistent camera geometry. This dual-conditioning design enables\nviewpoint changes far beyond the input video—such as rotating to the rear of the tiger or transitioning from a low-angle shot to a high\nbird’s-eye view—while preserving visual and motion coherence. Please refer to section “AR Demos” in the website for videos.\nEach trajectory is rendered into a 120-frame sequence at\na resolution of 1080 × 1080 pixels, providing dense tem-\nporal sampling with high visual fidelity. This yields three\nmulti-view video sequences per scene, each covering the\nfull motion duration with consistent lighting, textures, and\ngeometry. Overall, we rendered 1500 videos from 500 an-\nimations, each with 120 videos full grid rendering, leading\nto 180k videos.\nFor temporal-control training, we could sample any time\nvariants from these sequences, including slow motion, re-\nverse playback, bullet-time around arbitrary frames, and\nnon-monotonic time patterns such as forward–backward os-\ncillation. These augmented temporal signals are illustrated\nin Fig. 12.\n13\nFigure 11. Example of Cam×Time. Multi-view, densely sam-\npled sequences from the Cam×Time dataset. Each row shows\nframes from one camera trajectory, and each column samples dif-\nferent timesteps (0–120). The dataset provides diverse environ-\nments, human motions, and four camera paths per scene with full\n120-frame temporal coverage.\nFigure 12. Sampling from Cam×Time. By sampling from the\nCam×Time dataset, we can extract frames corresponding to arbi-\ntrary combinations of camera viewpoints and temporal positions,\nforming source-target pairs with rich camera and temporal control\nsignals.\nD. Additional Ablation Studies\nD.1. Temporal Warping Augmentation\nUsing [1, 2] as our default datasets, we compare training\njointly with static-scene datasets [18, 53] with applying\nonly temporal warping (TW) augmentation on the default\ndatasets (Sec. 3.2.2 in the main paper). Although static-\nscene datasets naturally support bullet-time effects, they do\nnot provide enough diversity of temporal control configura-\ntions for models to reliably learn time locking on their own,\nas shown in Fig. 14 (top). Please refer to section “Effective\nTemporal Warping” in the website for more videos.\nIn Fig. 14 (bottom), we further show that freezing tempo-\nral warping (3rd row) produces better results than training\nwithout freezing it. Please refer to section “Freeze Warping\nAblations” in the website for more videos.\nD.2. Significance of Cam×Time Dataset\nBesides the quantitative results in the main paper (Table 5),\nin Fig. 15 (top), we provide visual comparisons demonstrat-\ning the effectiveness of the proposed Cam×Time dataset.\nClear artifacts appear in baselines trained without additional\ndata or with only static-scene augmentation (highlighted\nin red boxes), whereas incorporating Cam×Time removes\nthese artifacts, demonstrating its significance. Please re-\nfer to section “Dataset Ablations” in the website for more\nvideos.\nD.3. Time Embedding Ablation\nAs promised in Sec. 3.2.1 in the main paper, we compare\nseveral time-embedding strategies. RoPE(f ′) can freeze the\nscene dynamics at t=40, but it also undesirably locks the\ncamera motion. Using MLP, by contrast, fails to lock the\ntemporal state at all (red boxes). Conditioning on the la-\ntent frame f ′ (with uniform sampling) introduces notice-\nable artifacts. In comparison, the proposed 1D-Conv em-\nbedding enables SpaceTimePilot to preserve the intended\nscene dynamics while still generating accurate camera mo-\ntion. Adding Cam×Time to training further enhances the\nresults. Please refer to section “Time-Embedding Method\nAblation” in the website for more examples.\nE. Additional Qualitative Visualizations\nWe show more qualitative results of SpaceTimePilot in\nFig. 13. Our model provides fully disentangled control over\ncamera motion and temporal dynamics. Each row presents\na different pairing of temporal control inputs (top-left icon)\nand camera trajectories. SpaceTimePilot reliably generates\ncoherent videos under diverse conditions, including normal\nand reverse playback, bullet-time, slow motion, replay mo-\ntion, and complex camera movements such as panning, tilt-\ning, zooming, and vertical translation. Please refer to sec-\ntion “Video Demonstrations” in the website for more exam-\nples.\n14\nbullet-time\nat t=40\nreverse\nmotion\nbounce\n40-80-40\nslow motion\n40-80\nFrame 0\nFrame 40\nFrame 80\n...\n...\nFrame 0\nFrame 40\nFrame 80\n...\n...\nbounce\n40-80-40\nbounce\n40-80-40\nslow motion\n40-80\nbounce\n40-80-40\nFigure 13. More Qualitative results. Our model provides fully disentangled control over camera motion and temporal dynamics. Each\nrow illustrates a different combination of temporal control inputs (top-left icon) and camera trajectories. SpaceTimePilot consistently\nproduces coherent videos across a wide range of controls, including normal and reverse playback, bullet-time, slow motion, replay motion,\nand complex camera paths such as panning, tilting, zooming, and vertical motion.\n15\nSrc Video\nDefault+\nRE10k+MC\nDefault \nw/ TW\nAblation 1: Temporal warping (TW) vs. Joint dataset training on bullet-time effect (t=40)\nSrc Video\n1D-Conv\n1DConv\n+Cam×Time\nw/o Freezing\nTemproal\nWarping\n1DConv\n+Cam×Time w\nFreezing\nTemporal\nWarping\nAblation 2: Varied temporal warping configuration on bullet-time effect (t=40) \nFigure 14. Ablation study. (Top) Using [1, 2] as default datasets, we compare the influence of adding static-scene datasets [18, 53] vs. just\ndoing temporal warping (TW) augmentation (Sec. 3.2.2 in the main paper). Temporal warping definitely provide more variety of time\ncontrol signals, allowing models to learn better camera-time disentanglement. (Bottom) We further compare different configurations of\nwarping, where we show freezing temporal warping (3rd row) leads to better results than those trained without freezing temporal warping.\n16\nSrc Video\nDefault  w/ TW\nDefault w/ TW\n＋ Cam×Time\nAblation 3: Dataset ablation: Cam×Time vs. static-scene datasets on bullet-time effect (t=40) \nDefault w/ TW\n＋ RE10k ＋ MC \nSrc Video\nMLP\nUniform\nSampling\n1D-Conv\n1D-Conv w/\nCam×Time\nAblation 4: Different time embedding schemes on bullet-time effect (t=40)\nRoPE(f’)\nFigure 15. Ablation study. (Top) We verify the efficacy of the proposed Cam×Time dataset. Considering [1, 2] as default datasets,\nwe compare the impact of different datasets on the generated videos. One can clearly see artifacts in baselines without any extra data or\naugmented with static-scene data, whereas training additionally with Cam×Time leads to no artifacts, confirming the usefulness of our\ndataset. (Bottom) We compare several time-embedding strategies. The MLP fails to lock the temporal state (red boxes), while RoPE(f ′)\ncorrectly freezes the scene dynamics at t=40 but unintentionally locks the camera motion too. Conditioning on the latent frame f ′ (with\nuniform sampling) introduces noticeable artifacts. In contrast, the proposed 1D-Conv embedding allows SpaceTimePilot to both freeze the\nscene dynamics at t=40 and produce intended camera motion. Incorporating Cam×Time during training further improves performance.\n17\n",
  "pages": [
    {
      "page_number": 1,
      "text": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across\nSpace and Time\nZhening Huang1,2\nHyeonho Jeong2\nXuelin Chen2\nYulia Gryaditskaya2\nTuanfeng Y. Wang2\nJoan Lasenby1\nChun-Hao Huang2\n1University of Cambridge\n2Adobe Research\nhttps://zheninghuang.github.io/Space-Time-Pilot/\nt=80\nt=80\nreverse\nmotion\nt=10\nt=0\nt=20\nt=40\nt=60\nSource Video\nSynthesized Video\nt=70\nt=80\nt=60\nt=40\nt=20\nt=0\nbullet-time\nat t=60\nSource Video\nSynthesized Video\nt=10\nt=0\nt=20\nt=40\nt=60\nt=60\nt=60\nt=60\nt=60\nt=60\nt=60\nslow motion\nx0.5\nSource Video\nSynthesized Video\nt=10\nt=0\nt=20\nt=40\nt=60\nt=5\nt=0\nt=10\nt=20\nt=30\nt=40\nFrame Number\n80\n你的段落⽂字\nTwo ballet dancers in blue and white costumes performing a graceful routine on a dark stage with a red velvet curtain and dramatic spotlights.\nA couple in 18th-century Baroque attire dancing on a theater stage in front of a live symphony orchestra; warm theatrical lighting.\nA couple performing a retro swing dance on a Parisian bridge; woman in a red polka-dot dress, classic Haussmann buildings in the background.\nCamera Trajactory Control\nAnimation Time Control\nSpaceTimePilot\nSpaceTimePilot\nSpaceTimePilot\n0\nt=80\nFigure 1. SpaceTimePilot enables unified control over both camera and time within a single diffusion model, producing continuous and\ncoherent videos along arbitrary space–time trajectories. Given a source video (odd rows), our model synthesizes new videos (even rows)\nwith retimed motion sequences, including slow motion, reverse motion, and bullet time, while precisely controlling camera movement\naccording to a given camera trajectory.\nAbstract\nWe present SpaceTimePilot, a video diffusion model that\ndisentangles space and time for controllable generative ren-\ndering.\nGiven a monocular video, SpaceTimePilot can\nindependently alter the camera viewpoint and the motion\nsequence within the generative process, re-rendering the\nscene for continuous and arbitrary exploration across space\nand time. To achieve this, we introduce an effective an-\nimation time-embedding mechanism in the diffusion pro-\ncess, allowing explicit control of the output video’s motion\nsequence with respect to that of the source video. As no\ndatasets provide paired videos of the same dynamic scene\nwith continuous temporal variations, we propose a simple\n1\narXiv:2512.25075v1  [cs.CV]  31 Dec 2025\n"
    },
    {
      "page_number": 2,
      "text": "yet effective temporal-warping training scheme that repur-\nposes existing multi-view datasets to mimic temporal dif-\nferences. This strategy effectively supervises the model to\nlearn temporal control and achieve robust space–time dis-\nentanglement.\nTo further enhance the precision of dual\ncontrol, we introduce two additional components: an im-\nproved camera-conditioning mechanism that allows alter-\ning the camera from the first frame, and Cam×Time, the first\nsynthetic Space and Time full-coverage rendering dataset\nthat provides fully free space–time video trajectories within\na scene. Joint training on the temporal-warping scheme\nand the Cam×Time dataset yields more precise temporal\ncontrol. We evaluate SpaceTimePilot on both real-world\nand synthetic data, demonstrating clear space–time disen-\ntanglement and strong results compared to prior work.\n1. Introduction\nVideos are 2D projections of an evolving 3D world, where\nthe underlying generative factors consist of spatial variation\n(camera viewpoint) and temporal evolution (dynamic scene\nmotion). Learning to understand and disentangle these fac-\ntors from observed videos is fundamental for tasks such\nas scene understanding, 4D reconstruction, video editing,\nand generative rendering, to name a few.\nIn this work,\nwe approach this challenge from the perspective of gen-\nerative rendering. Given a single observed video of a dy-\nnamic scene, our goal is to synthesize novel views (re-\nframe/reangle) and/or at different moments in time (retime),\nwhile remaining faithful to the underlying scene dynamic.\nA common strategy is to first reconstruct dynamic 3D\ncontent from 2D observations, i.e., perform 4D reconstruc-\ntion, and then re-render the scene. These methods model\nboth spatial and temporal variations using representations\nsuch as NeRFs [22, 25] or Dynamic Gaussian Splatting\n[15, 42], often aided by cues like geometry [27, 28], op-\ntical flow [19, 20], depth [6, 47], or long-term 2D tracks\n[17, 37].\nHowever, even full 4D reconstructions typi-\ncally show artifacts under novel viewpoints. More recent\nwork [21, 43] uses multi-view video diffusion to gener-\nate sparse, time-conditioned views and refines them via\nGaussian-splatting optimization, but rendering quality re-\nmains limited.\nAdvances in video diffusion models [3–\n5, 12, 16, 30, 39, 46, 51] further enable camera re-posing\nwith more lightweight point cloud representations, reduc-\ning the need for heavy 4D reconstruction. While effective in\npreserving identity, their reliance on per-frame depth and re-\nprojection limits robustness under large viewpoint changes.\nTo mitigate this, newer approaches condition generation\nsolely on camera parameters, achieving strong novel-view\nsynthesis on both static [14] and dynamic scenes [2, 9, 33].\nAutoregressive models like Genie-3 [29] even enable inter-\nactive scene exploration from a single image, showing that\nCamera\nCamera\nCamera\nCamera Control Models\n4D Multi-View Models\nSpaceTimePilot\nTime\nobserved input\nsynthesized views or starting frames of videos\nframe progression\nTime\nTime\nFigure 2. Space–time controllability across methods. Blue cells\ndenote the input video/views, while arrows and dots indicate gen-\nerated continuous videos or sparse frames. Camera-control V2V\nmodels [2, 33] modify only the camera trajectory while keeping\ntime strictly monotonic. 4D multi-view models [21, 43] synthe-\nsize discrete sparse views conditioned on space and time, but do\nnot generate continuous video sequences. SpaceTimePilot enables\nfree movement along both the camera and time axes with full con-\ntrol over direction and speed, supporting bullet-time, slow-motion,\nreverse playback, and mixed space–time trajectories.\ndiffusion models can encode implicit 4D priors. Nonethe-\nless, despite progress in spatial viewpoint control, current\nmethods still lack full 4D exploration, i.e., the ability to\nnavigate scenes freely across both space and time.\nIn this work, we introduce SpaceTimePilot, the first\nvideo diffusion model that enables joint spatial and tem-\nporal control. SpaceTimePilot introduces a new notion of\n“animation time” to capture the temporal status of scene\ndynamics in the source video. As such, it naturally disen-\ntangles temporal control and camera control by expressing\nthem as two independent signals. A high-level comparison\nbetween our approach and prior methods is illustrated in\nFig. 2. Unlike previous methods, SpaceTimePilot enables\nfree navigation along both the camera and time axes. Train-\ning such a model requires dynamic videos that exhibit mul-\ntiple forms of temporal playback while simultaneously be-\ning captured under multiple camera motions, which is only\nfeasible in a controlled studio setups. Although temporal di-\nversity can be increased by combining multiple real datasets\ne.g. [23, 53], as done in [41, 43], this approach remains sub-\noptimal, as the coverage of temporal variation is still insuf-\nficient to learn the underlying meaning of temporal control.\nExisting synthetic datasets [1, 2] also do not exhibit such\nproperties.\nTo address this limitation, we introduce a simple yet\neffective temporal-warping training scheme that augments\nexisting multi-view video datasets [1, 2] to simulate di-\nverse conditioning types while preserving continuous video\nstructure. By warping input sequences in time, the model\nis exposed to varied temporal behaviors without requiring\nadditional data collection.\nThis simple yet crucial strat-\negy allows the model to learn temporal control signals, en-\nabling it to directly exhibit space–time disentanglement ef-\nfects during generation. We further ablate various temporal-\nconditioning schemes and introduce a convolution-based\n2\n"
    },
    {
      "page_number": 3,
      "text": "temporal-control mechanism that enables finer-grained ma-\nnipulation of temporal behavior and supports effects such\nas bullet-time at any timestep within the video. While tem-\nporal warping increases temporal diversity, it can still en-\ntangle camera and scene dynamics – for example, tempo-\nral manipulation may inadvertently affect camera behavior.\nTo further strengthen disentanglement, we introduce a new\ndataset that spans the full grid of camera–time combinations\nalong a trajectory. Our synthetic Cam×Time dataset con-\ntains 180k videos rendered from 500 animations across 100\nscenes and three camera paths. Each path provides full-\nmotion sequences for every camera pose, yielding dense\nmulti-view and full-temporal coverage. This rich supervi-\nsion enables effective disentanglement of spatial and tem-\nporal control.\nExperimental results show that SpaceTimePilot success-\nfully disentangles space and time in generative rendering\nfrom single videos, outperforming adapted state-of-the-art\nbaselines by a significant margin. Our main contributions\nare summarized as follows:\n• We introduce SpaceTimePilot, the first video diffusion\nmodel that disentangles spatial and temporal factors to en-\nable continuous and controllable novel view synthesis as\nwell as temporal control from a single video.\n• We propose the temporal-warping strategy that repur-\nposes multi-view datasets to simulate diverse temporal\nvariations. By training on these warped sequences, the\nmodel effectively learns temporal control without the\nneed for explicitly constructed video pairs captured un-\nder different temporal settings.\n• We propose a more precise camera–time conditioning\nmechanism, illustrating how viewpoint and temporal em-\nbeddings can be jointly integrated into diffusion models\nto achieve fine-grained spatiotemporal control.\n• We construct the Cam×Time Dataset, providing dense\nspatiotemporal sampling of dynamic scenes across cam-\nera trajectories and motion sequences. This dataset sup-\nplies the necessary supervision for learning disentangled\n4D representations and supports precise camera–time\ncontrol in generative rendering.\n2. Related work\nWe aim to re-render a video from new viewpoints with tem-\nporal control, a task closely related to Novel View Synthesis\n(NVS) from monocular video inputs.\nVideo-based NVS. Prior video-based NVS methods can\nbe broadly characterized along two axes: (i) whether they\ntarget static or dynamic scenes, and (ii) whether they incor-\nporate explicit 3D geometry in the generation pipeline.\nFor static scenes, geometry-based methods reconstruct\nscene geometry from the input frames and use diffusion\nmodels to complete or hallucinate regions that are unseen\nunder new viewpoints [13, 31, 44, 49]. Although these ap-\nproaches achieve high rendering quality, they rely on heavy\n3D preprocessing. Geometry-free approaches [2, 33, 52]\nbypass explicit geometry and directly condition the diffu-\nsion process on observed views and camera poses to syn-\nthesize new viewpoints.\nFor dynamic scenes, inpainting-based methods such as\nTrajectoryCrafter [48], ReCapture [50], and Reangle [13]\nalso adopt warp-and-inpaint pipelines, while GEN3C [31]\nextends this with an evolving 3D cache and EPiC [40] im-\nproves efficiency via a lightweight ControlNet framework.\nGeometry-free dynamic models [1, 2, 33, 35, 36] instead\nlearn camera-conditioned generation from multi-view or 4D\ndatasets (e.g., Kubric-4D [7]), enabling smoother and more\nstable NVS with minimal 3D inductive bias. Proprietary\nsystems like Genie 3 [29] further demonstrate real-time,\ncontinuous camera control in dynamic scenes, underscor-\ning the potential of video diffusion models for interactive\nviewpoint manipulation.\nDisentangling Space and Time. Despite great progress in\ncamera controllability (space), the methods discussed above\ndo not address temporal control (time). Meanwhile, disen-\ntangling spatial and temporal factors has become a central\nfocus in 4D scene generation, recently advanced through\ndiffusion-based models. 4DiM [41] introduces a Masked\nFiLM mechanism that defaults to identity transformations\nwhen conditioning signals (e.g., camera pose or time) are\nabsent, enabling unified representations across both static\nand dynamic data through multi-modal supervision. Simi-\nlarly, CAT4D [43] leverages multi-view images to conduct\n4D dynamic reconstruction to achieve space–time disen-\ntanglement but remains constrained by its reliance on ex-\nplicit 4D reconstruction pipelines, which limits scalability\nand controllability. In contrast, our approach builds upon\ntext-to-video diffusion models and introduces a new tempo-\nral embeddings module and refined camera conditioning to\nachieve fully controllable 4D generative reconstruction.\n3. Method\nWe introduce SpaceTimePilot, a method that takes a source\nvideo Vsrc ∈RF ×C×H×W as input and synthesizes a tar-\nget video Vtrg ∈RF ×C×H×W , following an input cam-\nera trajectory ctrg ∈RF ×3×4 and temporal control signal\nttrg ∈RF . Here, F denotes the number of frames, C the\nnumber of color channels, and H and W are the frame\nheight and width, respectively. Each cf\ntrg ∈R3×4 represents\nthe camera extrinsic parameters (rotation and translation) at\nframe f, with respect to the 1st frame of Vsrc. The target\nvideo Vtrg preserves the scene’s underlying dynamics, ge-\nometry, and appearance in Vsrc, while adhering to the cam-\nera motion and temporal progression specified by ctrg and\nttrg. A key feature of our method is the disentanglement\n3\n"
    },
    {
      "page_number": 4,
      "text": "of spatial and temporal factors in the generative process,\nenabling effects such as bullet-time and retimed playback\nfrom novel viewpoints (see Fig. 1).\n3.1. Preliminaries\nOur framework builds upon recent advances in large-scale\ntext-to-video diffusion models and camera-conditioned\nvideo generation. We adopt a latent video diffusion back-\nbone similar to modern text-to-video foundation models\n[34], consisting of a 3D Variational Auto-Encoder (VAE)\nfor latent compression and a Transformer-based denoising\nmodel (DiT) operating over multi-modal tokens.\nAdditionally,\nour\ndesign\ndraws\ninspiration\nfrom\nReCamMaster [2],\nwhich introduces explicit camera\nconditioning for video synthesis. Given an input camera\ntrajectory c ∈RF ×3×4, spatial conditioning is achieved by\nfirst projecting the camera sequence to the space of video\ntokens and adding it to the features:\nx′ = x + Ecam (c) ,\n(1)\nwhere x is the output of the patchifying module and x′ is\nthe input to self-attention layers. The camera encoder Ecam\nmaps each flattened 3 × 4 camera matrix (12-dimensional)\ninto the target feature space, while also transforming the\ntemporal dimension from F to F ′.\n3.2. Disentangling Space and Time\nWe achieve spatial and temporal disentanglement through\na two-fold approach: a dedicated time representation and\nspecialized datasets.\n3.2.1. Time representation\nRecent video diffusion models include position embeddings\nfor latent frame index f ′, such as RoPE(f ′). However, we\nfound using RoPE(f ′) for temporal control to be ineffective,\nas it interferes with camera signals: RoPE(f ′) often con-\nstrains both temporal and camera motion simultaneously.\nTo address space and time disentanglement, we introduce a\ndedicated time control parameter t ∈RF . By manipulating\nttrg, we can control the temporal progression of the synthe-\nsized video Vtrg. For example, setting ttrg to a constant locks\nVtrg to a specific timestamp in Vsrc, while reversing the frame\nindices produces a playback of Vsrc in reverse.\n(Top) For multi-view dynamic scene datasets, a set of\ntemporal warping operations, including reverse, playback,\nzigzag motion, slow motion, and freeze are apppplied with\nteh source video as standford. This gives explicit supervi-\nsion for temporal control, without constructing additional\ntemporally varied training data.\n(Bottom) Existing camera-control and joint dataset train-\ning rely on monotonic time progression and static scene\nvideos, making it difficult for models to understand tem-\nporal variation. The introduced temporal mappings from\n1\n2\n3\n4\n5\n6\n7\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n1\n2\n3\n4\n5\n5\n5\n5\n6\n7\n8\n5\n5\n5\n5\n5\n5\n5\n5\n1\n2\n3\n4\n5\n6\n5\n4\n3\n2\n1\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n(f)  Freeze \n(a) Standard Forward\n(b) Reverse Playback\n(e) ZigZag Motion\n(d) Global Slow Motion x 0.5\n(c) Motion-to-Freeze\nPivot P, Where [N/2] < P < N\nFreezing P for N/2 steps, Where 0 < P < N\nPivot P\nRepeat P for whole video, Where 0 < P < N\nSource Video\nTarget Video\nStandard Forward\nframe index\n(a) Camera Control TI2V \n(c) Temporal Wrapping (ours)\ndynamic vidoes\n(2) Comparison of different training scheme for time control\n(b) Joint Dataset Training\nstatic scene videos\ndynamic vidoes\nframe index\nframe index\nanimation time t w.r.t. source \nanimation time t w.r.t. source \nanimation time t w.r.t. source \n(1) Temporal Warping Augmentation\nFigure 3. Temporal Wrapping for Spatiotemporal Disentan-\nglement. (Top) For multi-view dynamic scene datasets [2], a set\nof temporal warping operations (e.g.\nreverse playback, zigzag\nmotion, slow motion, and freeze) are applied to the target video,\nwith the source video kept as the standard forward reference, pro-\nviding explicit supervision for temporal control . (Bottom) Com-\npared with existing camera-control [2, 33] and joint-dataset train-\ning strategies [41, 43], which rely on monotonic time progression\nand static-scene videos to demonstrate temporal differences, Tem-\nporal Wrapping provide much more diverse and explicit signals of\ntemporal variation, leading to disentanglement of space and time.\nmulti-view video data, which provide diverse and clear sig-\nnal on tempral variation, and directly lead to disentangle-\nment of space and time.\nTime Embedding. To inject temporal control into the dif-\nfusion model, we analyze several approaches. First, we can\nencode time similar to a frame index using RoPE embed-\nding.\nHowever, we find it less suitable for time control\n(visual evaluations are provided in Supp. Mat.). Instead,\nwe adopt sinusoidal time embeddings applied at the latent\nframe f ′ level, which provide a stable and continuous rep-\nresentation of each frame’s temporal position and offer a\nfavorable trade-off between precision and stability. We fur-\nther observe that each latent frame corresponds to a con-\ntinuous temporal chunk, and propose using embeddings of\noriginal frame indices f to support finer granularity of time\ncontrol. To accomplish this, we introduce a time encod-\ning approach Eani(t), where t ∈RF . We first compute\nthe sinusoidal time embeddings to represent the temporal\nsequence, esrc = SinPE(tsrc), etrg = SinPE(ttrg), where\ntsrc, ttrg ∈RF . Next, we apply two 1D convolution lay-\ners to progressively project these embeddings into the latent\nframe space, e = Conv1D2(Conv1D1(e)). Finally, we add\nthese time features to the camera features and video tokens\n4\n"
    },
    {
      "page_number": 5,
      "text": "embeddings, updating Eq. (1) as follows:\nx′ = x + Ecam (c) + Eani (t) .\n(2)\nIn Sec. 4.2, we compare our approach with alternative\nconditioning strategies, such as using sinusoidal embed-\ndings where tsrc, ttrg are directly defined in RF ′, and em-\nploying an MLP instead of a 1D convolution for compres-\nsion. We demonstrate both qualitatively and quantitatively\nthe advantages of our proposed method.\n3.2.2. Datasets\nTo enable temporal manipulation in our approach, we re-\nquire paired training data that includes examples of time\nremapping.\nAchieving spatial-temporal disentanglement\nfurther requires data containing examples of both camera\nand temporal controls. To the best of our knowledge, no\npublicly available datasets satisfy these requirements. Only\na few prior works, such as 4DiM [41] and CAT4D [43], have\nattempted to address spatial-temporal disentanglement. A\ncommon strategy is to jointly train on static-scene datasets\nand multi-view video datasets [23, 53]. The limited con-\ntrol variability in these datasets leads to confusion between\ntemporal evolution and spatial movement, resulting in en-\ntangled or unstable behaviors [41, 43]. We address this lim-\nitation by augmenting existing multi-view video data with\ntemporal warping and by proposing a new synthetic dataset.\nTemporal Warping Augmentation. We introduce simple\naugmentations that add controllable temporal variations to\nmulti-view video datasets. During training, given a source\nvideo Vsrc = {If\nsrc}F\nf=1 and a target video Vtrg = {If\ntrg}F\nf=1,\nwe apply a temporal warping function τ : [1, F] →[1, F]\nto the target sequence, producing a warped video V ′\ntrg =\n{Iτ(f)\ntrg\n}F\nf=1.\nThe source animation timestamps are uni-\nformly sampled, tsrc = 1 : F.\nWarped timestamps,\nttrg = τ(tsrc), introduce non-linear temporal effects (see\nFig. 3 top b–e): (i) reversal, (ii) acceleration, (iii) freez-\ning, (iv) segmental slow motion, and (v) zigzag motion, in\nwhich the animation repeatedly reverses direction. After\nthese augmentations, the paired video sequences (Vsrc, V ′\ntrg)\ndiffer in both camera trajectories and temporal dynamics,\nproviding the model with a clear signal for learning disen-\ntangled spatiotemporal representations.\nSynthetic Cam×Time Dataset for Precise Spatiotempo-\nral Control.\nWhile our temporal warping augmentations\nencourage strong disentanglement between spatial and tem-\nporal factors, achieving fine-grained and continuous con-\ntrol — that is, smooth and precise adjustment of tempo-\nral dynamics — benefits from a dataset that systemati-\ncally covers both dimensions. To this end, we construct\nCam×Time, a new synthetic spatiotemporal dataset ren-\ndered in Blender. Given a camera trajectory and an ani-\nmated subject, Cam×Time exhaustively samples the cam-\nera–time grid, capturing each dynamic scene across diverse\nTable 1. Comparison of existing multi-view datasets for cam-\nera and temporal control against Cam×Time. Cam×Time pro-\nvides full-grid rendering (Figure 4), enabling target videos to sam-\nple arbitrary temporal variations over the full range from 0 to 120.\nDataset\nDynamic scenes\nSrc. Time: tsrc\nTgt. Time: ttrg\nCamera\nRE10k [53]\n✘\n1\n1\nMoving\nDL3DV10k [23]\n✘\n1\n1\nMoving\nMannequinChallenge [32]\n✘\n1\n1\nMoving\nKubric-4D [33]\n✔\n1:60\n1:60\nMoving\nReCamMaster [2]\n✔\n1:80\n1:80\nMoving\nSynCamMaster [1]\n✔\n1:80\n1:80\nFixed\nCam×Time (ours)\n✔\n1:120\n{1, 2, . . . , 120}120\nMoving\ncontinuous camera traj.\nanimation time\nfull space-time grid rendering \n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nsource \nvideo\n...\n...\n...\n...\nFigure 4. Cam×Time dataset visualization. (Top) A space-time\ngrid defined by a camera trajectory c = [c1, ..., cF ] and animation\nstatus t = [t1, ..., tF ]. Cam×Time renders images for all (c, t)\npairs, covering the full grid for learning disentangled spatial and\ntemporal control. Any two sampled sequences of F frames from\nthe grid can form a source-target pair. (Bottom) One typical choice\nof source videos is taking the diagonal cells in green.\ncombinations of camera viewpoints and temporal states\n(c, t), as illustrated in Fig. 4. The source video is obtained\nby sampling the diagonal frames of the dense grid (Fig. 4\n(bottom)), while the target videos are obtained by more\nfree-form sampling of continuous sequences.\nWe com-\npare Cam×Time against existing datasets in Tab. 1. While\n[23, 32, 53] are real videos with complex camera path anno-\ntations, they either do not provide time-synchronized video\npairs [32] or only provide pairs of static scenes [23, 53].\nSynthetic multi-view video datasets [1, 2, 33] provide pairs\nof dynamic videos but do not allow training for time control.\nIn contrast, Cam×Time enables fine-grained manipulation\nof both camera motion and temporal dynamics, enabling\nbullet-time effects, motion stabilization, and flexible combi-\nnations of the controls. We designate part of Cam×Time as\na test set, aiming for it to serve as a benchmark for control-\nlable video generation. We will release it to support future\nresearch on fine-grained spatiotemporal modeling.\n5\n"
    },
    {
      "page_number": 6,
      "text": "Table 2. Quantitative comparison across temporal controls (Direction (forward, backward motion), Speed (slow modes), Bullet Time). We\nreport PSNR↑, SSIM↑, and LPIPS↓. Best results are in bold. SpaceTimeMethod showcase best performance for temporal control overall.\nMethod\nPSNR↑\nSSIM↑\nLPIPS↓\nDir.\nSpeed\nBullet\nAvg\nDir.\nSpeed\nBullet\nAvg\nDir.\nSpeed\nBullet\nAvg\nReCamM+preshuffled†\n17.13\n14.84\n14.61\n15.52\n0.6623\n0.6050\n0.5965\n0.6213\n0.3930\n0.4793\n0.4863\n0.4529\nReCamM+jointdata\n18.32\n17.57\n17.69\n17.86\n0.7322\n0.7220\n0.7209\n0.7250\n0.2972\n0.3158\n0.3089\n0.3073\nSpaceTimePilot (Ours)\n21.75\n20.87\n20.85\n21.16\n0.7725\n0.7645\n0.7653\n0.7674\n0.1697\n0.1917\n0.1677\n0.1764\n† Uses simple frame-rearrangement operators (reversal, repetition, freezing) applied prior to inference to emulate temporal manipulation.\n3.3. Precise Camera Conditioning\nWe aim for full camera trajectory control in the target video.\nIn contrast, the previous novel-view synthesis approach [2]\nassumes that the first frame is identical in source and target\nvideos and that the target camera trajectory is defined rel-\native to it. This stems from the two limitations. First, the\nexisting approach ignores the source video trajectory, yield-\ning suboptimal source features computed using the target\ntrajectory for consistency:\nx′\nsrc = xsrc + Ecam (ctrg) ,\nx′\ntrg = xtrg + Ecam (ctrg) .\nSecond, it is trained on datasets where the first frame is al-\nways identical across the source and target videos. This\nlatter limitation is addressed in our training datasets design.\nTo overcome the former, we devise a source-aware cam-\nera conditioning. We estimate camera poses for both the\nsource and target videos using a pretrained pose estimator,\nand inject them jointly into the diffusion model to provide\nexplicit geometric context. Eq. 2 is therefore extended into:\nx′\nsrc = xsrc + Ecam (csrc) + Eani (tsrc) ,\n(3)\nx′\ntrg = xtrg + Ecam (ctrg) + Eani (ttrg) ,\nx′ = [x′\ntrg, x′\nsrc]frame-dim,\nwhere x′ denotes the input of the DiT model, which is the\nconcatenation of target and source tokens along the frame\ndimension. This formulation provides the model with both\nsource and target camera context, enabling spatially consis-\ntent generation and precise control over camera trajectories.\n3.4. Support for Longer Video Segments\nFinally, to showcase the full potential of our camera and\ntemporal control, we adopt a simple autoregressive video\ngeneration strategy, generating each new segment Vtrg con-\nditioned on the previously generated segment Vprv and a\nsource video Vsrc to produce longer videos.\nTo enable this capability during inference, we need to\nextend our training scenario to support conditioning on two\nvideos, where one serves as Vsrc and the other as Vprv.\nThe source video Vsrc is taken directly from the multi-view\ndatasets or from our synthetic dataset, as was described pre-\nviously. Vprv is constructed in a similar way to Vtrg — either\nusing temporal warping augmentations or by sampling from\nthe dense space-time grid of our synthetic dataset. When\ntemporal warping is applied, Vprv and Vtrg may originate\nfrom the same or different multi-view sequences represent-\ning the same time interval. To maintain full flexibility of\ncontrol, we do not enforce any other explicit correlations\nbetween Vprv and Vtrg, apart from specifying camera param-\neters relative to the selected source video frame.\nNote that not constraining the source and target videos\nto share the same first frame (as discussed in Sec. 3.3) is\ncrucial for achieving flexible camera control in longer se-\nquences. For instance, this design enables extended bullet-\ntime effects: we can first generate a rotation around a se-\nlected point up to 45◦(Vtrg,1), and then continue from 45◦\nto 90◦(Vtrg,2). Conditioning on two consecutive source seg-\nments allows the model to leverage information from newly\ngenerated viewpoints. In the bullet-time example, condi-\ntioning on the previously generated video enables the model\nto incorporate information from all newly synthesized view-\npoints, rather than relying solely on the viewpoint of the\ncorresponding moment in the source video.\n4. Experiments\nImplementation details.\nWe adopt the Wan-2.1 T2V-\n1.3B model [34], which produces F ′=21 latent frames\nand decodes them into F=81 RGB frames using a 3D-\nVAE. The network is conditioned on camera and animation-\ntime controls as defined in Eq. 3. Unless otherwise spec-\nified, SpaceTimePilot is trained with ReCamMaster and\nSynCamMaster datasets with the temporal warping aug-\nmentation described in Sec. 3.2.2, along with Cam×Time.\nPlease refer to Supp. Mat. for complete network architec-\nture and additional training details.\n4.1. Comparison with State-of-the-Art Baselines\n4.1.1. Time-Control Evaluation.\nWe first evaluate the retiming capability of our model. To\nfactor out the error induced by camera control, we condition\nSpaceTimePilot on a fixed camera pose while varying only\nthe temporal control signal.\nExperiments are performed\non the withheld Cam×Time test split, which contains 50\nscenes rendered with dense full-grid trajectories that can\n6\n"
    },
    {
      "page_number": 7,
      "text": "bullet-time\nat t=40\nbullet-time\nat t=40\nslow motion\n40-80\nreverse\nmotion\nslow motion\n40-80\nreverse\nmotion\nt=40\nt=0\nt=80\nt=20\nt=40\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nSource\n Video\nSynthesized\n Video\nFrame Number\n80\n0\nFrame Number\n80\n0\nFigure 5. Qualitative results of SpaceTimePilot. Our model enables fully disentangled control over camera motion and temporal dy-\nnamics. Each row shows a different combination of camera trajectory (left icons) and temporal warping (right icons). SpaceTimePilot\nproduces coherent videos under diverse controls, including normal playback, reverse playback, bullet-time, slow-motion, replay motion,\nand complex camera paths (pan, tilt, zoom, and vertical motion).\nbe retimed into arbitrary temporal sequences. For each test\ncase, we take a moving-camera source video but set the tar-\nget camera trajectory to the first-frame pose. We then ap-\nply a range of temporal control signals, including reverse,\nbullet-time, zigzag, slow motion, and normal playback, to\nsynthesize the corresponding retimed outputs.\nSince we\nhave ground-truth frames for all temporal configurations,\nwe report perceptual losses: PSNR, SSIM, and LPIPS.\nWe consider two baselines: (1) ReCamM+preshuffled:\noriginal ReCamMaster combined with input re-shuffling;\nand (2) ReCamM+jointdata: following [41, 43], we train\nReCamMaster with additional static-scene datasets [18, 53]\nwhich provide only one single temporal pattern.\nWhile frame shuffling may succeed in simple scenar-\nios, it fails to disentangle camera and temporal control. As\nshown in Table 2, this approach exhibits the weakest tem-\nporal controllability. Although incorporating static-scene\ndatasets improves performance, particularly in the bullet-\ntime category, relying on a single temporal control pattern\nremains insufficient for achieving robust temporal consis-\ntency. In contrast, SpaceTimePilot consistently outperforms\nall baselines across all temporal configurations.\nTable 3. VBench visual-quality evaluation across six dimensions.\nHigher is better for all metrics.\nMethod\nImgQ↑BGCons↑Motion↑SubjCons↑Flicker↑Aesthetic↑\nTraj-Crafter [48] 0.6389\n0.9376\n0.9888\n0.9463\n0.9816\n0.5172\nReCamM [2]\n0.6302\n0.9114\n0.9945\n0.9181\n0.9825\n0.5332\nReCamM+Aug\n0.6315\n0.9165\n0.9946\n0.9313\n0.9788\n0.5385\nSTPilot (Ours)\n0.6486\n0.9199\n0.9947\n0.9325\n0.9781\n0.5315\n4.1.2. Visual Quality Evaluation.\nNext, we evaluate the perceptual realism of our 1800 gen-\nerated videos using VBench [10]. We report all standard\nvisual quality metrics to provide a comprehensive assess-\nment of generative fidelity. Table 3 shows that our model\nachieves visual quality comparable to the baselines.\n4.1.3. Camera-Control Evaluation.\nFinlay, we evaluate the effectiveness of our camera control\nmechanism detailed in Sec. 3.3. Unlike the retiming evalu-\nation above, which relies on synthetic ground-truth videos,\nhere we construct a real-world 90-video evaluation set from\nOpenVideoHD [26], encompassing diverse dynamic human\nand object motions. Each method is evaluated across 20\ncamera trajectories: 10 starting from the same initial pose\nas the source video and 10 from different initial poses,\n7\n"
    },
    {
      "page_number": 8,
      "text": "Table 4. Camera accuracy and first-frame estimation. For camera control,\nthe enhanced camera control mechanism enables the generated video to start\nfrom an arbitrary camera angle while maintaining good camera accuracy.\nMethod\nRelRot↓RelTrans↓AbsRot↓AbsTrans↓Rot† ↓RTA15† ↑RTA30† ↑\nTraj-Crafter [48]\n5.94\n0.50\n6.93\n0.52\n9.76\n22.96%\n25.93%\nReCamM [2]\n4.26\n0.32\n10.08\n0.34\n7.49\n7.61%\n10.20%\nReCamM+Aug\n3.66\n0.43\n11.74\n0.46\n13.88\n3.89%\n5.93%\nSpaceTimePilot (ours)\n2.71\n0.33\n5.63\n0.34\n4.09\n35.19%\n54.44%\nTable 5. Time-embedding compressor ablation. The pro-\nposed time-embedding method, trained with temporal warp-\ning on the proposed dataset, yields sharper results overall.\nTime Embedding\nPSNR↑SSIM↑LPIPS↓\nUniform Sampling\n14.10 0.5981 0.5039\n1D-Conv\n14.75 0.6134 0.4878\n1D-Conv + Joint Data\n15.41 0.6252 0.4830\n1D-Conv +Cam×Time 21.16 0.7674 0.1764\n† Evaluation based on first-frame camera accuracy.\nt=0\nt=20\nt=40\nt=80\nt=80\nt=60\nt=40\nt=0\nt=80\nt=60\nt=40\nt=0\nt=80\nt=60\nt=40\nt=0\nSrc Video\nOurs\nReCamMaster\nTrajectory\nCrafter\nframe 0\nframe 20\nframe 40\nframe 80\nFigure 6. Qualitative comparison of disentangled camera-time\ncontrol. In this example, we apply reverse playback (time) and\na pan-right camera motion starting from the first-frame pose to a\nsource video (top), whose original camera motion is dolly-in (red\nto blue). SpaceTimePilot, by explicitly disentangling space and\ntime, achieves correct camera control (red boxes) together with\naccurate temporal control (green boxes). For TrajectoryCrafter, it\nfirst reverses the frames and then apply their method for viewpoint\ncontrol, resulting in incorrect camera motion. ReCamMaster (with\njoint-dataset training) is unable to perform temporal control, lead-\ning to failure cases.\nresulting in a total of 1800 generated videos. We apply\nSpatialTracker-v2 [45] to recover camera poses from the\ngenerated videos and compare them with the corresponding\ninput camera poses. To ensure consistent scale, we align the\nmagnitude of the first two camera locations. Trajectory ac-\ncuracy is quantified using RotErr and TransErr follow-\ning [8], under two protocols: (1) evaluating the raw trajecto-\nries defined w.r.t. the first frame (relative protocol, RelRot,\nRelTrans) and (2) evaluating after aligning to the estimated\npose of the first frame (absolute protocol, AbsRot, Ab-\nsTrans). Specifically, we transform the recovered raw tra-\njectories by multiplying the relative pose between the gener-\nated and source first frames, estimated by DUSt3R [38]. We\nalso compare this DUSt3R pose with the target trajectory’s\ninitial pose, and report RotErr, RTA@15 and RTA@30, as\ntranslation magnitude is scale-ambiguous.\nTo measure only the impact of source camera condition-\ning, we consider the original ReCamMaster [2] (ReCamM)\nand two variants. Since ReCamMaster is originally trained\nCamera: pan left.  Time: freeze at  t = 40.\nCamera: tilt-down.   Time: freeze at  t = 40.\nMLP Compressor 1D-Conv Compressor\nUniform Resampling\nframe 0\nframe 40\nframe 80\nMLP Compressor 1D-Conv Compressor\nUniform Resampling\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nframe 0\nframe 40\nframe 80\nt=40\nt=40\nt=40\nFigure 7. Temporal compression ablation. Comparing uniform\nresampling, MLP, and 1D-Conv compressors under tilt-down and\npan-right bullet-time controls, ttrg = [40, . . . , 40].\non datasets where the first frame of the source and target\nvideos are identical, the model always copies the first frame\nregardless of the input camera pose. For fairness, we re-\ntrain ReCamMaster with more data augmentations to in-\nclude non-identical first frames, denoted as ReCamM+Aug.\nNext, we condition the model additionally with source cam-\neras csrc following Eq. 3, denoted as ReCamM+Aug+csrc.\nFinally we also report the results of TrajectoryCrafter [48].\nIn Table 4, we observe that the absolute protocol pro-\nduces consistently higher errors, as trajectories must not\nonly match the overall shape (relative protocol) but also\nalign correctly in position and orientation.\nInterestingly,\nReCamM+Aug yields higher errors than the original Re-\nCamM, whereas incorporating source cameras csrc results\nin the best overall performance. This suggests that, with-\nout explicit reference to csrc, exposure to more augmented\nvideos with differing initial frames can instead confuse the\nmodel. The newly introduced conditioning signal on the\nsource video’s trajectory csrc achieves substantially better\ncamera-control accuracy across all metrics, more reliable\nfirst-frame alignment, and more faithful adherence to the\nfull trajectory than all baselines.\n4.1.4. Qualitative results.\nBesides the quantitative evaluation, we also demonstrate the\nstrength of SpaceTimePilot with visual examples. In Fig. 6,\nwe show that only our method correctly synthesizes both\nthe camera motion (red boxes) and the animation-time state\n(green boxes). While ReCamMaster handles camera control\nwell, it cannot modify the temporal state, such as enabling\n8\n"
    },
    {
      "page_number": 9,
      "text": "reverse playback. TrajectoryCrafter, in contrast, is confused\nby the reverse frame shuffle, causing the camera pose of the\nlast source frame (blue boxes) to incorrectly appear in the\nfirst frame of the generated video. More visual results can\nbe found in Fig. 5.\n4.2. Ablation Study\nTo validate the effectiveness of the proposed Time embed-\nding module, in Table 5, we follow the time-control eval-\nuation set up in Sec. 4.1.1 and compare our 1D convo-\nlutional time embedding against several variants and al-\nternatives discussed in Sec. 3.2.1: (1) Uniform-Sampling:\nsampling the 81-frame embedding uniformly to a 21-frame\nsequence, which is equivalent to adopting sinusoidal em-\nbeddings at the latent frame f ′ level; (2) 1D-Conv: us-\ning 1D convolution layers to compress from t ∈RF to\nt ∈RF ′, trained with ReCamMaster and SynCamMaster\ndatasets. (3) 1D-Conv+jointdata: row 2 but including addi-\ntionally static-scene datasets [18, 53]. (4) 1D-Conv (ours):\nrow 2 but instead including the proposed Cam×Time. We\nobserve that applying a 1D convolution to learn a compact\nrepresentation by compressing the fine-grained F-dim em-\nbeddings into a F ′-dim space performs notably better than\ndirectly constructing sinusoidal embeddings at the coarse\nf ′ level.\nIncorporating static-scene datasets yields only\nlimited improvements, likely due to their restricted tem-\nporal control patterns.\nBy contrast, using the proposed\nCam×Time consistently delivers the largest gains across\nall three metrics, confirming the effectiveness of our newly\nintroduced datasets. Furthermore, as shown in Fig. 7, we\npresent a visual comparison of bullet-time results using uni-\nform sampling and an MLP instead of the 1D convolu-\ntion for compressing the temporal control signal. Uniform\nsampling produces noticeable artifacts, and the MLP com-\npressor causes abrupt camera motion, whereas the 1D con-\nvolution effectively locks the animation time and enables\nsmooth camera movement.\n5. Conclusion\nWe present SpaceTimePilot, the first video diffusion model\nto provide fully disentangled spatial and temporal control,\nenabling 4D space-time exploration from a single monoc-\nular video.\nOur method introduces a new “animation\ntime” representation together with a source-aware camera-\ncontrol mechanism that leverages both source and target\nposes. This is supported by the synthetic Cam×Time and\na temporal-warping training scheme, which supply dense\nspatiotemporal supervision. These components allow pre-\ncise camera and time manipulation, arbitrary initial poses,\nand flexible multi-round generation. Across extensive ex-\nperiments, SpaceTimePilot consistently surpasses state-of-\nthe-art baselines, offering significantly improved camera-\ncontrol accuracy and reliable execution of complex retiming\neffects such as reverse playback, slow motion, and bullet-\ntime.\n6. Acknowledgement\nWe would like to extend our gratitude to Duygu Ceylan,\nPaul Guerrero, and Zifan Shi for insightful discussions and\nvaluable feedback on the manuscript. We also thank Rudi\nWu for helpful discussions on implementation details of\nCAT4D.\nReferences\n[1] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan,\nXiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and\nDi Zhang.\nSynCamMaster: Synchronizing multi-camera\nvideo generation from diverse viewpoints.\narXiv preprint\narXiv:2412.07760, 2024. 2, 3, 5, 14, 16, 17\n[2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui\nMu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei\nWan, and Di Zhang. ReCamMaster: Camera-controlled gen-\nerative rendering from a single video. In ICCV, 2025. 2, 3,\n4, 5, 6, 7, 8, 14, 16, 17\n[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her-\nrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur,\nGuanghui Liu, Amit Raj, et al. Lumiere: A space-time diffu-\nsion model for video generation. In SIGGRAPH Asia 2024\nConference Papers, pages 1–11, 2024. 2\n[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023.\n[5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh.\nVideo generation models as world simulators.\n2024. 2\n[6] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nICCV, pages 5712–5721, 2021. 2\n[7] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,\nYilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-\ngasam, Florian Golemo, Charles Herrmann, Thomas Kipf,\nAbhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-\nTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek\nNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-\nwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,\nMatan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,\nSuhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,\nFangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-\nable dataset generator. In CVPR, 2022. 3\n[8] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo\nDai, Hongsheng Li, and Ceyuan Yang.\nCameraCtrl: En-\nabling camera control for text-to-video generation. In ICLR,\n2025. 8\n[9] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng\nWei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and\n9\n"
    },
    {
      "page_number": 10,
      "text": "Hongsheng Li. CameraCtrl II: Dynamic scene exploration\nvia camera-controlled video diffusion models. arXiv preprint\narXiv:2503.10592, 2025. 2\n[10] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si,\nYuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin,\nNattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin\nWang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Com-\nprehensive benchmark suite for video generative models. In\nCVPR, 2024. 7\n[11] Adobe Systems Inc. Mixamo, 2018. Accessed: 2025-03-07.\n12\n[12] Hyeonho Jeong, Chun-Hao Paul Huang, Jong Chul Ye, Niloy\nMitra, and Duygu Ceylan. Track4Gen: Teaching video dif-\nfusion models to track points improves video generation. In\nCVPR, 2025. 2\n[13] Hyeonho Jeong, Suhyeon Lee, and Jong Chul Ye. Reangle-\nA-Video: 4d video generation as video-to-video translation.\nIn ICCV, 2025. 3\n[14] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi,\nTianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang\nXu. LVSM: A large view synthesis model with minimal 3d\ninductive bias. In ICLR, 2025. 2\n[15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¨uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Trans. Graph., 42(4):139–1,\n2023. 2\n[16] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024.\n2\n[17] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and\nKostas Daniilidis. MoSca: Dynamic gaussian fusion from\ncasual videos via 4d motion scaffolds. In CVPR, 2025. 2\n[18] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker,\nNoah Snavely, Ce Liu, and William T Freeman. Learning\nthe depths of moving people by watching frozen people. In\nCVPR, 2019. 7, 9, 14, 16\n[19] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In CVPR, pages 6498–6508, 2021. 2\n[20] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\nrendering. In CVPR, 2023. 2\n[21] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang,\nZhangyang Wang, Konstantinos N Plataniotis, Yao Zhao,\nand Yunchao Wei. Diffusion4D: Fast spatial-temporal con-\nsistent 4d generation via video diffusion models. In NeurIPS,\n2024. 2\n[22] Jinwei Lin.\nDynamic NeRF: A review.\narXiv preprint\narXiv:2405.08609, 2024. 2\n[23] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin,\nKun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu,\net al.\nDL3DV-10K: A large-scale scene dataset for deep\nlearning-based 3d vision.\nIn CVPR, pages 22160–22169,\n2024. 2, 5\n[24] Jiaxin Lu, Chun-Hao Paul Huang, Uttaran Bhattacharya,\nQixing Huang, and Yi Zhou. Humoto: A 4d dataset of mocap\nhuman object interactions. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n10886–10897, 2025. 12\n[25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, pages 99–106. ACM New York, NY, USA,\n2021. 2\n[26] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhen-\nheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai.\nOpenVid-1M: A large-scale high-quality dataset for text-to-\nvideo generation. arXiv preprint arXiv:2407.02371, 2024.\n7\n[27] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn ICCV, pages 5865–5874, 2021. 2\n[28] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B. Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHyperNeRF: A higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Transactions on Graphics (TOG), 40\n(6):238:1–238:12, 2021. 2\n[29] Jack Parker-Holder and Shlomi Fruchter. Genie 3: A new\nfrontier for world models. Google DeepMind Blog, 2025.\nAccessed: ¡insert date you retrieved¿. 2, 3\n[30] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,\nAnimesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-\nYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary,\nDingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan\nMisra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng\nLi, Luxin Zhang, Mannat Singh, Mary Williamson, Matt\nLe, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Pe-\nter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly,\nSai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak\nDatta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy,\nShelly Sheynin, Siddharth Bhattacharya, Simran Motwani,\nTao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xi-\naoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu,\nYi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian\nHe, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun\nMallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh\nWood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schon-\nfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff\nLiang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik\nSivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos\nGeorgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li,\nSimone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petro-\nvic, and Yuming Du. Movie gen: A cast of media foundation\nmodels, 2024. 2\n[31] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling,\nYifan Lu, Merlin Nimier-David, Thomas M¨uller, Alexander\nKeller, Sanja Fidler, and Jun Gao.\nGEN3C: 3d-informed\nworld-consistent video generation with precise camera con-\ntrol. In CVPR, 2025. 3\n10\n"
    },
    {
      "page_number": 11,
      "text": "[32] Chris Rockwell, Joseph Tung, Tsung-Yi Lin, Ming-Yu Liu,\nDavid F. Fouhey, and Chen-Hsuan Lin.\nDynamic camera\nposes and where to find them. In CVPR, 2025. 5\n[33] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sar-\ngent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi\nZheng, and Carl Vondrick. Generative camera dolly: Ex-\ntreme monocular dynamic novel view synthesis.\nECCV,\n2024. 2, 3, 4, 5\n[34] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao,\nChen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianx-\niao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jin-\ngren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao,\nKeyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang,\nPandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei\nZhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui,\nTingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang,\nWenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu,\nXianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu\nLv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yi-\ntong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun\nZheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi\nJiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open\nand advanced large-scale video generative models.\narXiv\npreprint arXiv:2503.20314, 2025. 4, 6\n[35] Chaoyang Wang,\nAshkan Mirzaei,\nVidit Goel,\nWilli\nMenapace, Aliaksandr Siarohin, Avalon Vinella, Michael\nVasilkovsky, Ivan Skorokhodov, Vladislav Shakhrai, Sergey\nKorolev, Sergey Tulyakov, and Peter Wonka. 4real-video-\nv2: Fused view-time attention and feedforward reconstruc-\ntion for 4d scene generation. In Adv. Neural Inform. Process.\nSyst., 2025. 3\n[36] Chaoyang Wang, Peiye Zhuang, Tuan Duc Ngo, Willi Mena-\npace, Aliaksandr Siarohin, Michael Vasilkovsky, Ivan Sko-\nrokhodov, Sergey Tulyakov, Peter Wonka, and Hsin-Ying\nLee. 4real-video: Learning generalizable photo-realistic 4d\nvideo diffusion. In CVPR, 2025. 3\n[37] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi\nLi, and Angjoo Kanazawa. Shape of motion: 4d reconstruc-\ntion from a single video. In ICCV, 2025. 2\n[38] Shuzhe Wang,\nVincent Leroy,\nYohann Cabon,\nBoris\nChidlovskii, and Jerome Revaud. DUSt3R: Geometric 3d\nvision made easy. In CVPR, 2024. 8\n[39] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu,\nPeiqing Yang, et al. Lavie: High-quality video generation\nwith cascaded latent diffusion models. IJCV, pages 1–20,\n2024. 2\n[40] Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon,\nYue Zhang, and Mohit Bansal. EPiC: Efficient Video Camera\nControl Learning with Precise Anchor-Video. arXiv preprint\narXiv:2505.21876, 2025. 3\n[41] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasac-\nchi, and David J. Fleet.\nControlling space and time with\ndiffusion models. In ICLR, 2025. 2, 3, 4, 5, 7\n[42] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 20310–20320, 2024. 2\n[43] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi\nZheng, Jonathan T Barron, and Aleksander Holynski.\nCat4D: Create anything in 4d with multi-view video diffu-\nsion models. In CVPR, 2024. 2, 3, 4, 5, 7\n[44] Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu,\nDahua Lin, and Gordon Wetzstein. Video world models with\nlong-term spatial memory. arXiv preprint arXiv:2506.05284,\n2025. 3\n[45] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Iurii\nMakarov, Bingyi Kang, Xin Zhu, Hujun Bao, Yujun Shen,\nand Xiaowei Zhou.\nSpatialTrackerV2: 3d point tracking\nmade easy. In ICCV, 2025. 8\n[46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 2\n[47] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park,\nand Jan Kautz.\nNovel view synthesis of dynamic scenes\nwith globally coherent depths from a monocular camera. In\nCVPR, pages 5339–5348, 2020. 2\n[48] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan.\nTra-\njectoryCrafter: Redirecting camera trajectory for monocular\nvideos via diffusion models. In ICCV, 2025. 3, 7, 8\n[49] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li,\nZhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan,\nand Yonghong Tian. Viewcrafter: Taming video diffusion\nmodels for high-fidelity novel view synthesis. arXiv preprint\narXiv:2409.02048, 2024. 3\n[50] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Kar-\nnad, David E Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng\nShou, Neal Wadhwa, and Nataniel Ruiz. ReCapture: Gen-\nerative video camera controls for user-provided videos using\nmasked video fine-tuning. In CVPR, 2024. 3\n[51] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation. IJCV, pages 1–15, 2024. 2\n[52] Jensen (Jinghao) Zhou, Hang Gao, Vikram Voleti, Aaryaman\nVasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian\nRupprecht, and Varun Jampani. Stable Virtual Camera: Gen-\nerative view synthesis with diffusion models. arXiv preprint,\n2025. 3\n[53] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magnification: Learning view syn-\nthesis using multiplane images. In SIGGRAPH, 2018. 2, 5,\n7, 9, 14, 16\n11\n"
    },
    {
      "page_number": 12,
      "text": "A. Network Architecture\nThe network architecture of SpaceTimePilot is depicted in\nFig. 8. The newly introduced animation-time embedder Eani\nencodes the source and target animation times, tsrc and ttrg,\ninto tensors matching the shapes of xsrc and xtrg, which are\nthen added to them respectively. During training, we train\nonly the camera embedder Ecam, the animation-time embed-\nder Eani, the self-attention (full-3D attention), and the pro-\njector layers before the cross-attention.\nfff\nff\nZsrc\nZtrg\nAdd noise\nPatchify\nSpace-Time DiT\nZ’trg\n3D-VAE\nDecoder\n3D-VAE\nencoder\nVt’\n /t\n/t\nCamera\nembedding\nTime\nEmbedding\nSelf-Attention\nProjector\nCross-Attention\nFeed-forward NN\ncam\ncam\ncam\ncam\ncam\ncam\ncam\ncam\ntime\ntime\ntime\ntime\ntime\ntime\ntime\ntime\n(patchified Ztrg)\n(patchified Zsrc)\nFigure 8. Architecture of SpaceTimePilot. Our model jointly\nconditions on camera trajectories and temporal control signals via\nspace–time attention, enabling non-monotonic motion generation\nsuch as reversals, repeats, accelerations, and zigzag time.\nB. Longer Space-Time Exploration Video with\nDisentangled Controls\nOne of the central advantages of SpaceTimePilot is its abil-\nity to freely navigate both spatial and temporal dimen-\nsions, with arbitrary starting points in each dimension and\nfully customizable trajectories through them.\nAlthough\neach individual generation is limited to an 81-frame win-\ndow, we show that SpaceTimePilot can effectively extend\nthis window indefinitely through a multi-turn autoregres-\nsive inference scheme, enabling continuous and control-\nlable space–time exploration from a single input video. The\noverall pipeline is illustrated in Fig. 9.\nThe core idea is to generate the final video in autore-\ngressive segments that connect seamlessly. For example,\ngiven a source video of 81 frames, we may first generate\na 0.5× slow-motion sequence covering frames 0–40 with\na new camera trajectory. Then, continuing both the visual\ncontext and the generated camera trajectory, we can pro-\nduce the next segment starting from the final camera pose\nof the previous output, while temporally traversing the re-\nmaining frames 40–81. This yields an autoregressive chain\nof viewpoint-controlled video segments that together create\na continuous long-range space–time trajectory.\nA key property that enables this behavior is that our\nmodel can generate video segments whose camera poses do\nnot need to start at the first frame. This allows precise con-\ntrol over the starting point, both in time and viewpoint, for\nevery generated chunk, ensuring smooth, consistent motion\nover extended sequences.\nTo maintain contextual coherence across iterations, we\nintroduce a lightweight memory mechanism. During train-\ning, the model is conditioned on a pair of source videos,\nwhich enables consistent chaining during inference. Specif-\nically:\n• At iteration i = 1, the model is conditioned only on the\noriginal source video.\n• At iteration i = 2, it is conditioned on both the source\nvideo and the previously generated 81-frame segment.\n• This process repeats, with each iteration conditioning on\nthe source video as well as the most recent generated seg-\nment.\nThis simple yet effective strategy allows SpaceTimePilot\nto generate arbitrarily long, smoothly connected sequences\nwith continuous and precise control over both temporal ma-\nnipulation and camera motion.\nHere, we showcase how this can be used to conduct large\nviewpoint changes, as demonstrated in Fig. 10.\nC.\nAdditional\nDetails\non\nthe\nProposed\nCam×Time Dataset.\nThe Cam×Time dataset is built using high-quality, com-\nmercially licensed 3D environments that include both real-\nistic indoor and outdoor scenes. For each environment, we\npopulate the space with multiple animated human charac-\nters. The character assets are sourced from Mixamo [11]\nand HUMOTO [24], and each character is manually tex-\ntured and refined to ensure realistic geometry, appearance,\nand material quality. The animations span a diverse range\nof human motions, including locomotion, gestures, and\nhuman-object interactions. Examples of scenes are shown\nin Fig. 11. Please refer to the complementary website for\nthe video examples.\nTo capture rich spatial coverage, we generate four dis-\ntinct camera trajectories for every scene.\nCamera paths\ninclude rotational orbits, linear tracking motions, and\nsmoothly curved arcs. A dedicated validity module ensures\nthat each trajectory: (1) begins at a collision-free location\nwith clear visibility of the main character, (2) maintains\nnon-intersecting movement with the environment through-\nout the path, and (3) preserves full subject visibility across\nall viewpoints.\n12\n"
    },
    {
      "page_number": 13,
      "text": "SpaceTimePilot\ndual src video\nSource video1 (81 frames)\ncamera traj 1\n(wrt src video 1)\ntemporal traj 1 \n(wrt src video 1)\nSpaceTimePilot\ndual src video\nOutput video 1\n(81 frames)\nOutput video 2\n(81 frames)\nVideo Stitching\nFinal Output.   (81 * n frames)\ncamera traj 2\n(wrt src video 1)\ntemporal traj 2 \n(wrt src video 1)\nSpaceTimePilot\ndual src video\ncamera traj 2\n(wrt src video 1)\ntemporal traj 2 \n(wrt src video 1)\nCopy of Source video1\n...\nFigure 9. Overview of the multi-turn autoregressive inference scheme. The model first generates an 81-frame segment conditioned\non the source video and a chosen space–time trajectory. The resulting output is then reused as a secondary source video for subsequent\niterations, each with its own camera and temporal trajectory. By chaining these iterations and stitching the outputs, SpaceTimePilot\nproduces a long, coherent video that follows an arbitrary space–time path.\nFigure 10.\nMulti-turn autoregressive generation with SpaceTimePilot. Top row: source video frames. Rows 2–4: Turn-1, Turn-2,\nand Turn-3 generations. At each turn, SpaceTimePilot jointly conditions on (1) the original source video and (2) the previously generated\nchunk, ensuring temporal continuity, stable motion progression, and consistent camera geometry. This dual-conditioning design enables\nviewpoint changes far beyond the input video—such as rotating to the rear of the tiger or transitioning from a low-angle shot to a high\nbird’s-eye view—while preserving visual and motion coherence. Please refer to section “AR Demos” in the website for videos.\nEach trajectory is rendered into a 120-frame sequence at\na resolution of 1080 × 1080 pixels, providing dense tem-\nporal sampling with high visual fidelity. This yields three\nmulti-view video sequences per scene, each covering the\nfull motion duration with consistent lighting, textures, and\ngeometry. Overall, we rendered 1500 videos from 500 an-\nimations, each with 120 videos full grid rendering, leading\nto 180k videos.\nFor temporal-control training, we could sample any time\nvariants from these sequences, including slow motion, re-\nverse playback, bullet-time around arbitrary frames, and\nnon-monotonic time patterns such as forward–backward os-\ncillation. These augmented temporal signals are illustrated\nin Fig. 12.\n13\n"
    },
    {
      "page_number": 14,
      "text": "Figure 11. Example of Cam×Time. Multi-view, densely sam-\npled sequences from the Cam×Time dataset. Each row shows\nframes from one camera trajectory, and each column samples dif-\nferent timesteps (0–120). The dataset provides diverse environ-\nments, human motions, and four camera paths per scene with full\n120-frame temporal coverage.\nFigure 12. Sampling from Cam×Time. By sampling from the\nCam×Time dataset, we can extract frames corresponding to arbi-\ntrary combinations of camera viewpoints and temporal positions,\nforming source-target pairs with rich camera and temporal control\nsignals.\nD. Additional Ablation Studies\nD.1. Temporal Warping Augmentation\nUsing [1, 2] as our default datasets, we compare training\njointly with static-scene datasets [18, 53] with applying\nonly temporal warping (TW) augmentation on the default\ndatasets (Sec. 3.2.2 in the main paper). Although static-\nscene datasets naturally support bullet-time effects, they do\nnot provide enough diversity of temporal control configura-\ntions for models to reliably learn time locking on their own,\nas shown in Fig. 14 (top). Please refer to section “Effective\nTemporal Warping” in the website for more videos.\nIn Fig. 14 (bottom), we further show that freezing tempo-\nral warping (3rd row) produces better results than training\nwithout freezing it. Please refer to section “Freeze Warping\nAblations” in the website for more videos.\nD.2. Significance of Cam×Time Dataset\nBesides the quantitative results in the main paper (Table 5),\nin Fig. 15 (top), we provide visual comparisons demonstrat-\ning the effectiveness of the proposed Cam×Time dataset.\nClear artifacts appear in baselines trained without additional\ndata or with only static-scene augmentation (highlighted\nin red boxes), whereas incorporating Cam×Time removes\nthese artifacts, demonstrating its significance. Please re-\nfer to section “Dataset Ablations” in the website for more\nvideos.\nD.3. Time Embedding Ablation\nAs promised in Sec. 3.2.1 in the main paper, we compare\nseveral time-embedding strategies. RoPE(f ′) can freeze the\nscene dynamics at t=40, but it also undesirably locks the\ncamera motion. Using MLP, by contrast, fails to lock the\ntemporal state at all (red boxes). Conditioning on the la-\ntent frame f ′ (with uniform sampling) introduces notice-\nable artifacts. In comparison, the proposed 1D-Conv em-\nbedding enables SpaceTimePilot to preserve the intended\nscene dynamics while still generating accurate camera mo-\ntion. Adding Cam×Time to training further enhances the\nresults. Please refer to section “Time-Embedding Method\nAblation” in the website for more examples.\nE. Additional Qualitative Visualizations\nWe show more qualitative results of SpaceTimePilot in\nFig. 13. Our model provides fully disentangled control over\ncamera motion and temporal dynamics. Each row presents\na different pairing of temporal control inputs (top-left icon)\nand camera trajectories. SpaceTimePilot reliably generates\ncoherent videos under diverse conditions, including normal\nand reverse playback, bullet-time, slow motion, replay mo-\ntion, and complex camera movements such as panning, tilt-\ning, zooming, and vertical translation. Please refer to sec-\ntion “Video Demonstrations” in the website for more exam-\nples.\n14\n"
    },
    {
      "page_number": 15,
      "text": "bullet-time\nat t=40\nreverse\nmotion\nbounce\n40-80-40\nslow motion\n40-80\nFrame 0\nFrame 40\nFrame 80\n...\n...\nFrame 0\nFrame 40\nFrame 80\n...\n...\nbounce\n40-80-40\nbounce\n40-80-40\nslow motion\n40-80\nbounce\n40-80-40\nFigure 13. More Qualitative results. Our model provides fully disentangled control over camera motion and temporal dynamics. Each\nrow illustrates a different combination of temporal control inputs (top-left icon) and camera trajectories. SpaceTimePilot consistently\nproduces coherent videos across a wide range of controls, including normal and reverse playback, bullet-time, slow motion, replay motion,\nand complex camera paths such as panning, tilting, zooming, and vertical motion.\n15\n"
    },
    {
      "page_number": 16,
      "text": "Src Video\nDefault+\nRE10k+MC\nDefault \nw/ TW\nAblation 1: Temporal warping (TW) vs. Joint dataset training on bullet-time effect (t=40)\nSrc Video\n1D-Conv\n1DConv\n+Cam×Time\nw/o Freezing\nTemproal\nWarping\n1DConv\n+Cam×Time w\nFreezing\nTemporal\nWarping\nAblation 2: Varied temporal warping configuration on bullet-time effect (t=40) \nFigure 14. Ablation study. (Top) Using [1, 2] as default datasets, we compare the influence of adding static-scene datasets [18, 53] vs. just\ndoing temporal warping (TW) augmentation (Sec. 3.2.2 in the main paper). Temporal warping definitely provide more variety of time\ncontrol signals, allowing models to learn better camera-time disentanglement. (Bottom) We further compare different configurations of\nwarping, where we show freezing temporal warping (3rd row) leads to better results than those trained without freezing temporal warping.\n16\n"
    },
    {
      "page_number": 17,
      "text": "Src Video\nDefault  w/ TW\nDefault w/ TW\n＋ Cam×Time\nAblation 3: Dataset ablation: Cam×Time vs. static-scene datasets on bullet-time effect (t=40) \nDefault w/ TW\n＋ RE10k ＋ MC \nSrc Video\nMLP\nUniform\nSampling\n1D-Conv\n1D-Conv w/\nCam×Time\nAblation 4: Different time embedding schemes on bullet-time effect (t=40)\nRoPE(f’)\nFigure 15. Ablation study. (Top) We verify the efficacy of the proposed Cam×Time dataset. Considering [1, 2] as default datasets,\nwe compare the impact of different datasets on the generated videos. One can clearly see artifacts in baselines without any extra data or\naugmented with static-scene data, whereas training additionally with Cam×Time leads to no artifacts, confirming the usefulness of our\ndataset. (Bottom) We compare several time-embedding strategies. The MLP fails to lock the temporal state (red boxes), while RoPE(f ′)\ncorrectly freezes the scene dynamics at t=40 but unintentionally locks the camera motion too. Conditioning on the latent frame f ′ (with\nuniform sampling) introduces noticeable artifacts. In contrast, the proposed 1D-Conv embedding allows SpaceTimePilot to both freeze the\nscene dynamics at t=40 and produce intended camera motion. Incorporating Cam×Time during training further improves performance.\n17\n"
    }
  ]
}