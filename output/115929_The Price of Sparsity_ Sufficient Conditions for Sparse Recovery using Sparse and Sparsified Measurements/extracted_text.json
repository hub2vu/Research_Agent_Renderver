{
  "filename": "115929_The Price of Sparsity_ Sufficient Conditions for Sparse Recovery using Sparse and Sparsified Measurements.pdf",
  "total_pages": 40,
  "full_text": "The Price of Sparsity: Sufficient Conditions for Sparse\nRecovery using Sparse and Sparsified Measurements\nYoussef Chaabouni\nOperations Research Center\nMassachusetts Institute of Technology\nCambridge, MA 02139, USA\nyouss404@mit.edu\nDavid Gamarnik\nOperations Research Center\nMassachusetts Institute of Technology\nCambridge, MA 02139, USA\ngamarnik@mit.edu\nAbstract\nWe consider the problem of recovering the support of a sparse signal using noisy\nprojections. While extensive work has been done on the dense measurement\nmatrix setting, the sparse setting remains less explored. In this work, we establish\nsufficient conditions on the sample size for successful sparse recovery using\nsparse measurement matrices. Bringing together our result with previously known\nnecessary conditions, we discover that, in the high-SNR regime where ds/p →\n+∞, sparse recovery using a sparse design exhibits a phase transition at an\ninformation-theoretic threshold of nSP\nINF = Θ (s log (p/s) / log (ds/p)) for the\nnumber of measurements, where p denotes the signal dimension, s the number\nof non-zero components of the signal, and d the expected number of non-zero\ncomponents per row of measurement. This expression makes the price of sparsity\nexplicit: restricting each measurement to d non-zeros inflates the required sample\nsize by a factor of log s/ log (ds/p), revealing a precise trade-off between sampling\ncomplexity and measurement sparsity. Additionally, we examine the effect of\nsparsifying an originally dense measurement matrix on sparse signal recovery. We\nprove in the regime of s = αp and d = ψp with α, ψ ∈(0, 1) and ψ small that a\nsample of size nSp-ified\nINF\n= Θ\n\u0000p/ψ2\u0001\nis sufficient for recovery, subject to a certain\nuniform integrability conjecture, the proof of which is work in progress.\n1\nIntroduction\nIn recent years, sparse signal recovery has gained significant attention, motivated by applications in\ncompressive sensing [7, 2, 5]; signal denoising [3]; sparse regression [13]; data stream computing\n[4, 11, 14]; combinatorial group testing [6]; etc. Practical examples range from the single-pixel\ncamera, MRI scanners and radar remote-sensing systems to error-correction schemes in digital\ncommunications and widely used image-compression formats [7, Chap. 1].\nThe problem can be formulated as follows. Consider a signal β⋆∈Rp, unknown but a priori s-sparse\nfor some given s ≤p, a random measurement matrix X ∈Rn×p (also referred to as design, features\nor data) and a noise vector Z ∼N(0, σ2In), where n ∈N denotes the sample size and σ2 > 0 a\nfixed constant. An vector of observations (also known as labels or annotations) is given by:\nY := Xβ⋆+ Z.\nSparse recovery refers to reconstructing β⋆given X and Y . Intuitively, this problem can be reduced\nto recovering the support S⋆of β⋆, i.e. the set of indices of its non-zero components. In fact, once\nthe support S⋆is identified, the full signal can be estimated using the corresponding columns of X\nvia the closed-form maximum-likelihood estimator formula βMLE = X+\nS⋆Y , where X+\nS⋆denotes the\nMoore-Penrose pseudoinverse of the submatrix formed by the columns of X with indices in S⋆[10].\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nTraditionally, X is considered to be a dense random matrix with sub-Gaussian entries. Previous\nworks have shown that the complexity of the problem in terms of required sample size exhibits two\nphase transitions at two thresholds nINF < nALG, yielding three regimes:\n• n < nINF: impossibility of recovery. Reeves et al. [16] show that if n ≤(1 −ε) nINF then the\nrecovery of any fraction of the support of the signal is impossible.\n• nINF < n < nALG: super-polynomial complexity. Gamarnik and Zadik [8] show that if n ≥\n(1 + ε) nINF then the maximum-likelihood estimator (MLE) recovers the support of β⋆. Although\nsolvable, the problem is widely believed to be algorithmically hard since the MLE exhibits an\nOverlap Gap Property (OGP) [8].\n• n > nALG: polynomial-time recovery. Wainwright [18] shows that if n ≥(1 + ε) nALG then the\nLasso [17] succeeds in recovering the support of β⋆.\n1.1\nSparse measurement setting\nWhile dense matrices offer an optimal sample size, they are costly in terms of storage and computation.\nSparse measurement matrices, where the number of non-zero entries per measurement vector scales\nsignificantly smaller than the signal dimension, mitigate these costs: they require significantly less\nstorage and allow for more efficient computations, as matrix-vector multiplications and incremental\nupdates can be performed faster. In addition, they enable efficient signal recovery algorithms by\ntaking advantage of the structural properties of the problem [9]. However, this sparsity comes at\nthe cost of increased sampling complexity [19]. This raises the following key question: How does\nmeasurement sparsity trade off with sampling complexity?\nSome of the prior studies have explored this sparse measurement setting. Wang et al. [19] establish\nnecessary conditions for sparse recovery for various measurement sparsity regimes. Let d denote\nthe expected number of non-zero components of a row of X. Their work reveals three regimes of\nbehavior depending on ds/p, the expected number of non-zero components of β⋆that align with\nnon-zero components of a row of X. The three regimes are: ds/p →+∞, ds/p = τ for some\nconstant τ > 0, and ds/p →0. They show that in each regime, the number of samples n must\nexceed a specific information-theoretic lower bound for any algorithm to reliably recover the signal’s\nsupport. In particular, in the first regime, where ds/p →+∞, the necessary condition threshold of\n[19] is the same as the one of the dense case, while it increases dramatically in the third case, where\nds/p →0. They work with entries rescaled so that Var (Xβ⋆) matches the dense case, while we\nkeep Var (Xij) = 1. The settings are equivalent since any scaling of X can be accounted for in β⋆.\nIn this work, we examine the opposite question: how many samples are enough to guarantee a\nreliable recovery? For simplicity, we assume the signal is binary, i.e. β⋆∈{0, 1}p. Note that in\nthis case, recovering the support is equivalent to recovering the signal. This assumption is very\ncommon in the literature [1, 16, 8]. Intuitively, detecting a component of size 1 is at least as hard as\ndetecting a stronger component, so the resulting thresholds are representative of signals with non-zero\nentries bounded away from zero by 1, i.e. β⋆∈\n\b\nβ ∈Rp : ∥β∥0 = s and minj∈[p] : βj̸=0 |βj| ≥1\n\t\n.\nOur first main result (Theorem 1) states that in the high signal-to-noise ratio (SNR) regime where\nds/p →+∞, if the number of samples n is larger than a threshold given by:\nnSP\nINF = Θ\n\u0012s log (p/s)\nlog (ds/p)\n\u0013\n,\n(1)\nthen the MLE asymptotically recovers the support of the signal. The proof uses large deviation\ntechniques to bound the probability that a different support has a lower error than the true one, and a\nunion bound over such supports. Bringing our result together with the necessary condition shown by\nWang et al. [19], we reveal that the problem exhibits a phase transition – similar to the one known\nin the dense case – at the information-theoretic threshold nSP\nINF. In fact, if there exists a constant\nε > 0 such that n ≤(1 −ε) nSP\nINF then it is information-theoretically impossible to ensure a reliable\nrecovery of the support of the signal, and if there exists a constant ε > 0 such that n ≥(1 + ε) nSP\nINF\nthen the MLE ensures a reliable recovery of the support. Our findings therefore answer the question\nof exactly how much data is needed for recovery. However, we note that the recovery that we show in\nthe sufficiency statement holds in a weaker sense than the non-recovery in the necessity statement.\nWe call the amount of additional observations in the sparsification setting compared to the dense one\nprice of sparsity. Precisely, restricting each measurement to d non-zeros inflates the required sample\n2\nsize by a factor of Γ = Θ (log s/ log (ds/p)), quantifying the sampling complexity vs. measurement\nsparsity trade-off.\nRegarding the computational complexity, Omidiran and Wainwright [15] show that the Lasso\nperforms as well in the sparse setting as in the dense setting, assuming a slow decay of sparsity. They\nshow that, under some slow sparsity assumption, it is sufficient for the sample size n to be larger than\nthe algorithmic threshold of the dense setting discussed above, given by:\nnALG = 2s log (p −s) ,\nspecifically for the Lasso to ensure a reliable polynomial time recovery of β⋆. Although the sparsity\nassumption under which this result holds allows for the density rate d/p to go to 0 as p →+∞, it\nstill doesn’t allow the measurements to be very sparse. In fact, it requires that:\nd/p = ω\n\u0010\ns−1/3\u0011\nand\nd/p = ω\n \u0012log log (p −s)\nlog (p −s)\n\u00131/3!\n.\n(2)\nThis raises a question about what happens in a more sparse regime. Although we don’t address the\nalgorithmic threshold, i.e. the question of polynomial-time recovery, we discover herein a sufficient\ncondition for recovery that allows for more sparsity, allowing for future investigation in this direction.\n1.2\nSparsification\nThe applications of the signal recovery problem [7, Chapter 1] considered in this paper can be broadly\ncategorized into two classes:\n- Applications where X is designed, e.g. involving signal compression and reconstruction.\n- Applications where X is observed, e.g. sparse regression, signal denoising, and error correction.\nIn light of this categorization, we note that measuring the trade-off between measurement sparsity\nand sampling complexity is particularly useful for the first class of problems. It provides practitioners\nwith an exact description of how the measurement matrix should be designed, in terms of size and\nsparsity, to optimize the computational cost of signal recovery. However, this is rendered useless in\nthe second class of problems when the measurement matrix is observed and dense. This motivates the\nsecond key question: given an initially dense measurement matrix, is there a way to make it sparse\nand still aim to recover the original signal?\nThis setting, in which the observations are generated with a dense measurement matrix, but the\nsignal is recovered using a sparsified version of it, is closely related to the “missing covariates”\nor “missing-at-random” framework studied in high-dimensional statistics. Prior work by Loh and\nWainwright [12], established algorithmic ℓ2-error bounds for regression under this model, assuming\ndense Gaussian designs and a constant missingness rate. Our analysis in Section 3 complements\nthis line of research by focusing instead on information-theoretic support-recovery thresholds and\nderiving the precise sample complexity cost incurred by sparsification in this regime.\nIn our examination of the sparsification question, we focus on the linear sparsity and linear sparsification\nregime where s = αp and d = ψp for constant α, ψ ∈(0, 1). Specifically, our second main result\n(Theorem 3) states that if the number of samples n is larger than a threshold given by:\n2h (α) p\nlog\n\u0010\n1 +\nδψ2\n(1−ψ)(2−δ(1−ψ))\n\u0011,\n(3)\nthen the minimizer of the mean squared error (MSE) based on sparsified measurements and accordingly-\nrescaled observations asymptotically recovers the true support up to error fraction δ ∈(0, 1). In\nparticular, support recovery is possible for arbitrarily aggressive sparsification, i.e. arbitrarily small ψ.\nIn the strong-sparsification regime where ψ →0, the sufficient threshold (3) effectively writes:\nnSp-ified\nINF\n= Θ\n\u0012 p\nψ2\n\u0013\n.\n(4)\nWe call the amount of additional observations in the sparsification setting compared to the dense one\nprice of sparsification. Unlike the price of sparsity, it is not due to the sparsity of the measurements\nbut rather to a bias in the observations. We also interpret our result as providing an expression of the\nsparsification budget: the level up to which one could sparsify their data and still recover the true\nsignal. Based on our result, this has order Θ\n\u0010p\np/n\n\u0011\nwhen n = Ω(p).\n3\n1.3\nContributions\nTo the best of our knowledge, this work is the first to address the following:\n1. Establish a necessary and sufficient condition for sparse recovery in the sparse setting.\n2. Provide a sufficient condition for sparse recovery after sparsifying an originally dense design,\nconditional on a mild uniform-integrability conjecture.\n1.4\nOutline and Notations\nWe organize the rest of the paper as follows. Section 2 studies the sparse measurement setting. Section\n3 examines recovery after sparsifying an originally dense measurement matrix. Section 4 concludes\nand sketches future work directions.\nThroughout this document, we will use the following notations:\n• We denote by h (·) the binary entropy: h (x) = −x log x −(1 −x) log (1 −x), x ∈(0, 1).\n• We call ℓ0-norm the number of non-zero coordinates of x ∈Rd, that is ∥x∥0 := Pd\ni=1 1 (xi ̸= 0).\n2\nSparse Recovery using Sparse Measurements\n2.1\nSetting\nLet n, p, s, d ∈N such that d, s ≤p. We define a sparse Gaussian matrix in Rn×p as follows.\nDefinition 2.1 (Sparse Gaussian matrix). We call X = [Xij]i∈[n],j∈[p] ∈Rn×p a sparse Gaussian\nmatrix with parameter d if for all i ∈[n], j ∈[p] we have:\nXij = BijNij,\nwhere (Bij)i∈[n],j∈[p]\ni.i.d.\n∼Ber (d/p) and (Nij)i∈[n],j∈[p]\ni.i.d.\n∼N (0, 1) are mutually independent.\nRemark 2.1. Note that d is the expected number of non-zero components per row of X. In our\nsetting, we think of d as being of smaller order of magnitude than p. In particular, d = o (p).\nLet X be a sparse Gaussian random matrix of parameter d, and Z be a random vector in Rn such that\nZ ∼N\n\u00000, σ2In\n\u0001\n, with σ > 0 a fixed constant. Let β⋆∈{0, 1}p be a deterministic vector such that\n∥β⋆∥0 = s. We define the random vector Y as:\nY := Xβ⋆+ Z.\n(5)\nOf particular interest is the signal-to-noise ratio (SNR), known to be an important quantity for\ncharacterizing the difficulty of sparse recovery problems [19, 16]. It’s defined as follows:\nSNR := E∥Xβ⋆∥2\n2\nE∥Z∥2\n2\n= ds\npσ2 .\n(6)\nThe maximum likelihood estimator (MLE) of β⋆is defined by the random vector:\nˆβ :=\narg min\nβ∈{0,1}p, ∥β∥0=s\n∥Y −Xβ∥2\n2 .\n(7)\nWe are interested in the minimum number of samples n required so that the MLE (7) asymptotically\nrecovers the true signal β⋆. We formalize the problem as follows.\n2.2\nProblem\nWe start by defining the support of a vector and the symmetric difference of supports.\nDefinition 2.2 (Support). Let u ∈Rp. We call support of u the set of indices of the non-zero\ncomponents of u and denote it Supp (u) := {i ∈[p]: ui ̸= 0}. Note that |Supp (u)| = ∥u∥0.\nDefinition 2.3 (Symmetric difference). We call symmetric difference between two sets S1 and S2 the\nset of elements in one but not the other and denote it S1 △S2 :=\n\u0000S1 ∪S2\n\u0001\n\\\n\u0000S1 ∩S2\n\u0001\n.\n4\nWe formalize the problem we address below as follows: given an error tolerance δ ∈(0, 1), we wish\nto determine the minimum number of samples n as a function of p, s and d required so that:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n−→1, as n, p, s, d →+∞.\n2.3\nResults\nOur first main result, Theorem 1, provides a sufficient condition on the sample size for reliable support\nrecovery when using sparse measurements.\nTheorem 1 (Sufficient conditions for sparse recovery using sparse measurement matrices). Suppose\np, s, d →+∞, d = o (p) and ds = ω (p) (i.e. SNR →+∞). Let δ ∈(0, 1). We consider two\ndifferent regimes.\n1. Assume s = o (p). Let\nn⋆\nslin :=\n2s log (p/s)\nlog (ds/p) + log (δ/(2σ2)).\nIf there exists ε > 0 such that n ≥(1 + ε) n⋆\nslin, then the MLE ˆβ recovers β⋆up to error δ w.h.p.:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εs log (p/s) + o\n\u0000s log (p/s)\n\u0001\u0011\n,\nas n, p, s, d →+∞.\n2. Assume there exists a constant α ∈(0, 1) such that s = αp. Let:\nn⋆\nlin :=\n2h (α) p\nlog d + log (δα/(2σ2)),\nwhere h (·) denote the entropy function. If there exists ε > 0 such that n ≥(1 + ε) n⋆\nlin, then the\nMLE ˆβ recovers β⋆up to error δ w.h.p.:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εh (α) p + o (p)\n\u0011\n,\nas n, p, s, d →+∞.\nThe proof of Theorem 1, given in appendix A.1, uses large deviation techniques to bound the\nprobability of a high-error support to have a lower MSE than the true one, then a union bound over\nsuch supports. We give below a brief proof sketch of Theorem 1.\nProof sketch. Let S denote the set of supports of cardinality s and S⋆= Supp (β⋆). For any S ∈S,\nwe denote by 1S the vector in {0, 1}p such that [1S]i = 1 (i ∈S) for all i ∈[p]. We define the loss\nfunction L over S such that L (S) := ∥Y −X1S∥2\n2, so that Supp\n\u0010\nˆβ\n\u0011\n= arg minS∈S L (S). As\np gets large, the event “L (S) < L (S⋆)” for any S such that |S △S⋆| ≥2δs is a rare event. The\nChernoff bound yields:\nlog P\n\u0010\nL (S) < L (S⋆)\n\u0011\n≤n\n2\n\u0012\nlog\n\u00122σ2p\nδds\n\u0013\n+ o (1)\n\u0013\n.\nThis step involves most of the technical work. Then, by union bound:\nP\n\u0010\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△S⋆\f\f\f < 2δs\n\u0011\n≥1 −\nX\nS : |S △S⋆|≥2δs\nP (L (S) < L (S⋆)) ≥1 −\n\u0012p\ns\n\u0013 \u00122σ2p\nδds\n\u0013n/2\n.\nSolving for n, we obtain a critical threshold of n⋆=\nlog(\np\ns)\nlog(ds/p)+log(δ/(2σ2)). We conclude.\nBringing together Theorem 1 with the necessary conditions shown by Wang et al. in [19], we obtain\nthe following corollary.\nCorollary 2 (Information-theoretic phase transition). The sparse recovery in the sparse setting\nproblem exhibits a phase transition at an information-theoretic threshold nSP\nINF.\n5\n1. In the first regime considered above, the expression of nSP\nINF is given by:\nnSP\nINF := 2s log (p/s)\nlog (ds/p) .\n2. In the second regime considered above, the expression of nSP\nINF is given by:\nnSP\nINF := 2h (α) p\nlog d .\nSpecifically, in each of these regimes:\n(i) If there exists ε > 0 such that n ≤(1 −ε) nSP\nINF then, as n, p, s, d →+∞, there exists no decoder\ng: Rn →{β ∈{0, 1}p : ∥β∥0 = s} such that:\nmax\nβ⋆∈{0,1}p, ∥β⋆∥0=s PX,Z\n\u0010\ng (Y ) ̸= Supp (β⋆)\n\u0011\n→0.\nIn this sense, it is information-theoretically impossible to ensure an asymptotically reliable recovery.\n(ii) If there exists ε > 0 such that n ≥(1 + ε) nSP\nINF, then as n, p, s, d →+∞:\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f\n2s\n−→0,\nin probability. In this sense, the MLE (7) ensures an asymptotically reliable recovery.\nThe proof of Corollary 2 is given in appendix A.2. Statement (i) is due to Wang et al. [19], while\nstatement (ii) follows from Theorem 1 and is the main contribution of this section.\nRemark 2.2 (Limitations). We note that the definition of “recovery” is not the same in statements (i)\nand (ii) of Corollary 2. The necessity statement (i) is about the impossibility a vanishing probability\nof exact equality of supports, which is stronger than the sufficiency statement of vanishing rescaled\nerror in (ii). In particular, it is possible in theory that both statements hold simultaneously. To the best\nof our knowledge, the literature on the dense setting also suffers from this gap (see [8, 18]). While\nReeves et al. [16] establish a clean information-theoretic phase transition in the dense setting (using\nthe same definition of “recovery” for both bounds), their analysis considers different problem settings\nand convergence guarantees than ours.\nWe interpret Theorem 1 and Corollary 2 as follows.\n• Phase transition. For simplicity, we only discuss the sublinear sparsity regime, defined by s = o (p).\nPrevious works on sparse recovery in the dense case ([16],[8]) have shown the existence of an\ninformation-theoretic threshold:\nnINF = 2s log (p/s)\nlog s\n,\n(8)\nat which the complexity of support recovery in terms of sample size exhibits a phase transition, where\nthe recovery of any fraction of the support is impossible for n ≤(1 −ε) nINF, and full recovery\nis guaranteed by the MLE for n ≥(1 + ε) nINF. In light of this, we ask if the support recovery\nproblem for the class of sparse measurement matrices described above exhibits a similar behavior.\nIn Corollary 2, we show that indeed, it exhibits a similar phase transition at an information-theoretic\nthreshold given by:\nnSP\nINF = 2s log (p/s)\nlog (ds/p) .\n(9)\nIn Table 1, we summarize these information-theoretic thresholds alongside known algorithmic\nthresholds for the sublinear sparsity regime\n\u0000s = o (p)\n\u0001\n, highlighting the comparison between\ndense and sparse measurements in the high-SNR setting.\n• Price of Sparsity. In particular, we notice that nSP\nINF ≥nINF. This confirms the intuition that sparse\nrecovery requires more samples in the sparse measurement case: in fact, due to the sparsity of the\nmeasurement matrix, there is a low probability for the coefficient of a component of the signal at a\ngiven observation to be non-zero, and hence the need for a larger sample size for recovery. In light\n6\nTable 1: Comparison of Sample Complexity Thresholds for Sublinear Sparsity\n\u0000s = o (p)\n\u0001\n.\nMeasurement\nInfo-Theoretic\nNecessary\nInfo-Theoretic\nSufficient\nAlgorithmic\nNecessary\nAlgorithmic\nSufficient\nDense\n2s log(p/s)\nlog s\n[16, 19]\n2s log(p/s)\nlog s\n[8, 16]\n2s log(p −s) [8]\n2s log(p −s) [18]\nSparse,\nhigh SNR\n2s log(p/s)\nlog(ds/p) [19]\n2s log(p/s)\nlog(ds/p) (Thm 1)\nUnknown\n2s log(p −s) [15]†\n†Holds under the slow decay of sparsity assumption in [15], see (2).\nof this, Corollary 2 is to be interpreted as providing an exact value for the price of sparsity, i.e. the\nextra amount of observations required in the sparse setting compared to the dense one, which is\ngiven by:\nΓ := nSP\nINF\nnINF\n=\nlog s\nlog (ds/p) > 1.\n• Dependence on d and s. Note that the expression of the price of sparsity heavily depends on the\nregimes of d and s. The smaller the density rate d/p, the more “expensive” the desired sparsity of\nthe measurements is, as suggested by the expression of Γ. In particular, Γ could take any value in\n(1, +∞), depending on the regimes of d and s w.r.t. p.\nExample 2.1. Consider the setting where s = pα, d = pβ with α, β ∈(0, 1) such that α + β > 1.\nThen Γ = α/ (α + β −1) ∈(1, +∞) , which goes to 1 when β goes to 1 (low measurement\nsparsity), and goes to +∞when α is fixed and β goes to 1 −α (high measurement sparsity).\n• Measurement sparsity vs. Sampling complexity trade-off. In this context, we conclude the\nexistence of a measurement sparsity vs. sampling complexity trade-off, which can also be interpreted\nas a trade-off between sampling complexity and computational cost. We consider an example that\nhighlights this trade-off.\nExample 2.2. Let ψ: [e, +∞) −→[e, +∞) such that ψ (x) = x/ log x. Consider two mea-\nsurement matrices: X1 a dense Gaussian in Rn1×p and X2 a sparse Gaussian Rn2×p with\nonly d = min\n\u0000po(1), ψ−1 (o (ψ (p)))\n\u0001\nexpected non-zero entries per row; and an s-sparse signal\nβ⋆∈Rp, in the linear sparsity regime where s = αp for constant α ∈(0, 1). On one hand, the\nnumber of samples required for reliable recovery raises from n1 = nINF = Θ\n\u0000p/ log p\n\u0001\nin the\ndense case to n2 = nSP\nINF = Θ\n\u0000p/ log d\n\u0001\nin the sparse one. On the other hand the computational\ncost of recovery the support is better in the sparse case, as matrix-vector multiplication cost drops\nfrom n1p = Θ\n\u0000p2/ log p\n\u0001\nto n2d = Θ\n\u0000pd/ log d\n\u0001\n. This highlights a trade-off between sampling\ncomplexity and computational cost. Proofs of these statements are given in appendix A.3.\n• Allowing for more sparsity. Note that the sparsity assumption under which Theorem 1 guarantees\nreliable recovery when there exists ε > 0 such that n ≥(1 + ε) nSP\nINF, which is:\nds/p →+∞,\n(10)\nis weaker than the sparsity assumption of the sufficient algorithmic threshold of Omidiran and\nWainwright [15] which guarantees polynomial-time recovery if there exists ε > 0 such that\nn ≥(1 + ε) nALG. As they show, this holds under the assumption that:\n\u0012d\np\n\u00133\nmin\n\u001a\ns, log log (p −s)\nlog (p −s)\n\u001b\n→+∞.\nFor example, when s = Θ (p), this requires that d = ω\n\u0000p2/3\u0001\n, while our result holds under the\nweaker assumption of d = ω (1). Our result allows for a significantly better sparsity, but this comes\nat the cost of potential super-polynomial computational complexity, since computing the MLE (7)\nis exponential-time in general.\n3\nImproving Sparse Recovery via Sparsification\n3.1\nSetting\nLet n, p, s, d ∈N and β⋆∈{0, 1}p s-sparse, defined as in Section 2.1. Let X ∈Rn×p such that\n(Xi,j)i∈[n],j∈[p]\ni.i.d.\n∼N (0, 1) and Z ∼N\n\u00000, σ2In\n\u0001\n, with σ > 0 constant. Let Y := Xβ⋆+ Z ∈Rn.\n7\nLet (Bij)i∈[n],j∈[p]\ni.i.d.\n∼Ber (d/p). We define the following sparsified version of X:\n˜X ∈Rn×p such that\n˜Xij := BijXij, ∀i ∈[n], j ∈[p].\n(11)\nIn addition, we define a rescaled version of Y as follows:\n˜Y := d\np Y ∈Rn.\n(12)\nAn estimator of β⋆is defined by the random vector:\nˆβ :=\narg min\nβ∈{0,1}p, ∥β∥0=s\n\r\r\r ˜Y −˜Xβ\n\r\r\r\n2\n2 .\n(13)\nThat is, the observations are generated with a dense measurement matrix (X), but the signal is\nrecovered using a sparsification of that matrix ( ˜X). We formalize the problem we address below as\nfollows: given an error tolerance δ ∈(0, 1), we wish to determine the minimum number of samples n\nin terms of p, s and d required so that:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n−→1, as n, p, s, d →+∞.\n3.2\nResults\nOur second main result, Theorem 3, provides a sufficient condition on the sample size for reliable\nsupport recovery after sparsifying an originally dense measurement matrix.\nTheorem 3 (Sufficient conditions for sparse recovery using sparsified measurements). Suppose that\np →+∞and there exist α, ψ ∈(0, 1) such that s = αp and d = ψp. Let δ ∈(0, 1). Let:\nn⋆\nSp-ified :=\n2h (α) p\nlog\n\u0010\n1 +\nδψ2\n(1−ψ)(2−δ(1−ψ))\n\u0011.\n(14)\nIf there exists ε > 0 such that n ≥(1 + ε) n⋆\nSp-ified, then, under Conjecture B.1, ˆβ recovers β⋆up to\nerror δ w.h.p.:\nP\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εh (α) p + o (p)\n\u0011\n,\nas n, p, s, d →+∞.\nRemark 3.1 (Limitations). We show that this result holds under a mild uniform-integrability\nconjecture. The statement of the conjecture is deferred to appendix B (Conjecture B.1) as it requires\ndelving into technical details of the proof. We expect it to follow from standard concentration bounds\nas all relevant random terms are sub-Gaussian, and verification is work in progress.\nThe proof of Theorem 3 is given in appendix B. It follows the same general outline as the proof\nof Theorem 1, but is much more technically involved. In particular, deriving the large-deviation\nbound is harder because the MSE no longer decomposes neatly: ˜Y does not correspond to the true\nobservations of the sparsified data but rather to a rescaling of the original observations (12). Instead\nof repeating a sketch here, we simply refer the reader back to the earlier proof sketch (see section 2.3).\nWe interpret Theorem 3 as follows.\n• Arbitrary sparsification rate. According to Theorem 3, support recovery is possible for arbitrarily\naggressive sparsification, i.e. arbitrarily small ψ, provided a large enough sample size.\n• Strong-sparsification regime. In the strong-sparsification regime where ψ →0, the denominator\nof n⋆\nSp-ified in (14) is effectively δψ2/ (2 −δ), and hence the sufficient condition upper bound writes:\nnSp-ified\nINF\n= 2 (2 −δ) h (α) p\nδψ2\n= Θ\n\u0012 p\nψ2\n\u0013\n.\n(15)\n• Price of Sparsification. We interpret our result as providing a value for the price of sparsification, i.e.\nthe extra amount of observations required due to the information loss resulting from sparsification.\nIn the linear sparsity and strong-sparsification regime, it writes:\nΓSp-cation := nSp-ified\nINF\nnINF\n=\nΘ\n\u0000p/ψ2\u0001\nΘ (p/ log p) = Θ\n\u0012log p\nψ2\n\u0013\n.\n8\nUnlike the intrinsically-sparse-observations setting studied in Section 2, this extra amount of\nrequired observations is not due to the sparsity of the measurements. In fact, one can check\nfrom (24) that in the linear sparsity regime where d = Θ (p), the price of sparsity is constant (i.e.\nΓ = Θ (1)). Instead, the price of sparsification is due to a bias in the observations that we explain\nby the fact that the sparsified observations ˜Y were not obtained as noisy projections of the true\nsignal as in the original model (5), but rather via a naïve rescaling of the original observations (12).\nBy simply rescaling the observations we did not discard the information in Y coming from the\nnullified components of X, hence introducing a bias.\n• Sparsification budget. Given dense data and a fixed large enough sample size n, by up to how\nmuch could we sparsify the data and still get recovery? We call this the sparsification budget.\nAccording to our result (15), its expression in the strong-sparsification regime is given by:\nψbudget = Θ\n\u0010p\np/n\n\u0011\n.\nIn particular, the above expression only makes sense when n = Ω(p), below which Theorem 3\ndoes not hold.\n4\nConclusion and Future Work\nIn the first part of this paper, we have studied the problem of recovery of a binary signal β⋆∈{0, 1}p\nbased on a sparse measurement matrix and noisy observations. Our main result is that, if the\nmeasurements have density rate d/p then, assuming that the measurements and the signal are together\nnot too sparse – in particular if ds = ω (p), i.e. high-SNR regime – it is possible to recover the\ntrue support asymptotically when the sample size is larger than the threshold given by Theorem 1.\nCombining our work with the necessary conditions of Wang el al. [19], we reveal an information-\ntheoretic phase transition. The expression of the phase-transition threshold makes the price of sparsity\nexplicit, revealing a precise trade-off between sampling complexity and measurement sparsity. In\nthe following, we present a quick summary of all – to the best of our knowledge – results on sparse\nrecovery in the sparse measurement setting, along with some future work directions.\n• Information-theoretic threshold, sufficient conditions. In Theorem 1, we establish a sufficient\ncondition for reliable recovery. However, this result is conditional on the high-SNR (ds/p →+∞)\nassumption. This raises the question of sufficient conditions when the measurements and signal are\neven more sparse.\n• Informational-theoretic threshold, necessary conditions.\n– Wang et al. [19] have studied this problem. Their work reveals three regimes of behavior\ndepending on the scaling of the expected number of non-zeros of β⋆aligning with non-zeros of a\nrow of X: ds/p = ω (1), ds/p →τ for some τ > 0, and ds/p = o (1). For their model, where\nthe variance of the non-zero components of X scales in a way that makes the second moment of\nthe projected signal Xβ⋆remain the same as in the dense case: the necessary condition threshold\nis on the order of magnitude of the one in the dense case in the regime where ds/p = ω (1),\nwhile it increases dramatically in the regime where ds/p = o (1).\n– In the dense setting, Reeves et al. [16] have shown that even the recovery of a fixed fraction of\nthe support is information theoretically impossible below the phase transition threshold: this is\nwhat they call the all-or-nothing property. It would be interesting to extend this property to the\nsparse setting.\n• Algorithmic threshold, sufficient conditions. Omidiran and Wainwright [15] have shown that under\na low-sparsity assumption on the measurements, the sufficient condition of the dense setting, i.e.\nn ≥(1 + ε) nALG, is sufficient for the sparse setting as well. It would be interesting to explore the\nquestion of polynomial-time recovery in a stronger sparsity regime.\n• Algorithmic threshold, necessary conditions. Although we cannot really hope to provide necessary\nconditions for polynomial-time recovery – unless conditionally on P ̸= NP – it would be interesting\nto provide a threshold under which the problem is believed to be algorithmically hard, as done by\nGamarnik and Zadik [8] in the dense setting.\nIn the second part of this paper, we have studied the problem of recovering the signal based on\nsparsified – but originally dense – measurements and accordingly-rescaled observations. Our main\n9\nresult is that, in the linear sparsity and linear sparsification regime where s = αp and d = ψp for\nconstant α, ψ ∈(0, 1), it is possible to recover the true support asymptotically when the sample\nsize is larger than a threshold given by Theorem 3. This reveals that support recovery is possible\nfor arbitrarily aggressive sparsification provided a large enough sample size, and provides an upper\nbound on the price of sparsification.\nNevertheless, we believe that the sparsification problem is infeasible for strong enough regimes of\nsparsification. In particular, we conjecture that the recovery is information-theoretically impossible\nno matter the sample size in the sub-linear sparsification regime where d = o (p). We leave the\nexploration of this regime for future work.\nAcknowledgments and Disclosure of Funding\nThis work was supported by the National Science Foundation (NSF) under grant CISE-2233897.\nYoussef Chaabouni thanks Soulef Frikha for long-standing mathematical inspiration, and Mehdi\nMakni for many years of insightful discussions. He also thanks Marouane Nejjar, Panos Tsimpos, and\nMalo Lahogue for valuable feedback and exchanges.\nReferences\n[1] Shuchin Aeron, Venkatesh Saligrama, and Manqi Zhao. Information theoretic bounds for\ncompressed sensing. IEEE Transactions on Information Theory, 56(10):5111–5130, 2010.\n[2] Emmanuel J Candès, Justin Romberg, and Terence Tao. Robust uncertainty principles: exact\nsignal reconstruction from highly incomplete frequency information. IEEE Transactions on\ninformation theory, 52(2):489–509, 2006.\n[3] Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by\nbasis pursuit. SIAM review, 43(1):129–159, 2001.\n[4] Graham Cormode and Marios Hadjieleftheriou. Finding the frequent items in streams of data.\nCommunications of the ACM, 52(10):97–105, 2009.\n[5] David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289–\n1306, 2006.\n[6] Ding-Zhu Du and Frank Kwang-ming Hwang. Combinatorial group testing and its applications,\nvolume 12. World Scientific, 1999.\n[7] Simon Foucart, Holger Rauhut, Simon Foucart, and Holger Rauhut. An invitation to compressive\nsensing. Springer, 2013.\n[8] David Gamarnik and Ilias Zadik. Sparse high-dimensional linear regression. estimating squared\nerror and a phase transition. The Annals of Statistics, 50(2):880–903, 2022.\n[9] Anna Gilbert and Piotr Indyk. Sparse recovery using sparse matrices. Proceedings of the IEEE,\n98(6):937–947, 2010.\n[10] Trevor Hastie. The elements of statistical learning: data mining, inference, and prediction, 2009.\n[11] Piotr Indyk. Sketching, streaming and sublinear-space algorithms. Graduate course notes,\navailable at, 33:617, 2007.\n[12] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing\ndata: Provable guarantees with non-convexity. Advances in neural information processing\nsystems, 24, 2011.\n[13] Alan Miller. Subset selection in regression. chapman and hall/CRC, 2002.\n[14] Shanmugavelayutham Muthukrishnan et al. Data streams: algorithms and applications. Founda-\ntions and Trends® in Theoretical Computer Science, 1(2):117–236, 2005.\n10\n[15] Dapo Omidiran and Martin J Wainwright. High-dimensional subset recovery in noise: sparsified\nmeasurements without loss of statistical efficiency. arXiv preprint arXiv:0805.3005, 2008.\n[16] Galen Reeves, Jiaming Xu, and Ilias Zadik. The all-or-nothing phenomenon in sparse linear\nregression. In Conference on Learning Theory, pages 2652–2663. PMLR, 2019.\n[17] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal\nStatistical Society Series B: Statistical Methodology, 58(1):267–288, 1996.\n[18] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery\nusing ℓ1-constrained quadratic programming (lasso). IEEE transactions on information theory,\n55(5):2183–2202, 2009.\n[19] Wei Wang, Martin J Wainwright, and Kannan Ramchandran. Information-theoretic limits on\nsparse signal recovery: dense versus sparse measurement matrices. IEEE Transactions on\nInformation Theory, 56(6):2967–2979, 2010.\n11\nA\nSparse Recovery using Sparse Measurements: Proofs\nA.1\nProof of Theorem 1\nProof of Theorem 1. For any i ∈[n], we denote by Xi := (Xij)j∈[p], Bi := (Bij)j∈[p], Ni :=\n(Nij)j∈[p]. We denote by S⋆:= Supp (β⋆) the support of β⋆. Let S := {S ⊂[p] : |S| = s}. We\ndefine the function:\nL : S −→[0, +∞)\nS 7−→∥Y −X1S∥2\n2 ,\nwhere 1S denotes the vector in {0, 1}p such that [1S]j = 1 (j ∈S) for all j ∈[p]. Note that, since\nX and Y are random, L(S) is a random variable for every S ∈S. In addition, note that:\nL(S) = ∥Z∥2\n2 + ∥X (1S⋆−1S)∥2\n2 + 2⟨Z, X (1S⋆−1S)⟩\n∀S ∈S,\nand, in particular:\nL(S⋆) = ∥Z∥2\n2 =\nn\nX\ni=1\nZ2\ni .\nFix S ∈S such that M := |S △S⋆| /2 ≥δs, and let U := S⋆\\ S, V := S \\ S⋆. Note that\n|U| = |V | = M. We define:\n∆:= L(S) −L(S⋆).\nProposition A.1. As n, p, s, d →+∞:\nP (∆≤0) ≤\n\u00122σ2p\nδds\n\u0013n/2\neo(n).\nProof. See appendix A.1.1.\nHence, we obtain:\nP\n\u0010\n∥Y −X1S∥2\n2 ≤∥Y −X1S⋆∥2\n2\n\u0011\n≤\n\u00122σ2p\nδds\n\u0013n/2\neo(n),\n(16)\nfor any S ∈{0, 1}p such that |S| = s and |S △S⋆| ≥2δs.\nUsing (16) and the union bound over the set of supports S s.t. |S △S⋆| ≥2δs, we obtain:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥PX,Z\n\u0010\n∥Y −X1S∥2\n2 > ∥Y −X1S⋆∥2\n2 , ∀S : |S △S⋆| ≥2δs\n\u0011\n= 1 −PX,Z\n\u0010\n∃S : |S △S⋆| ≥2δs, ∥Y −X1S∥2\n2 ≤∥Y −X1S⋆∥2\n2\n\u0011\nU.B.\n≥1 −\nX\nS : |S △S⋆|≥2δs\nPX,Z\n\u0010\n∥Y −X1S∥2\n2 ≤∥Y −X1S⋆∥2\n2\n\u0011\n≥1 −\n\u0012p\ns\n\u0013 \u00122σ2p\nδds\n\u0013n/2\neo(n).\nFirst regime: s = o (p).\nUsing the Corollary of Stirling:\nlog\n\u0012p\ns\n\u0013\n= s log (p/s) (1 + o (1)) ,\nin the RHS of the inequality above, we obtain:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0014\ns log (p/s) (1 + o (1)) −n\n2\n\u0012\nlog\n\u0012 δds\n2σ2p\n\u0013\n+ o (1)\n\u0013\u0015\n.\n12\nLet n⋆\nslin :=\n2s log(p/s)\nlog(ds/p)+log(δ/(2σ2)). Then if n ≥(1 + ε) n⋆\nslin for some constant ε > 0, we have:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0014\ns log (p/s) (1 + o (1)) −(1 + ε) n⋆\nslin\n2\n\u0012\nlog\n\u0012 δds\n2σ2p\n\u0013\n+ o (1)\n\u0013\u0015\n= 1 −exp\n\u0014\ns log (p/s)\n\u0012\n−ε + o (1) −\n1 + ε\nlog (ds/p) + log (δ/(2σ2)) o (1)\n\u0013\u0015\n= 1 −exp\n\u0010\ns log (p/s)\n\u0000−ε + o (1)\n\u0001\u0011\n.\nHence, as n, p, s, d →+∞:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εs log (p/s) + o\n\u0000s log (p/s)\n\u0001\u0011\n−→1.\nSecond regime: s = αp , α ∈(0, 1).\nUsing the Corollary of Stirling:\nlog\n\u0012p\ns\n\u0013\n= ph (α) (1 + o (1)) ,\nwe get:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0014\nph (α) (1 + o (1)) −n\n2\n\u0012\nlog\n\u0012 δds\n2σ2p\n\u0013\n+ o (1)\n\u0013\u0015\n.\nSimilarly to above, we take n⋆\nlin :=\n2H(α)p\nlog d+log(δα/(2σ2)). If n ≥(1 + ε) n⋆\nlin for some constant ε > 0,\nthen we obtain, as p, s, d →+∞:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εh (α) p + o (p)\n\u0011\n−→1,\nconcluding the proof.\nA.1.1\nProof of Proposition A.1\nProof of Proposition A.1. We have:\n∆:= L(S) −L(S⋆)\n= ∥X (1S⋆−1S)∥2\n2 + 2⟨Z, X (1S⋆−1S)⟩\n=\nn\nX\ni=1\n⟨Xi, 1S⋆−1S⟩2 + 2\nn\nX\ni=1\nZi⟨Xi, 1S⋆−1S⟩.\nWe denote by (∆i)i∈[n] the terms of the sum in the above expression, that is:\n∆i := ⟨Xi, 1S⋆−1S⟩2 + 2Zi⟨Xi, 1S⋆−1S⟩.\nNote that (∆i)i∈[n] are i.i.d. and ∆= Pn\ni=1 ∆i.\nNow using the Chernoff bound:\nP (∆≤0) = P (−∆≥0) = inf\nθ≥0 P\n\u0000e−θ∆≥1\n\u0001\n≤inf\nθ≥0 M−∆i(θ)n.\n(17)\nWe now study the moment generating function of −∆i, i.e. M−∆i(·). We have:\nM−∆i(θ) = EXi,Zi\nh\ne−θ[⟨Xi,1S⋆−1S⟩2+2Zi⟨Xi,1S⋆−1S⟩]i\n= EXi\nh\ne−θ⟨Xi,1S⋆−1S⟩2EZi\nh\ne−2θZi⟨Xi,1S⋆−1S⟩\f\fXi\nii\n= EXi\nh\ne−θ⟨Xi,1S⋆−1S⟩2MZi|Xi (−2θ⟨Xi, 1S⋆−1S⟩)\ni\n= EXi\nh\ne−θ⟨Xi,1S⋆−1S⟩2e\n1\n2 (−2θ⟨Xi,1S⋆−1S⟩)2σ2i\n= EXi\nh\ne(−θ+2θ2σ2)⟨Xi,1S⋆−1S⟩2i\n= EXi\nh\ne(−θ+2θ2σ2)(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\n.\n13\nPlugging this expression into (17), we obtain:\nlog P (∆≤0) ≤n inf\nθ≥0 log EXi\nh\ne(−θ+2θ2σ2)(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\n.\nStudying the function θ 7→−θ + 2θ2σ2 on R≥0 leads to the change of variable:\ninf\nθ≥0 log EXi\nh\ne(−θ+2θ2σ2)(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\n(18)\n=\ninf\nθ∈(−∞,1/(8σ2)] log EXi\nh\ne−θ(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\n,\n(19)\nOne can check that the function θ 7→log EXi\nh\ne−θ(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\nis non-increasing over\n(−∞, 1/\n\u00008σ2\u0001\n]. Hence (19) is equal to:\nlog EXi\nh\ne−(\nP\nj∈U Xij−P\nj∈V Xij)\n2/(8σ2)i\n.\nTherefore, the Chernoff bound yields:\nlog P (∆≤0) ≤n log EXi\nh\ne−(\nP\nj∈U Xij−P\nj∈V Xij)\n2/(8σ2)i\n.\n(20)\nSince U ∩V = ∅, we have:\nX\nj∈U\nXij −\nX\nj∈V\nXij\nd=\nX\nj∈U∪V\nXij.\nTherefore:\nE\nh\ne−(\nP\nj∈U Xij−P\nj∈V Xij)\n2/(8σ2)i\n= E\nh\ne−(\nP\nj∈U∪V Xij)\n2/(8σ2)i\n= E\nh\ne−(\nP\nj∈U∪V BijNij)\n2/(8σ2)i\n= EBi\nh\nENi\nh\ne−(\nP\nj∈U∪V BijNij)\n2/(8σ2) \f\f Bi\nii\n= EBi\n\nENi\n\ne\n−\n P\nj∈U∪V Bij Nij\n√P\nj∈U∪V Bij\n!2\n×\nP\nj∈U∪V Bij\n8σ2\n\f\f\f\f\f Bi\n\n\n\n.\nIn addition, conditionally on Bi, we have:\nΓ :=\n\n\nP\nj∈U∪V BijNij\nqP\nj∈U∪V Bij\n\n\n2\nd= χ2(1).\nIts MGF is:\nE\n\u0002\netΓ | Bi\n\u0003\n= MΓ|Bi (t) =\n1\n√1 −2t, for t < 1/2.\nHence:\nEBi\n\nENi\n\ne\n−\n P\nj∈U∪V Bij Nij\n√P\nj∈U∪V Bij\n!2\n×\nP\nj∈U∪V Bij\n8σ2\n\f\f\f\f\f Bi\n\n\n\n= EBi\n\u0014\nMΓ|Bi\n\u0012\n−\nP\nj∈U∪V Bij\n8σ2\n\u0013\u0015\n= EBi\n\n\n2σ\nq\n4σ2 + P\nj∈U∪V Bij\n\n.\nLet U[δs] and V[δs] respectively denote the sets of δs smallest elements of U and V . Note that this\ndefinition is legitimate since |U| = |V | = M ≥δs. Since Bij ≥0 for all j ∈U ∪V , we have:\nX\nj∈U∪V\nBij ≥\nX\nj∈U[δs]∪V[δs]\nBij,\n14\nand hence:\nEBi\n\n\n2σ\nq\n4σ2 + P\nj∈U∪V Bij\n\n≤EBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n.\nTherefore, we get:\nE\nh\ne−(\nP\nj∈U Xij−P\nj∈V Xij)\n2/(8σ2)i\n≤EBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n,\nand plugging this into (20) yields:\nlog P (∆≤0) ≤n log\n\nEBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n\n\n.\n(21)\nNow note that, for any i ∈[n]:\nX\nj∈U[δs]∪V[δs]\nBij\nd= Bin\n\u0012\n2δs, d\np\n\u0013\n,\nIn addition, since d = o (p) and ds/p →+∞, we have:\nLemma A.1. For any i ∈[n], the following holds: as p →+∞,\n1\np\n2δds/p\n\n\nX\nj∈U[δs]∪V[δs]\nBij −2δds/p\n\n\ndist\n−→N (0, 1) .\nProof. The proof is a simple adaptation of the proof of the Central Limit Theorem. See appendix\nA.1.2.\nHence, there exists a choice of the underlying probability space and random variables (Bij)i∈[n],j∈[p]\nfor which the random variable above converges almost surely and:\nN :=\nlim\np→+∞\n1\np\n2δds/p\n\n\nX\nj∈U[δs]∪V[δs]\nBij −2δds/p\n\n∼N (0, 1) .\nThe above yields:\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n=\n2σ\nr\n4σ2 + 2δds/p +\np\n2δds/p N + o\n\u0010p\nds/p\n\u0011.\nLet:\nVp :=\n2σ\np\nds/p\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n.\nThen we have:\nVp =\n2σ\np\nds/p\nr\n4σ2 + 2δds/p +\np\n2δds/p N + o\n\u0010p\nds/p\n\u0011\n=\n2σ\nr\n2δ + 4σ2\nds/p +\nq\n2δ\nds/pN + o\n\u0010\n1\n\u000ep\nds/p\n\u0011.\nSince ds/p →+∞as p →+∞, the above yields:\nVp −→2σ\n√\n2δ\n=\nr\n2σ2\nδ .\nIn addition, we note the following:\n15\nLemma A.2. Vp is uniformly integrable, that is: there exists p′ ∈N such that\nlim\nT →+∞sup\np≥p′ E\n\u0002\n|Vp| 1{|Vp|>T }\n\u0003\n= 0.\nProof. See appendix A.1.3.\nTherefore, we get:\nlim\np→+∞E [Vp] =\nr\n2σ2\nδ .\nHence, we write:\nEBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n=\nr\n2σ2p\nδds + o\n \u0012ds\np\n\u0013−1/2!\n.\nWe conclude:\nlog P (∆≤0) ≤n log\n\nEBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n\n\n\n= n log\n r\n2σ2p\nδds + o\n \u0012ds\np\n\u0013−1/2!!\n= n\n \nlog\n r\n2σ2p\nδds\n!\n+ log\n\u0010\n1 + o (1)\n\u0011!\n= n\n \nlog\n r\n2σ2p\nδds\n!\n+ o (1)\n!\n= n log\n r\n2σ2p\nδds\n!\n+ o(n),\nwhich yields the desired result:\nP (∆≤0) ≤\n\u00122σ2p\nδds\n\u0013n/2\neo(n).\nA.1.2\nProof of Lemma A.1\nProof of Lemma A.1. Let:\nNp :=\n1\np\n2δds/p\n\n\nX\nj∈U[δs]∪V[δs]\nBij −2δds/p\n\n.\n16\nIts characteristic function writes:\nΦNp (t)\n= E\n\u0002\neitNp\u0003\n= e−it√\n2δds/p \u0010\nE\nh\nexp\n\u0010\nitBij/\np\n2δds/p\n\u0011i\u00112δs\n= e−it√\n2δds/p \u0010\n1 −d/p + d/p exp\n\u0010\nit/\np\n2δds/p\n\u0011\u00112δs\n= e−it√\n2δds/p \u0010\n1 + d/p\n\u0010\nexp\n\u0010\nit/\np\n2δds/p\n\u0011\n−1\n\u0011\u00112δs\n= e−it√\n2δds/p\n \n1 + d/p\n \nit\np\n2δds/p\n−\nt2\n4δds/p + O\n \n−it3\n(2δds/p)3/2\n!!!2δs\n= e−it√\n2δds/p\n \n1 + d/p\n \nit\np\n2δds/p\n−\nt2\n4δds/p + O\n \n1\n(2δds/p)3/2\n!!!2δs\n= exp\n \n−it\np\n2δds/p + 2δs log\n \n1 + d/p\n \nit\np\n2δds/p\n−\nt2\n4δds/p + O\n \n1\n(2δds/p)3/2\n!!!!\n= e−it√\n2δds/p exp\n \n2δs\n \nd/p\n \nit\np\n2δds/p\n−\nt2\n4δds/p + O\n \n1\n(2δds/p)3/2\n!!\n+ O\n\u0012 d\nsp\n\u0013!!\n= exp\n \n−it\np\n2δds/p + it\np\n2δds/p −t2/2 + O\n \n1\np\n2δds/p\n!\n+ O\n\u00122δd\np\n\u0013!\n= exp\n\u0000−t2/2 + o (1)\n\u0001 p→+∞\n−→exp\n\u0000−t2/2\n\u0001\n= ΦN(0,1) (t) ,\nfor all t ∈R. The result follows.\nA.1.3\nProof of Lemma A.2\nProof of Lemma A.2. Fix T > 2. We have:\nVp =\n2σ\np\nds/p\nq\n4σ2 + P\ni∈U[δs]∪V[δs] Bij\n=\n2σ\ns\n2δ + 4σ2\nds/p +\nq\n2δ\nds/pN + o\n\u0012\n1\n√\nds/p\n\u0013.\nBy total probability, we have:\nE [|Vp| 1 {|Vp| > T}] = E\nh\n|Vp| 1 {|Vp| > T}\n\f\f N ≥−(ds/p)1/4i\nP\n\u0010\nN ≥−(ds/p)1/4\u0011\n+ E\nh\n|Vp| 1 {|Vp| > T}\n\f\f N < −(ds/p)1/4i\nP\n\u0010\nN < −(ds/p)1/4\u0011\n.\nLet\nΞ1 := E\nh\n|Vp| 1 {|Vp| > T}\n\f\f N ≥−(ds/p)1/4i\nP\n\u0010\nN ≥−(ds/p)1/4\u0011\n,\nand\nΞ2 := E\nh\n|Vp| 1 {|Vp| > T}\n\f\f N < −(ds/p)1/4i\nP\n\u0010\nN < −(ds/p)1/4\u0011\n,\nso that E [|Vp| 1 {|Vp| > T}] = Ξ1 + Ξ2. We address the two terms separately.\n• First case: N ≥−(ds/p)1/4. We have:\n4σ2\nds/p +\ns\n2δ\nds/pN + o\n \n1\np\nds/p\n!\n≥4σ2\nds/p −\n√\n2δ\n\u0012ds\np\n\u0013−1/4\n+ o\n \n1\np\nds/p\n!\n.\n17\nThe RHS above goes to 0 as p →+∞, and therefore it is ≥−δ for p large enough, say for all\np ≥p1 for some p1 ∈N. Hence we have:\n|Vp| = Vp =\n2σ\ns\n2δ + 4σ2\nds/p +\nq\n2δ\nds/pN + o\n\u0012\n1\n√\nds/p\n\u0013 ≤2σ\n√\nδ\n,\nfor all p ≥p1. In addition, this implies:\n1 {|Vp| > T} ≤1\n\u001a\u0012 2σ\n√\nδ\n\u0013\n> T\n\u001b\n.\nIn addition, we have:\nP\n\u0010\nN ≥−(ds/p)1/4\u0011\n≤1,\nwhich all together yield:\nΞ1 ≤2σ\n√\nδ\n1\n\u001a\u0012 2σ\n√\nδ\n\u0013\n> T\n\u001b\n.\n• Second case: N < −(ds/p)1/4. We have:\n|Vp| = Vp =\n2σ\np\nds/p\nq\n4σ2 + P\ni∈U[δs]∪V[δs] Bij\n≤\np\nds/p,\nwhich also implies:\n1 {|Vp| > T} ≤1\nnp\nds/p > T\no\n= 1\n\u001a\nT 2 < ds\np\n\u001b\n.\nIn addition, by Gaussian tail bounds we have:\nP\n\u0010\nN < −(ds/p)1/4\u0011\n≤e−\n√\nds/p\n2\n,\nfor p large enough, say for all p ≥p2 for some p2 ∈N. Therefore:\nΞ2 ≤\np\nds/p e−\n√\nds/p\n2\n1\n\u001a\nT 2 < ds\np\n\u001b\n.\nNote that the term\np\nds/p e−\n√\nds/p\n2\nis decreasing as ds/p increases (we have ds/p > T 2 > 2).\nTherefore:\nΞ2 ≤T e−T/2 1\n\u001a\nT 2 < ds\np\n\u001b\n≤T e−T/2.\nWe hence conclude that:\nE [|Vp| 1 {|Vp| > T}] = Ξ1 + Ξ2 ≤2σ\n√\nδ\n1\n\u001a\u0012 2σ\n√\nδ\n\u0013\n> T\n\u001b\n+ T e−T/2,\nfor all p ≥p′ := p1 ∨p2. Therefore we have:\nsup\np≥p′ E\n\u0014\n|Vp| 2σ\n√\nδ\n1 {|Vp| > T}\n\u0015\n≤2σ\n√\nδ\n1\n\u001a\u0012 2σ\n√\nδ\n\u0013\n> T\n\u001b\n+ T e−T/2.\nThis holds for any T > 2. Taking T to +∞, the result follows:\nlim\nT →+∞sup\np≥p′ E [|Vp| 1 {|Vp| > T}] = 0.\n18\nA.2\nProof of Corollary 2\nProof of Corollary 2. This proof relies on bringing together Theorem 1 with the following result\nfrom Wang et al. [19].\nTheorem 4 (Necessary condition for sparse ensembles, Corollary 2 of [19]). Let the measurement\nmatrix X ∈Rn×p be drawn with i.i.d. elements from the following distribution:\nXij =\n(\nN\n\u0010\n0, 1\nγ\n\u0011\n,\nw.p. γ\n0,\nw.p. 1 −γ\n,\nfor all i ∈[n], j ∈[p];\n(22)\nwhere γ ∈(0, 1]. Let λ > 0 and\nCp,s (λ) :=\n\u001a\nβ ∈Rp \f\f |Supp (β)| = s,\nmin\ni∈Supp(β) |βi| = λ\n\u001b\n.\nAssume that σ2 = 1. Then, in the regime where γs →+∞, a necessary condition for asymptotically\nreliable recovery over the signal class Cp,s (λ) is given by:\nn >\nlog\n\u0000p\ns\n\u0001\n−1\n1\n2 log (1 + sλ2).\nNote that, while Theorem 4 is not stated on the exact same signal space Cp,s (λ) in [19] but rather on\nthe larger:\n\u001a\nβ ∈Rp \f\f |Supp (β)| = s,\nmin\ni∈Supp(β) |βi| ≥λ\n\u001b\n,\nit follows directly from their result on “restricted ensembles” where the signal components under\nconsideration are set exactly to λ (see section III.A. in [19]). We now proceed to prove Corollary 2.\n(i) We show that (i) holds using Theorem 4, but this requires adapting our problem to the framework\nused by Wang et al. in [19]. In fact, note that the model used in Theorem 4 is different from the one\nwe use in this paper, that we defined in (5). In their model, Wang et al. [19] rescale the non-zero\ncomponents of X by multiplying them by 1/√γ, and require that the noise variance is σ2 = 1.\nTherefore, we cannot directly use Theorem 4 in our setting. However, this difference can be fixed\nby a simple rescaling of our model. Note that our model defined by (5), where X follows the sparse\nGaussian distribution defined in Definition 2.1 and Z ∼N\n\u00000, σ2In\n\u0001\n, can be equivalently written\nas:\nY0 = X0β⋆\n0 + Z0,\nwhere:\nY0 := 1\nσ Y,\nX0 :=\n1\np\nd/p\nX,\nβ⋆\n0 :=\np\nd/p\nσ\nβ⋆,\nand\nZ0 := 1\nσ Z ∼N (0, In) .\nHence, it is a particular case of the model defined in (22), with:\nγ := d/p\nand\nλ :=\n√γ\nσ .\nIn addition, the regime we consider of d = ω (p/s) corresponds exactly to the regime considered\nin Theorem 4 where γs →+∞. Therefore, using Theorem 4, a necessary condition for an\nasymptotically reliable recovery of β⋆in the considered regime is given by:\nn >\nlog\n\u0000p\ns\n\u0001\n−1\n1\n2 log (1 + sλ2) =\n2 log\n\u0000p\ns\n\u0001\n−2\nlog (1 + ds/ (pσ2)) = 2 log\n\u0000p\ns\n\u0001\nlog (ds/p) (1 + o (1)) .\n(23)\n19\nFirst regime: s = o (p).\nUsing the Corollary of Stirling:\nlog\n\u0012p\ns\n\u0013\n= s log (p/s) (1 + o (1)) ,\nthe necessary condition (23) writes:\nn > 2s log (p/s)\nlog (ds/p) (1 + o (1)) .\nLet:\nnSP\nINF := 2s log (p/s)\nlog (ds/p) .\nAssume there exists ε > 0 such that n ≤(1 −ε) nSP\nINF. We know that, for large enough n, p, s, d we\nhave:\nn ≤(1 −ε) nSP\nINF < 2s log (p/s)\nlog (ds/p) (1 + o (1)) ,\nwhich contradicts the necessary condition. Therefore, it is information-theoretically impossible to\nensure a reliable recovery of the support of β⋆.\nSecond regime: s = αp.\nUsing the Corollary of Stirling:\nlog\n\u0012p\ns\n\u0013\n= ph (α) (1 + o (1)) ,\nthe necessary condition (23) writes:\nn > 2h (α) p\nlog d\n(1 + o (1)) .\nLet:\nnSP\nINF := 2h (α) p\nlog d .\nSimilarly to above, we conclude that if there exists ε > 0 such that n ≤(1 −ε) nSP\nINF then it is\ninformation-theoretically impossible to ensure a reliable recovery of the support of β⋆.\n(ii) We show that (ii) holds using Theorem 1.\nFirst regime: s = o (p).\nLet δ > 0 and:\nn⋆\nslin :=\n2s log (p/s)\nlog (ds/p) + log (δ/(2σ2)).\nNote that:\nnSP\nINF = 2s log (p/s)\nlog (ds/p) = n⋆\nslin (1 + o (1)) .\nAssume there exists ε such that n ≥(1 + ε) nSP\nINF. Then:\nn ≥(1 + ε) (1 + o (1)) n⋆\nslin = (1 + ε + o (1)) n⋆\nslin ≥\n\u00001 + ε/2\n\u0001\nn⋆\nslin,\nfor n, p, s, d large enough. Using Theorem 1, we have:\nPX,Z\n\u0012 1\n2s\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f < δ\n\u0013\n≥1 −exp\n\u0010\n−εs log (p/s) + o\n\u0000s log (p/s)\n\u0001\u0011\n.\nTherefore, we obtain:\nPX,Z\n\u0012 1\n2s\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f < δ\n\u0013\n−→1,\nas n, p, s, d →+∞. Since this holds for all δ > 0, we conclude:\nSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n2s\n−→0,\nin probability, as n, p, s, d →+∞.\n20\nSecond regime: s = αp.\nLet δ > 0 and:\nn⋆\nlin :=\n2h (α) p\nlog d + log (δα/(2σ2)).\nNote that:\nnSP\nINF = 2h (α) p\nlog d\n= n⋆\nlin (1 + o (1)) .\nAssume there exists ε such that n ≥(1 + ε) nSP\nINF. Then:\nn ≤(1 + ε) (1 + o (1)) n⋆\nlin = (1 + ε + o (1)) n⋆\nlin ≥\n\u00001 + ε/2\n\u0001\nn⋆\nlin,\nfor n, p, s, d large enough. Using Theorem 1, we have:\nPX,Z\n\u0012 1\n2s\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f < δ\n\u0013\n≥1 −exp\n\u0010\n−εh (α) p + o (p)\n\u0011\n.\nTherefore, we obtain:\nPX,Z\n\u0012 1\n2s\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f < δ\n\u0013\n−→1,\nas n, p, s, d →+∞. Since this holds for all δ > 0, we conclude:\nSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n2s\n−→0,\nin probability, as n, p, s, d →+∞.\nA.3\nProof of Example 2.2\nProof of Example 2.2. We have d = min\n\u0000po(1), ψ−1 (o (ψ (p)))\n\u0001\n, hence:\n\u001alog d/ log p = o (1)\nψ (d) /ψ (p) = o (1)\n.\nIn addition:\nn1 = nINF = Θ\n\u0000p/ log p\n\u0001\n,\nn2 = nSP\nINF = Θ\n\u0000p/ log d\n\u0001\n.\nTherefore, we have:\nn2\nn1\n= Θ (p/ log d)\nΘ (p/ log p) = Θ\n\u0012log p\nlog d\n\u0013\n= ω (1) ,\nOn one hand, the number of samples required for reliable recovery is better in the dense case:\nn1 = Θ\n\u0000p/ log p\n\u0001\n= o\n\u0000p/ log d\n\u0001\n= o (n2) .\n(24)\nOn the other hand, the computational cost of recovering the support is better in the sparse case. In\nfact, matrix-vector multiplications are made easier by sparsity: in the dense case, multiplying X1\nwith a vector in Rp costs:\nn1p = Θ\n\u0000p2/ log p\n\u0001\nreal number multiplications, while multiplying X2 with a vector in Rp costs\nn2d = Θ\n\u0000pd/ log d\n\u0001\n= pΘ\n\u0000ψ (d)\n\u0001\n= po\n\u0000ψ (p)\n\u0001\n= o\n\u0000p2/ log p\n\u0001\n= o (n1p) ,\n(25)\nreal number multiplications.\nThis highlights the trade-off between sampling complexity and\ncomputational cost.\n21\nB\nImproving Sparse Recovery via Sparsification: Proof of Theorem 3\nProof of Theorem 3. Let S := {S ∈[p]: |S| = s}. We define the error function:\nL : S −→[0, +∞)\nS 7−→\n\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2 .\nNote that:\nL(S) =\n\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2\n=\n\r\r\r\r\nd\npY −˜X1S\n\r\r\r\r\n2\n2\n=\n\r\r\r\r\nd\np (X1S⋆+ Z) −˜X1S\n\r\r\r\r\n2\n2\n=\n\r\r\r\r\nd\npX1S⋆−˜X1S + d\npZ\n\r\r\r\r\n2\n2\n.\nIn particular, we have:\nL(S⋆) =\n\r\r\r\r\n\u0012d\npX −˜X\n\u0013\n1S⋆+ d\npZ\n\r\r\r\r\n2\n2\n.\nLet S ∈S. We define A (S) := S⋆\\ S, B (S) := S \\ S⋆, C (S) := S⋆∩S and M (S) := |A| =\n|B| = |S⋆△S| /2. Note that |C| = s −M. We are interested in bounding the following probability:\nP (L (S) −L (S⋆) ≤0) .\nNote that the term above only depends on S through M. In fact, for any S, S′ ∈S such that\n|S △S⋆| = |S′ △S⋆| we have L (S)\nd= L (S′). In light of this, we define a family of random\nvariables (∆(η))ηs∈[s] indexed by the rescaled symmetric difference η = M/s as L (S) −L (S⋆)\nfor some S such that |S △S⋆| = 2M, that is:\nFor any η ∈\n\u001a i\ns : i ∈[s]\n\u001b\n, ∆(η) := L (S) −L (S⋆) , for some S s.t. |S △S⋆| = 2ηs.\nWe extend the notation above to η ∈[0, 1] by defining ∆(η) := ∆\n\u0010\n⌈ηs⌉\ns\n\u0011\n.\nProposition B.1. Let η ∈(0, 1] and S ∈S such that S △S⋆= ⌈ηs⌉. For any θ ∈(0, +∞), define:\nγ (θ) := 2d2θσ2/p2.\nLet B ∈{0, 1}p denote a random vector such that Bj\ni.i.d.\n∼Ber (d/p). For any θ ∈(0, +∞), we define\nthe following random variables:\nσ2\nU (θ, B) := θ2\nX\nj∈A(S)∪B(S)\nBj,\nσ2\nV (θ, B) := 4d2s\np2\n+\n \n\u00001 + γ (θ)\n\u00012 −4d\n\u00001 + γ (θ)\n\u0001\np\n! X\nj∈A(S)\nBj\n+\n\u00001 −γ (θ)\n\u00012\nX\nj∈B(S)\nBij + 4\n\u0012\n1 −2d\np\n\u0013 X\nj∈C(S)\nBj,\nCov(U,V ) (θ, B) := θ\n\n\n\u0012\n1 + γ (θ) −2d\np\n\u0013 X\nj∈A(S)\nBj −\n\u00001 −γ (θ)\n\u0001 X\nj∈B(S)\nBj\n\n,\nρ (θ, B) :=\nCov(U,V ) (θ, B)\nσU (θ, B) σV (θ, B).\n22\nIn addition, for any θ ∈(0, +∞) we define the random variable:\nf (θ, B) :=\n\n\n\n1\nσU(θ,B)σV (θ,B)\nr\u0010\n1\nσU (θ,B)σV (θ,B) −ρ(θ,B)\n\u00112−1\nif\n1\nσU(θ,B)σV (θ,B) −ρ (θ, B) > 1,\n+∞\nelse.\n(26)\nThen we have:\nP (∆(η) ≤0) ≤\n\u0010\ninf\nθ>0 EB\n\u0002\nf (θ, B)\n\u0003\u0011n\nProof. See section B.1.\nNow recall that s and d are both linear in p. In particular, there exist α, ψ ∈(0, 1) such that:\n\u001as = αp\nd = ψp\n.\nWe have:\nγ (θ) = 2d2θσ2/p2 = 2σ2ψ2θ.\n(27)\nProposition B.2. We define:\nC⋆(η) :=\nψ\n(1 −ψ) (2 −η (1 −ψ)) > 0.\nDefine the function:\nξ : N −→(0, +∞)\np 7−→C⋆(η)\n2αψp .\nConsider the random vector B ∈{0, 1}p : Bj\ni.i.d.\n∼Ber (ψ). We define the random variable:\nHp (B) = f (ξ (p) , B) .\nThen the following hold:\n(i) Hp (B)\na.s.\n−→\nq\n1\n1+ηψC⋆(η) as p →+∞.\n(ii) P (∆(η) ≤0) ≤\n\u0010\nEB [Hp (B)]\n\u0011n\n.\nProof. See section B.2.\nConjecture B.1. The exists p0 such that (Hp (B))p≥p0 is uniformly integrable, that is:\nlim\nT →+∞sup\np≥p0\nEB\nh\nHp (B) 1\n\u0000Hp (B) > T\n\u0001i\n= 0.\nHence we have Hp (B)\na.s.\n−→\nq\n1\n1+ηψC⋆(η), which by Conjecture B.1 yields:\nlim\np→+∞EB [Hp (B)] =\ns\n1\n1 + ηψC⋆(η).\nTherefore, we obtain:\nEB [Hp (B)] =\ns\n1\n1 + ηψC⋆(η) + o (1) .\n23\nThus, by Proposition B.2 we have:\n1\nn log\n\u0010\nP (∆(η) ≤0)\n\u0011\n≤log\n\u0010\nEB [Hp (B)]\n\u0011\n= log\n s\n1\n1 + ηψC⋆(η) + o (1)\n!\n= −1\n2 log\n\u0010\n1 + ηψC⋆(η)\n\u0011\n+ o (1) .\nHence:\nP (∆(η) ≤0) ≤\n\u0010\n1 + ηψC⋆(η)\n\u0011−n/2\neo(n).\nTaking the union bound:\nP\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥P\n\u0012\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2 >\n\r\r\r ˜Y −˜X1S⋆\n\r\r\r\n2\n2 , ∀S : |S △S⋆| ≥2δs\n\u0013\n= 1 −P\n\u0012\n∃S : |S △S⋆| ≥2δs,\n\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2 ≤\n\r\r\r ˜Y −˜X1S⋆\n\r\r\r\n2\n2\n\u0013\n= 1 −\nX\nS : |S △S⋆|≥2δs\nP\n\u0012\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2 ≤\n\r\r\r ˜Y −˜X1S⋆\n\r\r\r\n2\n2\n\u0013\n= 1 −\nX\nη∈{i/s: i∈[s]}: η≥δ\nX\nS : |S △S⋆|=2ηs\nP (L (S) ≤L (S⋆))\n= 1 −\nX\nη∈{i/s: i∈[s]}: η≥δ\nX\nS : |S △S⋆|=2ηs\nP (∆(η) ≤0)\n≥1 −\n\u0012p\ns\n\u0013\nX\nη∈{i/s: i∈[s]}: η≥δ\n\u0010\n1 + ηψC⋆(η)\n\u0011−n/2\neo(n).\nIn addition, we have:\nX\nη∈{i/s: i∈[s]}: η≥δ\n(1 + ηψC⋆(η))−n/2 ≤\nX\nη∈{i/s: i∈[s]}: η≥δ\n(1 + δψC⋆(δ))−n/2\n(28)\n≤s (1 + δψC⋆(δ))−n/2 .\n(29)\nwhere (28) holds because the term inside the sum is non-increasing in η, and (29) holds because the\ncardinality of the index set of the sum is upper bounded by s. Therefore:\nP\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −\n\u0012p\ns\n\u0013\ns\n\u0010\n1 + δψC⋆(δ)\n\u0011−n/2\neo(n)\n= 1 −exp\n\u0014\nph (α)\n\u00001 + o (1)\n\u0001\n+ log s −n\n\u00121\n2 log\n\u0010\n1 + δψC⋆(δ)\n\u0011\n+ o (1)\n\u0013\u0015\n= 1 −exp\n\u0014\nph (α)\n\u00001 + o (1)\n\u0001\n−n\n\u00121\n2 log\n\u0010\n1 + δψC⋆(δ)\n\u0011\n+ o (1)\n\u0013\u0015\n.\nWe define:\nn⋆:=\n2h (α) p\nlog\n\u0010\n1 + δψC⋆(δ)\n\u0011.\n24\nLet ε > 0. Then if n ≥(1 + ε) n⋆, we have:\nP\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0014\nph (α)\n\u00001 + o (1)\n\u0001\n−n\n\u00121\n2 log\n\u0010\n1 + δψC⋆(δ)\n\u0011\n+ o (1)\n\u0013\u0015\n= 1 −exp\nh\nph (α)\n\u0000−ε + o (1)\n\u0001i\np→+∞\n−→1.\nB.1\nProof of Proposition B.1\nProof of Proposition B.1. Since η and S are fixed, we will simplify notations in this proof as follows:\nwe will write ∆for ∆(η), and A, B, C, M respectively for A (S) , B (S) , C (S) , M (S). We have:\n∆= L(S) −L(S⋆)\n=\n\r\r\r\r\nd\npX1S⋆−˜X1S + d\npZ\n\r\r\r\r\n2\n2\n−\n\r\r\r\r\n\u0012d\npX −˜X\n\u0013\n1S⋆+ d\npZ\n\r\r\r\r\n2\n2\n=\n\r\r\r ˜X1S\n\r\r\r\n2\n2 −\n\r\r\r ˜X1S⋆\n\r\r\r\n2\n2 + 2 d\np ⟨X1S⋆+ Z, ˜X (1S⋆−1S)⟩\n=\nn\nX\ni=1\n⟨˜Xi, 1S⟩2 −\nn\nX\ni=1\n⟨˜Xi, 1S⋆⟩2 + 2 d\np\nn\nX\ni=1\n(⟨Xi, 1S⋆⟩+ Zi)\n\u0010\n⟨˜Xi, 1S⋆⟩−⟨˜Xi, 1S⟩\n\u0011\n=\nn\nX\ni=1\n\u0012\n⟨˜Xi, 1S⟩2 −⟨˜Xi, 1S⋆⟩2 + 2 d\np (⟨Xi, 1S⋆⟩+ Zi)\n\u0010\n⟨˜Xi, 1S⋆⟩−⟨˜Xi, 1S⟩\n\u0011\u0013\nLet:\n∆i := ⟨˜Xi, 1S⟩2 −⟨˜Xi, 1S⋆⟩2 + 2 d\np (⟨Xi, 1S⋆⟩+ Zi)\n\u0010\n⟨˜Xi, 1S⋆⟩−⟨˜Xi, 1S⟩\n\u0011\n.\nSo that ∆= Pn\ni=1 ∆i. Now using the Chernoff bound:\nP (∆≤0) = P (−∆≥0) = inf\nθ≥0 P\n\u0000e−θ∆≥1\n\u0001\n≤inf\nθ≥0 M−∆i(θ)n.\n(30)\nWe have:\n∆i = ⟨˜Xi, 1S⟩2 −⟨˜Xi, 1S⋆⟩2 + 2 d\np (⟨Xi, 1S⋆⟩+ Zi)\n\u0010\n⟨˜Xi, 1S⋆⟩−⟨˜Xi, 1S⟩\n\u0011\n=\n\nX\nj∈S\n˜Xij\n\n\n2\n−\n\nX\nj∈S⋆\n˜Xij\n\n\n2\n+ 2 d\np\n\nX\nj∈S⋆\nXij + Zi\n\n\n\nX\nj∈S⋆\n˜Xij −\nX\nj∈S\n˜Xij\n\n\n= −\n\nX\nj∈S⋆\n˜Xij −\nX\nj∈S\n˜Xij\n\n\n\nX\nj∈S⋆\n˜Xij +\nX\nj∈S\n˜Xij −2 d\np\nX\nj∈S⋆\nXij\n\n\n+ 2 d\np Zi\n\nX\nj∈S⋆\n˜Xij −\nX\nj∈S\n˜Xij\n\n\nThen:\n∆i = −\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\nX\nj∈A\n˜Xij +\nX\nj∈B\n˜Xij + 2\nX\nj∈C\n˜Xij −2 d\np\nX\nj∈S⋆\nXij\n\n\n+ 2 d\np Zi\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n.\n25\nLet:\nKi := −\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\nX\nj∈A(S)\n˜Xij +\nX\nj∈B(S)\n˜Xij + 2\nX\nj∈C(S)\n˜Xij −2 d\np\nX\nj∈S⋆\nXij,\n\n\nso that:\n∆i = Ki + 2 d\np Zi\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n.\nThe MGF of −∆i can be expressed as:\nM−∆i (θ) = E [exp (−∆iθ)]\n= EXi,Bi\n\nEZi\n\nexp\n\n−θ\n\nKi + 2 d\np Zi\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\n\n\n\n\f\f\f\fXi, Bi\n\n\n\n\n= EXi,Bi\n\ne−θKiEZi\n\nexp\n\n−2dθ\np\nZi\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\n\n\f\f\f\fXi, Bi\n\n\n\n\n= EXi,Bi\n\ne−θKiMZi | Xi,Bi\n\n−2dθ\np\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\n\n\n\n= EXi,Bi\n\ne−θKi exp\n\n\n1\n2\n\n−2dθ\np\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\n\n2\nσ2\n\n\n\n\n\n= EXi,Bi\n\nexp\n\n\n−θKi + 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n\n\n\n\nThus, by the tower rule:\nM−∆i (θ) = EBi\n\nEXi\n\nexp\n\n\n−θKi + 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n\n\n\f\f\f\f\fBi\n\n\n\n.\n(31)\nFix θ > 0. Then:\n−θKi + 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n= θ\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\nX\nj∈A\n˜Xij +\nX\nj∈B\n˜Xij + 2\nX\nj∈C\n˜Xij −2 d\np\nX\nj∈S⋆\nXij\n\n\n+ 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n= θ\n\nX\nj∈A\nBijXij −\nX\nj∈B\nBijXij\n\n×\n\nX\nj∈A\n\u0012\u00001 + γ (θ)\n\u0001\nBij −2d\np\n\u0013\nXij +\nX\nj∈B\n(1 −γ (θ)) BijXij + 2\nX\nj∈C\n\u0012\nBij −d\np\n\u0013\nXij\n\n,\n26\nwhere γ (θ) = 2d2θσ2/p2. Since θ is fixed for now, we simplify the notation γ (θ) by simply writing\nγ. Let:\n\n\n\nU := θ\n\u0010P\nj∈A BijXij −P\nj∈B BijXij\n\u0011\nV := P\nj∈A\n\u0010\n(1 + γ) Bij −2d\np\n\u0011\nXij + P\nj∈B (1 −γ) BijXij + 2 P\nj∈C\n\u0010\nBij −d\np\n\u0011\nXij\n,\nso that:\n−θKi + 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n= UV.\n(32)\nPlugging (32) in (31), we obtain:\nM−∆i (θ) = EBi\n\u0002\nEXi\n\u0002\neUV | Bi\n\u0003\u0003\n.\n(33)\nNote that, conditionally on Bi, we have:\nU\nd= N\n\n0, θ2\nX\nj∈A∪B\nB2\nij\n\n,\n(34)\nand:\nV\nd= N\n\n0,\nX\nj∈A\n\u0012\n(1 + γ) Bij −2d\np\n\u00132\n+\nX\nj∈B\n(1 −γ)2 B2\nij + 4\nX\nj∈C\n\u0012\nBij −d\np\n\u00132\n\n.\n(35)\nLet: \n\n\nσ2\nU := θ2 P\nj∈A∪B B2\nij\nσ2\nV := P\nj∈A\n\u0010\n(1 + γ) Bij −2d\np\n\u00112\n+ P\nj∈B (1 −γ)2 B2\nij + 4 P\nj∈C\n\u0010\nBij −d\np\n\u00112\n.\nNote that:\nσ2\nU = θ2\nX\nj∈A∪B\nBij,\nand\nσ2\nV =\nX\nj∈A\n\u0012\n(1 + γ) Bij −2d\np\n\u00132\n+\nX\nj∈B\n(1 −γ)2 B2\nij + 4\nX\nj∈C\n\u0012\nBij −d\np\n\u00132\n=\nX\nj∈A\n\u0012\n(1 + γ)2 B2\nij −4d (1 + γ)\np\nBij\n\u0013\n+ 4d2\np2 M +\nX\nj∈B\n(1 −γ)2 B2\nij\n+ 4\nX\nj∈C\n\u0012\nB2\nij −2d\np Bij\n\u0013\n+ 4d2\np2 (s −M)\n= 4d2s\np2\n+\nX\nj∈A\n\u0012\n(1 + γ)2 −4d (1 + γ)\np\n\u0013\nBij +\nX\nj∈B\n(1 −γ)2 Bij + 4\nX\nj∈C\n\u0012\n1 −2d\np\n\u0013\nBij\n= 4d2s\np2\n+\n\u0012\n(1 + γ)2 −4d (1 + γ)\np\n\u0013 X\nj∈A\nBij + (1 −γ)2 X\nj∈B\nBij + 4\n\u0012\n1 −2d\np\n\u0013 X\nj∈C\nBij.\nIn addition, we have:\nCov (U, V ) = θ\n\nX\nj∈A\n\u0012\n(1 + γ) Bij −2d\np\n\u0013\nBij −\nX\nj∈B\n(1 −γ) B2\nij\n\n\n= θ\n\n\n\u0012\n1 + γ −2d\np\n\u0013 X\nj∈A\nBij −(1 −γ)\nX\nj∈B\nBij\n\n.\n27\nLemma B.1. Let N1\nd= N\n\u00000, σ2\n1\n\u0001\n, N2\nd= N\n\u00000, σ2\n2\n\u0001\nand ρ := corr (N1, N2). Then:\nE\n\u0002\neN1N2\u0003\n=\n\n\n\n1\nσ1σ2\nr\u0010\n1\nσ1σ2 −ρ\n\u00112−1\nif\n1\nσ1σ2 −ρ > 1,\n+∞\nelse.\nProof. See section B.1.1.\nNow using Lemma B.1 with Gaussian random variables U and V , we conclude that:\nEXi\n\u0002\neUV | Bi\n\u0003\n= f (θ, Bi) ,\n(36)\nwith f (·, ·) defined in (26). Plugging this into (33), we obtain:\nM−∆i (θ) = EBi [f (θ, Bi)] .\n(37)\nLet B ∈{0, 1}p be a random vector such that Bj\ni.i.d.\n= Ber (d/p). Note that B\nd= Bi. Then (37) yields:\nM−∆i (θ) = EB [f (θ, B)] .\nPlugging this into the Chernoff bound (30), we conclude:\nP (∆≤0) ≤\n\u0010\ninf\nθ>0 EB\n\u0002\nf (θ, B)\n\u0003\u0011n\n.\nB.1.1\nProof of Lemma B.1\nProof of Lemma B.1. We have:\nE\n\u0002\neN1N2\u0003\n=\nZ\nx∈R\nZ\ny∈R\nexy\n2πσ1σ2\np\n1 −ρ2 exp\n \n−\n1\n2 (1 −ρ2)\n \u0012 x\nσ1\n\u00132\n−2ρ\n\u0012 x\nσ1\n\u0013 \u0012 y\nσ2\n\u0013\n+\n\u0012 y\nσ2\n\u00132!!\ndydx\n=\nZ\nx∈R\nZ\ny∈R\n1\n2πσ1σ2\np\n1 −ρ2 exp\n \nxy −\n1\n2 (1 −ρ2)\n \u0012 x\nσ1\n\u00132\n−2ρ\n\u0012 x\nσ1\n\u0013 \u0012 y\nσ2\n\u0013\n+\n\u0012 y\nσ2\n\u00132!!\ndydx\n=\nZ\nx∈R\nZ\ny∈R\n1\n2π\np\n1 −ρ2 exp\n\u0012\nσ1σ2xy −x2 −2ρxy + y2\n2 (1 −ρ2)\n\u0013\ndydx\n=\nZ\nx∈R\nZ\ny∈R\n2\n\u00001 −ρ2\u0001\n2π\np\n1 −ρ2 exp\n\u00002σ1σ2\n\u00001 −ρ2\u0001\nxy −x2 + 2ρxy −y2\u0001\ndydx\n=\nZ\nx∈R\nZ\ny∈R\np\n1 −ρ2\nπ\nexp\n\u00002σ1σ2\n\u00001 −ρ2\u0001\nxy −x2 + 2ρxy −y2\u0001\ndydx\nTherefore:\nE\n\u0002\neN1N2\u0003\n=\np\n1 −ρ2\nπ\nZ\nx∈R\nZ\ny∈R\ne2σ1σ2(1−ρ2)xy−x2+2ρxy−y2dydx\n=\np\n1 −ρ2\nπ\nZ\nx∈R\nZ\ny∈R\ne\n−(y−x(σ1σ2(1−ρ2)+ρ))\n2+x2\u0010\n(σ1σ2(1−ρ2)+ρ)\n2−1\n\u0011\ndydx\n=\np\n1 −ρ2\nπ\nZ\nx∈R\ne\nx2\u0010\n(σ1σ2(1−ρ2)+ρ)\n2−1\n\u0011 Z\ny∈R\ne−(y−x(σ1σ2(1−ρ2)+ρ))\n2\ndydx\n=\np\n1 −ρ2\nπ\nZ\nx∈R\ne\nx2\u0010\n(σ1σ2(1−ρ2)+ρ)\n2−1\n\u0011 Z\ny∈R\n√π ΦN(x(σ1σ2(1−ρ2)+ρ),1/\n√\n2)(y) dydx\n=\np\n1 −ρ2\nπ\nZ\nx∈R\nexp\n\u0010\nx2 \u0010\u0000σ1σ2\n\u00001 −ρ2\u0001\n+ ρ\n\u00012 −1\n\u0011\u0011 √πdx.\n28\nHence we obtain:\nE\n\u0002\neN1N2\u0003\n=\np\n1 −ρ2\n√π\nZ\nx∈R\nexp\n\u0010\nx2 \u0010\u0000σ1σ2\n\u00001 −ρ2\u0001\n+ ρ\n\u00012 −1\n\u0011\u0011\ndx.\nLet ξ := σ1σ2\n\u00001 −ρ2\u0001\n+ ρ. Note that, if ξ ≥1, the above explodes to +∞. However, if ξ < 1, the\nabove yields:\nE\n\u0002\neN1N2\u0003\n=\np\n1 −ρ2\n√π\nZ\nx∈R\nexp\n\u0000x2 \u0000ξ2 −1\n\u0001\u0001\ndx\n=\np\n1 −ρ2\n√π\np\n1 −ξ2\nZ\nx∈R\nexp\n\u0000−x2\u0001\ndx\n=\np\n1 −ρ2\n√π\np\n1 −ξ2\nZ\nx∈R\n√π ΦN(0,1/\n√\n2) (x) dx\n=\np\n1 −ρ2\np\n1 −ξ2 .\nPlugging the expression of ξ in the above, we obtain:\nE\n\u0002\neN1N2\u0003\n=\np\n1 −ρ2\nq\n1 −ρ2 −σ2\n1σ2\n2 (1 −ρ2)2 −2ρ (1 −ρ2) σ1σ2\n=\n1\np\n1 −σ2\n1σ2\n2 (1 −ρ2) −2ρσ1σ2\n=\n1\np\n1 + ρ2σ2\n1σ2\n2 −2ρσ1σ2 −σ2\n1σ2\n2\n=\n1\nq\n(ρσ1σ2 −1)2 −σ2\n1σ2\n2\n=\n1\nσ1σ2\nr\u0010\n1\nσ1σ2 −ρ\n\u00112\n−1\n.\nIn addition, note that:\nξ < 1 ⇐⇒σ1σ2\n\u00001 −ρ2\u0001\n+ ρ < 1\n⇐⇒σ1σ2 (1 + ρ) (1 −ρ) < 1 −ρ\n⇐⇒σ1σ2 (1 + ρ) < 1\n⇐⇒1 <\n1\nσ1σ2\n−ρ.\nB.2\nProof of Proposition B.2\nProof of Proposition B.2. Fix η ∈(0, 1]. To simplify notations, we will write C⋆instead of C⋆(η).\nAll asymptotic statements are as p →+∞.\nWe start by showing (i). First, we note the following.\nLemma B.2.\nX\nj∈A(S)\nBj = ηαψp\n\u00001 + o (1)\n\u0001\n,\nX\nj∈B(S)\nBj = ηαψp\n\u00001 + o (1)\n\u0001\n,\nX\nj∈C(S)\nBj = (1 −η) αψp\n\u00001 + o (1)\n\u0001\n.\nProof. See section B.2.1.\n29\nNext, plugging the expressions given by Lemma B.2 into the expressions of σ2\nU (θ, B), σ2\nV (θ, B) and\nCov(U,V ) (θ, B) given in the statement of Proposition B.1, we obtain the following.\nProposition B.3. We have:\n\n\n\n\n\nσ2\nU (θ, B) = 2ηαψpθ2\u00001 + o (1)\n\u0001\nσ2\nV (θ, B) = 2αψp\nh\n2 −2ψ + η\n\u0010\n(ψ −γ (θ))2 −(1 −ψ)2\u0011i \u00001 + o (1)\n\u0001\nCov(U,V ) (θ, B) = 2ηαψpθ (γ (θ) −ψ)\n\u00001 + o (1)\n\u0001\n.\nProof. See section B.2.2.\nPlugging ξ (p) in the expression of γ (·) given by (27), we obtain:\nγ (ξ (p)) = 2σ2ψ2ξ (p) = 2σ2ψ2C⋆\n2αψp\n= σ2ψC⋆\nαp\n= o (1) .\nTherefore, we have by Proposition B.3:\nσ2\nU (ξ (p) , B) σ2\nV (ξ (p) , B) = 4ηα2ψ2p2\n\u0012 C⋆\n2αψp\n\u00132 \u0010\n2 −2ψ + η\n\u0010\n(ψ −γ (ξ (p)))2 −(1 −ψ)2\u0011\u0011\u0010\n1 + o (1)\n\u0011\n= ηC⋆2\u0010\n2 −2ψ + η\n\u0010\n(ψ −o (1))2 −(1 −ψ)2\u0011\u0011\u0010\n1 + o (1)\n\u0011\n= ηC⋆2\u0010\n2 −2ψ + η (2ψ −1)\n\u0011\u0010\n1 + o (1)\n\u0011\na.s.\n−→ηC⋆2\u0010\n2 −2ψ + 2ηψ −η\n\u0011\n.\nNote that the above is > 0 because η (1 −2ψ) < 2 (1 −ψ), which follows from the facts that η ≤1\nand ψ ∈(0, 1). In addition, by Proposition B.3:\nCov(U,V ) (ξ (p) , B) = 2ηαψp\n\u0012 C⋆\n2αψp\n\u0013 \u0000γ (ξ (p)) −ψ\n\u0001\u00001 + o (1)\n\u0001\n= ηC⋆\u0000o (1) −ψ\n\u0001\u00001 + o (1)\n\u0001\na.s.\n−→−ηψC⋆.\nBringing these together, we obtain:\nlim\np→+∞\n\u001a\u0010\n1 −Cov(U,V ) (ξ (p) , B)\n\u00112\n−\n\u0010\nσ2\nU (ξ (p) , B) σ2\nV (ξ (p) , B) + 1 + ηψC⋆\u0011\u001b\n=\n\u0010\n1 + ηψC⋆\u00112\n−η\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2 −ηψC⋆−1\n= η2ψ2C⋆2 + 2ηψC⋆+ 1 −η\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2 −ηψC⋆−1\n=\n\u0010\nη2ψ2 −2η + 2ηψ −2η2ψ + η2\u0011\nC⋆2 + ηψC⋆\n= ηC⋆\n \n−(1 −ψ)\n\u0010\n2 −η (1 −ψ)\n\u0011\nC⋆+ ψ\n!\n= 0.\n30\nIn addition:\nlim\np→+∞\n\u001a\n1\nσU (ξ (p) , B) σV (ξ (p) , B) −ρ (ξ (p) , B)\n\u001b2\n=\nlim\np→+∞\n\u001a\n1 −Cov(U,V ) (ξ (p) , B)\nσU (ξ (p) , B) σV (ξ (p) , B)\n\u001b2\n=\n\u0010\n1 + ηψC⋆\u00112\nη\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2\n= 1 +\n\u0010\n1 + ηψC⋆\u00112\n−η\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2\nη\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2\n= 1 +\n1 + ηψC⋆\n\u0010\n1 + ηψC⋆\n\u00112\n−\n\u0010\n1 + ηψC⋆\n\u0011\n= 1 +\n1\nηψC⋆> 1.\nTherefore, there exists p′ ∈N such that, for all p ≥p′ we have:\n\u001a\n1\nσU (ξ (p) , B) σV (ξ (p) , B) −ρ (ξ (p) , B)\n\u001b2\n> 1.\nThen note that, for all p ≥p′ we have:\n1\nσU (ξ (p) , B) σV (ξ (p) , B) −ρ (ξ (p) , B) > 1,\nand hence:\nf (ξ (p) , B) =\n1\nσU (ξ (p) , B) σV (ξ (p) , B)\nr\u0010\n1\nσU(ξ(p),B)σV (ξ(p),B) −ρ (ξ (p) , B)\n\u00112\n−1\n=\n1\nr\u0010\n1 −Cov(U,V ) (ξ (p) , B)\n\u00112\n−σ2\nU (ξ (p) , B) σ2\nV (ξ (p) , B)\na.s.\n−→\nr\n1\n1 + ηψC⋆.\nTherefore, we conclude that (i) holds. We now show (ii). We have:\ninf\nθ>0 EB\n\u0002\nf (θ, B)\n\u0003\n≤EB\n\u0002\nf (ξ (p) , B)\n\u0003\n= EB [Hp (B)] .\nPlugging this into the Chernoff bound obtained in Proposition B.1, we obtain:\nP (∆≤0) ≤\n\u0010\ninf\nθ>0 EB\n\u0002\nf (θ, B)\n\u0003\u0011n\n≤\n\u0010\nEB [Hp (B)]\n\u0011n\n.\nB.2.1\nProof of Lemma B.2\nProof of Lemma B.2. We know that Bj\ni.i.d.\n∼Ber (ψ). Therefore:\nX\nj∈A(S)\nBj = |A (S)| ψ +\np\n|A (S)| ψNA + o\n\u0010p\n|A (S)| ψ\n\u0011\n,\nwhere NA ∼N (0, 1). In addition, we have:\n|A (S)| = ⌈ηs⌉= ηs\n\u00001 + o (1)\n\u0001\n.\n31\nHence:\nX\nj∈A(S)\nBj = ηαψp\n\u00001 + o (1)\n\u0001\n.\nSimilarly, we have:\nX\nj∈B(S)\nBj = ηαψp\n\u00001 + o (1)\n\u0001\nand\nX\nj∈C(S)\nBj = (1 −η) αψp\n\u00001 + o (1)\n\u0001\n.\nB.2.2\nProof of Proposition B.3\nProof of Proposition B.3. We have by Proposition B.1:\nσ2\nU (θ, B) = θ2\nX\nj∈A(S)∪B(S)\nBj,\nσ2\nV (θ, B) = 4d2s\np2\n+\n \n\u00001 + γ (θ)\n\u00012 −4d\n\u00001 + γ (θ)\n\u0001\np\n! X\nj∈A(S)\nBj\n+\n\u00001 −γ (θ)\n\u00012\nX\nj∈B(S)\nBij + 4\n\u0012\n1 −2d\np\n\u0013 X\nj∈C(S)\nBj,\nCov(U,V ) (θ, B) = θ\n\n\n\u0012\n1 + γ (θ) −2d\np\n\u0013 X\nj∈A(S)\nBij −\n\u00001 −γ (θ)\n\u0001 X\nj∈B(S)\nBij\n\n.\nNow by Lemma B.2 we have:\n\n\n\n\n\nP\nj∈A(S) Bj = ηαψp\n\u00001 + o (1)\n\u0001\nP\nj∈B(S) Bj = ηαψp\n\u00001 + o (1)\n\u0001\nP\nj∈C(S) Bj = (1 −η) αψp\n\u00001 + o (1)\n\u0001\n.\nHence, the expression of σ2\nU (θ, B) writes:\nσ2\nU (θ, B) = θ2\nX\nj∈A(S)\nBj + θ2\nX\nj∈B(S)\nBj\n= θ2ηαψp\n\u00001 + o (1)\n\u0001\n+ θ2ηαψp\n\u00001 + o (1)\n\u0001\n= 2ηαψpθ2\u00001 + o (1)\n\u0001\n.\n32\nThe expression of σ2\nV (θ, B) writes:\nσ2\nV (θ, B)\n= 4d2s\np2\n+\n \n\u00001 + γ (θ)\n\u00012 −4d\n\u00001 + γ (θ)\n\u0001\np\n! X\nj∈A(S)\nBj\n+\n\u00001 −γ (θ)\n\u00012\nX\nj∈B(S)\nBij + 4\n\u0012\n1 −2d\np\n\u0013 X\nj∈C(S)\nBj\n= 4αψ2p +\n\u00001 + γ (θ)\n\u0001\n(1 + γ (θ) −4ψ) ηαψp\n\u00001 + o (1)\n\u0001\n+\n\u00001 −γ (θ)\n\u00012ηαψp\n\u00001 + o (1)\n\u0001\n+ 4 (1 −2ψ) (1 −η) αψp\n\u00001 + o (1)\n\u0001\n= αψp\n\u0010\n4ψ + η\n\u00001 + γ (θ)\n\u0001\u00001 + γ (θ) −4ψ\n\u0001\n+ η\n\u00001 −γ (θ)\n\u00012 + 4 (1 −2ψ) (1 −η)\n\u0011\u00001 + o (1)\n\u0001\n= αψp\n\u0012\n4 −4ψ + η\n\u0010\u00001 + γ (θ)\n\u00012 +\n\u00001 −γ (θ)\n\u00012 −4ψ\n\u00001 + γ (θ)\n\u0001\n−4 (1 −2ψ)\n\u0011\u0013\u00001 + o (1)\n\u0001\n= αψp\n\u0012\n4 −4ψ + η\n\u0010\n2 + 2γ (θ)2 −4ψγ (θ) + 4ψ −4\n\u0011\u0013\u00001 + o (1)\n\u0001\n= 2αψp\n\u0012\n2 −2ψ + η\n\u0010\nγ (θ)2 −2ψγ (θ) + 2ψ −1\n\u0011\u0013\u00001 + o (1)\n\u0001\n= 2αψp\n\u0012\n2 −2ψ + η\n\u0010\nγ (θ)2 −2ψγ (θ) + ψ2 −ψ2 + 2ψ −1\n\u0011\u0013\u00001 + o (1)\n\u0001\n= 2αψp\n\u0014\n2 −2ψ + η\n\u0010\u0000ψ −γ (θ)\n\u00012 −\n\u00001 −ψ\n\u00012\u0011\u0015\u00001 + o (1)\n\u0001\n.\nThe expression of Cov(U,V ) (θ, B) writes:\nCov(U,V ) (θ, B) = θ\n\n\n\u0012\n1 + γ (θ) −2d\np\n\u0013 X\nj∈A(S)\nBij −\n\u00001 −γ (θ)\n\u0001 X\nj∈B(S)\nBij\n\n\n= θ\n\u0012\u0010\n1 + γ (θ) −2ψ\n\u0011\nηαψp\n\u00001 + o (1)\n\u0001\n−\n\u00001 −γ (θ)\n\u0001\nηαψp\n\u00001 + o (1)\n\u0001\u0013\n= ηαψpθ\n\u0012\u0010\n1 + γ (θ) −2ψ\n\u0011\n−\n\u00001 −γ (θ)\n\u0001\u0013\u00001 + o (1)\n\u0001\n= 2ηαψpθ\n\u0000γ (θ) −ψ\n\u0001\u00001 + o (1)\n\u0001\n,\nas desired.\n33\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: Theorem 1, Corollary 2 and Theorem 3 clearly state the main claims.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims made\nin the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: Limitations where discussed:\n• Briefly in the introduction (section 1).\n• Thoroughly in Remark 2.2 and Remark 3.1.\n• Future work directions in the conclusion (section 4).\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to address\nproblems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an important\nrole in developing norms that preserve the integrity of the community. Reviewers will\nbe specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\n34\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: Complete proofs of theoretical results are provided: for Theorem 1 in appendix\nA.1, for Corollary 2 in appendix A.2, for Example 2.2 in appendix A.3, and for Theorem 3\nin appendix B.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main\nexperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [NA]\nJustification: The paper does not include experiments. In order to observe our results we\nneed the dimension p of the problem to be large, which cannot be ran experimentally since\ncomputing the MLE (7), (13) is exponential-time.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived well\nby the reviewers: Making the paper reproducible is important, regardless of whether\nthe code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all\nsubmissions to provide some reasonable avenue for reproducibility, which may depend\non the nature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct the\ndataset).\n35\n(d) We recognize that reproducibility may be tricky in some cases, in which case authors\nare welcome to describe the particular way they provide for reproducibility. In the\ncase of closed-source models, it may be that access to the model is limited in some\nway (e.g., to registered users), but it should be possible for other researchers to have\nsome path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions\nto faithfully reproduce the main experimental results, as described in supplemental material?\nAnswer: [NA]\nJustification: The paper does not include experiments.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run\nto reproduce the results.\nSee the NeurIPS code and data submission guidelines\n(https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [NA]\nJustification: The paper does not include experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [NA]\nJustification: The paper does not include experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confidence\nintervals, or statistical significance tests, at least for the experiments that support the\nmain claims of the paper.\n36\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error of\nthe mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the computer\nresources (type of compute workers, memory, time of execution) needed to reproduce the\nexperiments?\nAnswer: [NA]\nJustification: The paper does not include experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The research conducted in the paper conforms, in every respect, with the\nNeurIPS Code of Ethics.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special\nconsideration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: The main positive societal impact of our work is reducing computational cost\nin a range of applications via the measurement sparsity vs. computational cost trade-off.\nApplications of the sparse recovery problem are discussed in the introduction and an example\nof computational cost reduction is given in Example 2.2.\n37\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper poses no such risks.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: The paper does not use existing assets. Previous theoretical results are properly\ncredited to their authors.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n38\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to the\nasset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: The paper does not release new assets. We provide our interpretation of the\ntheoretical results in sections 2 and 3.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main\ncontribution of the paper involves human subjects, then as much detail as possible\nshould be included in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n39\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: The core method development in this research does not involve LLMs as any\nimportant, original, or non-standard components.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n40\n",
  "pages": [
    {
      "page_number": 1,
      "text": "The Price of Sparsity: Sufficient Conditions for Sparse\nRecovery using Sparse and Sparsified Measurements\nYoussef Chaabouni\nOperations Research Center\nMassachusetts Institute of Technology\nCambridge, MA 02139, USA\nyouss404@mit.edu\nDavid Gamarnik\nOperations Research Center\nMassachusetts Institute of Technology\nCambridge, MA 02139, USA\ngamarnik@mit.edu\nAbstract\nWe consider the problem of recovering the support of a sparse signal using noisy\nprojections. While extensive work has been done on the dense measurement\nmatrix setting, the sparse setting remains less explored. In this work, we establish\nsufficient conditions on the sample size for successful sparse recovery using\nsparse measurement matrices. Bringing together our result with previously known\nnecessary conditions, we discover that, in the high-SNR regime where ds/p →\n+∞, sparse recovery using a sparse design exhibits a phase transition at an\ninformation-theoretic threshold of nSP\nINF = Θ (s log (p/s) / log (ds/p)) for the\nnumber of measurements, where p denotes the signal dimension, s the number\nof non-zero components of the signal, and d the expected number of non-zero\ncomponents per row of measurement. This expression makes the price of sparsity\nexplicit: restricting each measurement to d non-zeros inflates the required sample\nsize by a factor of log s/ log (ds/p), revealing a precise trade-off between sampling\ncomplexity and measurement sparsity. Additionally, we examine the effect of\nsparsifying an originally dense measurement matrix on sparse signal recovery. We\nprove in the regime of s = αp and d = ψp with α, ψ ∈(0, 1) and ψ small that a\nsample of size nSp-ified\nINF\n= Θ\n\u0000p/ψ2\u0001\nis sufficient for recovery, subject to a certain\nuniform integrability conjecture, the proof of which is work in progress.\n1\nIntroduction\nIn recent years, sparse signal recovery has gained significant attention, motivated by applications in\ncompressive sensing [7, 2, 5]; signal denoising [3]; sparse regression [13]; data stream computing\n[4, 11, 14]; combinatorial group testing [6]; etc. Practical examples range from the single-pixel\ncamera, MRI scanners and radar remote-sensing systems to error-correction schemes in digital\ncommunications and widely used image-compression formats [7, Chap. 1].\nThe problem can be formulated as follows. Consider a signal β⋆∈Rp, unknown but a priori s-sparse\nfor some given s ≤p, a random measurement matrix X ∈Rn×p (also referred to as design, features\nor data) and a noise vector Z ∼N(0, σ2In), where n ∈N denotes the sample size and σ2 > 0 a\nfixed constant. An vector of observations (also known as labels or annotations) is given by:\nY := Xβ⋆+ Z.\nSparse recovery refers to reconstructing β⋆given X and Y . Intuitively, this problem can be reduced\nto recovering the support S⋆of β⋆, i.e. the set of indices of its non-zero components. In fact, once\nthe support S⋆is identified, the full signal can be estimated using the corresponding columns of X\nvia the closed-form maximum-likelihood estimator formula βMLE = X+\nS⋆Y , where X+\nS⋆denotes the\nMoore-Penrose pseudoinverse of the submatrix formed by the columns of X with indices in S⋆[10].\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n"
    },
    {
      "page_number": 2,
      "text": "Traditionally, X is considered to be a dense random matrix with sub-Gaussian entries. Previous\nworks have shown that the complexity of the problem in terms of required sample size exhibits two\nphase transitions at two thresholds nINF < nALG, yielding three regimes:\n• n < nINF: impossibility of recovery. Reeves et al. [16] show that if n ≤(1 −ε) nINF then the\nrecovery of any fraction of the support of the signal is impossible.\n• nINF < n < nALG: super-polynomial complexity. Gamarnik and Zadik [8] show that if n ≥\n(1 + ε) nINF then the maximum-likelihood estimator (MLE) recovers the support of β⋆. Although\nsolvable, the problem is widely believed to be algorithmically hard since the MLE exhibits an\nOverlap Gap Property (OGP) [8].\n• n > nALG: polynomial-time recovery. Wainwright [18] shows that if n ≥(1 + ε) nALG then the\nLasso [17] succeeds in recovering the support of β⋆.\n1.1\nSparse measurement setting\nWhile dense matrices offer an optimal sample size, they are costly in terms of storage and computation.\nSparse measurement matrices, where the number of non-zero entries per measurement vector scales\nsignificantly smaller than the signal dimension, mitigate these costs: they require significantly less\nstorage and allow for more efficient computations, as matrix-vector multiplications and incremental\nupdates can be performed faster. In addition, they enable efficient signal recovery algorithms by\ntaking advantage of the structural properties of the problem [9]. However, this sparsity comes at\nthe cost of increased sampling complexity [19]. This raises the following key question: How does\nmeasurement sparsity trade off with sampling complexity?\nSome of the prior studies have explored this sparse measurement setting. Wang et al. [19] establish\nnecessary conditions for sparse recovery for various measurement sparsity regimes. Let d denote\nthe expected number of non-zero components of a row of X. Their work reveals three regimes of\nbehavior depending on ds/p, the expected number of non-zero components of β⋆that align with\nnon-zero components of a row of X. The three regimes are: ds/p →+∞, ds/p = τ for some\nconstant τ > 0, and ds/p →0. They show that in each regime, the number of samples n must\nexceed a specific information-theoretic lower bound for any algorithm to reliably recover the signal’s\nsupport. In particular, in the first regime, where ds/p →+∞, the necessary condition threshold of\n[19] is the same as the one of the dense case, while it increases dramatically in the third case, where\nds/p →0. They work with entries rescaled so that Var (Xβ⋆) matches the dense case, while we\nkeep Var (Xij) = 1. The settings are equivalent since any scaling of X can be accounted for in β⋆.\nIn this work, we examine the opposite question: how many samples are enough to guarantee a\nreliable recovery? For simplicity, we assume the signal is binary, i.e. β⋆∈{0, 1}p. Note that in\nthis case, recovering the support is equivalent to recovering the signal. This assumption is very\ncommon in the literature [1, 16, 8]. Intuitively, detecting a component of size 1 is at least as hard as\ndetecting a stronger component, so the resulting thresholds are representative of signals with non-zero\nentries bounded away from zero by 1, i.e. β⋆∈\n\b\nβ ∈Rp : ∥β∥0 = s and minj∈[p] : βj̸=0 |βj| ≥1\n\t\n.\nOur first main result (Theorem 1) states that in the high signal-to-noise ratio (SNR) regime where\nds/p →+∞, if the number of samples n is larger than a threshold given by:\nnSP\nINF = Θ\n\u0012s log (p/s)\nlog (ds/p)\n\u0013\n,\n(1)\nthen the MLE asymptotically recovers the support of the signal. The proof uses large deviation\ntechniques to bound the probability that a different support has a lower error than the true one, and a\nunion bound over such supports. Bringing our result together with the necessary condition shown by\nWang et al. [19], we reveal that the problem exhibits a phase transition – similar to the one known\nin the dense case – at the information-theoretic threshold nSP\nINF. In fact, if there exists a constant\nε > 0 such that n ≤(1 −ε) nSP\nINF then it is information-theoretically impossible to ensure a reliable\nrecovery of the support of the signal, and if there exists a constant ε > 0 such that n ≥(1 + ε) nSP\nINF\nthen the MLE ensures a reliable recovery of the support. Our findings therefore answer the question\nof exactly how much data is needed for recovery. However, we note that the recovery that we show in\nthe sufficiency statement holds in a weaker sense than the non-recovery in the necessity statement.\nWe call the amount of additional observations in the sparsification setting compared to the dense one\nprice of sparsity. Precisely, restricting each measurement to d non-zeros inflates the required sample\n2\n"
    },
    {
      "page_number": 3,
      "text": "size by a factor of Γ = Θ (log s/ log (ds/p)), quantifying the sampling complexity vs. measurement\nsparsity trade-off.\nRegarding the computational complexity, Omidiran and Wainwright [15] show that the Lasso\nperforms as well in the sparse setting as in the dense setting, assuming a slow decay of sparsity. They\nshow that, under some slow sparsity assumption, it is sufficient for the sample size n to be larger than\nthe algorithmic threshold of the dense setting discussed above, given by:\nnALG = 2s log (p −s) ,\nspecifically for the Lasso to ensure a reliable polynomial time recovery of β⋆. Although the sparsity\nassumption under which this result holds allows for the density rate d/p to go to 0 as p →+∞, it\nstill doesn’t allow the measurements to be very sparse. In fact, it requires that:\nd/p = ω\n\u0010\ns−1/3\u0011\nand\nd/p = ω\n \u0012log log (p −s)\nlog (p −s)\n\u00131/3!\n.\n(2)\nThis raises a question about what happens in a more sparse regime. Although we don’t address the\nalgorithmic threshold, i.e. the question of polynomial-time recovery, we discover herein a sufficient\ncondition for recovery that allows for more sparsity, allowing for future investigation in this direction.\n1.2\nSparsification\nThe applications of the signal recovery problem [7, Chapter 1] considered in this paper can be broadly\ncategorized into two classes:\n- Applications where X is designed, e.g. involving signal compression and reconstruction.\n- Applications where X is observed, e.g. sparse regression, signal denoising, and error correction.\nIn light of this categorization, we note that measuring the trade-off between measurement sparsity\nand sampling complexity is particularly useful for the first class of problems. It provides practitioners\nwith an exact description of how the measurement matrix should be designed, in terms of size and\nsparsity, to optimize the computational cost of signal recovery. However, this is rendered useless in\nthe second class of problems when the measurement matrix is observed and dense. This motivates the\nsecond key question: given an initially dense measurement matrix, is there a way to make it sparse\nand still aim to recover the original signal?\nThis setting, in which the observations are generated with a dense measurement matrix, but the\nsignal is recovered using a sparsified version of it, is closely related to the “missing covariates”\nor “missing-at-random” framework studied in high-dimensional statistics. Prior work by Loh and\nWainwright [12], established algorithmic ℓ2-error bounds for regression under this model, assuming\ndense Gaussian designs and a constant missingness rate. Our analysis in Section 3 complements\nthis line of research by focusing instead on information-theoretic support-recovery thresholds and\nderiving the precise sample complexity cost incurred by sparsification in this regime.\nIn our examination of the sparsification question, we focus on the linear sparsity and linear sparsification\nregime where s = αp and d = ψp for constant α, ψ ∈(0, 1). Specifically, our second main result\n(Theorem 3) states that if the number of samples n is larger than a threshold given by:\n2h (α) p\nlog\n\u0010\n1 +\nδψ2\n(1−ψ)(2−δ(1−ψ))\n\u0011,\n(3)\nthen the minimizer of the mean squared error (MSE) based on sparsified measurements and accordingly-\nrescaled observations asymptotically recovers the true support up to error fraction δ ∈(0, 1). In\nparticular, support recovery is possible for arbitrarily aggressive sparsification, i.e. arbitrarily small ψ.\nIn the strong-sparsification regime where ψ →0, the sufficient threshold (3) effectively writes:\nnSp-ified\nINF\n= Θ\n\u0012 p\nψ2\n\u0013\n.\n(4)\nWe call the amount of additional observations in the sparsification setting compared to the dense one\nprice of sparsification. Unlike the price of sparsity, it is not due to the sparsity of the measurements\nbut rather to a bias in the observations. We also interpret our result as providing an expression of the\nsparsification budget: the level up to which one could sparsify their data and still recover the true\nsignal. Based on our result, this has order Θ\n\u0010p\np/n\n\u0011\nwhen n = Ω(p).\n3\n"
    },
    {
      "page_number": 4,
      "text": "1.3\nContributions\nTo the best of our knowledge, this work is the first to address the following:\n1. Establish a necessary and sufficient condition for sparse recovery in the sparse setting.\n2. Provide a sufficient condition for sparse recovery after sparsifying an originally dense design,\nconditional on a mild uniform-integrability conjecture.\n1.4\nOutline and Notations\nWe organize the rest of the paper as follows. Section 2 studies the sparse measurement setting. Section\n3 examines recovery after sparsifying an originally dense measurement matrix. Section 4 concludes\nand sketches future work directions.\nThroughout this document, we will use the following notations:\n• We denote by h (·) the binary entropy: h (x) = −x log x −(1 −x) log (1 −x), x ∈(0, 1).\n• We call ℓ0-norm the number of non-zero coordinates of x ∈Rd, that is ∥x∥0 := Pd\ni=1 1 (xi ̸= 0).\n2\nSparse Recovery using Sparse Measurements\n2.1\nSetting\nLet n, p, s, d ∈N such that d, s ≤p. We define a sparse Gaussian matrix in Rn×p as follows.\nDefinition 2.1 (Sparse Gaussian matrix). We call X = [Xij]i∈[n],j∈[p] ∈Rn×p a sparse Gaussian\nmatrix with parameter d if for all i ∈[n], j ∈[p] we have:\nXij = BijNij,\nwhere (Bij)i∈[n],j∈[p]\ni.i.d.\n∼Ber (d/p) and (Nij)i∈[n],j∈[p]\ni.i.d.\n∼N (0, 1) are mutually independent.\nRemark 2.1. Note that d is the expected number of non-zero components per row of X. In our\nsetting, we think of d as being of smaller order of magnitude than p. In particular, d = o (p).\nLet X be a sparse Gaussian random matrix of parameter d, and Z be a random vector in Rn such that\nZ ∼N\n\u00000, σ2In\n\u0001\n, with σ > 0 a fixed constant. Let β⋆∈{0, 1}p be a deterministic vector such that\n∥β⋆∥0 = s. We define the random vector Y as:\nY := Xβ⋆+ Z.\n(5)\nOf particular interest is the signal-to-noise ratio (SNR), known to be an important quantity for\ncharacterizing the difficulty of sparse recovery problems [19, 16]. It’s defined as follows:\nSNR := E∥Xβ⋆∥2\n2\nE∥Z∥2\n2\n= ds\npσ2 .\n(6)\nThe maximum likelihood estimator (MLE) of β⋆is defined by the random vector:\nˆβ :=\narg min\nβ∈{0,1}p, ∥β∥0=s\n∥Y −Xβ∥2\n2 .\n(7)\nWe are interested in the minimum number of samples n required so that the MLE (7) asymptotically\nrecovers the true signal β⋆. We formalize the problem as follows.\n2.2\nProblem\nWe start by defining the support of a vector and the symmetric difference of supports.\nDefinition 2.2 (Support). Let u ∈Rp. We call support of u the set of indices of the non-zero\ncomponents of u and denote it Supp (u) := {i ∈[p]: ui ̸= 0}. Note that |Supp (u)| = ∥u∥0.\nDefinition 2.3 (Symmetric difference). We call symmetric difference between two sets S1 and S2 the\nset of elements in one but not the other and denote it S1 △S2 :=\n\u0000S1 ∪S2\n\u0001\n\\\n\u0000S1 ∩S2\n\u0001\n.\n4\n"
    },
    {
      "page_number": 5,
      "text": "We formalize the problem we address below as follows: given an error tolerance δ ∈(0, 1), we wish\nto determine the minimum number of samples n as a function of p, s and d required so that:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n−→1, as n, p, s, d →+∞.\n2.3\nResults\nOur first main result, Theorem 1, provides a sufficient condition on the sample size for reliable support\nrecovery when using sparse measurements.\nTheorem 1 (Sufficient conditions for sparse recovery using sparse measurement matrices). Suppose\np, s, d →+∞, d = o (p) and ds = ω (p) (i.e. SNR →+∞). Let δ ∈(0, 1). We consider two\ndifferent regimes.\n1. Assume s = o (p). Let\nn⋆\nslin :=\n2s log (p/s)\nlog (ds/p) + log (δ/(2σ2)).\nIf there exists ε > 0 such that n ≥(1 + ε) n⋆\nslin, then the MLE ˆβ recovers β⋆up to error δ w.h.p.:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εs log (p/s) + o\n\u0000s log (p/s)\n\u0001\u0011\n,\nas n, p, s, d →+∞.\n2. Assume there exists a constant α ∈(0, 1) such that s = αp. Let:\nn⋆\nlin :=\n2h (α) p\nlog d + log (δα/(2σ2)),\nwhere h (·) denote the entropy function. If there exists ε > 0 such that n ≥(1 + ε) n⋆\nlin, then the\nMLE ˆβ recovers β⋆up to error δ w.h.p.:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εh (α) p + o (p)\n\u0011\n,\nas n, p, s, d →+∞.\nThe proof of Theorem 1, given in appendix A.1, uses large deviation techniques to bound the\nprobability of a high-error support to have a lower MSE than the true one, then a union bound over\nsuch supports. We give below a brief proof sketch of Theorem 1.\nProof sketch. Let S denote the set of supports of cardinality s and S⋆= Supp (β⋆). For any S ∈S,\nwe denote by 1S the vector in {0, 1}p such that [1S]i = 1 (i ∈S) for all i ∈[p]. We define the loss\nfunction L over S such that L (S) := ∥Y −X1S∥2\n2, so that Supp\n\u0010\nˆβ\n\u0011\n= arg minS∈S L (S). As\np gets large, the event “L (S) < L (S⋆)” for any S such that |S △S⋆| ≥2δs is a rare event. The\nChernoff bound yields:\nlog P\n\u0010\nL (S) < L (S⋆)\n\u0011\n≤n\n2\n\u0012\nlog\n\u00122σ2p\nδds\n\u0013\n+ o (1)\n\u0013\n.\nThis step involves most of the technical work. Then, by union bound:\nP\n\u0010\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△S⋆\f\f\f < 2δs\n\u0011\n≥1 −\nX\nS : |S △S⋆|≥2δs\nP (L (S) < L (S⋆)) ≥1 −\n\u0012p\ns\n\u0013 \u00122σ2p\nδds\n\u0013n/2\n.\nSolving for n, we obtain a critical threshold of n⋆=\nlog(\np\ns)\nlog(ds/p)+log(δ/(2σ2)). We conclude.\nBringing together Theorem 1 with the necessary conditions shown by Wang et al. in [19], we obtain\nthe following corollary.\nCorollary 2 (Information-theoretic phase transition). The sparse recovery in the sparse setting\nproblem exhibits a phase transition at an information-theoretic threshold nSP\nINF.\n5\n"
    },
    {
      "page_number": 6,
      "text": "1. In the first regime considered above, the expression of nSP\nINF is given by:\nnSP\nINF := 2s log (p/s)\nlog (ds/p) .\n2. In the second regime considered above, the expression of nSP\nINF is given by:\nnSP\nINF := 2h (α) p\nlog d .\nSpecifically, in each of these regimes:\n(i) If there exists ε > 0 such that n ≤(1 −ε) nSP\nINF then, as n, p, s, d →+∞, there exists no decoder\ng: Rn →{β ∈{0, 1}p : ∥β∥0 = s} such that:\nmax\nβ⋆∈{0,1}p, ∥β⋆∥0=s PX,Z\n\u0010\ng (Y ) ̸= Supp (β⋆)\n\u0011\n→0.\nIn this sense, it is information-theoretically impossible to ensure an asymptotically reliable recovery.\n(ii) If there exists ε > 0 such that n ≥(1 + ε) nSP\nINF, then as n, p, s, d →+∞:\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f\n2s\n−→0,\nin probability. In this sense, the MLE (7) ensures an asymptotically reliable recovery.\nThe proof of Corollary 2 is given in appendix A.2. Statement (i) is due to Wang et al. [19], while\nstatement (ii) follows from Theorem 1 and is the main contribution of this section.\nRemark 2.2 (Limitations). We note that the definition of “recovery” is not the same in statements (i)\nand (ii) of Corollary 2. The necessity statement (i) is about the impossibility a vanishing probability\nof exact equality of supports, which is stronger than the sufficiency statement of vanishing rescaled\nerror in (ii). In particular, it is possible in theory that both statements hold simultaneously. To the best\nof our knowledge, the literature on the dense setting also suffers from this gap (see [8, 18]). While\nReeves et al. [16] establish a clean information-theoretic phase transition in the dense setting (using\nthe same definition of “recovery” for both bounds), their analysis considers different problem settings\nand convergence guarantees than ours.\nWe interpret Theorem 1 and Corollary 2 as follows.\n• Phase transition. For simplicity, we only discuss the sublinear sparsity regime, defined by s = o (p).\nPrevious works on sparse recovery in the dense case ([16],[8]) have shown the existence of an\ninformation-theoretic threshold:\nnINF = 2s log (p/s)\nlog s\n,\n(8)\nat which the complexity of support recovery in terms of sample size exhibits a phase transition, where\nthe recovery of any fraction of the support is impossible for n ≤(1 −ε) nINF, and full recovery\nis guaranteed by the MLE for n ≥(1 + ε) nINF. In light of this, we ask if the support recovery\nproblem for the class of sparse measurement matrices described above exhibits a similar behavior.\nIn Corollary 2, we show that indeed, it exhibits a similar phase transition at an information-theoretic\nthreshold given by:\nnSP\nINF = 2s log (p/s)\nlog (ds/p) .\n(9)\nIn Table 1, we summarize these information-theoretic thresholds alongside known algorithmic\nthresholds for the sublinear sparsity regime\n\u0000s = o (p)\n\u0001\n, highlighting the comparison between\ndense and sparse measurements in the high-SNR setting.\n• Price of Sparsity. In particular, we notice that nSP\nINF ≥nINF. This confirms the intuition that sparse\nrecovery requires more samples in the sparse measurement case: in fact, due to the sparsity of the\nmeasurement matrix, there is a low probability for the coefficient of a component of the signal at a\ngiven observation to be non-zero, and hence the need for a larger sample size for recovery. In light\n6\n"
    },
    {
      "page_number": 7,
      "text": "Table 1: Comparison of Sample Complexity Thresholds for Sublinear Sparsity\n\u0000s = o (p)\n\u0001\n.\nMeasurement\nInfo-Theoretic\nNecessary\nInfo-Theoretic\nSufficient\nAlgorithmic\nNecessary\nAlgorithmic\nSufficient\nDense\n2s log(p/s)\nlog s\n[16, 19]\n2s log(p/s)\nlog s\n[8, 16]\n2s log(p −s) [8]\n2s log(p −s) [18]\nSparse,\nhigh SNR\n2s log(p/s)\nlog(ds/p) [19]\n2s log(p/s)\nlog(ds/p) (Thm 1)\nUnknown\n2s log(p −s) [15]†\n†Holds under the slow decay of sparsity assumption in [15], see (2).\nof this, Corollary 2 is to be interpreted as providing an exact value for the price of sparsity, i.e. the\nextra amount of observations required in the sparse setting compared to the dense one, which is\ngiven by:\nΓ := nSP\nINF\nnINF\n=\nlog s\nlog (ds/p) > 1.\n• Dependence on d and s. Note that the expression of the price of sparsity heavily depends on the\nregimes of d and s. The smaller the density rate d/p, the more “expensive” the desired sparsity of\nthe measurements is, as suggested by the expression of Γ. In particular, Γ could take any value in\n(1, +∞), depending on the regimes of d and s w.r.t. p.\nExample 2.1. Consider the setting where s = pα, d = pβ with α, β ∈(0, 1) such that α + β > 1.\nThen Γ = α/ (α + β −1) ∈(1, +∞) , which goes to 1 when β goes to 1 (low measurement\nsparsity), and goes to +∞when α is fixed and β goes to 1 −α (high measurement sparsity).\n• Measurement sparsity vs. Sampling complexity trade-off. In this context, we conclude the\nexistence of a measurement sparsity vs. sampling complexity trade-off, which can also be interpreted\nas a trade-off between sampling complexity and computational cost. We consider an example that\nhighlights this trade-off.\nExample 2.2. Let ψ: [e, +∞) −→[e, +∞) such that ψ (x) = x/ log x. Consider two mea-\nsurement matrices: X1 a dense Gaussian in Rn1×p and X2 a sparse Gaussian Rn2×p with\nonly d = min\n\u0000po(1), ψ−1 (o (ψ (p)))\n\u0001\nexpected non-zero entries per row; and an s-sparse signal\nβ⋆∈Rp, in the linear sparsity regime where s = αp for constant α ∈(0, 1). On one hand, the\nnumber of samples required for reliable recovery raises from n1 = nINF = Θ\n\u0000p/ log p\n\u0001\nin the\ndense case to n2 = nSP\nINF = Θ\n\u0000p/ log d\n\u0001\nin the sparse one. On the other hand the computational\ncost of recovery the support is better in the sparse case, as matrix-vector multiplication cost drops\nfrom n1p = Θ\n\u0000p2/ log p\n\u0001\nto n2d = Θ\n\u0000pd/ log d\n\u0001\n. This highlights a trade-off between sampling\ncomplexity and computational cost. Proofs of these statements are given in appendix A.3.\n• Allowing for more sparsity. Note that the sparsity assumption under which Theorem 1 guarantees\nreliable recovery when there exists ε > 0 such that n ≥(1 + ε) nSP\nINF, which is:\nds/p →+∞,\n(10)\nis weaker than the sparsity assumption of the sufficient algorithmic threshold of Omidiran and\nWainwright [15] which guarantees polynomial-time recovery if there exists ε > 0 such that\nn ≥(1 + ε) nALG. As they show, this holds under the assumption that:\n\u0012d\np\n\u00133\nmin\n\u001a\ns, log log (p −s)\nlog (p −s)\n\u001b\n→+∞.\nFor example, when s = Θ (p), this requires that d = ω\n\u0000p2/3\u0001\n, while our result holds under the\nweaker assumption of d = ω (1). Our result allows for a significantly better sparsity, but this comes\nat the cost of potential super-polynomial computational complexity, since computing the MLE (7)\nis exponential-time in general.\n3\nImproving Sparse Recovery via Sparsification\n3.1\nSetting\nLet n, p, s, d ∈N and β⋆∈{0, 1}p s-sparse, defined as in Section 2.1. Let X ∈Rn×p such that\n(Xi,j)i∈[n],j∈[p]\ni.i.d.\n∼N (0, 1) and Z ∼N\n\u00000, σ2In\n\u0001\n, with σ > 0 constant. Let Y := Xβ⋆+ Z ∈Rn.\n7\n"
    },
    {
      "page_number": 8,
      "text": "Let (Bij)i∈[n],j∈[p]\ni.i.d.\n∼Ber (d/p). We define the following sparsified version of X:\n˜X ∈Rn×p such that\n˜Xij := BijXij, ∀i ∈[n], j ∈[p].\n(11)\nIn addition, we define a rescaled version of Y as follows:\n˜Y := d\np Y ∈Rn.\n(12)\nAn estimator of β⋆is defined by the random vector:\nˆβ :=\narg min\nβ∈{0,1}p, ∥β∥0=s\n\r\r\r ˜Y −˜Xβ\n\r\r\r\n2\n2 .\n(13)\nThat is, the observations are generated with a dense measurement matrix (X), but the signal is\nrecovered using a sparsification of that matrix ( ˜X). We formalize the problem we address below as\nfollows: given an error tolerance δ ∈(0, 1), we wish to determine the minimum number of samples n\nin terms of p, s and d required so that:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n−→1, as n, p, s, d →+∞.\n3.2\nResults\nOur second main result, Theorem 3, provides a sufficient condition on the sample size for reliable\nsupport recovery after sparsifying an originally dense measurement matrix.\nTheorem 3 (Sufficient conditions for sparse recovery using sparsified measurements). Suppose that\np →+∞and there exist α, ψ ∈(0, 1) such that s = αp and d = ψp. Let δ ∈(0, 1). Let:\nn⋆\nSp-ified :=\n2h (α) p\nlog\n\u0010\n1 +\nδψ2\n(1−ψ)(2−δ(1−ψ))\n\u0011.\n(14)\nIf there exists ε > 0 such that n ≥(1 + ε) n⋆\nSp-ified, then, under Conjecture B.1, ˆβ recovers β⋆up to\nerror δ w.h.p.:\nP\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εh (α) p + o (p)\n\u0011\n,\nas n, p, s, d →+∞.\nRemark 3.1 (Limitations). We show that this result holds under a mild uniform-integrability\nconjecture. The statement of the conjecture is deferred to appendix B (Conjecture B.1) as it requires\ndelving into technical details of the proof. We expect it to follow from standard concentration bounds\nas all relevant random terms are sub-Gaussian, and verification is work in progress.\nThe proof of Theorem 3 is given in appendix B. It follows the same general outline as the proof\nof Theorem 1, but is much more technically involved. In particular, deriving the large-deviation\nbound is harder because the MSE no longer decomposes neatly: ˜Y does not correspond to the true\nobservations of the sparsified data but rather to a rescaling of the original observations (12). Instead\nof repeating a sketch here, we simply refer the reader back to the earlier proof sketch (see section 2.3).\nWe interpret Theorem 3 as follows.\n• Arbitrary sparsification rate. According to Theorem 3, support recovery is possible for arbitrarily\naggressive sparsification, i.e. arbitrarily small ψ, provided a large enough sample size.\n• Strong-sparsification regime. In the strong-sparsification regime where ψ →0, the denominator\nof n⋆\nSp-ified in (14) is effectively δψ2/ (2 −δ), and hence the sufficient condition upper bound writes:\nnSp-ified\nINF\n= 2 (2 −δ) h (α) p\nδψ2\n= Θ\n\u0012 p\nψ2\n\u0013\n.\n(15)\n• Price of Sparsification. We interpret our result as providing a value for the price of sparsification, i.e.\nthe extra amount of observations required due to the information loss resulting from sparsification.\nIn the linear sparsity and strong-sparsification regime, it writes:\nΓSp-cation := nSp-ified\nINF\nnINF\n=\nΘ\n\u0000p/ψ2\u0001\nΘ (p/ log p) = Θ\n\u0012log p\nψ2\n\u0013\n.\n8\n"
    },
    {
      "page_number": 9,
      "text": "Unlike the intrinsically-sparse-observations setting studied in Section 2, this extra amount of\nrequired observations is not due to the sparsity of the measurements. In fact, one can check\nfrom (24) that in the linear sparsity regime where d = Θ (p), the price of sparsity is constant (i.e.\nΓ = Θ (1)). Instead, the price of sparsification is due to a bias in the observations that we explain\nby the fact that the sparsified observations ˜Y were not obtained as noisy projections of the true\nsignal as in the original model (5), but rather via a naïve rescaling of the original observations (12).\nBy simply rescaling the observations we did not discard the information in Y coming from the\nnullified components of X, hence introducing a bias.\n• Sparsification budget. Given dense data and a fixed large enough sample size n, by up to how\nmuch could we sparsify the data and still get recovery? We call this the sparsification budget.\nAccording to our result (15), its expression in the strong-sparsification regime is given by:\nψbudget = Θ\n\u0010p\np/n\n\u0011\n.\nIn particular, the above expression only makes sense when n = Ω(p), below which Theorem 3\ndoes not hold.\n4\nConclusion and Future Work\nIn the first part of this paper, we have studied the problem of recovery of a binary signal β⋆∈{0, 1}p\nbased on a sparse measurement matrix and noisy observations. Our main result is that, if the\nmeasurements have density rate d/p then, assuming that the measurements and the signal are together\nnot too sparse – in particular if ds = ω (p), i.e. high-SNR regime – it is possible to recover the\ntrue support asymptotically when the sample size is larger than the threshold given by Theorem 1.\nCombining our work with the necessary conditions of Wang el al. [19], we reveal an information-\ntheoretic phase transition. The expression of the phase-transition threshold makes the price of sparsity\nexplicit, revealing a precise trade-off between sampling complexity and measurement sparsity. In\nthe following, we present a quick summary of all – to the best of our knowledge – results on sparse\nrecovery in the sparse measurement setting, along with some future work directions.\n• Information-theoretic threshold, sufficient conditions. In Theorem 1, we establish a sufficient\ncondition for reliable recovery. However, this result is conditional on the high-SNR (ds/p →+∞)\nassumption. This raises the question of sufficient conditions when the measurements and signal are\neven more sparse.\n• Informational-theoretic threshold, necessary conditions.\n– Wang et al. [19] have studied this problem. Their work reveals three regimes of behavior\ndepending on the scaling of the expected number of non-zeros of β⋆aligning with non-zeros of a\nrow of X: ds/p = ω (1), ds/p →τ for some τ > 0, and ds/p = o (1). For their model, where\nthe variance of the non-zero components of X scales in a way that makes the second moment of\nthe projected signal Xβ⋆remain the same as in the dense case: the necessary condition threshold\nis on the order of magnitude of the one in the dense case in the regime where ds/p = ω (1),\nwhile it increases dramatically in the regime where ds/p = o (1).\n– In the dense setting, Reeves et al. [16] have shown that even the recovery of a fixed fraction of\nthe support is information theoretically impossible below the phase transition threshold: this is\nwhat they call the all-or-nothing property. It would be interesting to extend this property to the\nsparse setting.\n• Algorithmic threshold, sufficient conditions. Omidiran and Wainwright [15] have shown that under\na low-sparsity assumption on the measurements, the sufficient condition of the dense setting, i.e.\nn ≥(1 + ε) nALG, is sufficient for the sparse setting as well. It would be interesting to explore the\nquestion of polynomial-time recovery in a stronger sparsity regime.\n• Algorithmic threshold, necessary conditions. Although we cannot really hope to provide necessary\nconditions for polynomial-time recovery – unless conditionally on P ̸= NP – it would be interesting\nto provide a threshold under which the problem is believed to be algorithmically hard, as done by\nGamarnik and Zadik [8] in the dense setting.\nIn the second part of this paper, we have studied the problem of recovering the signal based on\nsparsified – but originally dense – measurements and accordingly-rescaled observations. Our main\n9\n"
    },
    {
      "page_number": 10,
      "text": "result is that, in the linear sparsity and linear sparsification regime where s = αp and d = ψp for\nconstant α, ψ ∈(0, 1), it is possible to recover the true support asymptotically when the sample\nsize is larger than a threshold given by Theorem 3. This reveals that support recovery is possible\nfor arbitrarily aggressive sparsification provided a large enough sample size, and provides an upper\nbound on the price of sparsification.\nNevertheless, we believe that the sparsification problem is infeasible for strong enough regimes of\nsparsification. In particular, we conjecture that the recovery is information-theoretically impossible\nno matter the sample size in the sub-linear sparsification regime where d = o (p). We leave the\nexploration of this regime for future work.\nAcknowledgments and Disclosure of Funding\nThis work was supported by the National Science Foundation (NSF) under grant CISE-2233897.\nYoussef Chaabouni thanks Soulef Frikha for long-standing mathematical inspiration, and Mehdi\nMakni for many years of insightful discussions. He also thanks Marouane Nejjar, Panos Tsimpos, and\nMalo Lahogue for valuable feedback and exchanges.\nReferences\n[1] Shuchin Aeron, Venkatesh Saligrama, and Manqi Zhao. Information theoretic bounds for\ncompressed sensing. IEEE Transactions on Information Theory, 56(10):5111–5130, 2010.\n[2] Emmanuel J Candès, Justin Romberg, and Terence Tao. Robust uncertainty principles: exact\nsignal reconstruction from highly incomplete frequency information. IEEE Transactions on\ninformation theory, 52(2):489–509, 2006.\n[3] Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by\nbasis pursuit. SIAM review, 43(1):129–159, 2001.\n[4] Graham Cormode and Marios Hadjieleftheriou. Finding the frequent items in streams of data.\nCommunications of the ACM, 52(10):97–105, 2009.\n[5] David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289–\n1306, 2006.\n[6] Ding-Zhu Du and Frank Kwang-ming Hwang. Combinatorial group testing and its applications,\nvolume 12. World Scientific, 1999.\n[7] Simon Foucart, Holger Rauhut, Simon Foucart, and Holger Rauhut. An invitation to compressive\nsensing. Springer, 2013.\n[8] David Gamarnik and Ilias Zadik. Sparse high-dimensional linear regression. estimating squared\nerror and a phase transition. The Annals of Statistics, 50(2):880–903, 2022.\n[9] Anna Gilbert and Piotr Indyk. Sparse recovery using sparse matrices. Proceedings of the IEEE,\n98(6):937–947, 2010.\n[10] Trevor Hastie. The elements of statistical learning: data mining, inference, and prediction, 2009.\n[11] Piotr Indyk. Sketching, streaming and sublinear-space algorithms. Graduate course notes,\navailable at, 33:617, 2007.\n[12] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing\ndata: Provable guarantees with non-convexity. Advances in neural information processing\nsystems, 24, 2011.\n[13] Alan Miller. Subset selection in regression. chapman and hall/CRC, 2002.\n[14] Shanmugavelayutham Muthukrishnan et al. Data streams: algorithms and applications. Founda-\ntions and Trends® in Theoretical Computer Science, 1(2):117–236, 2005.\n10\n"
    },
    {
      "page_number": 11,
      "text": "[15] Dapo Omidiran and Martin J Wainwright. High-dimensional subset recovery in noise: sparsified\nmeasurements without loss of statistical efficiency. arXiv preprint arXiv:0805.3005, 2008.\n[16] Galen Reeves, Jiaming Xu, and Ilias Zadik. The all-or-nothing phenomenon in sparse linear\nregression. In Conference on Learning Theory, pages 2652–2663. PMLR, 2019.\n[17] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal\nStatistical Society Series B: Statistical Methodology, 58(1):267–288, 1996.\n[18] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery\nusing ℓ1-constrained quadratic programming (lasso). IEEE transactions on information theory,\n55(5):2183–2202, 2009.\n[19] Wei Wang, Martin J Wainwright, and Kannan Ramchandran. Information-theoretic limits on\nsparse signal recovery: dense versus sparse measurement matrices. IEEE Transactions on\nInformation Theory, 56(6):2967–2979, 2010.\n11\n"
    },
    {
      "page_number": 12,
      "text": "A\nSparse Recovery using Sparse Measurements: Proofs\nA.1\nProof of Theorem 1\nProof of Theorem 1. For any i ∈[n], we denote by Xi := (Xij)j∈[p], Bi := (Bij)j∈[p], Ni :=\n(Nij)j∈[p]. We denote by S⋆:= Supp (β⋆) the support of β⋆. Let S := {S ⊂[p] : |S| = s}. We\ndefine the function:\nL : S −→[0, +∞)\nS 7−→∥Y −X1S∥2\n2 ,\nwhere 1S denotes the vector in {0, 1}p such that [1S]j = 1 (j ∈S) for all j ∈[p]. Note that, since\nX and Y are random, L(S) is a random variable for every S ∈S. In addition, note that:\nL(S) = ∥Z∥2\n2 + ∥X (1S⋆−1S)∥2\n2 + 2⟨Z, X (1S⋆−1S)⟩\n∀S ∈S,\nand, in particular:\nL(S⋆) = ∥Z∥2\n2 =\nn\nX\ni=1\nZ2\ni .\nFix S ∈S such that M := |S △S⋆| /2 ≥δs, and let U := S⋆\\ S, V := S \\ S⋆. Note that\n|U| = |V | = M. We define:\n∆:= L(S) −L(S⋆).\nProposition A.1. As n, p, s, d →+∞:\nP (∆≤0) ≤\n\u00122σ2p\nδds\n\u0013n/2\neo(n).\nProof. See appendix A.1.1.\nHence, we obtain:\nP\n\u0010\n∥Y −X1S∥2\n2 ≤∥Y −X1S⋆∥2\n2\n\u0011\n≤\n\u00122σ2p\nδds\n\u0013n/2\neo(n),\n(16)\nfor any S ∈{0, 1}p such that |S| = s and |S △S⋆| ≥2δs.\nUsing (16) and the union bound over the set of supports S s.t. |S △S⋆| ≥2δs, we obtain:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥PX,Z\n\u0010\n∥Y −X1S∥2\n2 > ∥Y −X1S⋆∥2\n2 , ∀S : |S △S⋆| ≥2δs\n\u0011\n= 1 −PX,Z\n\u0010\n∃S : |S △S⋆| ≥2δs, ∥Y −X1S∥2\n2 ≤∥Y −X1S⋆∥2\n2\n\u0011\nU.B.\n≥1 −\nX\nS : |S △S⋆|≥2δs\nPX,Z\n\u0010\n∥Y −X1S∥2\n2 ≤∥Y −X1S⋆∥2\n2\n\u0011\n≥1 −\n\u0012p\ns\n\u0013 \u00122σ2p\nδds\n\u0013n/2\neo(n).\nFirst regime: s = o (p).\nUsing the Corollary of Stirling:\nlog\n\u0012p\ns\n\u0013\n= s log (p/s) (1 + o (1)) ,\nin the RHS of the inequality above, we obtain:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0014\ns log (p/s) (1 + o (1)) −n\n2\n\u0012\nlog\n\u0012 δds\n2σ2p\n\u0013\n+ o (1)\n\u0013\u0015\n.\n12\n"
    },
    {
      "page_number": 13,
      "text": "Let n⋆\nslin :=\n2s log(p/s)\nlog(ds/p)+log(δ/(2σ2)). Then if n ≥(1 + ε) n⋆\nslin for some constant ε > 0, we have:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0014\ns log (p/s) (1 + o (1)) −(1 + ε) n⋆\nslin\n2\n\u0012\nlog\n\u0012 δds\n2σ2p\n\u0013\n+ o (1)\n\u0013\u0015\n= 1 −exp\n\u0014\ns log (p/s)\n\u0012\n−ε + o (1) −\n1 + ε\nlog (ds/p) + log (δ/(2σ2)) o (1)\n\u0013\u0015\n= 1 −exp\n\u0010\ns log (p/s)\n\u0000−ε + o (1)\n\u0001\u0011\n.\nHence, as n, p, s, d →+∞:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εs log (p/s) + o\n\u0000s log (p/s)\n\u0001\u0011\n−→1.\nSecond regime: s = αp , α ∈(0, 1).\nUsing the Corollary of Stirling:\nlog\n\u0012p\ns\n\u0013\n= ph (α) (1 + o (1)) ,\nwe get:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0014\nph (α) (1 + o (1)) −n\n2\n\u0012\nlog\n\u0012 δds\n2σ2p\n\u0013\n+ o (1)\n\u0013\u0015\n.\nSimilarly to above, we take n⋆\nlin :=\n2H(α)p\nlog d+log(δα/(2σ2)). If n ≥(1 + ε) n⋆\nlin for some constant ε > 0,\nthen we obtain, as p, s, d →+∞:\nPX,Z\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0010\n−εh (α) p + o (p)\n\u0011\n−→1,\nconcluding the proof.\nA.1.1\nProof of Proposition A.1\nProof of Proposition A.1. We have:\n∆:= L(S) −L(S⋆)\n= ∥X (1S⋆−1S)∥2\n2 + 2⟨Z, X (1S⋆−1S)⟩\n=\nn\nX\ni=1\n⟨Xi, 1S⋆−1S⟩2 + 2\nn\nX\ni=1\nZi⟨Xi, 1S⋆−1S⟩.\nWe denote by (∆i)i∈[n] the terms of the sum in the above expression, that is:\n∆i := ⟨Xi, 1S⋆−1S⟩2 + 2Zi⟨Xi, 1S⋆−1S⟩.\nNote that (∆i)i∈[n] are i.i.d. and ∆= Pn\ni=1 ∆i.\nNow using the Chernoff bound:\nP (∆≤0) = P (−∆≥0) = inf\nθ≥0 P\n\u0000e−θ∆≥1\n\u0001\n≤inf\nθ≥0 M−∆i(θ)n.\n(17)\nWe now study the moment generating function of −∆i, i.e. M−∆i(·). We have:\nM−∆i(θ) = EXi,Zi\nh\ne−θ[⟨Xi,1S⋆−1S⟩2+2Zi⟨Xi,1S⋆−1S⟩]i\n= EXi\nh\ne−θ⟨Xi,1S⋆−1S⟩2EZi\nh\ne−2θZi⟨Xi,1S⋆−1S⟩\f\fXi\nii\n= EXi\nh\ne−θ⟨Xi,1S⋆−1S⟩2MZi|Xi (−2θ⟨Xi, 1S⋆−1S⟩)\ni\n= EXi\nh\ne−θ⟨Xi,1S⋆−1S⟩2e\n1\n2 (−2θ⟨Xi,1S⋆−1S⟩)2σ2i\n= EXi\nh\ne(−θ+2θ2σ2)⟨Xi,1S⋆−1S⟩2i\n= EXi\nh\ne(−θ+2θ2σ2)(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\n.\n13\n"
    },
    {
      "page_number": 14,
      "text": "Plugging this expression into (17), we obtain:\nlog P (∆≤0) ≤n inf\nθ≥0 log EXi\nh\ne(−θ+2θ2σ2)(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\n.\nStudying the function θ 7→−θ + 2θ2σ2 on R≥0 leads to the change of variable:\ninf\nθ≥0 log EXi\nh\ne(−θ+2θ2σ2)(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\n(18)\n=\ninf\nθ∈(−∞,1/(8σ2)] log EXi\nh\ne−θ(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\n,\n(19)\nOne can check that the function θ 7→log EXi\nh\ne−θ(\nP\nj∈U Xij−P\nj∈V Xij)\n2i\nis non-increasing over\n(−∞, 1/\n\u00008σ2\u0001\n]. Hence (19) is equal to:\nlog EXi\nh\ne−(\nP\nj∈U Xij−P\nj∈V Xij)\n2/(8σ2)i\n.\nTherefore, the Chernoff bound yields:\nlog P (∆≤0) ≤n log EXi\nh\ne−(\nP\nj∈U Xij−P\nj∈V Xij)\n2/(8σ2)i\n.\n(20)\nSince U ∩V = ∅, we have:\nX\nj∈U\nXij −\nX\nj∈V\nXij\nd=\nX\nj∈U∪V\nXij.\nTherefore:\nE\nh\ne−(\nP\nj∈U Xij−P\nj∈V Xij)\n2/(8σ2)i\n= E\nh\ne−(\nP\nj∈U∪V Xij)\n2/(8σ2)i\n= E\nh\ne−(\nP\nj∈U∪V BijNij)\n2/(8σ2)i\n= EBi\nh\nENi\nh\ne−(\nP\nj∈U∪V BijNij)\n2/(8σ2) \f\f Bi\nii\n= EBi\n\nENi\n\ne\n−\n P\nj∈U∪V Bij Nij\n√P\nj∈U∪V Bij\n!2\n×\nP\nj∈U∪V Bij\n8σ2\n\f\f\f\f\f Bi\n\n\n\n.\nIn addition, conditionally on Bi, we have:\nΓ :=\n\n\nP\nj∈U∪V BijNij\nqP\nj∈U∪V Bij\n\n\n2\nd= χ2(1).\nIts MGF is:\nE\n\u0002\netΓ | Bi\n\u0003\n= MΓ|Bi (t) =\n1\n√1 −2t, for t < 1/2.\nHence:\nEBi\n\nENi\n\ne\n−\n P\nj∈U∪V Bij Nij\n√P\nj∈U∪V Bij\n!2\n×\nP\nj∈U∪V Bij\n8σ2\n\f\f\f\f\f Bi\n\n\n\n= EBi\n\u0014\nMΓ|Bi\n\u0012\n−\nP\nj∈U∪V Bij\n8σ2\n\u0013\u0015\n= EBi\n\n\n2σ\nq\n4σ2 + P\nj∈U∪V Bij\n\n.\nLet U[δs] and V[δs] respectively denote the sets of δs smallest elements of U and V . Note that this\ndefinition is legitimate since |U| = |V | = M ≥δs. Since Bij ≥0 for all j ∈U ∪V , we have:\nX\nj∈U∪V\nBij ≥\nX\nj∈U[δs]∪V[δs]\nBij,\n14\n"
    },
    {
      "page_number": 15,
      "text": "and hence:\nEBi\n\n\n2σ\nq\n4σ2 + P\nj∈U∪V Bij\n\n≤EBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n.\nTherefore, we get:\nE\nh\ne−(\nP\nj∈U Xij−P\nj∈V Xij)\n2/(8σ2)i\n≤EBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n,\nand plugging this into (20) yields:\nlog P (∆≤0) ≤n log\n\nEBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n\n\n.\n(21)\nNow note that, for any i ∈[n]:\nX\nj∈U[δs]∪V[δs]\nBij\nd= Bin\n\u0012\n2δs, d\np\n\u0013\n,\nIn addition, since d = o (p) and ds/p →+∞, we have:\nLemma A.1. For any i ∈[n], the following holds: as p →+∞,\n1\np\n2δds/p\n\n\nX\nj∈U[δs]∪V[δs]\nBij −2δds/p\n\n\ndist\n−→N (0, 1) .\nProof. The proof is a simple adaptation of the proof of the Central Limit Theorem. See appendix\nA.1.2.\nHence, there exists a choice of the underlying probability space and random variables (Bij)i∈[n],j∈[p]\nfor which the random variable above converges almost surely and:\nN :=\nlim\np→+∞\n1\np\n2δds/p\n\n\nX\nj∈U[δs]∪V[δs]\nBij −2δds/p\n\n∼N (0, 1) .\nThe above yields:\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n=\n2σ\nr\n4σ2 + 2δds/p +\np\n2δds/p N + o\n\u0010p\nds/p\n\u0011.\nLet:\nVp :=\n2σ\np\nds/p\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n.\nThen we have:\nVp =\n2σ\np\nds/p\nr\n4σ2 + 2δds/p +\np\n2δds/p N + o\n\u0010p\nds/p\n\u0011\n=\n2σ\nr\n2δ + 4σ2\nds/p +\nq\n2δ\nds/pN + o\n\u0010\n1\n\u000ep\nds/p\n\u0011.\nSince ds/p →+∞as p →+∞, the above yields:\nVp −→2σ\n√\n2δ\n=\nr\n2σ2\nδ .\nIn addition, we note the following:\n15\n"
    },
    {
      "page_number": 16,
      "text": "Lemma A.2. Vp is uniformly integrable, that is: there exists p′ ∈N such that\nlim\nT →+∞sup\np≥p′ E\n\u0002\n|Vp| 1{|Vp|>T }\n\u0003\n= 0.\nProof. See appendix A.1.3.\nTherefore, we get:\nlim\np→+∞E [Vp] =\nr\n2σ2\nδ .\nHence, we write:\nEBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n=\nr\n2σ2p\nδds + o\n \u0012ds\np\n\u0013−1/2!\n.\nWe conclude:\nlog P (∆≤0) ≤n log\n\nEBi\n\n\n2σ\nq\n4σ2 + P\nj∈U[δs]∪V[δs] Bij\n\n\n\n\n= n log\n r\n2σ2p\nδds + o\n \u0012ds\np\n\u0013−1/2!!\n= n\n \nlog\n r\n2σ2p\nδds\n!\n+ log\n\u0010\n1 + o (1)\n\u0011!\n= n\n \nlog\n r\n2σ2p\nδds\n!\n+ o (1)\n!\n= n log\n r\n2σ2p\nδds\n!\n+ o(n),\nwhich yields the desired result:\nP (∆≤0) ≤\n\u00122σ2p\nδds\n\u0013n/2\neo(n).\nA.1.2\nProof of Lemma A.1\nProof of Lemma A.1. Let:\nNp :=\n1\np\n2δds/p\n\n\nX\nj∈U[δs]∪V[δs]\nBij −2δds/p\n\n.\n16\n"
    },
    {
      "page_number": 17,
      "text": "Its characteristic function writes:\nΦNp (t)\n= E\n\u0002\neitNp\u0003\n= e−it√\n2δds/p \u0010\nE\nh\nexp\n\u0010\nitBij/\np\n2δds/p\n\u0011i\u00112δs\n= e−it√\n2δds/p \u0010\n1 −d/p + d/p exp\n\u0010\nit/\np\n2δds/p\n\u0011\u00112δs\n= e−it√\n2δds/p \u0010\n1 + d/p\n\u0010\nexp\n\u0010\nit/\np\n2δds/p\n\u0011\n−1\n\u0011\u00112δs\n= e−it√\n2δds/p\n \n1 + d/p\n \nit\np\n2δds/p\n−\nt2\n4δds/p + O\n \n−it3\n(2δds/p)3/2\n!!!2δs\n= e−it√\n2δds/p\n \n1 + d/p\n \nit\np\n2δds/p\n−\nt2\n4δds/p + O\n \n1\n(2δds/p)3/2\n!!!2δs\n= exp\n \n−it\np\n2δds/p + 2δs log\n \n1 + d/p\n \nit\np\n2δds/p\n−\nt2\n4δds/p + O\n \n1\n(2δds/p)3/2\n!!!!\n= e−it√\n2δds/p exp\n \n2δs\n \nd/p\n \nit\np\n2δds/p\n−\nt2\n4δds/p + O\n \n1\n(2δds/p)3/2\n!!\n+ O\n\u0012 d\nsp\n\u0013!!\n= exp\n \n−it\np\n2δds/p + it\np\n2δds/p −t2/2 + O\n \n1\np\n2δds/p\n!\n+ O\n\u00122δd\np\n\u0013!\n= exp\n\u0000−t2/2 + o (1)\n\u0001 p→+∞\n−→exp\n\u0000−t2/2\n\u0001\n= ΦN(0,1) (t) ,\nfor all t ∈R. The result follows.\nA.1.3\nProof of Lemma A.2\nProof of Lemma A.2. Fix T > 2. We have:\nVp =\n2σ\np\nds/p\nq\n4σ2 + P\ni∈U[δs]∪V[δs] Bij\n=\n2σ\ns\n2δ + 4σ2\nds/p +\nq\n2δ\nds/pN + o\n\u0012\n1\n√\nds/p\n\u0013.\nBy total probability, we have:\nE [|Vp| 1 {|Vp| > T}] = E\nh\n|Vp| 1 {|Vp| > T}\n\f\f N ≥−(ds/p)1/4i\nP\n\u0010\nN ≥−(ds/p)1/4\u0011\n+ E\nh\n|Vp| 1 {|Vp| > T}\n\f\f N < −(ds/p)1/4i\nP\n\u0010\nN < −(ds/p)1/4\u0011\n.\nLet\nΞ1 := E\nh\n|Vp| 1 {|Vp| > T}\n\f\f N ≥−(ds/p)1/4i\nP\n\u0010\nN ≥−(ds/p)1/4\u0011\n,\nand\nΞ2 := E\nh\n|Vp| 1 {|Vp| > T}\n\f\f N < −(ds/p)1/4i\nP\n\u0010\nN < −(ds/p)1/4\u0011\n,\nso that E [|Vp| 1 {|Vp| > T}] = Ξ1 + Ξ2. We address the two terms separately.\n• First case: N ≥−(ds/p)1/4. We have:\n4σ2\nds/p +\ns\n2δ\nds/pN + o\n \n1\np\nds/p\n!\n≥4σ2\nds/p −\n√\n2δ\n\u0012ds\np\n\u0013−1/4\n+ o\n \n1\np\nds/p\n!\n.\n17\n"
    },
    {
      "page_number": 18,
      "text": "The RHS above goes to 0 as p →+∞, and therefore it is ≥−δ for p large enough, say for all\np ≥p1 for some p1 ∈N. Hence we have:\n|Vp| = Vp =\n2σ\ns\n2δ + 4σ2\nds/p +\nq\n2δ\nds/pN + o\n\u0012\n1\n√\nds/p\n\u0013 ≤2σ\n√\nδ\n,\nfor all p ≥p1. In addition, this implies:\n1 {|Vp| > T} ≤1\n\u001a\u0012 2σ\n√\nδ\n\u0013\n> T\n\u001b\n.\nIn addition, we have:\nP\n\u0010\nN ≥−(ds/p)1/4\u0011\n≤1,\nwhich all together yield:\nΞ1 ≤2σ\n√\nδ\n1\n\u001a\u0012 2σ\n√\nδ\n\u0013\n> T\n\u001b\n.\n• Second case: N < −(ds/p)1/4. We have:\n|Vp| = Vp =\n2σ\np\nds/p\nq\n4σ2 + P\ni∈U[δs]∪V[δs] Bij\n≤\np\nds/p,\nwhich also implies:\n1 {|Vp| > T} ≤1\nnp\nds/p > T\no\n= 1\n\u001a\nT 2 < ds\np\n\u001b\n.\nIn addition, by Gaussian tail bounds we have:\nP\n\u0010\nN < −(ds/p)1/4\u0011\n≤e−\n√\nds/p\n2\n,\nfor p large enough, say for all p ≥p2 for some p2 ∈N. Therefore:\nΞ2 ≤\np\nds/p e−\n√\nds/p\n2\n1\n\u001a\nT 2 < ds\np\n\u001b\n.\nNote that the term\np\nds/p e−\n√\nds/p\n2\nis decreasing as ds/p increases (we have ds/p > T 2 > 2).\nTherefore:\nΞ2 ≤T e−T/2 1\n\u001a\nT 2 < ds\np\n\u001b\n≤T e−T/2.\nWe hence conclude that:\nE [|Vp| 1 {|Vp| > T}] = Ξ1 + Ξ2 ≤2σ\n√\nδ\n1\n\u001a\u0012 2σ\n√\nδ\n\u0013\n> T\n\u001b\n+ T e−T/2,\nfor all p ≥p′ := p1 ∨p2. Therefore we have:\nsup\np≥p′ E\n\u0014\n|Vp| 2σ\n√\nδ\n1 {|Vp| > T}\n\u0015\n≤2σ\n√\nδ\n1\n\u001a\u0012 2σ\n√\nδ\n\u0013\n> T\n\u001b\n+ T e−T/2.\nThis holds for any T > 2. Taking T to +∞, the result follows:\nlim\nT →+∞sup\np≥p′ E [|Vp| 1 {|Vp| > T}] = 0.\n18\n"
    },
    {
      "page_number": 19,
      "text": "A.2\nProof of Corollary 2\nProof of Corollary 2. This proof relies on bringing together Theorem 1 with the following result\nfrom Wang et al. [19].\nTheorem 4 (Necessary condition for sparse ensembles, Corollary 2 of [19]). Let the measurement\nmatrix X ∈Rn×p be drawn with i.i.d. elements from the following distribution:\nXij =\n(\nN\n\u0010\n0, 1\nγ\n\u0011\n,\nw.p. γ\n0,\nw.p. 1 −γ\n,\nfor all i ∈[n], j ∈[p];\n(22)\nwhere γ ∈(0, 1]. Let λ > 0 and\nCp,s (λ) :=\n\u001a\nβ ∈Rp \f\f |Supp (β)| = s,\nmin\ni∈Supp(β) |βi| = λ\n\u001b\n.\nAssume that σ2 = 1. Then, in the regime where γs →+∞, a necessary condition for asymptotically\nreliable recovery over the signal class Cp,s (λ) is given by:\nn >\nlog\n\u0000p\ns\n\u0001\n−1\n1\n2 log (1 + sλ2).\nNote that, while Theorem 4 is not stated on the exact same signal space Cp,s (λ) in [19] but rather on\nthe larger:\n\u001a\nβ ∈Rp \f\f |Supp (β)| = s,\nmin\ni∈Supp(β) |βi| ≥λ\n\u001b\n,\nit follows directly from their result on “restricted ensembles” where the signal components under\nconsideration are set exactly to λ (see section III.A. in [19]). We now proceed to prove Corollary 2.\n(i) We show that (i) holds using Theorem 4, but this requires adapting our problem to the framework\nused by Wang et al. in [19]. In fact, note that the model used in Theorem 4 is different from the one\nwe use in this paper, that we defined in (5). In their model, Wang et al. [19] rescale the non-zero\ncomponents of X by multiplying them by 1/√γ, and require that the noise variance is σ2 = 1.\nTherefore, we cannot directly use Theorem 4 in our setting. However, this difference can be fixed\nby a simple rescaling of our model. Note that our model defined by (5), where X follows the sparse\nGaussian distribution defined in Definition 2.1 and Z ∼N\n\u00000, σ2In\n\u0001\n, can be equivalently written\nas:\nY0 = X0β⋆\n0 + Z0,\nwhere:\nY0 := 1\nσ Y,\nX0 :=\n1\np\nd/p\nX,\nβ⋆\n0 :=\np\nd/p\nσ\nβ⋆,\nand\nZ0 := 1\nσ Z ∼N (0, In) .\nHence, it is a particular case of the model defined in (22), with:\nγ := d/p\nand\nλ :=\n√γ\nσ .\nIn addition, the regime we consider of d = ω (p/s) corresponds exactly to the regime considered\nin Theorem 4 where γs →+∞. Therefore, using Theorem 4, a necessary condition for an\nasymptotically reliable recovery of β⋆in the considered regime is given by:\nn >\nlog\n\u0000p\ns\n\u0001\n−1\n1\n2 log (1 + sλ2) =\n2 log\n\u0000p\ns\n\u0001\n−2\nlog (1 + ds/ (pσ2)) = 2 log\n\u0000p\ns\n\u0001\nlog (ds/p) (1 + o (1)) .\n(23)\n19\n"
    },
    {
      "page_number": 20,
      "text": "First regime: s = o (p).\nUsing the Corollary of Stirling:\nlog\n\u0012p\ns\n\u0013\n= s log (p/s) (1 + o (1)) ,\nthe necessary condition (23) writes:\nn > 2s log (p/s)\nlog (ds/p) (1 + o (1)) .\nLet:\nnSP\nINF := 2s log (p/s)\nlog (ds/p) .\nAssume there exists ε > 0 such that n ≤(1 −ε) nSP\nINF. We know that, for large enough n, p, s, d we\nhave:\nn ≤(1 −ε) nSP\nINF < 2s log (p/s)\nlog (ds/p) (1 + o (1)) ,\nwhich contradicts the necessary condition. Therefore, it is information-theoretically impossible to\nensure a reliable recovery of the support of β⋆.\nSecond regime: s = αp.\nUsing the Corollary of Stirling:\nlog\n\u0012p\ns\n\u0013\n= ph (α) (1 + o (1)) ,\nthe necessary condition (23) writes:\nn > 2h (α) p\nlog d\n(1 + o (1)) .\nLet:\nnSP\nINF := 2h (α) p\nlog d .\nSimilarly to above, we conclude that if there exists ε > 0 such that n ≤(1 −ε) nSP\nINF then it is\ninformation-theoretically impossible to ensure a reliable recovery of the support of β⋆.\n(ii) We show that (ii) holds using Theorem 1.\nFirst regime: s = o (p).\nLet δ > 0 and:\nn⋆\nslin :=\n2s log (p/s)\nlog (ds/p) + log (δ/(2σ2)).\nNote that:\nnSP\nINF = 2s log (p/s)\nlog (ds/p) = n⋆\nslin (1 + o (1)) .\nAssume there exists ε such that n ≥(1 + ε) nSP\nINF. Then:\nn ≥(1 + ε) (1 + o (1)) n⋆\nslin = (1 + ε + o (1)) n⋆\nslin ≥\n\u00001 + ε/2\n\u0001\nn⋆\nslin,\nfor n, p, s, d large enough. Using Theorem 1, we have:\nPX,Z\n\u0012 1\n2s\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f < δ\n\u0013\n≥1 −exp\n\u0010\n−εs log (p/s) + o\n\u0000s log (p/s)\n\u0001\u0011\n.\nTherefore, we obtain:\nPX,Z\n\u0012 1\n2s\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f < δ\n\u0013\n−→1,\nas n, p, s, d →+∞. Since this holds for all δ > 0, we conclude:\nSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n2s\n−→0,\nin probability, as n, p, s, d →+∞.\n20\n"
    },
    {
      "page_number": 21,
      "text": "Second regime: s = αp.\nLet δ > 0 and:\nn⋆\nlin :=\n2h (α) p\nlog d + log (δα/(2σ2)).\nNote that:\nnSP\nINF = 2h (α) p\nlog d\n= n⋆\nlin (1 + o (1)) .\nAssume there exists ε such that n ≥(1 + ε) nSP\nINF. Then:\nn ≤(1 + ε) (1 + o (1)) n⋆\nlin = (1 + ε + o (1)) n⋆\nlin ≥\n\u00001 + ε/2\n\u0001\nn⋆\nlin,\nfor n, p, s, d large enough. Using Theorem 1, we have:\nPX,Z\n\u0012 1\n2s\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f < δ\n\u0013\n≥1 −exp\n\u0010\n−εh (α) p + o (p)\n\u0011\n.\nTherefore, we obtain:\nPX,Z\n\u0012 1\n2s\n\f\f\fSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n\f\f\f < δ\n\u0013\n−→1,\nas n, p, s, d →+∞. Since this holds for all δ > 0, we conclude:\nSupp\n\u0010\nˆβ\n\u0011\n△Supp (β⋆)\n2s\n−→0,\nin probability, as n, p, s, d →+∞.\nA.3\nProof of Example 2.2\nProof of Example 2.2. We have d = min\n\u0000po(1), ψ−1 (o (ψ (p)))\n\u0001\n, hence:\n\u001alog d/ log p = o (1)\nψ (d) /ψ (p) = o (1)\n.\nIn addition:\nn1 = nINF = Θ\n\u0000p/ log p\n\u0001\n,\nn2 = nSP\nINF = Θ\n\u0000p/ log d\n\u0001\n.\nTherefore, we have:\nn2\nn1\n= Θ (p/ log d)\nΘ (p/ log p) = Θ\n\u0012log p\nlog d\n\u0013\n= ω (1) ,\nOn one hand, the number of samples required for reliable recovery is better in the dense case:\nn1 = Θ\n\u0000p/ log p\n\u0001\n= o\n\u0000p/ log d\n\u0001\n= o (n2) .\n(24)\nOn the other hand, the computational cost of recovering the support is better in the sparse case. In\nfact, matrix-vector multiplications are made easier by sparsity: in the dense case, multiplying X1\nwith a vector in Rp costs:\nn1p = Θ\n\u0000p2/ log p\n\u0001\nreal number multiplications, while multiplying X2 with a vector in Rp costs\nn2d = Θ\n\u0000pd/ log d\n\u0001\n= pΘ\n\u0000ψ (d)\n\u0001\n= po\n\u0000ψ (p)\n\u0001\n= o\n\u0000p2/ log p\n\u0001\n= o (n1p) ,\n(25)\nreal number multiplications.\nThis highlights the trade-off between sampling complexity and\ncomputational cost.\n21\n"
    },
    {
      "page_number": 22,
      "text": "B\nImproving Sparse Recovery via Sparsification: Proof of Theorem 3\nProof of Theorem 3. Let S := {S ∈[p]: |S| = s}. We define the error function:\nL : S −→[0, +∞)\nS 7−→\n\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2 .\nNote that:\nL(S) =\n\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2\n=\n\r\r\r\r\nd\npY −˜X1S\n\r\r\r\r\n2\n2\n=\n\r\r\r\r\nd\np (X1S⋆+ Z) −˜X1S\n\r\r\r\r\n2\n2\n=\n\r\r\r\r\nd\npX1S⋆−˜X1S + d\npZ\n\r\r\r\r\n2\n2\n.\nIn particular, we have:\nL(S⋆) =\n\r\r\r\r\n\u0012d\npX −˜X\n\u0013\n1S⋆+ d\npZ\n\r\r\r\r\n2\n2\n.\nLet S ∈S. We define A (S) := S⋆\\ S, B (S) := S \\ S⋆, C (S) := S⋆∩S and M (S) := |A| =\n|B| = |S⋆△S| /2. Note that |C| = s −M. We are interested in bounding the following probability:\nP (L (S) −L (S⋆) ≤0) .\nNote that the term above only depends on S through M. In fact, for any S, S′ ∈S such that\n|S △S⋆| = |S′ △S⋆| we have L (S)\nd= L (S′). In light of this, we define a family of random\nvariables (∆(η))ηs∈[s] indexed by the rescaled symmetric difference η = M/s as L (S) −L (S⋆)\nfor some S such that |S △S⋆| = 2M, that is:\nFor any η ∈\n\u001a i\ns : i ∈[s]\n\u001b\n, ∆(η) := L (S) −L (S⋆) , for some S s.t. |S △S⋆| = 2ηs.\nWe extend the notation above to η ∈[0, 1] by defining ∆(η) := ∆\n\u0010\n⌈ηs⌉\ns\n\u0011\n.\nProposition B.1. Let η ∈(0, 1] and S ∈S such that S △S⋆= ⌈ηs⌉. For any θ ∈(0, +∞), define:\nγ (θ) := 2d2θσ2/p2.\nLet B ∈{0, 1}p denote a random vector such that Bj\ni.i.d.\n∼Ber (d/p). For any θ ∈(0, +∞), we define\nthe following random variables:\nσ2\nU (θ, B) := θ2\nX\nj∈A(S)∪B(S)\nBj,\nσ2\nV (θ, B) := 4d2s\np2\n+\n \n\u00001 + γ (θ)\n\u00012 −4d\n\u00001 + γ (θ)\n\u0001\np\n! X\nj∈A(S)\nBj\n+\n\u00001 −γ (θ)\n\u00012\nX\nj∈B(S)\nBij + 4\n\u0012\n1 −2d\np\n\u0013 X\nj∈C(S)\nBj,\nCov(U,V ) (θ, B) := θ\n\n\n\u0012\n1 + γ (θ) −2d\np\n\u0013 X\nj∈A(S)\nBj −\n\u00001 −γ (θ)\n\u0001 X\nj∈B(S)\nBj\n\n,\nρ (θ, B) :=\nCov(U,V ) (θ, B)\nσU (θ, B) σV (θ, B).\n22\n"
    },
    {
      "page_number": 23,
      "text": "In addition, for any θ ∈(0, +∞) we define the random variable:\nf (θ, B) :=\n\n\n\n1\nσU(θ,B)σV (θ,B)\nr\u0010\n1\nσU (θ,B)σV (θ,B) −ρ(θ,B)\n\u00112−1\nif\n1\nσU(θ,B)σV (θ,B) −ρ (θ, B) > 1,\n+∞\nelse.\n(26)\nThen we have:\nP (∆(η) ≤0) ≤\n\u0010\ninf\nθ>0 EB\n\u0002\nf (θ, B)\n\u0003\u0011n\nProof. See section B.1.\nNow recall that s and d are both linear in p. In particular, there exist α, ψ ∈(0, 1) such that:\n\u001as = αp\nd = ψp\n.\nWe have:\nγ (θ) = 2d2θσ2/p2 = 2σ2ψ2θ.\n(27)\nProposition B.2. We define:\nC⋆(η) :=\nψ\n(1 −ψ) (2 −η (1 −ψ)) > 0.\nDefine the function:\nξ : N −→(0, +∞)\np 7−→C⋆(η)\n2αψp .\nConsider the random vector B ∈{0, 1}p : Bj\ni.i.d.\n∼Ber (ψ). We define the random variable:\nHp (B) = f (ξ (p) , B) .\nThen the following hold:\n(i) Hp (B)\na.s.\n−→\nq\n1\n1+ηψC⋆(η) as p →+∞.\n(ii) P (∆(η) ≤0) ≤\n\u0010\nEB [Hp (B)]\n\u0011n\n.\nProof. See section B.2.\nConjecture B.1. The exists p0 such that (Hp (B))p≥p0 is uniformly integrable, that is:\nlim\nT →+∞sup\np≥p0\nEB\nh\nHp (B) 1\n\u0000Hp (B) > T\n\u0001i\n= 0.\nHence we have Hp (B)\na.s.\n−→\nq\n1\n1+ηψC⋆(η), which by Conjecture B.1 yields:\nlim\np→+∞EB [Hp (B)] =\ns\n1\n1 + ηψC⋆(η).\nTherefore, we obtain:\nEB [Hp (B)] =\ns\n1\n1 + ηψC⋆(η) + o (1) .\n23\n"
    },
    {
      "page_number": 24,
      "text": "Thus, by Proposition B.2 we have:\n1\nn log\n\u0010\nP (∆(η) ≤0)\n\u0011\n≤log\n\u0010\nEB [Hp (B)]\n\u0011\n= log\n s\n1\n1 + ηψC⋆(η) + o (1)\n!\n= −1\n2 log\n\u0010\n1 + ηψC⋆(η)\n\u0011\n+ o (1) .\nHence:\nP (∆(η) ≤0) ≤\n\u0010\n1 + ηψC⋆(η)\n\u0011−n/2\neo(n).\nTaking the union bound:\nP\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥P\n\u0012\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2 >\n\r\r\r ˜Y −˜X1S⋆\n\r\r\r\n2\n2 , ∀S : |S △S⋆| ≥2δs\n\u0013\n= 1 −P\n\u0012\n∃S : |S △S⋆| ≥2δs,\n\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2 ≤\n\r\r\r ˜Y −˜X1S⋆\n\r\r\r\n2\n2\n\u0013\n= 1 −\nX\nS : |S △S⋆|≥2δs\nP\n\u0012\r\r\r ˜Y −˜X1S\n\r\r\r\n2\n2 ≤\n\r\r\r ˜Y −˜X1S⋆\n\r\r\r\n2\n2\n\u0013\n= 1 −\nX\nη∈{i/s: i∈[s]}: η≥δ\nX\nS : |S △S⋆|=2ηs\nP (L (S) ≤L (S⋆))\n= 1 −\nX\nη∈{i/s: i∈[s]}: η≥δ\nX\nS : |S △S⋆|=2ηs\nP (∆(η) ≤0)\n≥1 −\n\u0012p\ns\n\u0013\nX\nη∈{i/s: i∈[s]}: η≥δ\n\u0010\n1 + ηψC⋆(η)\n\u0011−n/2\neo(n).\nIn addition, we have:\nX\nη∈{i/s: i∈[s]}: η≥δ\n(1 + ηψC⋆(η))−n/2 ≤\nX\nη∈{i/s: i∈[s]}: η≥δ\n(1 + δψC⋆(δ))−n/2\n(28)\n≤s (1 + δψC⋆(δ))−n/2 .\n(29)\nwhere (28) holds because the term inside the sum is non-increasing in η, and (29) holds because the\ncardinality of the index set of the sum is upper bounded by s. Therefore:\nP\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −\n\u0012p\ns\n\u0013\ns\n\u0010\n1 + δψC⋆(δ)\n\u0011−n/2\neo(n)\n= 1 −exp\n\u0014\nph (α)\n\u00001 + o (1)\n\u0001\n+ log s −n\n\u00121\n2 log\n\u0010\n1 + δψC⋆(δ)\n\u0011\n+ o (1)\n\u0013\u0015\n= 1 −exp\n\u0014\nph (α)\n\u00001 + o (1)\n\u0001\n−n\n\u00121\n2 log\n\u0010\n1 + δψC⋆(δ)\n\u0011\n+ o (1)\n\u0013\u0015\n.\nWe define:\nn⋆:=\n2h (α) p\nlog\n\u0010\n1 + δψC⋆(δ)\n\u0011.\n24\n"
    },
    {
      "page_number": 25,
      "text": "Let ε > 0. Then if n ≥(1 + ε) n⋆, we have:\nP\n\u0010\f\f\fSupp (β⋆) △Supp\n\u0010\nˆβ\n\u0011\f\f\f < 2δs\n\u0011\n≥1 −exp\n\u0014\nph (α)\n\u00001 + o (1)\n\u0001\n−n\n\u00121\n2 log\n\u0010\n1 + δψC⋆(δ)\n\u0011\n+ o (1)\n\u0013\u0015\n= 1 −exp\nh\nph (α)\n\u0000−ε + o (1)\n\u0001i\np→+∞\n−→1.\nB.1\nProof of Proposition B.1\nProof of Proposition B.1. Since η and S are fixed, we will simplify notations in this proof as follows:\nwe will write ∆for ∆(η), and A, B, C, M respectively for A (S) , B (S) , C (S) , M (S). We have:\n∆= L(S) −L(S⋆)\n=\n\r\r\r\r\nd\npX1S⋆−˜X1S + d\npZ\n\r\r\r\r\n2\n2\n−\n\r\r\r\r\n\u0012d\npX −˜X\n\u0013\n1S⋆+ d\npZ\n\r\r\r\r\n2\n2\n=\n\r\r\r ˜X1S\n\r\r\r\n2\n2 −\n\r\r\r ˜X1S⋆\n\r\r\r\n2\n2 + 2 d\np ⟨X1S⋆+ Z, ˜X (1S⋆−1S)⟩\n=\nn\nX\ni=1\n⟨˜Xi, 1S⟩2 −\nn\nX\ni=1\n⟨˜Xi, 1S⋆⟩2 + 2 d\np\nn\nX\ni=1\n(⟨Xi, 1S⋆⟩+ Zi)\n\u0010\n⟨˜Xi, 1S⋆⟩−⟨˜Xi, 1S⟩\n\u0011\n=\nn\nX\ni=1\n\u0012\n⟨˜Xi, 1S⟩2 −⟨˜Xi, 1S⋆⟩2 + 2 d\np (⟨Xi, 1S⋆⟩+ Zi)\n\u0010\n⟨˜Xi, 1S⋆⟩−⟨˜Xi, 1S⟩\n\u0011\u0013\nLet:\n∆i := ⟨˜Xi, 1S⟩2 −⟨˜Xi, 1S⋆⟩2 + 2 d\np (⟨Xi, 1S⋆⟩+ Zi)\n\u0010\n⟨˜Xi, 1S⋆⟩−⟨˜Xi, 1S⟩\n\u0011\n.\nSo that ∆= Pn\ni=1 ∆i. Now using the Chernoff bound:\nP (∆≤0) = P (−∆≥0) = inf\nθ≥0 P\n\u0000e−θ∆≥1\n\u0001\n≤inf\nθ≥0 M−∆i(θ)n.\n(30)\nWe have:\n∆i = ⟨˜Xi, 1S⟩2 −⟨˜Xi, 1S⋆⟩2 + 2 d\np (⟨Xi, 1S⋆⟩+ Zi)\n\u0010\n⟨˜Xi, 1S⋆⟩−⟨˜Xi, 1S⟩\n\u0011\n=\n\nX\nj∈S\n˜Xij\n\n\n2\n−\n\nX\nj∈S⋆\n˜Xij\n\n\n2\n+ 2 d\np\n\nX\nj∈S⋆\nXij + Zi\n\n\n\nX\nj∈S⋆\n˜Xij −\nX\nj∈S\n˜Xij\n\n\n= −\n\nX\nj∈S⋆\n˜Xij −\nX\nj∈S\n˜Xij\n\n\n\nX\nj∈S⋆\n˜Xij +\nX\nj∈S\n˜Xij −2 d\np\nX\nj∈S⋆\nXij\n\n\n+ 2 d\np Zi\n\nX\nj∈S⋆\n˜Xij −\nX\nj∈S\n˜Xij\n\n\nThen:\n∆i = −\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\nX\nj∈A\n˜Xij +\nX\nj∈B\n˜Xij + 2\nX\nj∈C\n˜Xij −2 d\np\nX\nj∈S⋆\nXij\n\n\n+ 2 d\np Zi\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n.\n25\n"
    },
    {
      "page_number": 26,
      "text": "Let:\nKi := −\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\nX\nj∈A(S)\n˜Xij +\nX\nj∈B(S)\n˜Xij + 2\nX\nj∈C(S)\n˜Xij −2 d\np\nX\nj∈S⋆\nXij,\n\n\nso that:\n∆i = Ki + 2 d\np Zi\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n.\nThe MGF of −∆i can be expressed as:\nM−∆i (θ) = E [exp (−∆iθ)]\n= EXi,Bi\n\nEZi\n\nexp\n\n−θ\n\nKi + 2 d\np Zi\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\n\n\n\n\f\f\f\fXi, Bi\n\n\n\n\n= EXi,Bi\n\ne−θKiEZi\n\nexp\n\n−2dθ\np\nZi\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\n\n\f\f\f\fXi, Bi\n\n\n\n\n= EXi,Bi\n\ne−θKiMZi | Xi,Bi\n\n−2dθ\np\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\n\n\n\n= EXi,Bi\n\ne−θKi exp\n\n\n1\n2\n\n−2dθ\np\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\n\n2\nσ2\n\n\n\n\n\n= EXi,Bi\n\nexp\n\n\n−θKi + 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n\n\n\n\nThus, by the tower rule:\nM−∆i (θ) = EBi\n\nEXi\n\nexp\n\n\n−θKi + 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n\n\n\f\f\f\f\fBi\n\n\n\n.\n(31)\nFix θ > 0. Then:\n−θKi + 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n= θ\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n\nX\nj∈A\n˜Xij +\nX\nj∈B\n˜Xij + 2\nX\nj∈C\n˜Xij −2 d\np\nX\nj∈S⋆\nXij\n\n\n+ 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n= θ\n\nX\nj∈A\nBijXij −\nX\nj∈B\nBijXij\n\n×\n\nX\nj∈A\n\u0012\u00001 + γ (θ)\n\u0001\nBij −2d\np\n\u0013\nXij +\nX\nj∈B\n(1 −γ (θ)) BijXij + 2\nX\nj∈C\n\u0012\nBij −d\np\n\u0013\nXij\n\n,\n26\n"
    },
    {
      "page_number": 27,
      "text": "where γ (θ) = 2d2θσ2/p2. Since θ is fixed for now, we simplify the notation γ (θ) by simply writing\nγ. Let:\n\n\n\nU := θ\n\u0010P\nj∈A BijXij −P\nj∈B BijXij\n\u0011\nV := P\nj∈A\n\u0010\n(1 + γ) Bij −2d\np\n\u0011\nXij + P\nj∈B (1 −γ) BijXij + 2 P\nj∈C\n\u0010\nBij −d\np\n\u0011\nXij\n,\nso that:\n−θKi + 2d2θ2σ2\np2\n\nX\nj∈A\n˜Xij −\nX\nj∈B\n˜Xij\n\n\n2\n= UV.\n(32)\nPlugging (32) in (31), we obtain:\nM−∆i (θ) = EBi\n\u0002\nEXi\n\u0002\neUV | Bi\n\u0003\u0003\n.\n(33)\nNote that, conditionally on Bi, we have:\nU\nd= N\n\n0, θ2\nX\nj∈A∪B\nB2\nij\n\n,\n(34)\nand:\nV\nd= N\n\n0,\nX\nj∈A\n\u0012\n(1 + γ) Bij −2d\np\n\u00132\n+\nX\nj∈B\n(1 −γ)2 B2\nij + 4\nX\nj∈C\n\u0012\nBij −d\np\n\u00132\n\n.\n(35)\nLet: \n\n\nσ2\nU := θ2 P\nj∈A∪B B2\nij\nσ2\nV := P\nj∈A\n\u0010\n(1 + γ) Bij −2d\np\n\u00112\n+ P\nj∈B (1 −γ)2 B2\nij + 4 P\nj∈C\n\u0010\nBij −d\np\n\u00112\n.\nNote that:\nσ2\nU = θ2\nX\nj∈A∪B\nBij,\nand\nσ2\nV =\nX\nj∈A\n\u0012\n(1 + γ) Bij −2d\np\n\u00132\n+\nX\nj∈B\n(1 −γ)2 B2\nij + 4\nX\nj∈C\n\u0012\nBij −d\np\n\u00132\n=\nX\nj∈A\n\u0012\n(1 + γ)2 B2\nij −4d (1 + γ)\np\nBij\n\u0013\n+ 4d2\np2 M +\nX\nj∈B\n(1 −γ)2 B2\nij\n+ 4\nX\nj∈C\n\u0012\nB2\nij −2d\np Bij\n\u0013\n+ 4d2\np2 (s −M)\n= 4d2s\np2\n+\nX\nj∈A\n\u0012\n(1 + γ)2 −4d (1 + γ)\np\n\u0013\nBij +\nX\nj∈B\n(1 −γ)2 Bij + 4\nX\nj∈C\n\u0012\n1 −2d\np\n\u0013\nBij\n= 4d2s\np2\n+\n\u0012\n(1 + γ)2 −4d (1 + γ)\np\n\u0013 X\nj∈A\nBij + (1 −γ)2 X\nj∈B\nBij + 4\n\u0012\n1 −2d\np\n\u0013 X\nj∈C\nBij.\nIn addition, we have:\nCov (U, V ) = θ\n\nX\nj∈A\n\u0012\n(1 + γ) Bij −2d\np\n\u0013\nBij −\nX\nj∈B\n(1 −γ) B2\nij\n\n\n= θ\n\n\n\u0012\n1 + γ −2d\np\n\u0013 X\nj∈A\nBij −(1 −γ)\nX\nj∈B\nBij\n\n.\n27\n"
    },
    {
      "page_number": 28,
      "text": "Lemma B.1. Let N1\nd= N\n\u00000, σ2\n1\n\u0001\n, N2\nd= N\n\u00000, σ2\n2\n\u0001\nand ρ := corr (N1, N2). Then:\nE\n\u0002\neN1N2\u0003\n=\n\n\n\n1\nσ1σ2\nr\u0010\n1\nσ1σ2 −ρ\n\u00112−1\nif\n1\nσ1σ2 −ρ > 1,\n+∞\nelse.\nProof. See section B.1.1.\nNow using Lemma B.1 with Gaussian random variables U and V , we conclude that:\nEXi\n\u0002\neUV | Bi\n\u0003\n= f (θ, Bi) ,\n(36)\nwith f (·, ·) defined in (26). Plugging this into (33), we obtain:\nM−∆i (θ) = EBi [f (θ, Bi)] .\n(37)\nLet B ∈{0, 1}p be a random vector such that Bj\ni.i.d.\n= Ber (d/p). Note that B\nd= Bi. Then (37) yields:\nM−∆i (θ) = EB [f (θ, B)] .\nPlugging this into the Chernoff bound (30), we conclude:\nP (∆≤0) ≤\n\u0010\ninf\nθ>0 EB\n\u0002\nf (θ, B)\n\u0003\u0011n\n.\nB.1.1\nProof of Lemma B.1\nProof of Lemma B.1. We have:\nE\n\u0002\neN1N2\u0003\n=\nZ\nx∈R\nZ\ny∈R\nexy\n2πσ1σ2\np\n1 −ρ2 exp\n \n−\n1\n2 (1 −ρ2)\n \u0012 x\nσ1\n\u00132\n−2ρ\n\u0012 x\nσ1\n\u0013 \u0012 y\nσ2\n\u0013\n+\n\u0012 y\nσ2\n\u00132!!\ndydx\n=\nZ\nx∈R\nZ\ny∈R\n1\n2πσ1σ2\np\n1 −ρ2 exp\n \nxy −\n1\n2 (1 −ρ2)\n \u0012 x\nσ1\n\u00132\n−2ρ\n\u0012 x\nσ1\n\u0013 \u0012 y\nσ2\n\u0013\n+\n\u0012 y\nσ2\n\u00132!!\ndydx\n=\nZ\nx∈R\nZ\ny∈R\n1\n2π\np\n1 −ρ2 exp\n\u0012\nσ1σ2xy −x2 −2ρxy + y2\n2 (1 −ρ2)\n\u0013\ndydx\n=\nZ\nx∈R\nZ\ny∈R\n2\n\u00001 −ρ2\u0001\n2π\np\n1 −ρ2 exp\n\u00002σ1σ2\n\u00001 −ρ2\u0001\nxy −x2 + 2ρxy −y2\u0001\ndydx\n=\nZ\nx∈R\nZ\ny∈R\np\n1 −ρ2\nπ\nexp\n\u00002σ1σ2\n\u00001 −ρ2\u0001\nxy −x2 + 2ρxy −y2\u0001\ndydx\nTherefore:\nE\n\u0002\neN1N2\u0003\n=\np\n1 −ρ2\nπ\nZ\nx∈R\nZ\ny∈R\ne2σ1σ2(1−ρ2)xy−x2+2ρxy−y2dydx\n=\np\n1 −ρ2\nπ\nZ\nx∈R\nZ\ny∈R\ne\n−(y−x(σ1σ2(1−ρ2)+ρ))\n2+x2\u0010\n(σ1σ2(1−ρ2)+ρ)\n2−1\n\u0011\ndydx\n=\np\n1 −ρ2\nπ\nZ\nx∈R\ne\nx2\u0010\n(σ1σ2(1−ρ2)+ρ)\n2−1\n\u0011 Z\ny∈R\ne−(y−x(σ1σ2(1−ρ2)+ρ))\n2\ndydx\n=\np\n1 −ρ2\nπ\nZ\nx∈R\ne\nx2\u0010\n(σ1σ2(1−ρ2)+ρ)\n2−1\n\u0011 Z\ny∈R\n√π ΦN(x(σ1σ2(1−ρ2)+ρ),1/\n√\n2)(y) dydx\n=\np\n1 −ρ2\nπ\nZ\nx∈R\nexp\n\u0010\nx2 \u0010\u0000σ1σ2\n\u00001 −ρ2\u0001\n+ ρ\n\u00012 −1\n\u0011\u0011 √πdx.\n28\n"
    },
    {
      "page_number": 29,
      "text": "Hence we obtain:\nE\n\u0002\neN1N2\u0003\n=\np\n1 −ρ2\n√π\nZ\nx∈R\nexp\n\u0010\nx2 \u0010\u0000σ1σ2\n\u00001 −ρ2\u0001\n+ ρ\n\u00012 −1\n\u0011\u0011\ndx.\nLet ξ := σ1σ2\n\u00001 −ρ2\u0001\n+ ρ. Note that, if ξ ≥1, the above explodes to +∞. However, if ξ < 1, the\nabove yields:\nE\n\u0002\neN1N2\u0003\n=\np\n1 −ρ2\n√π\nZ\nx∈R\nexp\n\u0000x2 \u0000ξ2 −1\n\u0001\u0001\ndx\n=\np\n1 −ρ2\n√π\np\n1 −ξ2\nZ\nx∈R\nexp\n\u0000−x2\u0001\ndx\n=\np\n1 −ρ2\n√π\np\n1 −ξ2\nZ\nx∈R\n√π ΦN(0,1/\n√\n2) (x) dx\n=\np\n1 −ρ2\np\n1 −ξ2 .\nPlugging the expression of ξ in the above, we obtain:\nE\n\u0002\neN1N2\u0003\n=\np\n1 −ρ2\nq\n1 −ρ2 −σ2\n1σ2\n2 (1 −ρ2)2 −2ρ (1 −ρ2) σ1σ2\n=\n1\np\n1 −σ2\n1σ2\n2 (1 −ρ2) −2ρσ1σ2\n=\n1\np\n1 + ρ2σ2\n1σ2\n2 −2ρσ1σ2 −σ2\n1σ2\n2\n=\n1\nq\n(ρσ1σ2 −1)2 −σ2\n1σ2\n2\n=\n1\nσ1σ2\nr\u0010\n1\nσ1σ2 −ρ\n\u00112\n−1\n.\nIn addition, note that:\nξ < 1 ⇐⇒σ1σ2\n\u00001 −ρ2\u0001\n+ ρ < 1\n⇐⇒σ1σ2 (1 + ρ) (1 −ρ) < 1 −ρ\n⇐⇒σ1σ2 (1 + ρ) < 1\n⇐⇒1 <\n1\nσ1σ2\n−ρ.\nB.2\nProof of Proposition B.2\nProof of Proposition B.2. Fix η ∈(0, 1]. To simplify notations, we will write C⋆instead of C⋆(η).\nAll asymptotic statements are as p →+∞.\nWe start by showing (i). First, we note the following.\nLemma B.2.\nX\nj∈A(S)\nBj = ηαψp\n\u00001 + o (1)\n\u0001\n,\nX\nj∈B(S)\nBj = ηαψp\n\u00001 + o (1)\n\u0001\n,\nX\nj∈C(S)\nBj = (1 −η) αψp\n\u00001 + o (1)\n\u0001\n.\nProof. See section B.2.1.\n29\n"
    },
    {
      "page_number": 30,
      "text": "Next, plugging the expressions given by Lemma B.2 into the expressions of σ2\nU (θ, B), σ2\nV (θ, B) and\nCov(U,V ) (θ, B) given in the statement of Proposition B.1, we obtain the following.\nProposition B.3. We have:\n\n\n\n\n\nσ2\nU (θ, B) = 2ηαψpθ2\u00001 + o (1)\n\u0001\nσ2\nV (θ, B) = 2αψp\nh\n2 −2ψ + η\n\u0010\n(ψ −γ (θ))2 −(1 −ψ)2\u0011i \u00001 + o (1)\n\u0001\nCov(U,V ) (θ, B) = 2ηαψpθ (γ (θ) −ψ)\n\u00001 + o (1)\n\u0001\n.\nProof. See section B.2.2.\nPlugging ξ (p) in the expression of γ (·) given by (27), we obtain:\nγ (ξ (p)) = 2σ2ψ2ξ (p) = 2σ2ψ2C⋆\n2αψp\n= σ2ψC⋆\nαp\n= o (1) .\nTherefore, we have by Proposition B.3:\nσ2\nU (ξ (p) , B) σ2\nV (ξ (p) , B) = 4ηα2ψ2p2\n\u0012 C⋆\n2αψp\n\u00132 \u0010\n2 −2ψ + η\n\u0010\n(ψ −γ (ξ (p)))2 −(1 −ψ)2\u0011\u0011\u0010\n1 + o (1)\n\u0011\n= ηC⋆2\u0010\n2 −2ψ + η\n\u0010\n(ψ −o (1))2 −(1 −ψ)2\u0011\u0011\u0010\n1 + o (1)\n\u0011\n= ηC⋆2\u0010\n2 −2ψ + η (2ψ −1)\n\u0011\u0010\n1 + o (1)\n\u0011\na.s.\n−→ηC⋆2\u0010\n2 −2ψ + 2ηψ −η\n\u0011\n.\nNote that the above is > 0 because η (1 −2ψ) < 2 (1 −ψ), which follows from the facts that η ≤1\nand ψ ∈(0, 1). In addition, by Proposition B.3:\nCov(U,V ) (ξ (p) , B) = 2ηαψp\n\u0012 C⋆\n2αψp\n\u0013 \u0000γ (ξ (p)) −ψ\n\u0001\u00001 + o (1)\n\u0001\n= ηC⋆\u0000o (1) −ψ\n\u0001\u00001 + o (1)\n\u0001\na.s.\n−→−ηψC⋆.\nBringing these together, we obtain:\nlim\np→+∞\n\u001a\u0010\n1 −Cov(U,V ) (ξ (p) , B)\n\u00112\n−\n\u0010\nσ2\nU (ξ (p) , B) σ2\nV (ξ (p) , B) + 1 + ηψC⋆\u0011\u001b\n=\n\u0010\n1 + ηψC⋆\u00112\n−η\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2 −ηψC⋆−1\n= η2ψ2C⋆2 + 2ηψC⋆+ 1 −η\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2 −ηψC⋆−1\n=\n\u0010\nη2ψ2 −2η + 2ηψ −2η2ψ + η2\u0011\nC⋆2 + ηψC⋆\n= ηC⋆\n \n−(1 −ψ)\n\u0010\n2 −η (1 −ψ)\n\u0011\nC⋆+ ψ\n!\n= 0.\n30\n"
    },
    {
      "page_number": 31,
      "text": "In addition:\nlim\np→+∞\n\u001a\n1\nσU (ξ (p) , B) σV (ξ (p) , B) −ρ (ξ (p) , B)\n\u001b2\n=\nlim\np→+∞\n\u001a\n1 −Cov(U,V ) (ξ (p) , B)\nσU (ξ (p) , B) σV (ξ (p) , B)\n\u001b2\n=\n\u0010\n1 + ηψC⋆\u00112\nη\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2\n= 1 +\n\u0010\n1 + ηψC⋆\u00112\n−η\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2\nη\n\u0010\n2 −2ψ + 2ηψ −η\n\u0011\nC⋆2\n= 1 +\n1 + ηψC⋆\n\u0010\n1 + ηψC⋆\n\u00112\n−\n\u0010\n1 + ηψC⋆\n\u0011\n= 1 +\n1\nηψC⋆> 1.\nTherefore, there exists p′ ∈N such that, for all p ≥p′ we have:\n\u001a\n1\nσU (ξ (p) , B) σV (ξ (p) , B) −ρ (ξ (p) , B)\n\u001b2\n> 1.\nThen note that, for all p ≥p′ we have:\n1\nσU (ξ (p) , B) σV (ξ (p) , B) −ρ (ξ (p) , B) > 1,\nand hence:\nf (ξ (p) , B) =\n1\nσU (ξ (p) , B) σV (ξ (p) , B)\nr\u0010\n1\nσU(ξ(p),B)σV (ξ(p),B) −ρ (ξ (p) , B)\n\u00112\n−1\n=\n1\nr\u0010\n1 −Cov(U,V ) (ξ (p) , B)\n\u00112\n−σ2\nU (ξ (p) , B) σ2\nV (ξ (p) , B)\na.s.\n−→\nr\n1\n1 + ηψC⋆.\nTherefore, we conclude that (i) holds. We now show (ii). We have:\ninf\nθ>0 EB\n\u0002\nf (θ, B)\n\u0003\n≤EB\n\u0002\nf (ξ (p) , B)\n\u0003\n= EB [Hp (B)] .\nPlugging this into the Chernoff bound obtained in Proposition B.1, we obtain:\nP (∆≤0) ≤\n\u0010\ninf\nθ>0 EB\n\u0002\nf (θ, B)\n\u0003\u0011n\n≤\n\u0010\nEB [Hp (B)]\n\u0011n\n.\nB.2.1\nProof of Lemma B.2\nProof of Lemma B.2. We know that Bj\ni.i.d.\n∼Ber (ψ). Therefore:\nX\nj∈A(S)\nBj = |A (S)| ψ +\np\n|A (S)| ψNA + o\n\u0010p\n|A (S)| ψ\n\u0011\n,\nwhere NA ∼N (0, 1). In addition, we have:\n|A (S)| = ⌈ηs⌉= ηs\n\u00001 + o (1)\n\u0001\n.\n31\n"
    },
    {
      "page_number": 32,
      "text": "Hence:\nX\nj∈A(S)\nBj = ηαψp\n\u00001 + o (1)\n\u0001\n.\nSimilarly, we have:\nX\nj∈B(S)\nBj = ηαψp\n\u00001 + o (1)\n\u0001\nand\nX\nj∈C(S)\nBj = (1 −η) αψp\n\u00001 + o (1)\n\u0001\n.\nB.2.2\nProof of Proposition B.3\nProof of Proposition B.3. We have by Proposition B.1:\nσ2\nU (θ, B) = θ2\nX\nj∈A(S)∪B(S)\nBj,\nσ2\nV (θ, B) = 4d2s\np2\n+\n \n\u00001 + γ (θ)\n\u00012 −4d\n\u00001 + γ (θ)\n\u0001\np\n! X\nj∈A(S)\nBj\n+\n\u00001 −γ (θ)\n\u00012\nX\nj∈B(S)\nBij + 4\n\u0012\n1 −2d\np\n\u0013 X\nj∈C(S)\nBj,\nCov(U,V ) (θ, B) = θ\n\n\n\u0012\n1 + γ (θ) −2d\np\n\u0013 X\nj∈A(S)\nBij −\n\u00001 −γ (θ)\n\u0001 X\nj∈B(S)\nBij\n\n.\nNow by Lemma B.2 we have:\n\n\n\n\n\nP\nj∈A(S) Bj = ηαψp\n\u00001 + o (1)\n\u0001\nP\nj∈B(S) Bj = ηαψp\n\u00001 + o (1)\n\u0001\nP\nj∈C(S) Bj = (1 −η) αψp\n\u00001 + o (1)\n\u0001\n.\nHence, the expression of σ2\nU (θ, B) writes:\nσ2\nU (θ, B) = θ2\nX\nj∈A(S)\nBj + θ2\nX\nj∈B(S)\nBj\n= θ2ηαψp\n\u00001 + o (1)\n\u0001\n+ θ2ηαψp\n\u00001 + o (1)\n\u0001\n= 2ηαψpθ2\u00001 + o (1)\n\u0001\n.\n32\n"
    },
    {
      "page_number": 33,
      "text": "The expression of σ2\nV (θ, B) writes:\nσ2\nV (θ, B)\n= 4d2s\np2\n+\n \n\u00001 + γ (θ)\n\u00012 −4d\n\u00001 + γ (θ)\n\u0001\np\n! X\nj∈A(S)\nBj\n+\n\u00001 −γ (θ)\n\u00012\nX\nj∈B(S)\nBij + 4\n\u0012\n1 −2d\np\n\u0013 X\nj∈C(S)\nBj\n= 4αψ2p +\n\u00001 + γ (θ)\n\u0001\n(1 + γ (θ) −4ψ) ηαψp\n\u00001 + o (1)\n\u0001\n+\n\u00001 −γ (θ)\n\u00012ηαψp\n\u00001 + o (1)\n\u0001\n+ 4 (1 −2ψ) (1 −η) αψp\n\u00001 + o (1)\n\u0001\n= αψp\n\u0010\n4ψ + η\n\u00001 + γ (θ)\n\u0001\u00001 + γ (θ) −4ψ\n\u0001\n+ η\n\u00001 −γ (θ)\n\u00012 + 4 (1 −2ψ) (1 −η)\n\u0011\u00001 + o (1)\n\u0001\n= αψp\n\u0012\n4 −4ψ + η\n\u0010\u00001 + γ (θ)\n\u00012 +\n\u00001 −γ (θ)\n\u00012 −4ψ\n\u00001 + γ (θ)\n\u0001\n−4 (1 −2ψ)\n\u0011\u0013\u00001 + o (1)\n\u0001\n= αψp\n\u0012\n4 −4ψ + η\n\u0010\n2 + 2γ (θ)2 −4ψγ (θ) + 4ψ −4\n\u0011\u0013\u00001 + o (1)\n\u0001\n= 2αψp\n\u0012\n2 −2ψ + η\n\u0010\nγ (θ)2 −2ψγ (θ) + 2ψ −1\n\u0011\u0013\u00001 + o (1)\n\u0001\n= 2αψp\n\u0012\n2 −2ψ + η\n\u0010\nγ (θ)2 −2ψγ (θ) + ψ2 −ψ2 + 2ψ −1\n\u0011\u0013\u00001 + o (1)\n\u0001\n= 2αψp\n\u0014\n2 −2ψ + η\n\u0010\u0000ψ −γ (θ)\n\u00012 −\n\u00001 −ψ\n\u00012\u0011\u0015\u00001 + o (1)\n\u0001\n.\nThe expression of Cov(U,V ) (θ, B) writes:\nCov(U,V ) (θ, B) = θ\n\n\n\u0012\n1 + γ (θ) −2d\np\n\u0013 X\nj∈A(S)\nBij −\n\u00001 −γ (θ)\n\u0001 X\nj∈B(S)\nBij\n\n\n= θ\n\u0012\u0010\n1 + γ (θ) −2ψ\n\u0011\nηαψp\n\u00001 + o (1)\n\u0001\n−\n\u00001 −γ (θ)\n\u0001\nηαψp\n\u00001 + o (1)\n\u0001\u0013\n= ηαψpθ\n\u0012\u0010\n1 + γ (θ) −2ψ\n\u0011\n−\n\u00001 −γ (θ)\n\u0001\u0013\u00001 + o (1)\n\u0001\n= 2ηαψpθ\n\u0000γ (θ) −ψ\n\u0001\u00001 + o (1)\n\u0001\n,\nas desired.\n33\n"
    },
    {
      "page_number": 34,
      "text": "NeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: Theorem 1, Corollary 2 and Theorem 3 clearly state the main claims.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims made\nin the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: Limitations where discussed:\n• Briefly in the introduction (section 1).\n• Thoroughly in Remark 2.2 and Remark 3.1.\n• Future work directions in the conclusion (section 4).\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to address\nproblems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an important\nrole in developing norms that preserve the integrity of the community. Reviewers will\nbe specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\n34\n"
    },
    {
      "page_number": 35,
      "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: Complete proofs of theoretical results are provided: for Theorem 1 in appendix\nA.1, for Corollary 2 in appendix A.2, for Example 2.2 in appendix A.3, and for Theorem 3\nin appendix B.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main\nexperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [NA]\nJustification: The paper does not include experiments. In order to observe our results we\nneed the dimension p of the problem to be large, which cannot be ran experimentally since\ncomputing the MLE (7), (13) is exponential-time.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived well\nby the reviewers: Making the paper reproducible is important, regardless of whether\nthe code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all\nsubmissions to provide some reasonable avenue for reproducibility, which may depend\non the nature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct the\ndataset).\n35\n"
    },
    {
      "page_number": 36,
      "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors\nare welcome to describe the particular way they provide for reproducibility. In the\ncase of closed-source models, it may be that access to the model is limited in some\nway (e.g., to registered users), but it should be possible for other researchers to have\nsome path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions\nto faithfully reproduce the main experimental results, as described in supplemental material?\nAnswer: [NA]\nJustification: The paper does not include experiments.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run\nto reproduce the results.\nSee the NeurIPS code and data submission guidelines\n(https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [NA]\nJustification: The paper does not include experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [NA]\nJustification: The paper does not include experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confidence\nintervals, or statistical significance tests, at least for the experiments that support the\nmain claims of the paper.\n36\n"
    },
    {
      "page_number": 37,
      "text": "• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error of\nthe mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the computer\nresources (type of compute workers, memory, time of execution) needed to reproduce the\nexperiments?\nAnswer: [NA]\nJustification: The paper does not include experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The research conducted in the paper conforms, in every respect, with the\nNeurIPS Code of Ethics.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special\nconsideration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: The main positive societal impact of our work is reducing computational cost\nin a range of applications via the measurement sparsity vs. computational cost trade-off.\nApplications of the sparse recovery problem are discussed in the introduction and an example\nof computational cost reduction is given in Example 2.2.\n37\n"
    },
    {
      "page_number": 38,
      "text": "Guidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: The paper poses no such risks.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: The paper does not use existing assets. Previous theoretical results are properly\ncredited to their authors.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n38\n"
    },
    {
      "page_number": 39,
      "text": "• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to the\nasset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: The paper does not release new assets. We provide our interpretation of the\ntheoretical results in sections 2 and 3.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main\ncontribution of the paper involves human subjects, then as much detail as possible\nshould be included in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n39\n"
    },
    {
      "page_number": 40,
      "text": "• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: The core method development in this research does not involve LLMs as any\nimportant, original, or non-standard components.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n40\n"
    }
  ]
}