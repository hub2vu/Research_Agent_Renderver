{
  "filename": "118823_How many measurements are enough_ Bayesian recovery in inverse problems with general distributions.pdf",
  "total_pages": 42,
  "full_text": "How many measurements are enough? Bayesian\nrecovery in inverse problems with general distributions\nBen Adcock\nDepartment of Mathematics\nSimon Fraser University\nCanada\nNick Huang\nDepartment of Mathematics\nSimon Fraser University\nCanada\nAbstract\nWe study the sample complexity of Bayesian recovery for solving inverse prob-\nlems with general prior, forward operator and noise distributions. We consider\nposterior sampling according to an approximate prior P, and establish sufficient\nconditions for stable and accurate recovery with high probability. Our main result\nis a non-asymptotic bound that shows that the sample complexity depends on (i)\nthe intrinsic complexity of P, quantified by its approximate covering number, and\n(ii) concentration bounds for the forward operator and noise distributions. As a key\napplication, we specialize to generative priors, where P is the pushforward of a\nlatent distribution via a Deep Neural Network (DNN). We show that the sample\ncomplexity scales log-linearly with the latent dimension k, thus establishing the\nefficacy of DNN-based priors. Generalizing existing results on deterministic (i.e.,\nnon-Bayesian) recovery for the important problem of random sampling with an\northogonal matrix U, we show how the sample complexity is determined by the co-\nherence of U with respect to the support of P. Hence, we establish that coherence\nplays a fundamental role in Bayesian recovery as well. Overall, our framework\nunifies and extends prior work, providing rigorous guarantees for the sample\ncomplexity of solving Bayesian inverse problems with arbitrary distributions.\n1\nIntroduction\nInverse problems are of fundamental importance in science, engineering and industry. In a standard\nsetting, the aim is to recover an unknown vector (e.g., an signal or image) x∗∈Rn from measurements\ny = Ax∗+ e ∈Rm.\n(1.1)\nHere e ∈Rm is measurement noise and A ∈Rm×n, often termed the measurement matrix, represents\nthe forwards operator. While simple, the discrete, linear problem (1.1) is sufficient to model many\nimportant applications [5,25,64,68]. It is common to solve (1.1) using a Bayesian approach (see,\ne.g., [30, 71]), where one assumes that x∗is drawn from some prior distribution R. However, in\npractice, R is never known exactly. Especially in modern settings that employ Deep Learning\n(DL) [12,33], it is typical to learn an approximate prior P and then recover x∗from y by approximate\nposterior sampling, i.e., sampling ˆx from the posterior P(·|y, A). An increasingly popular approach\ninvolves using generative models to learn P (see, e.g., [12,21,33,68,70,80] and references therein).\nA major concern in many inverse problems is that the number of measurements m is highly limited,\ndue to physical constraints such as time (e.g., in Magnetic Resonance Imaging (MRI)), power (e.g.,\nin portable sensors), money (e.g., seismic imaging), radiation exposure (e.g., X-Ray CT), or other\nfactors [5,64,68]. Hence, one aims to recover x∗well while keeping the number of measurements m\nas small as possible. With this in mind, in this work we address the following broad question: How\nmany measurements suffice for stable and accurate recovery of x∗∼R via approximate posterior\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nsampling ˆx ∼P(·|y, A), and what are conditions on R, P and the distributions A and E of the\nmeasurement matrix A and noise e, respectively, that ensure this recovery?\n1.1\nOverview\nIn this work, we strive to answer to this question in the broadest possible terms, with a theoretical\nframework that allows for very general types of distributions. We now describe the corresponding\nconditions needed and present simplified versions of our main results.\n(i) Closeness of the real and approximate distributions. We assume that Wp(R, P) is small for\nsome 1 ≤p ≤∞, where Wp denotes the Wassertstein p-metric.\n(ii) Low-complexity of P. Since m ≪n in many applications, to have any prospect for accurate\nrecovery we need to impose that P (or equivalently R, in view of the previous assumption) has\nan inherent low complexity. Following [49], we quantify this in terms of its approximate covering\nnumber Covη,δ(P). This is equal to the minimum number of balls of radius η required to cover a\nregion of Rn having P-measure at least 1 −δ. See Definition 2.1 for the full definition.\n(iii) Concentration of A. We consider constants Clow(t) = Clow(t; A, D) ≥0 and Cupp(t) =\nCupp(t; A, D) ≥0 (see Definition 2.3 for the full definition) such that\nPA∼A[∥Ax∥≤t∥x∥] ≤Clow(t),\nPA∼A[∥Ax∥≥t∥x∥] ≤Cupp(t)\nfor all x ∈D := supp(P) −supp(P) and t > 0. Here, and throughout this paper, we write\nS −S = {x1 −x2 : x1, x2 ∈S} ⊆Rn for the difference set associated with a set S ⊆Rn. We\nalso write supp(P) for the support of the measure P (see §2.1 for the definition). Furthermore, we\nwrite ∥·∥for the ℓ2-norm and ∥·∥∞for the ℓ∞-norm. If A is isotropic, i.e., EA∼A∥Ax∥2 = ∥x∥2,\n∀x ∈Rn, as is often the case in practice, these constants measure how fast ∥Ax∥2 concentrates\naround its mean. Notice that this condition is imposed only on D = supp(P) −supp(P), rather\nthan the whole space Rn. As we see later, this is crucial in obtaining meaningful recovery guarantees.\nFinally, in order to present a simplified result in this section, we now make several simplifying\nassumptions. Both of these will be relaxed in our full result, Theorem 3.1.\n(iv) Gaussian noise. Specifically, we assume that E = N(0, σ2\nm I) for some σ > 0.\n(v) Bounded forwards operators. We assume that ∥A∥≤θ a.s. for A ∼A and some θ > 0.\nTheorem 1.1 (Simplified main result). Let 1 ≤p ≤∞, 0 < δ ≤1/4, ε, η > 0 and suppose that\nconditions (i)–(v) hold with Wp(R, P) ≤ε/(2mθ) and σ ≥ε/δ1/p. Suppose that x∗∼R, A ∼A,\ne ∼E independently and ˆx ∼P(·|y, A), where y = Ax∗+ e. Then, for any d ≥2,\nP\n\u0002\n∥x∗−ˆx∥≥(8d2 + 2)(η + σ)\n\u0003\n≲δ + Covη,δ(P)\nh\nClow(1/d) + Cupp(d) + e−m/16i\n.\n(1.2)\nNote that in this and all subsequent results, the term on the left-hand side of (1.2) is the probability\nwith respect to all variables, i.e., x∗∼R, A ∼A, e ∼E and ˆx ∼P(·|y, A). Theorem 1.1 is\nextremely general, in that it allows for essentially arbitrary (real and approximate) signal distributions\nR and P and an essentially arbitrary distribution A for the forwards operator. In broad terms, it\nbounds the probability that the error ∥x∗−ˆx∥of posterior sampling exceeds a constant times the\nnoise level σ plus an arbitrary parameter η. It does so in terms of the approximate covering number\nCovη,δ(P), which measures the complexity of the approximate distribution P, the concentration\nbounds Clow and Cupp for A, which measure how much A elongates or shrinks a fixed vector, and\nan exponentially-decaying term e−m/16, which stems from the (Gaussian) noise. In particular, by\nanalyzing these terms for different classes of distributions P and A, we can derive concrete bounds\nfor various exemplar problems. We next describe two such problems.\n1.2\nExamples\nWe first consider A to be a distribution of subgaussian random matrices. Here A ∼A if its entries\nare i.i.d. subgaussian random variables with mean zero and variance 1/m (see Definition 3.4).\nTheorem 1.2 (Subgaussian measurement matrices, simplified). Consider the setup of Theorem 1.1,\nwhere A is a distribution of subgaussian random matrices. Then there is a constant c > 0 (depending\non the subgaussian parameters β, κ > 0; see Definition 3.4) such that\nP [∥x∗−ˆx∥≥34(η + σ)] ≲δ,\nwhenever m ≥c · [log(Covη,δ(P)) + log(1/δ)] .\n2\nThis theorem shows the efficacy of Bayesian recovery with subgaussian random matrices: namely,\nthe sample complexity scales linearly in the distribution complexity, i.e., the log of the approximate\ncovering number. Later in Theorem 3.5, we slightly refine and generalize this result.\nGaussian random matrices are very commonly studied, due to their amenability to analysis and tight\ntheoretical bounds [5,22,34,49], with Theorem 1.2 being a case in point. However, they are largely\nirrelevant to practical inverse problems, where physical constraints impose certain structures on the\nforwards operator distribution A [5,64,68]. For instance, in MRI physical constraints mean that the\nmeasurements are samples of the Fourier transform of the image. This has motivated researchers to\nconsider much more practically-relevant distributions, in particular, so-called subsampled orthogonal\ntransforms (see, e.g., [5]). Here U ∈Rn×n is a fixed orthogonal matrix – for example, the matrix\nof the Discrete Fourier Transform (DFT) in the case of MRI – and the distribution A is defined by\nrandomly selecting m rows of U. See Definition 3.7 for the formal definition.\nTheorem 1.3 (Subsampled orthogonal transforms, simplified). Consider the setup of Theorem 1.1,\nwhere A is a distribution of subsampled orthogonal matrices based on a matrix U. Then there is a\nuniversal constant c > 0 such that\nP [∥x∗−ˆx∥≥34(η + σ)] ≲δ,\nwhenever m ≥c · µ(U; D) · [log(Covη,δ(P)) + log(1/δ)] ,\nwhere D = supp(P) −supp(P) and µ(U; D) is the coherence of U relative to D, defined as\nµ(U; D) = n sup\nn\n∥Ux∥2\n∞/∥x∥2 : x ∈D, x ̸= 0\no\n.\nThis result shows that similar stable and accurate recovery to the Gaussian case can be achieved\nusing subsampled orthogonal matrices, provided the number of measurements scales linearly with the\ncoherence. We discuss this term further in §3.2, where we also present the full result, Theorem 3.9.\nIn general, Theorems 1.2 and 1.3 establish a key condition for successful Bayesian recovery in\ninverse problems, in each case relating the number of measurements m to the intrinsic complexity\nlog(Covη,δ(P)) of P. It is therefore informative to see how this complexity behaves for cases of\ninterest. As noted, it is common to use a generative model to learn P. This means that P = G♯γ,\nwhere G : Rk →Rn is a Deep Neural Network (DNN) and γ is some fixed probability measure on\nthe latent space Rk. Typically, γ = N(0, I). If G is L-Lipschitz, we show in Proposition 4.1 that\nlog(Covη,δ(P)) = O(k log[L(\n√\nk + log(1/δ))/η]).\n(1.3)\nThis scales log-linearly in the latent space dimension k, confirming the intrinsic low-complexity of P.\nCombining (1.3) with Theorems 1.2-1.3 we see that posterior sampling achieves stable and accurate\nrecovery, provided the number of measurements m scales near-optimally with k, i.e., as O(k log(k)).\nFurther, in order to compare with deterministic settings such as classical compressed sensing, which\nconcerns the recovery of s-sparse vectors, we also consider distributions P = Ps of s-sparse vectors.\nIn this case, we show in Proposition 4.3 that\nlog(Covη,δ(P)) = O(s log(n/s) + log[(√s + log(1/δ))/η]).\n(1.4)\nHence s measurements, up to log terms, are sufficient for recovery of approximately sparse vectors.\nThis extends a classical result for deterministic compressed sensing to the Bayesian setting.\n1.3\nSignificance\nThe significance of this work is as follows. See §1.4 for additional discussion.\n1. We provide the first results for Bayesian recovery with arbitrary real and approximate prior\ndistributions R, P and forwards operator and noise distributions A, E.\n2. Unlike much of the theory of Bayesian inverse problems, which is asymptotic in nature [12,30,71],\nour results are non-asymptotic. They hold for arbitrary values of the various parameters within given\nranges (e.g., the failure probability δ, the noise level σ, the number of measurements m, and so forth).\n3. For priors defined by Lipschitz generative DNNs, we establish the first result demonstrating that\nthe sample complexity of Bayesian recovery depends log-linearly on the latent dimension k and\nlogarithmically on the Lipschitz constant.\n3\n4. For the important class of subsampled orthogonal transforms, we show that the sample complexity\nof Bayesian recovery depends on the coherence, thus resolving several key open problems in the\nliterature (see next).\n5. It is increasingly well-known that DL-based methods for inverse problems are susceptible to\nhallucinations and other undesirable effects [11,15,18,25,29,40,42,45,47,60–63,65,66,79]. This is\na major issue that may limit the uptake of these methods in safety-critical domains such as medical\nimaging [25,55,57,61,74,76,77]. Our results provide theoretical guarantees for stable and accurate,\nand therefore show conditions under which hallucinations provably cannot occur. This is not only\ntheoretically interesting, but it also has practical consequences in the development of robust DL\nmethods for inverse problems – a topic we intend to explore in future work.\n1.4\nRelated work\nBayesian methods for inverse problems have become increasingly popular over the last several\ndecades [30,71], and many state-of-the-art DL methods for inverse problems now follow a Bayesian\napproach (see [7,12,21,46,56,64] and references therein). Learned priors, such as those stemming\nfrom generative models, are now also increasingly used in applications [1,7,12,21,28,33,38,46,48,\n51,53,54,58,64,68,70,78,80].\nThis work is motivated in part by the (non-Bayesian) theory of generative models for solving\ninverse problems (see [22,33,43,44,48,68–70,80] and references therein). This was first developed\nin [22], where compressed sensing techniques were used to show recovery guarantees for a Gaussian\nrandom matrix A when computing an approximate solution to (1.1) in the range Σ := ran(G) of\na Lipschitz map G : Rk →Rn, typically assumed to be a generative DNN. This is a deterministic\napproach. Besides the random forward operator and (potentially) random noise, it recovers a fixed\n(i.e., nonrandom) underlying signal x∗in a deterministic fashion with a point estimator ˆx that is\nobtained as a minimizer of the empirical ℓ2-loss minz∈Σ ∥Az −y∥2. In particular, no information\nabout the latent space distribution γ is used. In this work, following, e.g., [1,21,48,49,70] and others,\nwe consider a Bayesian setting, where x∗∼R is random and where we quantify the number of\nmeasurements that suffice for accurate and stable recovery via posterior sampling ˆx ∼P(·|y, A).\nOur work is a generalization of [49], which considered Bayesian recovery with Gaussian random\nmatrices and standard Gaussian noise. We significantly extend [49] to allow for arbitrary distributions\nA and E for the forward operator and noise, respectively. A key technical step in doing this is the\nintroduction of the concentration bounds Clow and Cupp. In particular, these bounds are imposed\nonly over the subset D = supp(P) −supp(P). This is unnecessary in the Gaussian case considered\nin Theorem 1.2, but crucial to obtain meaningful bounds in, for instance, the case of subsampled\northogonal transforms considered in Theorem 1.3 (see Remarks 3.2 and 3.10 for further discussion).\nAs noted, this case is very relevant to applications. In particular, when U is taken as a DFT matrix our\nwork addresses open problems posed in [48, §3] and [68, §II.F] on recovery guarantees with Fourier\nmeasurements. We also derive bounds for the approximate covering number of distributions given\nby Lipschitz generative DNNs (see (1.3) and Proposition 4.1) and distributions of sparse vectors\n(see (1.4) and Proposition 4.3), addressing an open problem posed in [49, §6]. In particular, we\ndemonstrate stable and accurate recovery in a Bayesian sense with a number of measurements that\nscales linearly in the model complexity, i.e., k in the former case and s in the latter case.\nRecently [16,17] generalized the results of [22] in the non-Bayesian setting from Gaussian random\nmatrices to subsampled orthogonal transforms. Theorem 1.3 provides a Bayesian analogue of this\nwork, as discussed above, where we consider posterior sampling rather than a deterministic point\nestimator. We also extend the setup of [16,17] by allowing for general measurement distributions A\nand priors P. In particular, in [16,17] the quantity Σ, which is the deterministic analogue of the prior\ndistribution P in our work, was assumed to be the range of a ReLU generative DNN. Like in [16],\nwe make use of the concept of coherence (see Theorem 1.3). However, our proof techniques are\ncompletely different to those of [16,17].\nClassical compressed sensing considers the recovery of approximately s-sparse vectors from (1.1).\nHowever, it has been extended to consider much more general types of low-complexity signal\nmodels, such as joint or block sparse vectors, tree sparse vectors, cosparse vectors and many\nothers [4,6,13,23,31,36,73]. However, most recovery guarantees for general model classes consider\nonly (sub)Gaussian measurement matrices (see e.g., [13, 34]). Recently, [3] introduced a general\n4\nframework for compressed sensing that allows for general low-complexity models Σ ⊆Rn contained\nwithin a finite union of finite-dimensional subspaces and arbitrary (random) measurement matrices.\nOur work is a Bayesian analogue of this deterministic setting. Similar to [3], a key feature of our work\nis that we consider arbitrary real and approximate signal distributions P, R (analogous to arbitrary\nlow-complexity models Σ) and arbitrary distributions A for the forwards operator. Unsurprisingly, a\nnumber of the conditions in our main result, Theorem 1.1 – namely, the low-complexity condition (ii)\nand concentration condition (iii) – share similarities to those that ensure stable and accurate recovery\nin non-Bayesian compressed sensing. See Remarks 2.2 and 3.3 for further discussion. However, the\nproof techniques used in this work are once more entirely different.\nFinally, while the focus of this work is to establish guarantees for posterior sampling, we mention\nin passing related work on information-theoretically optimal recovery methods. Methods such as\nApproximate Message Passing (AMP) [14,35] are well studied, and asymptotic information-theoretic\nbounds for Gaussian random matrices are known [52]. AMP methods are fast point estimation\nalgorithms, whereas we focus on sampling-based methods and do not consider computational\nimplementations (see §5 for some further discussion). Note that information-theoretic lower bounds\nfor posterior sampling have also been shown for the Gaussian case in [49].\n2\nPreliminaries\n2.1\nNotation\nWe now introduce some further notation. We let Br(x) = {z ∈Rn : ∥z −x∥≤r} and, when x = 0,\nwe write Br := Br(0). Given a set X ⊆Rn, we write Xc = Rn\\X for its complement. We also\nwrite Br(X) = S\nx∈X Br(x) for the r-neighbourhood of X.\nLet (X, F, µ) be a Borel probability space. We write supp(µ) for its support, i.e., the smallest closed\nset A ⊆X for which µ(A) = 1. Given probability spaces (X, F1, µ), (Y, F2, ν), we write Γ = Γµ,ν\nfor the set of couplings, i.e., probability measures on the product space (X × Y, σ(F1 ⊗F2)) whose\nmarginals are µ and ν, respectively. Given a cost function c : X × Y →[0, ∞) and 1 ≤p < ∞, the\nWasserstein-p metric is defined as\nWp(µ, ν) = inf\nγ∈Γ\n\u0012Z\nX×Y\nc(x, y)pdγ(x, y)\n\u00131/p\n.\nIf p = ∞, then W∞(µ, ν) = infγ∈Γ(esssupγc(x, y)). In this paper, unless stated otherwise,\nX = Y = Rn and the cost function c is the Euclidean distance.\n2.2\nApproximate covering numbers\nAs a measure of complexity of measures, we use the concept of approximate covering numbers as\nintroduced in [49].\nDefinition 2.1 (Approximate covering number). Let (X, F, P) be a probability space and δ, η ≥0.\nThe η, δ-approximate covering number of P is defined as\nCovη,δ(P) = min\n(\nk ∈N : ∃{xi}k\ni=1 ⊆supp(P), P\n k[\ni=1\nBη(xi)\n!\n≥1 −δ\n)\n.\nThis quantity measures how many balls of radius η are required to cover at least 1 −δ of the P-\nmass of Rn. See [49] for further discussion. Note that [49] does not require the centres xi of the\napproximate cover belong to supp(P). However, this is useful in our more general setting and\npresents no substantial restriction. At worst, this requirement changes η by a factor of 1/2.\nRemark 2.2 (Relation to non-Bayesian compressed sensing) Note that when δ = 0, the approxi-\nmate covering number Covη,0(P) ≡Covη(supp(P)) is just the classical covering number of the\nset supp(P), i.e., the minimal number of balls of radius η that cover supp(P). Classical covering\nnumbers play a key role in (non-Bayesian) compressed sensing theory. Namely, the covering number\nof (the unit ball of) the model class Σ ⊆Rn directly determines the number of measurements that\nsuffice for stable and accurate recovery. See, e.g., [3,34]. In the Bayesian setting, the approximate\ncovering number plays the same role; see Theorem 1.1.\n5\n2.3\nBounds for A and E\nSince our objective is to establish results that hold for arbitrary measurement and noise distributions\nA and E, we require several key definitions. These are variety of (concentration) bounds.\nDefinition 2.3 (Concentration bounds for A). Let A be a distribution on Rm×n, t ≥0 and D ⊆Rn.\nThen a lower concentration bound for A is any constant Clow(t) = Clow(t; A, D) ≥0 such that\nPA∼A{∥Ax∥≤t∥x∥} ≤Clow(t; A, D),\n∀x ∈D.\nSimilarly, an upper concentration bound for A is any constant Cupp(t) = Cupp(t; A, D) ≥0 such\nthat\nPA∼A{∥Ax∥≥t∥x∥} ≤Cupp(t; A, D),\n∀x ∈D.\nFinally, given t, s ≥0 an (upper) absolute concentration bound for A is any constant Cabs(s, t; A, D)\nsuch that\nPA∼A(∥Ax∥> t) ≤Cabs(s, t; A, D),\n∀x ∈D, ∥x∥≤s.\nNotice that if A is isotropic, i.e., E∥Ax∥2 = ∥x∥2, ∀x ∈Rn, then Clow and Cupp determine\nhow well ∥Ax∥2 concentrates around its mean ∥x∥2 for any fixed x ∈D. To obtain desirable\nsample complexity estimates (e.g., Theorems 1.2 and 1.3), we need concentration bounds that decay\nexponentially in m. A crucial component of this analysis is considering concentration bounds over\nsome subset D (related to the support of P), as, in general, one cannot expect fast concentration over\nthe whole of Rn. See Remarks 3.2 and 3.10.\nDefinition 2.4 (Concentration bound for E). Let E be a distribution in Rm and t ≥0. Then an\n(upper) concentration bound for E is any constant Dupp(t) = Dupp(t; E) ≥0 such that\nE(Bc\nt ) = Pe∼E(∥e∥≥t) ≤Dupp(t; E).\nNotice that this bound just measures the probability that the noise is large. We also need the following\nconcept, which estimates how much the density of E changes in a τ-neighbourhood of the origin\nwhen perturbed by an amount ε.\nDefinition 2.5 (Density shift bounds for E). Let E be a distribution in Rm with density pE and\nε, τ ≥0. Then a density shift bound for E is any constant Dshift(ε, τ) = Dshift(ε, τ; E) ≥0 (possibly\n+∞) such that\npE(u) ≤Dshift(ε, τ; E)pE(v),\n∀u, v ∈Rn, ∥u∥≤τ, ∥u −v∥≤ε.\n3\nMain results\nWe now present our main results. The first, an extension of Theorem 1.1, is a general result that holds\nfor arbitrary distributions R, P, A and E.\nTheorem 3.1. Let 1 ≤p ≤∞, 0 ≤δ ≤1/4, ε, η, t > 0, c, c′ ≥1 and σ ≥ε/δ1/p. Let E be a\ndistribution on Rm and R, P be distributions on Rn satisfying Wp(R, P) ≤ε and\nmin(log Covη,δ(R), log Covη,δ(P)) ≤k\n(3.1)\nfor some k ∈N. Suppose that x∗∼R, A ∼A, e ∼E independently and ˆx ∼P(·|y, A), where\ny = Ax∗+ e. Then p := P[∥x∗−ˆx∥≥(c + 2)(η + σ)] satisfies\np ≤2δ + Cabs(ε/δ1/p, tε/δ1/p; A, D1) + Dupp(c′σ; E)\n+ 2Dshift(tε/δ1/p, c′σ; E)ek\n\"\nClow\n \n2\n√\n2\n√c ; A, D2\n!\n+ Cupp\n\u0012 √c\n2\n√\n2; A, D2\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n√\n2; E\n\u0013 #\n,\nwhere\nD1 = Bε/δ1/p(supp(P)) ∩supp(R) −supp(P)\n(3.2)\nand\nD2 =\n\u001asupp(P) −supp(P)\nif P attains the minimum in (3.1)\nsupp(P) −supp(R)\notherwise\n.\n(3.3)\n6\nThis theorem bounds the probability p of unstable or inaccurate recovery in terms of the various\nparameters using the constants introduced in the previous section and the approximate covering\nnumbers of R, P. This result is powerful in its generality, but as a consequence, rather opaque. In\nparticular, since it considers arbitrary measurement and noise distributions, the number of measure-\nments m does not explicitly enter the bound. For typical distributions, a dependence on m is found\nin the concentration bounds Clow, Cupp, Dupp, as well as the terms Dshift and Cabs. For instance, the\nformer decay exponentially-fast in m for the examples introduced in §1.2, and therefore compensate\nfor the exponentially-large scaling in k in the main bound (see §B for precise estimates, as well as the\ndiscussion in §3.1-3.2). Note that Theorem 3.1 also considers general noise distributions E. While\nGaussian noise is arguably the most important example – and will be used in all our subsequent\nexamples – this additional generality comes at little cost in terms of the technicality of the proofs.\nRemark 3.2 (The concentration bounds in Theorem 3.1) A particularly important facet of this\nresult, for the reasons discussed above, is that the various concentration bounds Cabs, Clow and Cupp\nare taken over sets D1, D2 – given by (3.2) and (3.3), respectively, and related to the support of P\nand R – rather than the whole space Rn. We exploit this fact crucially later in Theorem 3.9.\nRemark 3.3 (Relation to non-Bayesian compressed sensing) The constants Clow and Cupp are\nsimilar, albeit not identical to similar conditions such as the Restricted Isometry Property (RIP)\n(see, e.g., [5, Chpt. 5]) or Restricted Eigenvalue Condition (REC) [19,22] that appear in non-Bayesian\ncompressed sensing. There, one considers a fixed model class Σ ⊆Rn, such as the set Σs of s-sparse\nvectors or, as in [22], the range ran(G) of a generative DNN. Conditions such as the RIP or REC\nimpose that ∥Ax∥is concentrated around ∥x∥for all x belonging to the difference set Σ −Σ. In\nTheorem 3.1, assuming P attains the minimum in (3.1), there is a similar condition with Σ replaced\nby supp(P). Indeed, Clow(2\n√\n2/√c; A, D2) measures how small ∥Ax∥is in relation to ∥x∥and\nCupp(√c/(2\n√\n2); A, D2) measures how large ∥Ax∥is in relation to ∥x∥.\n3.1\nExample: Subgaussian random matrices with Gaussian noise\nWe now apply this theorem to the first example introduced in §1.2. Recall that a random variable X\non R is subgaussian with parameters β, κ > 0 if P(|X| ≥t) ≤βe−κt2 for all t > 0.\nDefinition 3.4 (Subgaussian random matrix). A random matrix A ∈Rm×n is subgaussian with\nparameters β, κ > 0 if A =\n1\n√m eA, where the entries of eA are independent mean-zero sugaussian\nrandom variables with variance 1 and the same subgaussian parameters β, κ.\nNote that 1/√m is a scaling factor that ensures that A is isotropic, i.e., E∥Ax∥2 = ∥x∥2, ∀x ∈Rn.\nTheorem 3.5. Let 1 ≤p ≤∞, 0 ≤δ ≤1/4, ε, η > 0 and σ ≥ε/δ1/p. Let E = N(0, σ2\nm I)\nand A be a distribution of subgaussian random matrices with parameters β, κ > 0. Let R, P be\ndistributions on Rn and suppose that x∗∼R, A ∼A, e ∼E independently and ˆx ∼P(·|y, A),\nwhere y = Ax∗+ e. Then there is a constant c(β, κ) > 0 depending on β, κ only such that\nP[∥x∗−ˆx∥≥34(η + σ)] ≲δ,\nprovided Wp(R, P) ≤ε/c(β, κ) and\nm ≥c(β, κ) · [min(log Covη,δ(R), log Covη,δ(P)) + log(1/δ)] .\n(3.4)\nThis theorem is derived from Theorem 3.1 by showing that the various concentration bounds are\nexponentially small in m for subgaussian random matrices (see §B). It is a direct generalization\nof [49], which considered the Gaussian case only. It shows that subgaussian random matrices are\nnear-optimal for Bayesian recovery, in the sense that m scales linearly with the log of the approximate\ncovering number (3.4). We estimate these covering numbers for several key cases in §4.\nIt is worth at this stage discussing how (3.4) behaves with respect to the various parameters. First,\nsuppose that η decreases so that the error bound becomes smaller. Then Covη,δ(·) increases, meaning,\nas expected, that more measurements are required to meet (3.4). Second, suppose that δ decreases, so\nthat the failure probability shrinks. Then Covη,δ(·) and log(1/δ) both increase, meaning, once again,\nthat more measurements are needed for (3.4) to hold. Both behaviours are as expected.\n7\nRemark 3.6 (Relation to Johnson–Lindenstrauss) Suppose that P = R is a sum of d Diracs\nlocated at X = {x1, . . . , xd} ⊆Rn. Since the matrix A ∈Rm×n is a linear dimensionality-reducing\nmap, the Johnson-Lindenstrauss Lemma states that distances in X are preserved under A if and only\nif m = O(log(d)). In this setting, preserving the distances in X is equivalent to being able to stably\nidentify the mode from which a signal is drawn when observing its measurements. In agreement with\nthis argument, Theorem 3.5 also predicts recovery from roughly m = O(log(d)) measurements.\n3.2\nExample: Randomly-subsampled orthogonal transforms with Gaussian noise\nAs discussed, subgaussian random matrices are largely impractical. We now consider the more\npractical case of subsampled orthogonal transforms.\nDefinition 3.7 (Randomly-subsampled orthogonal transform). Let U ∈Rn×n be orthogonal (i.e.,\nU ⊤U = UU ⊤= I) and write u1, . . . , un ∈Rn for its rows. Let X1, . . . , Xn ∼i.i.d. Ber(m/n) be\nindependent Bernoulli random variables with P(Xi = 1) = m/n and P(Xi = 0) = 1 −m/n. Then\nwe define a distribution A as follows. We say that A ∼A if\nA =\nr n\nm\n\n\nu⊤\ni1...\nu⊤\niq\n\n,\nwhere {i1, . . . , iq} ⊆{1, . . . , n} is the set of indices i for which Xi = 1.\nThe factor\np\nn/m ensures that E(A⊤A) = I. Note that the number of measurements q in this model\nis itself a random variable, with E(q) = m. However, q concentrates exponentially around its mean.\nDefinition 3.8. Let U ∈Rn×n and D ⊆Rn. The coherence of U relative to D is\nµ(U; D) = n · sup\nn\n∥Ux∥2\n∞/∥x∥2 : x ∈D, x ̸= 0\no\n.\nCoherence is a well-known concept in classical compressed sensing with sparse vectors. Definition\n3.8 is a generalization that allows for arbitrary model classes D. This definition is similar to that\nof [16], which considered non-Bayesian compressed sensing with generative models. It is also related\nto the more general concept of variation introduced in [3].\nTheorem 3.9. Let 1 ≤p ≤∞, 0 ≤δ ≤1/4, ε, η > 0 and σ ≥ε/δ1/p. Let E = N(0, σ2\nm I) and\nA be a distribution of randomly-subsampled orthogonal matrices based on a matrix U. Let R, P\nbe distributions on Rn and suppose that x∗∼R, A ∼A, e ∼E independently and ˆx ∼P(·|y, A),\nwhere y = Ax∗+ e. Then there is a universal constant c > 0 such that\nP [∥x∗−ˆx∥≥34(η + σ)] ≲δ,\nprovided Wp(R, P) ≤ε/(2√mn) and\nm ≥c · µ(U; D) · [log(Covη,δ(P)) + log(1/δ)] ,\nwhere D = supp(P) −supp(P).\n(3.5)\nThis theorem is a Bayesian analogue of the deterministic results shown in [3, 16]. In [3, 16], the\nmeasurement conditions scale linearly with µ(U; D), where D = Σ −Σ and Σ ⊆Rn is the low-\ncomplexity model class. Similarly, the number of measurements (3.5) scales linearly with respect to\nthe coherence relative to D = supp(P) −supp(P), which, as discussed in Remark 3.3, plays the\nrole of the low-complexity model class in the Bayesian setting. Note that the measurement condition\n(3.5) involves the approximate distribution P only. This is relevant, since the quantities µ(U; D) and\nCovη,δ(P) can be estimated either numerically or analytically in various cases, such as when P is\ngiven by a generative model. Indeed, we estimate Covη,δ(P) analytically for Lipschitz generative\nmodels in Proposition 4.1 below. The coherence µ(U; D) was estimated analytically in [16] for\nReLU DNNs with random weights (see Remark 4.2 below). It can also be estimated numerically for\nmore general types of generative models [2,16]. Overall, by estimating these quantities, one can use\n(3.5) to gauge how well one can recover with a given P. Note that this may not be possible if (3.5)\ninvolved R as well, since this distribution is typically unknown.\nIn classical compressed sensing, coherence determines the sample complexity of recovering sparse\nvectors from randomly-subsampled orthogonal transforms [26]. A similar argument can be made in\nthe Bayesian setting. Notice that µ(U; D) ≤µ(U; Rn) = n. However, we are particularly interested\nin cases where µ(U; D) ≪n, in which case (3.5) may be significantly smaller than the ambient\ndimension n. We discuss this in the context of several examples in the next section.\n8\nRemark 3.10 (Concentration over subsets) Theorem 3.9 illustrates why it is important that Theo-\nrem 3.1 involves concentration bounds over subsets of Rn. To derive Theorem 3.9 from Theorem 3.1\n(see §B), we show exponentially-fast concentration in m/µ(U; D). Had we considered the whole of\nRn, then, since µ(U; Rn) = n, this would have lead to an undesirable measurement condition of the\nform m = O(n) scaling linearly in the ambient dimension n.\n4\nCovering number and sample complexity estimates\nWe conclude by applying our results to two different types of approximate prior distributions.\n4.1\nGenerative DNNs\nProposition 4.1 (Approximate covering number for a Lipschitz pushforward of a Gaussian measure).\nLet G : Rk →Rn be Lipschitz with constant L ≥0, i.e., ∥G(x) −G(z)∥≤L∥x −z∥, ∀x, z ∈Rk,\nand define P = G♯γ, where γ = N(0, I) is the standard normal distribution on Rk. Then\nlog(Covη,δ(P)) ≤k log\n\"\n1 + 2\n√\nkL\nη\n \n1 +\nr\n2\nk log(1/δ)\n!#\n.\n(4.1)\nThis result shows that P has low complexity, since log(Covη,δ(P)) scales log-linearly in k. Combined\nwith Theorem 3.5, it shows that accurate and stable Bayesian recovery with such a prior with a sample\ncomplexity that is near-optimal in the latent dimension k, i.e., O(k log(k)).\nNotice that L only appears logarithmically in (4.1). While it is not the main focus of this work, we\nnote that Lipschitz constants of DNNs have been studied quite extensively [37,72,75]. Moreover, it\nis also possible to design and train DNNs with small Lipschitz constants [59].\nRemark 4.2 (Quadratic bottleneck and high coherence) In Theorem 3.9, the measurement con-\ndition (3.5) also depends on the coherence. This quantity has been considered in [16] for the case\nof ReLU DNNs. In particular, if a ReLU DNN G : Rk →Rn has random weights drawn from a\nstandard normal distribution, then its coherence µ(U; D) scales like O(k) up to log factors [16, Thm.\n3]. Combining this with Theorem 3.9 and Proposition 4.1, we see that the overall sample complexity\nfor Bayesian recovery scales like O(k2 log(k)) in this case. This is worse than the subgaussian case,\nwhere there is no coherence factor and the sample complexity, as noted above, is O(k log(k)). Such\na quadratic bottleneck also arises in the non-Bayesian setting [16]. Its removal is an open problem\n(see §5). Note that the coherence is also not guaranteed to be small for general (in particular, trained)\nDNNs. However, [16] also discuss strategies for training generative models to have small coherence.\nNumerically, they show that generative models with smaller coherence achieve better recovery from\nthe same number of measurements than those with larger coherence.\n4.2\nDistributions of sparse vectors\nLet s ∈{1, . . . , n}. We define a distribution P = Ps of s-sparse vectors in Rn as follows. To draw\nx ∼P, we first choose a support set S ⊆{1, . . . , n}, |S| = s, uniformly at random amongst all\npossible\n\u0000n\ns\n\u0001\nsuch subsets. We then define xi = 0, i /∈S, and for each i ∈S we draw xi randomly\nand independently from N(0, 1). Note that there are other ways to define distributions of sparse\nvectors, which can be analyzed similarly. However, for brevity we only consider the above setup.\nProposition 4.3 (Approximate covering number for distributions of sparse vectors). Let P = Ps be\na distribution of s-sparse vectors in Rn. Then\nlog(Covη,δ(Ps)) ≤s\n\"\nlog\n\u0010en\ns\n\u0011\n+ log\n \n1 + 2√s\nη\n \n1 +\nr\n2\ns log(1/δ)\n!!#\n.\n(4.2)\nAs in the previous case, we deduce Bayesian recovery from O(s log(n/s)) subgaussian measurements,\ni.e., near-optimal, log-linear sample complexity. In the case of randomly-subsampled orthogonal\nmatrices, we also need to consider the coherence. For P = Ps as above, one can easily show that\nµ(U; supp(Ps) −supp(Ps)) ≤2sµ∗(U),\nwhere µ∗(U) = n · max\ni,j |uij|2.\n(4.3)\n9\nThe term µ∗(U) is often referred to as the coherence of U (see, e.g., [5, Defn. 5.8] or [26]). Notice\nthat µ∗(U) ≈1 for DFT matrices, which is one reason why subsampled Fourier transforms are\nparticularly effective in (non-Bayesian) compressed sensing. Our work implies as similar conclusion\nin the Bayesian setting: indeed, substituting (4.2) and (4.3) into (3.5) we immediately deduce that the\nmeasurement condition for Bayesian recovery with Ps behaves like m = O(s2) up to log terms.\nRemark 4.4 (Quadratic bottleneck) Once more we witness a quadratic bottleneck. In the non-\nBayesian setting, one can show stable and accurate recovery of sparse vectors from O(s) measure-\nments, up to log terms (see, e.g., [5, Cor. 13.15]). However, this requires specialized theoretical\ntechniques that heavily leverage the structure of the set of sparse vectors. In the setting of this paper,\nthe bottleneck arises from the generality of the approach considered in this work: specifically, the\nfact that our main results hold for arbitrary probability distributions P.\n5\nLimitations and future work\nWe end by discussing a number of limitations and avenues for future work. First, although our\nmain result Theorem 3.1 is very general, we have only applied it to a number of different cases,\nsuch as Lipschitz pushforward measures and Gaussian random matrices or subsampled orthogonal\ntransforms. We believe many other important problems can be studied as corollaries of our main\nresults. This includes sampling with heavy-tailed vectors [50], sampling with random convolutions\n[67], multi-sensor acquisition problems [27], generative models augmented with sparse deviations\n[32], block sampling [3,20,24], with applications to practical MRI acquisition, sparse tomography [8],\ndeconvolution and inverse source problems [9]. We believe our framework can also be applied to\nvarious types of non-Gaussian noise, as well as problems involving sparsely-corrupted measurements\n[50]. We are actively investigating applying our framework to these problems.\nSecond, as noted in Remarks 4.2 and 4.4 there is a quadratic bottleneck when considering subsampled\northogonal transforms. In the non-Bayesian case, this can be overcome in the case of (structured)\nsparse models using more technical arguments [3]. We believe similar ideas could also be exploited\nin the Bayesian setting. On a related note, both [16, 22] consider ReLU generative models in the\nnon-Bayesian setting, and derive measurement conditions that do not involve the Lipschitz constant\nL of the DNN. It is unclear whether analogous results can be established in the Bayesian setting.\nThird, our main result involves the density shift bound (Definition 2.5). In particular, the noise\ndistribution should have a density. This rather unpleasant technical assumption stems from Lemma\nC.6, which is a key step in proving the main result, Theorem 3.1. This lemma allows one to replace\nthe ‘real’ distribution R in the probability term p in Theorem 3.1 by the approximate distribution P.\nThis is done in order to align the prior and the posterior, which is necessary for the subsequent steps\nof the proof of Theorem 3.1. It would be interesting to see if this assumption on the noise could be\nremoved through a refined analysis.\nFourth, as noted in [16], the coherence µ(U; D) arising in Theorem 3.9 may be high. In the non-\nBayesian setting, this has been addressed in [2,3,17] by using a nonuniform probability distribution\nfor drawing rows of U, with probabilities given in terms of so-called local coherences of U with\nrelative to D. As shown therein, this can lead to significant performance gains over sampling\nuniformly at random. We believe a similar approach can be considered in the Bayesian setting as a\nconsequence of our general framework. We intend to explore this in future work.\nFinally, our results in this paper are theoretical, and strive to study the sample complexity of Bayesian\nrecovery. We do not address the practical problem of sampling from the posterior. This is a key\ncomputational challenge in Bayesian inverse problems [12]. However, efficient techniques for doing\nthis are emerging. See, e.g., [48,54,70] and references therein. We believe an advantage of our results\nis their independence from the choice of posterior sampling algorithm, whose analysis can therefore\nbe performed separately. This is an interesting problem we intend to examine in the future. In future\nwork we also intend present numerical experiments for various practical settings that further support\nthe theory developed in this paper.\n10\nAcknowledgments and Disclosure of Funding\nBA acknowledges the support of the Natural Sciences and Engineering Research Council of Canada\nof Canada (NSERC) through grant RGPIN-2021-611675. NH acknowledges support from an NSERC\nCanada Graduate Scholarship. Both authors would like to thank Paul Tupper and Weiran Sun for\nhelpful comments and feedback.\nReferences\n[1] A. Aali, M. Arvinte, S. Kumar, and J. I. Tamir. Solving inverse problems with score-based\ngenerative priors learned from noisy data. In 2023 57th Asilomar Conference on Signals,\nSystems, and Computers, pages 837–843, 2023.\n[2] B. Adcock, J. M. Cardenas, and N. Dexter. CS4ML: A general framework for active learning\nwith arbitrary data based on Christoffel functions. In A. Oh, T. Naumann, A. Globerson,\nK. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing\nSystems, volume 36, pages 19990–20037, 2023.\n[3] B. Adcock, J. M. Cardenas, and N. Dexter. A unified framework for learning with nonlinear\nmodel classes from arbitrary linear samples. In International Conference on Machine Learning,\n2024.\n[4] B. Adcock, A. Gelb, G. Song, and Y. Sui. Joint sparse recovery based on variances. SIAM J.\nSci. Comput., 41(1):A246–A268, 2019.\n[5] B. Adcock and A. C. Hansen. Compressive Imaging: Structure, Sampling, Learning. Cambridge\nUniversity Press, Cambridge, UK, 2021.\n[6] B. Adcock, A. C. Hansen, C. Poon, and B. Roman. Breaking the coherence barrier: a new\ntheory for compressed sensing. Forum Math. Sigma, 5:e4, 2017.\n[7] J. Adler and O. Öktem. Deep Bayesian inversion. In Data-Driven Models in Inverse Problems,\npages 359–412. De Gruyter, Berlin, Boston, 2025.\n[8] G. S. Alberti, A. Felisi, M. Santacesaria, and S. I. Trapasso. Compressed sensing for inverse\nproblems and the sample complexity of the sparse Radon transform. J. Eur. Math. Soc. (in\npress), 2025.\n[9] G. S. Alberti, A. Felisi, M. Santacesaria, and S. I. Trapasso. Compressed sensing for inverse\nproblems ii: applications to deconvolution, source recovery, and MRI. arXiv:2501.01929, 2025.\n[10] L. Ambrosio, E. Brué, and D. Semola. Lectures on Optimal Transport. UNITEXT. Springer,\nCham, Switzerland, 2nd edition, 2024.\n[11] V. Antun, F. Renna, C. Poon, B. Adcock, and A. C. Hansen. On instabilities of deep learning in\nimage reconstruction and the potential costs of AI. Proc. Natl. Acad. Sci. USA, 117(48):30088–\n30095, 2020.\n[12] S. Arridge, P. Maass, O. Öktem, and C.-B. Schönlieb. Solving inverse problems using data-\ndriven models. Acta Numer., 28:1–174, 2019.\n[13] R. G. Baraniuk, V. Cevher, M. F. Duarte, and C. Hedge. Model-based compressive sensing.\nIEEE Trans. Inform. Theory, 56(4):1982–2001, 2010.\n[14] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applica-\ntions to compressed sensing. IEEE Trans. Inform. Theory, 57(2):764–785, 2011.\n[15] C. Belthangady and L. A. Royer. Applications, promises, and pitfalls of deep learning for\nfluorescence image reconstruction. Nature methods, 16(12):1215–1225, 2019.\n[16] A. Berk, S. Brugiapaglia, B. Joshi, Y. Plan, M. Scott, and O. Yilmaz. A coherence parameter\ncharacterizing generative compressed sensing with fourier measurements. IEEE J. Sel. Areas\nInf. Theory, 3(3):502–512, 2022.\n[17] A. Berk, S. Brugiapaglia, Y. Plan, M. Scott, X. Sheng, and O. Yilmaz. Model-adapted Fourier\nsampling for generative compressed sensing. In NeurIPS 2023 Workshop on Deep Learning\nand Inverse Problems, 2023.\n[18] S. Bhadra, V. A. Kelkar, F. J. Brooks, and M. A. Anastasio. On hallucinations in tomographic\nimage reconstruction. IEEE Trans. Med. Imaging, 40(11):3249–3260, 2021.\n11\n[19] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.\nAnn. Statist., 37(4):1705–1732, 2009.\n[20] J. Bigot, C. Boyer, and P. Weiss. An analysis of block sampling strategies in compressed sensing.\nIEEE Trans. Inform. Theory, 62(4):2125–2139, 2016.\n[21] P. Bohra, J. Pham, T.-A. Dong, and M. Unser. Bayesian inversion for nonlinear imaging models\nusing deep generative priors. IEEE Trans. Comput. Imag., 8:1237–1249, 2023.\n[22] A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models.\nIn International Conference on Machine Learning, pages 537–546, 2017.\n[23] A. Bourrier, M. E. Davies, T. Peleg, P. Pérez, and R. Gribonval. Fundamental performance\nlimits for ideal decoders in high-dimensional linear inverse problems. IEEE Trans. Inform.\nTheory, 60(12):7928–7946, 2014.\n[24] C. Boyer, J. Bigot, and P. Weiss. Compressed sensing with structured sparsity and structured\nacquisition. Appl. Comput. Harmon. Anal., 46(2):312–350, 2019.\n[25] M. Burger and T. Roith. Learning in image reconstruction: A cautionary tale. SIAM News,\n57(08), Oct 2024.\n[26] E. J. Candès and Y. Plan. A probabilistic and RIPless theory of compressed sensing. IEEE\nTrans. Inform. Theory, 57(11):7235–7254, 2011.\n[27] I.-Y. Chun and B. Adcock. Compressed sensing and parallel acquisition. IEEE Trans. Inform.\nTheory, 63(8):4860–4882, 2017.\n[28] H. Chung and J. C. Ye. Score-based diffusion models for accelerated mri. Medical Image\nAnalysis, 80:102479, 2022.\n[29] M. J. Colbrook, V. Antun, and A. C. Hansen. The difficulty of computing stable and accurate\nneural networks: On the barriers of deep learning and smale’s 18th problem. Proc. Natl. Acad.\nSci. USA, 119(12):e2107151119, 2022.\n[30] M. Dashti and A. M. Stuart. The bayesian approach to inverse problems. In R. Ghanem et al.,\neditor, Handbook of Uncertainty Quantification. Springer, 2017.\n[31] M. A. Davenport, M. F. Duarte, Y. C. Eldar, and G. Kutyniok. Introduction to compressed\nsensing. In Y. C. Eldar and G. Kutyniok, editors, Compressed Sensing: Theory and Applications,\npages 1–64. Cambridge University Press, Cambridge, UK, 2012.\n[32] M. Dhar, A. Grover, and S. Ermon. Modeling sparse deviations for compressed sensing using\ngenerative models. In International Conference on Machine Learning, pages 1214–1223.\nPMLR, 2018.\n[33] A. G. Dimakis. Deep generative models and inverse problems. In P. Grohs and G. Kutyniok, ed-\nitors, Mathematical Aspects of Deep Learning, chapter 9, pages 400–421. Cambridge University\nPress, Cambridge, UK, 2022.\n[34] S. Dirksen. Dimensionality reduction with subgaussian matrices: a unified theory. Found.\nComput. Math., 16:1367–1396, 2016.\n[35] D. L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed\nsensing. Proc. Natl. Acad. Sci. USA, 106(45):18914–18919, 2009.\n[36] M. F. Duarte and Y. C. Eldar. Structured compressed sensing: from theory to applications. IEEE\nTrans. Signal Process., 59(9):4053–4085, 2011.\n[37] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J. Pappas. Efficient and accurate estimation\nof Lipschitz constants for deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d’Alché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 33. Curran Associates, Inc., 2019.\n[38] B. T. Feng, J. Smith, M. Rubinstein, H. Chang, K. L. Bouman, and W. T. Freeman. Score-based\ndiffusion models as principled priors for inverse imaging. In 2023 IEEE/CVF International\nConference on Computer Vision (ICCV), pages 10486–10497, 2023.\n[39] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Appl. Numer.\nHarmon. Anal. Birkhäuser, New York, NY, 2013.\n[40] M. Genzel, J. Macdonald, and M. Marz. Solving inverse problems with deep neural networks –\nrobustness included? IEEE Trans. Pattern Anal. Mach. Intell., 45(1):1119–1134, 2023.\n12\n[41] C. R. Givens and R. M. Shortt. A class of Wasserstein metrics for probability distributions.\nMichigan Math. J., 31(2):231–240, 1984.\n[42] N. M. Gottschling, V. Antun, A. C. Hansen, and B. Adcock. The troublesome kernel – on\nhallucinations, no free lunches and the accuracy-stability trade-off in inverse problems. SIAM\nRev., 67(1):73–104, 2025.\n[43] P. Hand, O. Leong, and V. Voroninski. Phase retrieval under a generative prior. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in\nNeural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n[44] P. Hand and V. Voroninski. Global guarantees for enforcing deep generative priors by empirical\nrisk. In S. Bubeck, V. Perchet, and P. Rigollet, editors, Proceedings of the Thirty-First Confer-\nence on Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages\n970–978. PMLR, 2018.\n[45] D. P. Hoffman, I. Slavitt, and C. A. Fitzpatrick. The promise and peril of deep learning in\nmicroscopy. Nature Methods, 18(2):131–132, 2021.\n[46] M. Holden, M. Pereyra, and K. C. Zygalakis. Bayesian imaging with data-driven priors encoded\nby neural networks. SIAM J. Imaging Sci., 15(2):892–924, 2022.\n[47] Y. Huang, T. Würfl, K. Breininger, L. Liu, G. Lauritsch, and A. Maier. Some investigations\non robustness of deep learning in limited angle tomography. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention, pages 145–153, 2018.\n[48] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir. Robust compressed\nsensing mri with deep generative priors. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S.\nLiang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems,\nvolume 34, pages 14938–14954. Curran Associates, Inc., 2021.\n[49] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price. Instance-optimal compressed sensing via\nposterior sampling. In 38th International Conference on Machine Learning, pages 4709–4720,\n2021.\n[50] A. Jalal, L. Liu, A. G. Dimakis, and C. Caramanis. Robust compressed sensing using generative\nmodels. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 713–727. Curran Associates, Inc.,\n2020.\n[51] Z. Kadkhodaie and E. Simoncelli. Stochastic solutions for linear inverse problems using the\nprior implicit in a denoiser. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,\npages 13242–13254. Curran Associates, Inc., 2021.\n[52] A. Karan, K. Shah, S. Chen, and Y. C. Eldar. Unrolled denoising networks provably learn\nto perform optimal Bayesian inference. In A. Globerson, L. Mackey, D. Belgrave, A. Fan,\nU. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing\nSystems, volume 37, pages 135264–135298. Curran Associates, Inc., 2024.\n[53] B. Kawar, M. Elad, S. Ermon, and J. Song. Denoising diffusion restoration models. In\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in\nNeural Information Processing Systems, volume 35, pages 23593–23606. Curran Associates,\nInc., 2022.\n[54] B. Kawar, G. Vaksman, and M. Elad. SNIPS: solving noisy inverse problems stochastically.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, volume 34, pages 21757–21769. Curran\nAssociates, Inc., 2021.\n[55] R. F. Laine, I. Arganda-Carreras, R. Henriques, and G. Jacquemet. Avoiding a replication crisis\nin deep-learning-based bioimage analysis. Nature Methods, 18(10):1136–1144, 2021.\n[56] R. Laumont, V. D. Bortoli, A. Almansa, J. Delon, A. Durmus, and M. Pereyra. Bayesian imaging\nusing plug & play priors: When langevin meets tweedie. SIAM J. Imaging Sci., 15(2):701–737,\n2022.\n[57] X. Liu, B. Glocker, M. M. McCradden, M. Ghassemi, A. K. Denniston, and L. Oakden-Rayner.\nThe medical algorithmic audit. The Lancet Digital Health, 4(5):e384–e397, 2022.\n13\n[58] G. Luo, M. Blumenthal, M. Heide, and M. Uecker. Bayesian MRI reconstruction with joint\nuncertainty estimation using diffusion models. Magn. Reson. Med., 90(1):295–311, 2023.\n[59] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative\nadversarial networks. In International Conference on Learning Representations, 2018.\n[60] J. N. Morshuis, S. Gatidis, M. Hein, and C. F. Baumgartner. Adversarial robustness of MR\nimage reconstruction under realistic perturbations. arXiv:2208.03161, 2022.\n[61] Matthew J Muckley, Bruno Riemenschneider, Alireza Radmanesh, Sunwoo Kim, Geunu Jeong,\nJingyu Ko, Yohan Jun, Hyungseob Shin, Dosik Hwang, Mahmoud Mostapha, et al. Results of\nthe 2020 fastMRI challenge for machine learning MR image reconstruction. IEEE Trans. Med.\nImaging, 2021.\n[62] M. Neyra-Nesterenko and B. Adcock. NESTANets: stable, accurate and efficient neural\nnetworks for analysis-sparse inverse problems. Sampl. Theory Signal Process. Data Anal., 21:4,\n2023.\n[63] C. R. Noordman, D. Yakar, J. Bosma, F. F. J. Simonis, and H. Huisman. Complexities of deep\nlearning-based undersampled MR image reconstruction. Eur. Radiol. Exp., 7:58, 2023.\n[64] G. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and R. Willett. Deep learning\ntechniques for inverse problems in imaging. IEEE J. Sel. Areas Inf. Theory, 1(1):39–56, 2020.\n[65] A. Raj, Y. Bresler, and B. Li. Improving robustness of deep-learning-based image reconstruction.\nIn Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages\n7932–7942. PMLR, 13–18 Jul 2020.\n[66] A. J. Reader and B. Pan. AI for PET image reconstruction. Brit. J. Radiol., 96(1150):20230292,\n2023.\n[67] J. Romberg. Compressive sensing by random convolution. SIAM J. Imaging Sci., 2(4):1098–\n1128, 2009.\n[68] J. Scarlett, R. Heckel, M. R. D. Rodrigues, P. Hand, and Y. C. Eldar. Theoretical perspectives on\ndeep learning methods in inverse problems. IEEE J. Sel. Areas Inf. Theory, 3(3):433–453, 2022.\n[69] V. Shah and C. Hegde. Solving linear inverse problems using GAN priors: An algorithm with\nprovable guarantees. In 2018 IEEE international conference on Acoustics, Speech and Signal\nProcessing (ICASSP) conference on acoustics, speech and signal processing (ICASSP), pages\n4609–4613. IEEE, 2018.\n[70] Y. Song, L. Shen, L. Xing, and S. Ermon. Solving inverse problems in medical imaging with\nscore-based generative models. In International Conference on Learning Representations, 2022.\n[71] A. M. Stuart. Inverse problems: a Bayesian perspective. Acta Numer., 19:451–559, 2010.\n[72] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, J. Ian Goodfellow, and R. Fergus.\nIntriguing properties of neural networks. In Proceedings of the International Conference on\nLearning Representations, 2014.\n[73] Y. Traonmilin and R. Gribonval. Stable recovery of low-dimensional cones in Hilbert spaces:\none RIP to rule them all. Appl. Comput. Harmon. Anal., 45(1):170–205, 2018.\n[74] G. Varoquaux and V. Cheplygina. Machine learning for medical imaging: methodological\nfailures and recommendations for the future. NPJ digital medicine, 5(1):1–8, 2022.\n[75] A. Virmaux and K. Scaman. Lipschitz regularity of deep neural networks: analysis and efficient\nestimation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran\nAssociates, Inc., 2018.\n[76] E. Wu, K. Wu, R. Daneshjou, D. Ouyang, D. E. Ho, and J. Zou. How medical AI devices\nare evaluated: limitations and recommendations from an analysis of FDA approvals. Nature\nMedicine, 27(4):582–584, 2021.\n[77] T. Yu, T. Hilbert, G. G. Piredda, A. Joseph, G. Bonanno, S. Zenkhri, P. Omoumi, M. B. Cuadra,\nE. J. Canales-Rodríguez, T. Kober, et al. Validation and generalizability of self-supervised\nimage reconstruction methods for undersampled MRI. arXiv:2201.12535, 2022.\n[78] M. Zach, F. Knoll, and T. Pock. Stable deep MRI reconstructions using generative priors. IEEE\nTrans. Med. Imag., 42(12):3817–3831, 2023.\n14\n[79] C. Zhang, J. Jia, B. Yaman, S. Moeller, S. Liu, M. Hong, and M. Akçakaya. Instabilities in\nconventional multi-coil MRI reconstruction with small adversarial perturbations. In 2021 55th\nAsilomar Conference on Signals, Systems, and Computers, pages 895–899, 2021.\n[80] Z. Zhao, J. C. Ye, and Y. Bresler. Generative models for inverse imaging problems: from\nmathematical foundations to physics-driven applications. IEEE Signal Process. Mag., 40(1):148–\n163, 2023.\n15\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: We thoroughly discuss the main claims made in the abstract and introduction\nand the necessary assumptions to show them. Our main theoretical contributions directly\naddress these claims. We also have several further remarks after these results to provide\nadditional context for our work.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We discuss limitations at the end of the paper in §5.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n16\nAnswer: [Yes]\nJustification: We have a detailed discussion of the assumptions needed to show our theoreti-\ncal results in §2. We provide further discussion and present the results themselves in §3-4.\nWe provide full proofs of our results in the supplemental material.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\n17\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not\nbe possible, so No is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n18\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We have complied with the NeurIPS Code of Ethics in the preparation of this\nmanuscript.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: This work is primarily foundational, and the examples considered do not\ndirectly impact society.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n19\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This paper is theoretical, and does not involve any data or models.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: This paper is theoretical, and does not use any existing assets.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n20\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: This paper is theoretical, and does not introduce any new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: No experiments were conducted in this paper.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: No experiments were conducted in this paper.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\n21\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: The core method development in this paper does not involve LLMs as an\nimportant, original, or non-standard component.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n22\nA\nCovering number estimates and the proofs of Propositions 4.1 and 4.3\nThe proof of Proposition 4.1 relies on the following two lemmas.\nLemma A.1 (Approximate covering number under a Lipschitz pushforward map). Let G : Rk →Rn\nbe Lipschitz with constant L ≥0, i.e.,\n∥G(x) −G(z)∥≤L∥x −z∥,\n∀x, z ∈Rk,\nand define P = G♯γ, where γ is any probability distribution on Rk. Then\nCovη,δ(P) ≤Covη/L,δ(γ),\n∀δ, η ≥0.\nProof. Let {xi}k\ni=1 ⊆supp(γ) and define zi = G(xi) ∈supp(G♯γ) for i = 1, . . . , k. Let\nz ∈G(Bη/L(xi)) and write z = G(x) for some x ∈Bη/L(xi). Then\n∥z −zi∥= ∥G(x) −G(xi)∥≤L∥x −xi∥≤η.\nHence z ∈Bη(zi). Since z was arbitrary, we deduce that G(Bη/L(xi)) ⊆Bη(zi). It follows that\nBη/L(xi) ⊆G−1(Bη(zi)). Now suppose that γ\nhSk\ni=1 Bη/L(xi)\ni\n≥1 −δ. Then, by definition of\nthe pushforward measure\nG♯γ\n\" k[\ni=1\nBη(zi)\n#\n= γ\n\" k[\ni=1\nG−1(Bη(zi))\n#\n≥γ\n\" k[\ni=1\nBη/L(xi)\n#\n≥1 −δ.\nThis gives the result.\nLemma A.2 (Approximate covering number of a normal distribution). Let P = N(0, σ2I) on Rn.\nThen its approximate covering number (Definition 2.1) satisfies\nCovη,δ(P) ≤\n\u0012\n1 + 2√nσt\nη\n\u0013n\n,\nwhere t = 1 +\nr\n2\nn log(1/δ).\nProof. Observe that, for t ≥0,\nP(Bc√nσt) = P(X ≥nt2) ≤(t2e1−t2)n/2.\nwhere X ∼χ2\nn is a chi-squared random variable, and the inequality follows from a standard Chernoff\nbound. Now t2 ≤e2t gives that\nP(Bc√nσt) ≤(e−(t−1)2)n/2.\nNow set t = 1 +\nq\n2\nn log(1/δ) so that P(Bc√nσt) ≤δ. Hence, we have shown that\nCovη,δ(P) ≤Covη(B√nσt),\nwhere Covη is the classical covering number of a set, i.e.,\nCovη(A) = min\n(\nk : ∃{xi}k\ni=1 ⊆A, A ⊆\nk[\ni=1\nBη(xi)\n)\n.\nUsing standard properties of covering numbers (see, e.g., [5, Lem. 13.22], we get\nCovη(B√nσt) = Covη/(√nσt)(B1) ≤\n\u0012\n1 + 2√nσt\nη\n\u0013n\n,\nas required.\nProof of Proposition 4.1. By Lemma A.1,\nCovη,δ(P) ≤Covη/L,δ(N(0, I)).\nThe result now follows from Lemma A.2.\n23\nTo prove Proposition 4.3, we first require the following lemma.\nLemma A.3 (Approximate covering number of a mixture). Let P = Pr\ni=1 piPi be a mixture of\nprobability distributions Pi on Rn, where pi ≥0, ∀i, and Pr\ni=1 pi = 1. Then\nCovη,δ(P) ≤\nr\nX\ni=1\nCovη,δ(Pi).\nProof. For each i = 1, . . . , r, let {x(i)\nj }ki\nj=1 ⊆Rn, and, in particular, 3x(i)\nj\n∈supp(Pi), be such that\nPi\n\n\nki\n[\nj=1\nBη(x(i)\nj )\n\n≥1 −δ.\nThen\nP\n\n\nr[\ni=1\nki\n[\nj=1\nBη(x(i)\nj )\n\n=\nr\nX\ni=1\npiPi\n\n\nr[\ni=1\nki\n[\nj=1\nBη(x(i)\nj )\n\n\n≥\nr\nX\ni=1\npiPi\n\n\nki\n[\nj=1\nBη(x(i)\nj )\n\n\n≥\nr\nX\ni=1\npi(1 −δ) = 1 −δ.\nNotice that supp(Pi) ⊆supp(P), therefore, x(i)\nj\n∈supp(P). The result now follows.\nProof of Proposition 4.3. Let S = {S : S ⊆{1, . . . , n}, |S| = s}. Then we can write Ps as the\nmixture\nPs =\n|S|\nX\ni=1\n1\n|S|PS,\nwhere PS is defined as follows: x ∼PS if xi = 0 for i /∈S and, for i ∈S, xi is drawn independently\nfrom the standard normal distribution on R. Notice that PS = G♯γ, where γ = N(0, I) is the\nstandard, multivariate normal distribution on Rs and G : Rs →Rn is a zero-padding map. The map\nG is Lipschitz with constant L = 1. Hence, by Lemmas A.1 and A.2,\nCovη,δ(PS) ≤\n\u0012\n1 + 2√st\nη\n\u0013s\n,\nt = 1 +\nr\n2\ns log(1/δ)\nWe now apply Lemma A.3 and the fact that\n|S| =\n\u0012n\ns\n\u0013\n≤\n\u0010en\ns\n\u0011s\n,\nthe latter being a standard bound, to obtain\nCovη,δ(Ps) ≤\n\u0010en\ns\n\u0011s \u0012\n1 + 2√st\nη\n\u0013s\n,\nt = 1 +\nr\n2\ns log(1/δ).\nTaking logarithms gives the result.\n24\nB\nConcentration inequalities and the proofs of Theorems 3.5 and 3.9\nWe now aim to prove Theorems 3.5 and 3.9. To do this, we first derive concentration inequalities for\nsubsgaussian random matrices and subsampled orthogonal transforms.\nB.1\nGaussian concentration and density shift bounds\nLemma B.1 (Concentration and density shift bounds for Gaussian noise). Let E = N(0, σ2\nm I). Then\nthe upper concentration bound Dupp(t; E) (Definition 2.4) can be taken as\nDupp(t; E) =\n\u0012 t2\nσ2 e1−t2\nσ2\n\u0013m/2\n,\n∀t > σ.\nand the density shift bound Dshift(ε, τ; E) (Definition 2.5) can be taken as\nDshift(ε, τ; E) = exp\n\u0012m(2τ + ε)\n2σ2\nε\n\u0013\n,\n∀ε, τ ≥0.\nProof. Write e ∼E as e =\nσ\n√mn, where n ∼N(0, I). Then\nP(∥e∥≥t) = P(∥n∥2 ≥t2m/σ2) = P(X ≥t2m/σ2),\nwhere X = ∥n∥2 ∼χ2\nm is a chi-squared random variable with m degrees of freedom. Using a\nstandard Chernoff bound once more, we have\nP(X ≥zm) ≤(ze1−z)m/2,\nfor any z > 1. Setting z = t2\nσ2 , we have\nP(∥e∥≥t) ≤\n\u0012 t2\nσ2 e1−t2\nσ2\n\u0013m/2\n,\nwhich gives the first result.\nFor the second result, we recall that E has density\npE(e) = (2πσ2/m)−m/2 exp\n\u0010\n−m\n2σ2 ∥e∥2\u0011\n.\nTherefore\npE(u)\npE(v) = exp\n\u0010 m\n2σ2 (∥v∥−∥u∥) (∥v∥+ ∥u∥)\n\u0011\n.\nNow suppose that ∥u∥≤τ and ∥u −v∥≤ε. Then\npE(u)\npE(v) ≤exp\n\u0012m(2τ + ε)\n2σ2\nε\n\u0013\n.\nHence\nDshift(ε, τ; E) ≤exp\n\u0012m(2τ + ε)\n2σ2\nε\n\u0013\n,\nwhich gives the second result.\nB.2\nSubgaussian concentration inequalities\nLemma B.2 (Lower and upper concentration bounds for subgaussian random matrices). Let A be a\ndistribution of subgaussian random matrices with parameters β, κ > 0 (Definition 3.4). Then the\nlower ad upper concentration bounds for A (Definition 2.3) can be taken as\nCupp(t; A, Rn) = Clow(1/t; A, Rn) = 2 exp(−c(t, β, κ)m)\nfor any t > 1, where c(t, β, κ) > 0 depends on β, κ only.\n25\nProof. Let x ∈Rn and observe that\nP(∥Ax∥≥t∥x∥) ≤P\n\u0010\f\f\f∥Ax∥2 −∥x∥2\f\f\f ≥(t2 −1)∥x∥2\u0011\nP(∥Ax∥≤t−1∥x∥) ≤P\n\u0010\f\f\f∥Ax∥2 −∥x∥2\f\f\f ≥(1 −t−2)∥x∥2\u0011.\n(B.1)\nWe now use [39, Lem. 9.8]. Note that this result only considers a bound of the form P(|∥Ax∥2 −\n∥x∥2| ≥s∥x∥2) for s ∈(0, 1). But the proof straightforwardly extends to s > 0.\nB.3\nConcentration inequalities for randomly-subsampled orthogonal transforms\nLemma B.3 (Concentration bounds for randomly-subsampled orthogonal transforms). Let D ⊆\nRn and A be a distribution of randomly-subsampled orthognal transforms based on a matrix U\n(Definition 3.7). Then the lower and upper concentration bounds for A (Definition 2.3) can be taken\nas\nCupp(t; A, D) = Clow(1/t; A, D) = 2 exp\n\u0012\n−mc(t)\nµ(U; D)\n\u0013\nfor any t > 1, where µ(U; D) is a in Definition 3.8 and c(t) > 0 depend on t only.\nProof. Due to (B.1), it suffices to bound\nP\n\u0010\f\f\f∥Ax∥2 −∥x∥2\f\f\f ≥s∥x∥2\u0011\nfor s > 0. The result uses Bernstein’s inequality for bounded random variables (see, e.g., [5, Thm.\n12.18]). Let x ∈D. By definition of A and the fact that U is orthogonal, we can write\n∥Ax∥2 −∥x∥2 =\nn\nX\ni=1\n\u0010 n\nmIXi=1 −1\n\u0011\n|⟨ui, x⟩|2 =:\nN\nX\ni=1\nZi.\nNotice that the random variables Zi are independent, with E(Zi) = 0. We also have\n|Zi| ≤n\nm|⟨ui, x⟩|2 ≤µ(U; D)\nm\n∥x∥2 =: K\nand\nn\nX\ni=1\nE|Zi|2 ≤\nn\nX\ni=1\nn2|⟨ui, x⟩|4\nm2\nE(I2\nXi=1) ≤K\nn\nX\ni=1\nn|⟨ui, x⟩|2\nm\nm\nn = K∥x∥2 =: σ2.\nTherefore, by Bernstein’s inequality,\nP(|∥Ax∥2 −∥x∥2| ≥s∥x∥2) ≤2 exp\n \n−\ns2∥x∥4/2\nσ2 + Ks∥x∥2/3)\n!\n= 2 exp\n\u0012\n−\nms2/2\nµ(U; D)(1 + s/3)\n\u0013\nfor any s > 0 and x ∈D. The result now follows.\nLemma B.4 (Absolute concentration bounds for subsampled orthogonal transforms). Let D ⊆Rn\nand A be a distribution of randomly-subsampled orthogonal transforms based on a matrix U\n(Definition 3.7). Then ∥A∥≤\np\nn/m a.s. for A ∼A, and consequently the absolute concentration\nbound for A (Definition 2.3) can be taken as Cabs(s, t; A, D) = 0 for any s ≥0 and t ≥s\np\nn/m.\nProof. Recall that A consists of q rows of an orthogonal matrix U multiplied by the scalar\np\nn/m.\nHence ∥A∥≤\np\nn/m∥U∥=\np\nn/m. Now let x ∈D with ∥x∥≤s. Then ∥Ax∥≤∥A∥∥x∥≤\n∥A∥s. Therefore ∥Ax∥\np\nn/ms ≤t, meaning that P(∥Ax∥> t) = 0. This gives the result.\n26\nB.4\nProofs of Theorems 3.5 and 3.9\nProof of Theorem 3.5. Let p = P [∥x∗−ˆx∥≥34(η + σ)]. We use Theorem 3.1 with c = 32, c′ = 2,\nt = 2 and ε replaced by ε/d, where d ≥1 is a constant that will be chosen later. Let ε′ = ε/(dδ1/p).\nThen Theorem 3.1 gives\np ≲δ + Cabs(ε′, 2ε′; A, Rn) + Dupp(2σ; E)\n+ 2Dshift(2ε′, 2σ; E)ek [Clow (1/2; A, Rn) + Cupp (2; A, Rn) + 2Dupp(2σ; E)] .\nConsider Cabs(ε′, 2ε′; A, Rn). If x ∈Rn with ∥x∥≤s, then\nP(∥Ax∥> t) ≤P(∥Ax∥> (t/s)∥x∥).\nHence, in this case, we may take\nCabs(ε′, 2ε′; A, Rn) = Cupp(2; A, Rn).\n(B.2)\nNow by Lemma B.2, we have that\nClow(1/2; A, Rn) = Cupp(2; A, Rn) = exp(−c(β, κ)m),\nwhere c(β, κ) > 0 depends on β, κ only. Also, by Lemma B.1, we have\nDupp(2σ; E) =\n\u00002e−1\u0001m/2 = exp(−cm),\nfor some universal constant c > 0 and\nDshift(2ε′, 2σ; E) = exp\n\u0012m(4σ + 2ε′)\n2σ2\n2ε′\n\u0013\n≤exp\n\u00126m\nd\n\u0013\n,\nwhere we used the facts that σ ≥ε/δ1/p = dϵ′ and d ≥1. We deduce that\np ≲δ + exp(k + 6m/d −c(β, κ)m)\nfor a possibly different constant c(β, κ) > 0. We now choose d = d(β, κ) = 12/c(β, κ). Up to\nanother possible change in c(β, κ), the condition (3.4) on m and (3.1) now give that p ≲δ, as\nrequired.\nProof of Theorem 3.9. Let p = P [∥x∗−ˆx∥≥34(η + σ)]. In this case, the forwards operator A\nsatisfies ∥A∥≤\np\nn/m (Lemma B.4). Hence we may apply Theorem 1.1 with θ =\np\nn/m and\nd = 2 to obtain\np ≲δ + Covη,δ(P) [Clow(1/2; A, D) + Cupp(2; A, D) + exp(−cm)]\nfor some universal constant c > 0, where D = supp(P) −supp(P). Lemma B.3 now gives that\np ≲δ + Covη,δ(P)\n\u0014\nexp\n\u0012\n−\ncm\nµ(U; D)\n\u0013\n+ exp(−cm)\n\u0015\nfor a possibly different constant c > 0. The result now follows from the condition (3.5) on m.\n27\nC\nProofs of Theorems 1.1 and 3.1\nWe finally consider the proofs of the two general results, Theorems 1.1 and 3.1. Our main effort will\nbe in establishing the latter, from which the former will follow after a short argument.\nTo prove Theorem 3.1, we first require some additional background on couplings, along with several\nlemmas. This is given in §C.1. We then establish a series of key technical lemmas, presented in §C.2,\nwhich are used in the main proof. Having shown these, the proof of Theorem 3.1 proceeds in §C.3\nvia a series of step. We now briefly describe these steps, and by doing so explain how the sets D1, D2\ndefined in (3.2)-(3.3) arise.\n(i) First, using Lemma C.7, we decompose P, R into distributions P′, R′ that are supported in\nballs of a given radius, plus remainder terms. The distributions P′, R′ are close (in W∞)\nto a discrete distribution Q supported at the centres of the balls that give the approximate\ncover satisfying (3.1).\n(ii) Next, we replace x∗∼R in the definition of the probability p in Theorem 3.1 by z∗∼P′.\nThe is done to align the prior with the posterior P(·|y, A), which is needed later in the proof.\nWe do this using Lemma C.6. Here we have to consider the action of A on vectors of the\nform x −z, where x ∈supp(R′) and z ∈supp(P′). After determining the supports of\nR′, P′, we see that x −z ∈D1, where D1 is the set defined in (3.2).\n(iii) We now decompose P′ into a mixture over the balls mentioned in (i). After a series of\narguments, we reduce the task to that of considering the probability that the conditional\ndistribution is drawn from one ball when the prior is drawn from another. Lemmas C.4 and\nC.5 handle this. They involve estimating the action of A on vectors x −z, where z is the\ncentre of one of the balls and x comes from another ball. Since the balls are supported in\nsupp(P) we have x ∈supp(P), and since the centres come from the approximate covering\nnumber bound (3.1), we have that z ∈supp(P) if P attains the minimum and z ∈supp(R)\notherwise. Hence x −z ∈D2, with D2 as in (3.3).\nC.1\nBackground on couplings\nFor a number of our results, we require some background on couplings. We first recall some notation.\nGiven probability spaces (X, F1, µ), (Y, F2, ν), we write Γ = Γµ,ν for the set of couplings, i.e.,\nprobability measures on the product space (X × Y, σ(F1 ⊗F2)) whose marginals are µ and ν,\nrespectively. For convenience, we write π1 : X × Y →X and π2 : X × Y →Y for the projections\nπ1(x, y) = x and likewise π2(x, y) = y. In particular, for any coupling γ we have π1♯γ = µ and\nπ2♯γ = ν, where ♯denotes the pushforward operation. As an immediate consequence, we observe\nthat for any measurable function φ : X →R,\nZ\nX\nφ(x) dµ(x) =\nZ\nX\nφ(x) dπ1♯γ(x) =\nZ\nX×Y\nφ(x) dγ(x, y).\n(C.1)\nGiven a cost function c : X × Y →[0, ∞), the Wasserstein-p metric is defined as\nWp(µ, ν) = inf\nγ∈Γ\n\u0012Z\nX×Y\nc(x, y)p dγ(x, y)\n\u00131/p\nfor 1 ≤p < ∞and\nW∞(µ, ν) = inf\nγ∈Γ\n\u0000esssupγc(x, y)\n\u0001\n.\nWe say that γ ∈Γ is a Wp-optimal coupling of µ and ν if\n\u0012Z\nX×Y\nc(x, y)p dγ(x, y)\n\u00131/p\n= Wp(µ, ν)\nfor 1 ≤p < ∞or\nesssupγc(x, y) = W∞(µ, ν)\nwhen p = ∞. Note that such a coupling exists whenever X, Y are Polish spaces, and when the cost\nfunction is lower semicontinuous [41]. In our case, we generally work with Euclidean spaces with\nthe cost function being the Euclidean norm, hence both conditions are satisfied.\n28\nFor convenience, if γ is probability measure on the product space (X ×Y, σ(F1 ⊗F2)), we will often\nwrite γ(E1, E2) instead of γ(E1 × E2) for Ei ∈Fi, i = 1, 2. Moreover, if x ∈X is a singleton, we\nwrite γ(x, E2) for γ({x} × E2) and likewise for γ(E1, y).\nWe now need several lemmas on couplings.\nLemma C.1. Suppose that (X, F1, µ), (Y, F2, ν) are Borel probability spaces, and let γ be a\ncoupling of µ, ν on the space (X × Y, σ(F1 ⊗F2)). Then supp(γ) ⊆supp(µ) × supp(ν).\nProof. Let (x, y) ∈supp(γ). Then γ(Ux,y) > 0 for every open set Ux,y ⊆X × Y that contains\n(x, y). Now, to show that (x, y) ∈supp(µ) × supp(ν), we show that x ∈supp(µ) and y ∈supp(ν).\nLet Ux ⊆X be open with x ∈Ux. By definition, µ(Ux) = γ(Ux × Rn). Since Ux × Rn is open and\ncontains (x, y), it follows that γ(Ux ×Rn) > 0. Since Ux was arbitrary, we deduce that x ∈supp(µ).\nThe argument that y ∈supp(ν) is identical.\nLemma C.2. Let X be a Polish space with a complete metric d. Let µ, ν be Borel probability\nmeasures on X. Let dH be the Hausdorff metric with respect to d and W∞be the Wasserstein-∞\nmetric with cost function d. Then\ndH(supp(µ), supp(ν)) ≤W∞(µ, ν).\nIn particular, supp(µ) ⊆Bη(supp(ν)) for any η ≥W∞(µ, ν).\nProof. Since\ndH(supp(µ), supp(ν)) = max\n(\nsup\nx∈supp(ν)\nd(x, supp(µ)),\nsup\ny∈supp(µ)\nd(y, supp(ν))\n)\nwe\nmay,\nwithout\nloss\nof\ngenerality,\nassume\nthat\nthe\nmaximum\nis\nachieved\nby\nsupx∈supp(ν) d(x, supp(µ))\n=:\nD.\nTake a sequence {xn}n∈N\n⊆\nsupp(ν) such that\nDn := d(xn, supp(µ)) →D. Since xn ∈supp(ν), for any ε > 0, we have ν(Bε(xn)) > 0. Note\nthat Bε(xn) is measurable as we assume X is Borel. For each n ∈N, define εn = 1\nnDn. We show\nthat for all x ∈Bεn(xn), y ∈supp(µ), d(x, y) > Dn(1 −1\nn). By triangle inequality, we have\nd(x, y) ≥d(xn, y) −d(xn, x) ≥Dn −Dn/n = Dn(1 −1/n).\nNotice also that Dn(1 −1\nn) ≤D and converges to D as n →∞. This implies that\nA := {(x′, y′) : d(x′, y′) > Dn(1 −1/n)} ⊇Bεn(xn) × supp(µ).\nNow consider any coupling γ ∈Γµ,ν. We have\nγ(A) ≥γ(Bεn(xn) × supp(µ)) = γ(Bεn(xn) × X) = ν(Bεn(xn)) > 0.\nTherefore ess supγ d(x, y) > Dn(1−1/n). Now since Dn(1−1\nn) →D we have ess supγ d(x, y) ≥\nD. This is holds for any coupling, therefore the result follows.\nWhen working with a coupling between a finitely-supported distribution and a continuous distribution,\nthe following lemma is often useful.\nLemma C.3. Let (X, F1, µ), (Y, F2, ν) be probability spaces, such that ν is finitely supported on a set\nS ⊆Y . Let γ be a coupling of µ, ν and E ⊆X × Y be γ-measurable. Write Ey = {x : (x, y) ∈E}\nfor the slice of E at y ∈Y . Then\nγ(E) =\nX\ns∈S,s∈π2(E)\nγ(Es × {s}).\nProof. Write\nγ(E) =\nX\ns∈S,s∈π2(E)\nγ(Es × {s}) + γ( ˆE),\nwhere ˆE = E\\ S\ns∈S,s∈π2(E)(Es × {s}). It suffices to show that γ( ˆE) = 0. Since F ⊆π−1\n2 (π2(F))\nfor any set F, we have\nγ( ˆE) ≤γ(π−1\n2 (π2( ˆE)) = ν(π2( ˆE)) = ν(π2( ˆE) ∩S).\nBut\nˆE = {(x, y) ∈E : y /∈S}\nand therefore π2( ˆE) ∩S = ∅. The result now follows.\n29\nC.2\nTechnical lemmas\nC.2.1\nSeparation lemma\nThe lemma considers a scenario where two random variables are drawn for a mixture of k probability\ndistributions. The second random variable is conditioned on the draw of the first. It then considers\nthe probability that the two random variables are drawn from different distributions in the mixture,\nbounding this in terms of their Total Variation (TV) distance. It generalizes [49, Lem. 3.1].\nLemma C.4 (Separation lemma). Let H1, . . . , Hk be Borel probability measures and consider the\nmixture H = Pk\ni=1 aiHi. Let y∗∼H and ˆy ∼Pk\ni=1 P(y∗∼Hi|y∗)Hi(·|y∗) where P(y∗∼\nHi|y∗) are the posterior weights. Then\nP[y∗∼Hi, ˆy ∼Hj(·|y∗)] ≤1 −TV(Hi, Hj).\nTo clarify, in this lemma and elsewhere we use the notation y∗∼Hi (and similar) to mean the event\nthat y∗is drawn from the ith distribution Hi.\nProof. Note that if the Hi have densities hi with respect to some measure, these weights are given by\nP(y∗∼Hi|y∗) =\naihi(y∗)\nPk\nj=1 ajhj(y∗)\n.\n(C.2)\nWe now write\np := P[y∗∼Hi, ˆy ∼Hj(·|y∗)] = P[y∗∼Hi]P[ˆy ∼Hj(·|y∗)|y∗∼Hj]\n= P[y∗∼Hi]E[P(ˆy ∼Hj(·|y∗)|y∗∼Hi]\n= aiE[P(ˆy ∼Hj(·|y∗))|y∗∼Hi].\nSince E[y∗|y∗∼Hi] ∼Hi, we have\np = aiE[P(ˆy ∼Hj(·|y∗))|y∗∼Hi] = ai\nZ\nP(ˆy ∼Hj(·|y∗)) dHi(y∗).\nNow, because of the mixture property, Hi ≪H and therefore its Radon-Nikodym derivative\nhi = dHi\ndH exists. This means we may write\np = ai\nZ\nP(ˆy ∼Hj(·|y∗))hi(y∗) dH(y∗).\nBy definition, we have P(ˆy ∼Hj(·|y∗)) = P(y∗∼Hj|y∗) and using (C.2), we deduce that\np =\nZ aiajhi(y∗)hj(y∗)\nPk\nl=1 alhl(y∗)\ndH(y∗)\nWe now write\np =\nZ\naihi(y∗)hj(y∗)aj\naihi(y∗) + ajhj(y∗) + P\nl̸=i,j alhl(y∗) dH(y∗)\n≤\nZ\naihi(y∗)hj(y∗)aj\naihi(y∗) + ajhj(y∗) dH(y∗)\n=\nZ\naihi(y∗)hj(y∗)aj\n(aihi(y∗) + ajhj(y∗)) dH(y∗)\n=\nZ\naihi(y∗)hj(y∗)aj\naihi(y∗) + ajhj(y∗) dH(y∗)\n≤\nZ\naihi(y∗)hj(y∗)aj\nmax{aihi(y∗), ajhj(y∗)} dH(y∗)\n=\nZ\nmin{aihi(y∗), ajhj(y∗)} dH(y∗)\n= 1 −\nZ 1\n2(hi(y∗) + hj(y∗)) −min{aihi(y∗), ajhj(y∗)} dH(y∗)\n= 1 −\nZ 1\n2|hi(y∗) −hj(y∗)| dH(y∗)\n= 1 −TV(Hi, Hj),\n30\nas required.\nC.2.2\nDisjointly-supported measures induce well-separated measurement distributions\nThe following lemma pertains to the pushforwards of measures supported in Rn via the forward\noperator A and noise e. Specifically, it states that if two distributions Pint and Pext are disjointly\nsupported then their corresponding pushforwards Hint,A and Hext,A are, on average with respect\nto A ∼A, well-separated, in the sense of their TV-distance. It is generalization of [49, Lem. 3.2]\nthat allows for arbitrary distributions A of the forward operators, as opposed to just distributions of\nGaussian random matrices.\nLemma C.5 (Disjointly-supported measures induce well-separated measurement distributions). Let\n˜x ∈Rn, σ ≥0, η ≥0, c ≥1, Pext be a distribution supported in the set\nS˜x,ext = {x ∈Rn : ∥x −˜x∥≥c(η + σ)}\nand Pint be a distribution supported in the set\nS˜x,int = {x ∈Rn : ∥x −˜x∥≤η}.\nGiven A ∈Rm×n, let Hint,A be the distribution of y = Ax∗+ e where x∗∼Pint and e ∼E\nindependently, and define Hext,A in a similar way. Then\nEA∼A[TV(Hint,A, Hext,A)] ≥1−\n\u0014\nClow\n\u0012 2\n√c; A, Dext\n\u0013\n+ Cupp\n\u0012√c\n2 ; A, Dint\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n; E\n\u0013\u0015\n,\nwhere Dext = {x −˜x : x ∈supp(Pext)}, Dint = {x −˜x : x ∈supp(Pint)} and Cupp(·; A),\nClow(·; A) and Dupp(·; E) are as in Definitions 2.3 and 2.4, respectively.\nNotice that the average TV-distance is bounded below by the concentration bounds Clow and Cupp\nfor A (Definition 2.3) and the concentration bound Dupp for E (Definition 2.4). This is unsurprising.\nThe pushforward measures are expected to be well-separated if, firstly, the action of A approximately\npreserves the lengths of vectors (which explains the appearance of Clow and Cupp) and, secondly,\nadding noise by E does not, with high probability, cause well-separated vectors to become close\nto each other (which explains the appearance of Dupp). Also as expected, as c increases, i.e., the\ndistributions Pint and Pext become further separated, the average TV-distance increases.\nProof. Given A ∈Rm×n, let\nBA = {y ∈Rm : ∥y −A˜x∥≤√c(η + σ)}.\nWe claim that\nEA[Hext,A(BA)] ≤Clow\n\u0012 2\n√c; A, Dext\n\u0013\n+ Dupp(σ√c; E),\n(C.3)\nEA[Hint,A(BA)] ≥1 −\n\u0014\nCupp\n\u0012√c\n2 ; A, Dint\n\u0013\n+ Dupp\n\u0012√c\n2 σ; E\n\u0013\u0015\n.\n(C.4)\nNotice that these claims immediately imply the result, since\nEA∼ATV(Hext,A, Hint,A) ≥EA∼A[Hint,A(BA)] −EA∼A[Hext,A(BA)].\nTherefore, the rest of the proof is devoted to showing (C.3) and (C.4). For the former, we write\nEA∼A[Hext,A(BA)] = EA∼A\n\u0014Z Z\n1BA(Ax + e) dPext(x) dE(e)\n\u0015\n= EA∼A\n\u0014Z Z\n1BA(Ax + e) dE(e) dPext(x)\n\u0015\n= EA∼A[Ex∼Pext[E(BA −Ax)]]\n= Ex∼Pext[EA∼A[E(BA −Ax)]],\n(C.5)\nwhere BA −Ax = {b−Ax : b ∈BA}. We now bound Ex∼Pext[EA∼AE(BA −Ax)]. Given x ∈Rn,\nlet Cx = {A : ∥Ax −A˜x∥< 2√c(η + σ)} ⊆Rm×n and write\nI1 = Ex∼Pext[EA∼AE(BA −Ax)1Cx],\nI2 = Ex∼Pext[EA∼AE(BA −Ax)1Cc\nx]\n31\nso that\nEx∼Pext[EA∼A[E(BA −Ax)]] = I1 + I2.\n(C.6)\nWe will bound I1, I2 separately. For I1, we first write\nI1 = Ex∼Pext[EA∼A[E(BA −Ax)1Cx]]\n≤Ex∼Pext[EA∼A[1Cx]]\n= Ex∼Pext[PA∼A(∥Ax −A˜x∥< 2√c(η + σ))],\nwhere the inequality follows from the fact that E(BA −Ax) ≤1. Now since x ∼Pext, we have\nx ∈S˜x,ext and therefore ∥x −˜x∥≥c(η + σ). Hence\nEx∼Pext[PA∼A(∥Ax −A˜x∥< 2√c(η + σ)] ≤Ex∼Pext\n\u0014\nPA∼A\n\u0012\n∥Ax −A˜x∥< 2\n√c∥x −˜x∥\n\u0013\u0015\n.\nSince the outer expectation term has x ∼Pext, we have that x ∈supp(Pext) with probability one.\nUsing Definition 2.3, we deduce that\nI1 ≤Clow\n\u0012 2\n√c; A, Dext\n\u0013\n.\n(C.7)\nWe now bound I2. Let x ∈S˜x,ext and A ∈Cc\nx, i.e., ∥A(x −˜x)∥> 2√c(η + σ). We now show\nthat BA ⊆BA,x, where BA,x = {y ∈Rm : ∥y −Ax∥≥√c(η + σ)}. Suppose that y ∈BA, i.e.,\n∥y −A˜x∥≤√c(η + σ). We have\n∥y −Ax∥= ∥y −A˜x + A˜x −Ax∥\n≥∥A(˜x −x)∥−∥y −A˜x∥\n> 2√c(η + σ) −√c(η + σ)\n= √c(η + σ),\nand therefore y ∈BA,x, as required. Using this, we have\nE(BA −Ax) ≤E(BA,x −Ax),\n∀A ∈Cc\nx, x ∈S˜x,ext,\nand therefore\nI2 = Ex∼Pext[EA∼A[E(BA −Ax)1Ccx]] ≤Ex∼Pext[EA∼A[E(BA,x −Ax)1Ccx]].\nBut we notice that BA,x −Ax = Bc√c(η+σ). Now since η ≥0, we have Bc√c(η+σ) ⊆Bc\nσ√c. Hence\nE(BA,x −Ax) = E(Bc√c(η+σ)) ≤E(Bc\nσ√c) ≤Dupp(σ√c; E),\nand therefore I2 ≤Dupp(σ√c; E). Combining this with (C.5), (C.6) and (C.7), we deduce that\nEA[Hext,A(BA)] ≤Ex∼Pext[EA∼A[E(BA−Ax)]] = I1+I2 ≤Clow(2/√c; A, Dext)+Cupp(σ√c; E),\nwhich shows (C.3).\nWe will now establish (C.4). With similar reasoning to (C.5), we have\nEA∼A[Hint,A(Bc\nA)] = Ex∼Pint[EA∼A[E(Bc\nA −Ax)]]\nProceeding as before, let Dx = {A : ∥Ax −A˜x∥<\n√c\n2 (η + σ)}, I1 = Ex∼Pint[EA∼A[E(Bc\nA −\nAx)]1Dcx], and I2 = Ex∼Pint[EA∼A[E(Bc\nA −Ax)]1Dx] so that\nEA∼A[Hint,A(Bc\nA)] = Ex∼Pint[EA∼A[E(Bc\nA −Ax)]] = I1 + I2.\n(C.8)\nThe terms I1, I2 are similar to those considered in the previous case. We bound them similarly. For\nI1, we have, by dropping the inner probability terms,\nI1 ≤Ex∼Pint[EA∼A[1Dcx]] = Ex∼Pint\n\u0014\nPA∼A(∥A(x −˜x)∥≥\n√c\n2 (η + σ))\n\u0015\n.\nSince x ∈S˜x,int, we have ∥x −˜x∥≤η ≤η + σ which gives\nEx∼Pint[PA∼A(∥A(x −˜x)∥≥\n√c\n2 (η + σ))] ≤Ex∼Pint\n\u0014\nPA∼A(∥A(x −˜x)∥≥\n√c\n2 ∥x −˜x∥)\n\u0015\n32\nand therefore\nI1 ≤Cupp\n\u0012√c\n2 ; A, Dint\n\u0013\n.\n(C.9)\nWe now bound I2. Let x ∈S˜x,int and suppose that A ∈Dx, i.e., ∥x −˜x∥≤η and ∥A(x −˜x)∥<\n√c\n2 (η + σ). Define ˆBA,x = {y ∈Rm : ∥y −Ax∥<\n√c\n2 (η + σ)}. We will show ˆBA,x ⊆BA in this\ncase. Let y ∈BA,x. Then\n∥y −A˜x∥≤∥y −Ax∥+ ∥Ax −A˜x∥<\n√c\n2 (η + σ) +\n√c\n2 (η + σ) = √c(η + σ),\nas required. This implies that Bc\nA ⊆ˆBc\nA,x. Hence\nE(Bc\nA −Ax) ≤E( ˆBc\nA,x −Ax) = E(Bc√c\n2 (η+σ)) ≤E(Bc√c\n2 σ)\nwhich implies that I2 ≤Dupp(\n√c\n2 σ; E). Combining with (C.8) and (C.9) we get\nEA[Hint,A(Bc\nA)] ≤Ex∼Pint[EA∼A[E((BA −Ax)c)]] ≤Cupp\n\u0012√c\n2 ; A, Dint\n\u0013\n+ Dupp\n\u0012√c\n2 σ; E\n\u0013\n,\nwhich implies (C.4). This completes the proof.\nC.2.3\nReplacing the real distribution with the approximate distribution\nWe next establish a result that allows one to upper bound the failure probability based on draws from\nthe real distribution R with the failure probability based on draws from the approximate distribution\nP. This lemma is a key technical step that aligns the prior distribution with the posterior. The specific\nbound is given in terms of the Wasserstein distance between R and P and several of the concentration\nbounds defined in §2. This is a significant generalization of [49, Lem. 3.3] that allows for arbitrary\ndistributions A, E for the forwards operator and noise.\nLemma C.6 (Replacing the real distribution with the approximate distribution). Let ε, σ, d, t ≥0,\nc ≥1, E be a distribution on Rm and R, P be distributions on Rn such that W∞(R, P) ≤ε. Let Π\nbe an W∞-optimal coupling of R and P and define the set D = {x∗−z∗: (x∗, z∗) ∈supp(Π)}.\nLet\np = Px∗∼R,A∼A,e∼E,ˆx∼P(·|Ax∗+e,A)[∥x∗−ˆx∥≥d + ε]\nand\nq = Pz∗∼P,A∼A,e∼E,ˆz∼P(·|Az∗+e,A)[∥z∗−ˆz∥≥d].\nThen\np ≤Cabs(ε, tε; A, D) + Dupp(cσ; E) + Dshift(tε, cσ; E)q,\nwhere Cabs(ε, tε; A, D), Dupp(cσ; E) and Dshift(tε, cσ; E) are as in Definitions 2.3, 2.4 and 2.5,\nrespectively.\nAs expected, this lemma involves a trade-off. The constant Cabs(ε, tε; A, D) is made smaller (for fixed\nε) by making the constant t larger. However, this increases Dshift(tε, cσ; E), which is compensated\nby making c smaller. However, this in turn increases the constant Dupp(cσ; E).\nProof. Define the events\nB1,ˆx = {x∗: ∥x∗−ˆx∥≥d + ε},\nB2,ˆz = {z∗: ∥z∗−ˆz∥≥d}\nso that\np = Px∗∼R,A∼A,e∼E,ˆx∼P(·|Ax∗+e,A)[x∗∈B1,ˆx]\nq = Pz∗∼P,A∼A,e∼E,ˆz∼P(·|Az∗+e,A) [z∗∈B2,ˆz].\n(C.10)\nObserve that\np = Ex∗∼R[EA∼AEy|A,x∗[Eˆx∼P(·|Ax∗+e,A)[1B1,ˆx]]]\n=\nZ Z Z Z\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx)dE(e)dA(A)dR(x∗)\n33\nand similarly\nq =\nZ Z Z Z\n1B2,ˆz(z∗)dP(·|Az∗+ e, A)(ˆz)dE(e)dA(A)dP(z∗).\nTherefore, to obtain the result, it suffices to replace samples from the real distribution R with samples\nfrom the approximate distribution P and to replace the indicator function of B1,ˆx by the indicator\nfunction over B2,ˆz. For the first task, we use couplings. Since W∞(R, P) ≤ε, there exists a coupling\nΠ between R, P with Π(∥x∗−z∗∥≤ε) = 1. By (C.1), we can write\np =\nZ Z Z Z\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx)dE(e)dA(A)dΠ(x∗, z∗).\nDefine E = {(x∗, z∗) : ∥x∗−z∗∥≤ε} and observe that Π(E) = 1. Then, for fixed A, e, we have\nZ Z\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx)dΠ(x∗, z∗) =\nZ\nE\nZ\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx)dΠ(x∗, z∗)\nZ Z\n1B2,ˆx(z∗)dP(·|Ax∗+ e, A)(ˆx)dΠ(x∗, z∗) =\nZ\nE\nZ\n1B2,ˆx(z∗)dP(·|Ax∗+ e, A)(ˆx)dΠ(x∗, z∗).\nWe now show 1B1,ˆx(x∗) ≤1B2,ˆx(z∗) for (x∗, z∗) ∈E. Let (x∗, z∗) ∈E and suppose that\nx∗∈B1,ˆx. Then ∥x∗−ˆx∥≥d + ε and, since ∥x∗−z∗∥≤ε, we also have that ∥z∗−ˆx∥≥d and\ntherefore z∗∈B2,ˆx, as required. Hence\nZ\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx) ≤\nZ\n1B2,ˆx(z∗)dP(·|Ax∗+ e, A)(ˆx)\nfor (x∗, z∗) ∈E. Now, since indicator functions are non-negative, Fubini’s theorem immediately\nimplies that\np =\nZ Z Z Z\n1B1,ˆx(x∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\n≤\nZ Z Z Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗).\nHaving introduced the coupling Π and replaced 1B1,ˆx by 1B2,ˆx, to establish the result it remains to\nreplace the conditional distribution P(·|Ax∗+ e, A) by P(·|Az∗+ e, A). With a similar technique\nto that used in the proof of Lemma C.5, we define Cx∗,z∗= {A : ∥A(x∗−z∗)∥> tε} and\nI1 =\nZ Z\n1Cx∗,z∗(A)\nZ Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\nI2 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\nso that\np ≤\nZ Z Z Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗) = I1 + I2.\n(C.11)\nWe first bound I1. As before, we write\nI1 =\nZ Z\n1Cx∗,z∗(A)\nZ Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\n≤\nZ Z\n1Cx∗,z∗(A) dA(A) dΠ(x∗, z∗).\nRecalling the definition of the set E above, we get\nZ Z\n1Cx∗,z∗(A) dA(A) dΠ(x∗, z∗) ≤\nZ\nE\nPA∼A{∥A(x∗−z∗)∥> tε} dΠ(x∗, z∗).\nUsing the definition of C0, E and D, we deduce that\nI1 ≤Cabs(ε, tε; A, D).\n(C.12)\n34\nNow we bound I2. We further split the integral I2 as follows:\nI2 = I21 + I22,\n(C.13)\nwhere\nI21 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bc\ncσ(e)\nZ\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\nI22 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bcσ(e)\nZ\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\nLet us first find an upper bound for I21. We have\nI21 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bccσ(e)\nZ\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\n≤\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bccσ(e) dE(e) dA(A) dΠ(x∗, z∗)\n≤\nZ\n1Bccσ(e) dE(e),\nand therefore, by Definition 2.4,\nI21 = E(Bc\ncσ) ≤Dupp(cσ; E).\n(C.14)\nWe now find a bound for I22. We first use Definition 2.5 to write\nI22 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bcσ(e)pE(e)\nZ\n1B2,ˆx(z∗)dP(·|Ax∗+ e, A)(ˆx) de dA(A) dΠ(x∗, z∗).\nNow define the new variable e′ = e + A(x∗−z∗). Since, in the integrand, ∥e∥≤cσ (due to\nthe indicator function 1Bcσ(e)) and ∥A(x∗−z∗)∥≤tε (due to the indicator function 1Cc\nx∗,z∗(A)),\nDefinition 2.5 yields the bound\nI22 ≤Dshift(tε, cσ; E)\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1B2σ(e′ −A(x∗−z∗))pE(e′)\n×\nZ\n1B2,ˆx(z∗) dP(·|Az∗+ e′, A)(ˆx) de′ dA(A) dΠ(x∗, z∗).\nWe now drop the first two indicator functions and relabel the variables e′ and ˆx as e and ˆz, respectively,\nto obtain\nI22 ≤Dshift(tε, cσ; E)\nZ Z Z Z\n1B2,ˆz(z∗) dP(·|Az∗+ e, A)(ˆz) dE(e) dA(A) dΠ(x∗, z∗).\nThis gives\nI22 ≤Dshift(tε, cσ; E)q,\nwhere q is as in (C.10). Combining this with (C.13), we deduce that\nI2 ≤Dupp(cσ, ε) + Dshift(tε, cσ; E)q.\nTo complete the proof, we combine this with (C.11) and (C.12), and then recall (C.10) once more.\nC.2.4\nDecomposing distributions\nThe following lemma is in large part similar to [49, Lem. A.1]. However, we streamline and rewrite\nits proof for clarity and completeness, fix a number of small issues and make an addition to the\nstatement (see item (v) below) that is important for proving our main result.\nLemma C.7 (Decomposing distributions). Let R, P be arbitrary distributions on Rn, p ≥1 and\nη, ρ, δ > 0. If Wp(R, P) ≤ρ and k ∈N is such that\nmin{log Covη,δ(P), log Covη,δ(R)} ≤k,\n(C.15)\nthen there exist distributions R′, R′′, P′, P′′, a constant 0 < δ′ ≤δ and a discrete distribution Q\nwith supp(Q) = S satisfying\n35\n(i) min{W∞(P′, Q), W∞(R′, Q)} ≤η,\n(ii) W∞(R′, P′) ≤\nρ\nδ1/p ,\n(iii) P = (1 −2δ′)P′ + (2δ′)P′′ and R = (1 −2δ′)R′ + (2δ′)R′′,\n(iv) |S| ≤ek,\n(v) and S ⊆supp(P) if P attains the minimum in (C.15) with S ⊆supp(R) otherwise.\nThis lemma states that two distributions that are close in Wasserstein p-distance, and for which at\nleast one has small approximate covering number (C.15), can be decomposed into mixtures (iii) of\ndistributions, where the following holds. One of the distributions, say P′, is close (i) in Wasserstein-∞\ndistance to a discrete distribution Q with the cardinality of its support (iv) bounded by the approximate\ncovering number. The other R′ is close in Wasserstein-∞distance to P′. Moreover, both mixtures\n(iii) are dominated by these distributions: the ‘remainder’ terms P′′ and R′′ are associated with a\nsmall constant δ′ ≤δ, meaning they are sampled with probability ≤δ when drawing from either P\nor R. Note that if p < ∞then the Wasserstein-∞distance between R′ and P′ may get larger as\nδ shrinks, i.e., as the remainder gets smaller. However, this does not occur when p = ∞, as (ii) is\nindependent of δ in this case.\nProof. Without loss of generality, we assume that log Covη,δ(P) ≤k. Then Covη,δ(P) ≤ek and\nhence there is a set S = {ui}l\ni=1 ⊆supp(P) with l ≤ek, where the ui are the centres of the balls\nused to cover at least 1 −δ of the measure of P. That is,\nP\n\" l[\ni=1\nB(ui, η)\n#\n= Px∼P\n\"\nx ∈\nl[\ni=1\nB(ui, η)\n#\n=: 1 −c∗≥1 −δ.\nWe now define f : Rn →R so that f(x) = 0 if x lies outside these balls, and otherwise, f(x) is the\nequal to the reciprocal of the number of balls in which x is contained. Namely,\nf(x) =\n(\n1\nPl\ni=1 1B(ui,η)(x)\nif x ∈Sl\ni=1 B(ui, η)\n0\notherwise\n.\nWe divide the remainder of the proof into a series of steps.\n1. Construction of Q′. We will now define a finite measure Q′. The point of Q′ is to, concentrate the\nmass of the measure P into the centres of the balls ui. If the sets B(ui, η) are disjoint, then this is\nstraightforward. However, to ensure that Q′ is indeed a probability measure, we need to normalize\nand account for any non-trivial intersections. This is done via the function f. Pick some arbitrary\nˆu /∈{u1, . . . , ul} and define\nQ′ =\nl\nX\ni=1\n Z\nB(ui,η)\nf(x) dP(x)\n!\nδui + c∗δˆu.\nObserve that\nZ\ndQ′(x) =\nl\nX\ni=1\nZ\nB(ui,η)\nf(x) dP(x) + c∗= P\n l[\ni=1\nB(ui, η)\n!\n+ c∗= (1 −c∗) + c∗= 1,\nand therefore Q′ is a probability distribution supported on the finite set S ∪{ˆu}.\n2. Coupling Q′, P. Now that we have associated all the mass of P with the points ui, we can define a\ncoupling Π between Q′ and P that associates the mass of P and ui with a single measure. Moreover,\nthis measure will keep points within η distance of each other with high probability. We define Π as\nfollows for measurable sets E, F ⊆Rn:\nΠ(E, F) =\nl\nX\ni=1\n1F (ui)\nZ\nB(ui,η)∩E\nf(x) dP(x) + 1F (ˆu)P\n \nE\\\nl[\ni=1\nB(ui, η)\n!\n.\nTo see that this is a coupling, we first observe that\nΠ(Rn, F) =\nl\nX\ni=1\n1F (ui)\nZ\nB(ui,η)\nf(x) dP(x) + 1F (ˆu)(1 −c∗) ≡Q′(F),\n36\nwhich gives the result for the first marginal. For the other, we have\nΠ(E, Rn) =\nl\nX\ni=1\nZ\nB(ui,η)∩E\nf(x) dP(x) + P\n \nE\\\nl[\ni=1\nB(ui, η)\n!\n.\nBy definition of f, this is precisely\nΠ(E, Rn) = P\n \nE ∩\nl[\ni=1\nB(ui, η)\n!\n+ P\n \nE\\\nl[\ni=1\nB(ui, η)\n!\n≡P(E),\nwhich gives the result for the second marginal. Note that Π was only defined for product sets,\nbut, since Q′ is finitely supported, it follows directly from Lemma C.3 that it extends to arbitrary\nmeasurable sets in the product sigma-algebra. We now show that Π[∥x1 −x2∥> η] ≤c∗≤δ. That\nis, we show that most points drawn from Π are within η distance of each other. By law of total\nprobability we have\nΠ(∥x1 −x2∥> η) =\nl\nX\ni=1\nΠ(∥x1 −x2∥> η|x2 = ui)Π(x2 = ui)\n+ Π(∥x1 −x2∥> η|x2 = ˆu)Π(x2 = ˆu)\n=\nl\nX\ni=1\nΠ(Ui, ui)Q′(ui) + Π( ˆU, ˆu)Q′(ˆu),\nwhere Ui = {x : ∥x −ui∥> η} and ˆU = {x : ∥x −ˆu∥> η}. Notice that Ui ∩B(ui, η) = ∅and\ntherefore\nΠ(Ui, ui) =\nZ\n{x:∥x−ui∥>η}∩B(ui,η)\nf dP = 0.\nHence\nl\nX\ni=1\nΠ(Ui, ui)Q′(ui) + Π( ˆU, ˆu)Q′(ˆu) = Π( ˆU, ˆu)Q′(ˆu)\nand, since Q′(ˆu) = c∗, we have Π( ˆU, ˆu)Q′(ˆu) ≤c∗≤δ. This gives\nΠ(∥x1 −x2∥> η) ≤δ,\n(C.16)\nas required.\n3. Coupling P, R. The next step is to introduce R. With the assumption that Wp(R, P) ≤ρ, by\ndefinition there exists a coupling Γ between P and R such that EΓ[∥x1 −x2∥p] ≤ρp. Markov’s\ninequality then gives that\nΓ\n\u0010\n∥x1 −x2∥≥\nρ\nδ1/p\n\u0011\n≤EΓ[∥x1 −x2∥p]\nρp\nδ\n≤δ.\n(C.17)\n4. Coupling P, Q′, R. We next couple P, Q′ and R. Before doing so, we first discuss the goal of our\nfinal coupling. Recall that we have the distribution P, the distribution Π that couples P, Q′ closely\nexcept for up to δ mass of P, and Γ which keeps P, R close again except for up to δ of the mass of\nΓ. We want to decompose P into the portions that are η close to Q′, and points that are not. These\nwill become P′ and P′′, respectively. At the same time, we want to decompose R to points that are\nρ\nδ1/p close to P′, and points that are not. Naturally this will become R′ and R′′. To achieve this, we\ncouple P, Q′ and R in this step and then use this to construct the final decomposition in the next step.\nWe have measures P, Q′ and R and couplings Π of P, Q′ and Γ of P, R. We will in a sense, couple\nΠ, Γ. Since (Rn)3 is a Polish space, by [10, Lem. 8.4], there exists a coupling Ωwith\nπ1,2♯Ω= Π,\nπ1,3♯Ω= Γ,\nwhere π1,2(x1, x2, x3) = (x1, x2) and likewise for π1,3. One should intuitively think of the x1\ncomponent as samples from P, the x2 component as samples from Q, and the x3 component as\nsamples from R. With the base measure defined, we still want to ensure that x1, x3 are sampled\n37\nclosely, and x1, x2 are as well. Consider the event such that x1, x3 are\nρ\nδ1/p close and x1, x2 are η\nclose: namely,\nE := {(x1, x2, x3) : ∥x1 −x3∥≤ρ/δ1/p and ∥x1 −x2∥≤η}.\nSplit up the negation of the two events of ∥x1 −x3∥≤ρ/δ1/p and ∥x1 −x2∥≤η into the events\nE1 := {(x1, x2, x3) : ∥x1 −x2∥> η, z ∈Rn},\nE2 := {(x1, x2, x3) : ∥x1 −x3∥> ρ/δ1/p, y ∈Rn},\nso that Ec = E1 ∪E2. We will now show Ω(E1) ≤δ. Write E1 = E′\n1 ×Rn where E′\n1 = {(x1, x2) :\n∥x1 −x2∥> η} satisfies Π(E′\n1) ≤δ′ by (C.16). Then\nδ′ ≥Π(E′\n1) =\nZ\n1E′\n1(x1, x2) dπ1,2♯Ω(x1, x2)\n=\nZ\n1E′\n1(p1,2(x1, x2, x3)) dΩ(x1, x2, x3)\n=\nZ\n1E1(x1, x2, x3) dΩ(x1, x2, x3)\n= Ω(E1),\nas required. Using (C.17), we also have the analogous result for E2. Hence Ω(Ec) = Ω(E1 ∪E2) ≤\nΩ(E1) + Ω(E2) =: 2δ′ ≤2δ′, and consequently,\nΩ(E) = 1 −2δ′ ≥1 −2δ,\nwhere E := {(x1, x2, x3) : ∥x1 −x3∥≤ρ/δ1/p and ∥x1 −x2∥≤η}.\n(C.18)\n4. Decomposing P, R. Finally, we define P′, P′′, R′, R′′ and Q by conditioning on the events E\nand Ec, as follows:\nP′(A) = Ω(A, Rn, Rn|E),\nR′(A) = Ω(Rn, Rn, A|E),\nP′′(A) = Ω(A, Rn, Rn|Ec),\nR′′(A) = Ω(Rn, Rn, A|Ec),\nQ(A) = Ω(Rn, A, Rn|E).\nThis gives\nP(A) = Ω(A, Rn, Rn) = Ω(E)Ω(A, Rn, Rn|E) + Ω(Ec)Ω(A, Rn, Rn|Ec)\n= (1 −2δ′)P′(A) + 2δ′P′′(A)\nand similarly\nR(A) = Ω(Rn, A, Rn) = Ω(E)Ω(Rn, A, Rn|E) + Ω(Ec)Ω(Rn, A, Rn|Ec)\n= (1 −2δ′)R′(A) + 2δ′R′′(A).\nWe now claim that these distributions satisfy (i)-(v) in the statement of the lemma. We have already\nshown that (iii) holds. To show (i), we define a coupling γ of P′, Q by γ(B) = Ω(B, Rn|E)\nfor any B ⊆(Rn)2. Observe that γ(A, Rn) = Ω(A, Rn, Rn|E) = P′(A) and γ(Rn, A) =\nΩ(Rn, A, Rn|E) = Q(A). Hence this is indeed a coupling of P′, Q. Therefore it suffices to\nshow that γ(B) = 0, where B is the event {∥x1 −x2∥> η}. We have γ(B) = Ω(B, Rn|E).\nRecall that for (x1, x2, x3) ∈E, we have ∥x1 −x2∥≤η. Hence Ω(B, Rn|E) = 0. Therefore,\nW∞(P′, Q) ≤η, which gives (i).\nSimilarly, for (ii) we define a coupling γ′ of R′, P′ by γ′(B) = Ω(B|E) where B := {(x1, x2, x3) :\n(x1x3) ∈B, x2 ∈Rn}. With similar reasoning as the previous case, γ′ is a coupling of R′, P′ and,\nfor (x1, x2, x3) ∈E we have ∥x1 −x3∥≤ρ/δ1/p, so letting B be the event {∥x1 −x3∥> ρ/δ1/p},\nwe conclude that W∞(R′, P′) ≤ρ/δ1/p. This gives (ii).\nFinally we verify (iv) and (v). First recall that both properties hold for Q′ by construction. The results\nnow follow from the fact that Q′(·) = Ω(Rn, ·, Rn) and Q(·) = Ω(Rn, ·, Rn|E).\n38\nC.3\nProof of Theorem 3.1\nWe now prove Theorem 3.1. This follows a similar approach to that of [49, Thm. 3.4], but with a\nseries of significant modifications to account for the substantially more general setup considered in\nthis work. We also streamline the proof and clarify a number of key steps.\nProof of Theorem 3.1. By Lemma C.7, we can decompose P, R into measures P′, P′′ and R′, R′′,\nand construct a finite distribution Q supported on a finite set S such that\n(i) min{W∞(P′, Q), W∞(R′, Q)} ≤η,\n(ii) W∞(R′, P′) ≤ε′ :=\nε\nδ1/p ,\n(iii) P = (1 −2δ′)P′ + (2δ′)P′′ and R = (1 −2δ′)R′ + (2δ′)R′′ for some 0 ≤δ′ ≤δ,\n(iv) |S| ≤ek,\n(v) and S ⊆supp(P) if P attains the minimum in (C.15) with S ⊆supp(R) otherwise.\nIt is helpful to briefly recall the construction of these sets. Beginning with δ, η as parameters for\nthe approximate covering numbers, the distribution Q concentrates 1 −δ of the mass of P into the\ncentres of the η-radius balls used. Then the distributions P′, R′ are the measures P, R within the\nballs. We now write\np := Px∗∼R,A∼A,e∼E,ˆx∼P(·|y,A)[∥x∗−ˆx∥≥(c + 2)η + (c + 2)σ]\n≤Px∗∼R,A∼A,e∼E,ˆx∼P(·|y,A)[∥x∗−ˆx∥≥(c + 2)η + (c + 1)σ + ε′]\n≤2δ′ + (1 −2δ′)Px∗∼R′,A∼A,e∼E,ˆx∼P(·|y,A)[∥x∗−ˆx∥≥(c + 1)(η + σ) + ε′]\n=: 2δ′ + (1 −2δ′)q.\n(C.19)\nHere, in the first inequality we used the fact that σ ≥ε′, and in the second, we used the decomposition\nR = (1 −2δ′)R′ + 2δ′R′′ and the fact that δ′ ≤δ. We now bound q by using Lemma C.6 to replace\nthe distribution R′ by the distribution P′. Writing u = Az∗+ e, this lemma and (ii) give that\nq ≤Cabs(ε′, tε′; A, D) + Dupp(c′σ; E) + Dshift(tε′, c′σ; E)r,\nwhere r = Pz∗∼P′,A∼A,e∼E,ˆz∼P(·|u,A)[∥z∗−ˆz∥≥(c + 1)(η + σ)]\n(C.20)\nand D = {x∗−z∗: (x∗, z∗) ∈supp(Π)}, for Π being the W∞-optimal coupling of R′, P′\nguaranteed by (ii). Lemma C.1 implies that supp(Π) ⊆supp(R′) × supp(P′) and therefore\nD ⊆supp(R′) −supp(P′).\nNow (iii) implies that supp(P′) ⊆supp(P). Similarly, (iii) implies that supp(R′) ⊆supp(R). But\nLemma C.2 and (ii) imply that supp(R′) ⊆Bε′(supp(P′)). Therefore\nD ⊆Bε′(supp(P)) ∩supp(R) −supp(P) = D1,\nwhere D1 as in (3.2).\nWe now bound r. Observe first that\nW∞(P′, Q) ≤η′ := η + ε′.\nIndeed, from (i) either W∞(P′, Q) ≤η or W∞(R′, Q) ≤η. In the former case, the inequality\ntrivially holds. In the latter case, we can use the triangle inequality and (ii) to obtain the desired\nbound. This implies that there is a coupling Γ of P′, Q with esssupΓ∥x −y∥≤η′. Fix ˜z ∈S and,\nfor any Borel set E ⊆Rn, define\nΓ˜z(E) = Γ(E, ˜z)\nQ(˜z) .\nThen it is readily checked that Γ˜z(·) defines a probability measure. Note also that Γ˜z is supported on\na ball of radius η′ around ˜z, since esssupΓ(∥x −y∥) ≤η′. Recall that Γ is a coupling between P′\nand Q. Let E ⊆Rn be a Borel set. Then Lemma C.3 gives that\nP′(E) = Γ(E, Rn) =\nX\n˜z∈S\nΓ((E, Rn)˜z, ˜z) =\nX\n˜z∈S\nΓ(E, ˜z) =\nX\n˜z∈S\nΓ˜z(E)Q(˜z).\n39\nTherefore, we can express P′ as the mixture\nP′(·) =\nX\n˜z∈S\nΓ˜z(·)Q(˜z).\nDefine the event E = {∥z∗−ˆz∥≥(c + 1)(η + σ)} ⊆Rn × Rn so that the probability r defined in\n(C.20) can be expressed as\nr = Ez∗∼P′,A∼A,e∼E,ˆz∼P(·|A,u)[1E].\nUsing the above expression for P′ we now write\nr =\nZ Z Z Z\n1E(z∗, ˆz) dP(·|A, u)(ˆz) dE(e) dA(A) dP′(z∗)\n=\nZ Z Z Z\n1E(z∗, ˆz) dP(·|A, u)(ˆz) dE(e) dA(A) d\n X\n˜z∈S\nQ(˜z)Γ˜z(·)\n!\n(z∗)\n=\nX\n˜z∈S\nQ(˜z)\n\u0012Z Z Z Z\n1E(z∗, ˆz) dP(·|A, u)(ˆz) dE(e) dA(A) dΓ˜z(z∗)\n\u0013\n,\nwhere the last line holds as Q(˜z) is a constant. Hence\nr =\nX\n˜z∈S\nQ(˜z)Pz∗∼Γ˜z,A∼A,e∼E,ˆz∼P(·|A,u)[E].\n(C.21)\nNow we bound each term in this sum. We do this by decomposing P into a mixture of three\nprobability measures depending on ˜z ∈S. To do this, let θ = c(η + σ) and observe that, for any\nBorel set E ⊆Rn,\nP(E) = P(E ∩Bθ(˜z)) + P(E ∩Bc\nθ(˜z))\n= P(E ∩Bθ(˜z)) + P(E ∩Bc\nθ(˜z)) + (1 −2δ′)Q(˜z)Γ˜z(E) −(1 −2δ′)Q(˜z)Γ˜z(E)\n= P(E ∩Bθ(˜z)) −(1 −2δ′)Q(˜z)Γ˜z(E ∩Bθ(˜z))\n+ P(E ∩Bc\nθ(˜z)) −(1 −2δ′)Q(˜z)Γ˜z(E ∩Bc\nθ(˜z))\n+ (1 −2δ′)Q(˜z)Γ˜z(E).\nNow define the constants\nc˜z,mid = P(Bθ(˜z)) −(1 −2δ′)Q(˜z)Γ˜z(Bθ(˜z)),\nc˜z,ext = P(Bc\nθ(˜z)) −(1 −2δ′)Q(˜z)Γ˜z(Bc\nθ(˜z)).\nand let\nP˜z,int(E) = Γ˜z(E)\nP˜z,mid(E) =\n1\nc˜z,mid\n(P(E ∩Bθ(z∗)) −(1 −2δ′)Q(z∗)Γ˜z(E ∩Bθ(z∗))) ,\nP˜z,ext(E) =\n1\nc˜z,ext\n(P(E ∩Bc\nθ(z∗)) −(1 −2δ′)Q(z∗)Γ˜z(E ∩Bc\nθ(z∗))).\nThen P can be expressed as the mixture\nP = (1 −2δ′)Q(˜z)P˜z,int + c˜z,midP˜z,mid + c˜z,extP˜z,ext.\n(C.22)\nTo ensure this is a well-defined mixture, we need to show that P˜z,mid and P˜z,ext are probability\nmeasures. However, by (iii) we have, for any Borel set E ⊆Rn,\nP(E) ≥(1 −2δ′)P′(E) = (1 −2δ′)\nX\n˜z∈S\nΓ˜z(E)Q(˜z) ≥(1 −2δ′)Γ˜z(E)Q(˜z).\nTherefore, P˜z,mid and P˜z,ext are well-defined, provided the constants c˜z,int, c˜z,ext > 0. However, if\none of these constants is zero, then we can simply exclude this term from the mixture (C.22). For the\nrest of the theorem, we will assume that, at least, c˜z,ext > 0.\nIt is now useful to note that\nsupp(P˜z,mid) ⊆Bθ(˜z)\nand\nsupp(P˜z,ext) ⊆Bc\nθ(˜z),\n40\nwhich follows immediately from their definitions, and also that\nsupp(P˜z,int) ⊆Bη′(˜z) ⊆Bθ(˜z).\nwhere in the second inclusion we used the fact that η′ = η + ε/δ1/p ≤η + σ ≤c(η + σ) = θ, as\nσ ≥ε/δ1/p and c ≥1.\nWe now return to the sum (C.21). Consider an arbitrary term. First, observe that, for z∗∼P, we\nhave P(z∗∼Γ˜z) = Q(˜z)(1 −2δ′) by (C.22). Hence\nQ(˜z)Ez∗∼Γ˜z,ˆz∼P(·|A,u)[1E] = P(z∗∼Γ˜z)\n1 −2δ′\nEz∗∼P,ˆz∼P(·|A,u)[1E|z∗∼Γ˜z]\n= P(z∗∼Γ˜z)\n(1 −2δ′)\n1\nP(z∗∼Γ˜z)\nZ Z\n1E1z∗∼Γ˜z dP(z∗) dP(·|A, u)(ˆz).\nRecall that z∗∼Γ˜z is supported in Bη′(˜z). Therefore, for the event E to occur, i.e., ∥z∗−ˆz∥>\n(c + 1)(η + σ), it must be that ˆz ∈Bc\nθ(˜z), which means that ˆz ∼P˜z,ext(·|A, u). Hence\nQ(˜z)Ez∗∼Γ˜z,ˆz∼P(·|A,u)[1E] ≤\n1\n1 −2δ′\nZ Z\n1ˆz∼P˜z,ext(·|A,u)1z∗∼Γ˜z dP(z∗) dP(·|A, u)(ˆz)\n=\n1\n1 −2δ′ P[z∗∼P˜z,int, ˆz ∼P˜z,ext(·|A, u)].\nNow fix A ∈Rm×n. Let H˜z,int,A be the distribution of y∗= Az∗+ e for z∗∼P˜z,int and e ∼E\nindependently, and define H˜z,ext,A similarly. Then, by Fubini’s theorem, we have\nQ(˜z)Ez∗∼Γ˜z,A∼A,e∼E,ˆz∼P(·|A,u)[1E] ≤\n1\n1 −2δ′ EA∼A,e∼EP[y∗∼H˜z,int,A, ˆy ∼H˜z,ext,A(·|y∗)].\nNow let H˜z,A be the distribution of y = Az + e for z ∼P and e ∼E independently. Then Lemma\nC.4 (with H = H˜z,A, H1 = H˜z,int,A, H2 = H˜z,mid,A, H3 = H˜z,ext,A and a1 = (1 −2δ′)Q(˜z),\na2 = c˜z,mid, a3 = c˜z,ext) gives\nQ(˜z)Ez∗∼Γ˜z,A∼A,e∼E,ˆz∼P(·|A,u)[1E] ≤\n1\n1 −2δ′ EA∼A [1 −TV(H˜z,int,A, H˜z,ext,A)] .\nFinally, summing over all ˜z we deduce that\nr =\nX\n˜z∈S\nQ(˜z)Ez∗∼Γ˜z,A∼A,e∼E,ˆz∼P(·|A,u)[1E] ≤\n1\n1 −2δ′\nX\n˜z∈S\nEA∼A[1 −TV(H˜z,int,A, H˜z,ext,A)].\n(C.23)\nNow recall that H˜z,int,A is the pushforward of a measure P˜z,int supported in Bη′(˜z), where η′ =\nη + ε′ ≤η + σ and H˜z,ext,A is the pushforward of a measure P˜z,ext supported in Bc\nθ(˜z), where\nθ = c(η + σ) ≥c\n2(η′ + σ). Therefore, Lemma C.5 (with c replaced by c/2) gives that\nEA∼A[1 −TV(H˜z,int,A, H˜z,ext,A)] ≤Clow\n \n2\n√\n2\n√c ; A, D˜z,ext\n!\n+ Cupp\n\u0012 √c\n2\n√\n2; A, D˜z,int\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n√\n2; E\n\u0013\n,\nwhere D˜z,ext = {x −˜z : x ∈supp(P˜z,ext)} and D˜z,int = {x −˜z : x ∈supp(P˜z,int)}. It follows\nimmediately from (C.22) that\nsupp(P˜z,int), supp(P˜z,ext) ⊆supp(P).\nMoreover, ˜z ∈S and therefore\nD˜z,ext, D˜z,int ⊆D2,\nwhere D2 is as in (3.3). Using this, the previous bound and (C.23), we deduce that\nr ≤\n|S|\n1 −2δ′\n\"\nClow\n \n2\n√\n2\n√c ; A, D2\n!\n+ Cupp\n\u0012 √c\n2\n√\n2; A, D2\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n√\n2; E\n\u0013#\n.\nTo complete the proof, now substitute this into (C.19) and (C.20), to obtain\np ≤2δ′ + [Cabs(ε′, tε′; A, D1) + Dupp(c′σ; E)]\n+ 2Dshift(tε′, c′σ; E)|S|\n\"\nClow\n \n2\n√\n2\n√c ; A, D2\n!\n+ Cupp\n\u0012 √c\n2\n√\n2; A, D2\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n√\n2; E\n\u0013#\n.\nThe result now follows after recalling (iv), i.e., |S| ≤ek, and the fact that δ′ ≤δ ≤1/4.\n41\nC.4\nProof of Theorem 1.1\nFinally, we now show how Theorem 3.1 implies the simplified result, Theorem 1.1.\nProof of Theorem 1.1. Let p = P\n\u0002\n∥x∗−ˆx∥≥(8d2 + 2)(η + σ)\n\u0003\n. We use Theorem 3.1 with ε\nreplaced by ε/(2mθ). Let c = 8d2, c′ = 2 , t = θ and ε′ = ε/(2δ1/pmθ). Then Theorem 3.1 gives\nthat\np ≲δ + Cabs(ε′, θε′; A, D1) + Dupp(2σ; E)\n+ 2Dshift(θε′, 2σ; E)ek [Clow (1/d; A, D2) + Cupp (d; A, D2) + 2Dupp(dσ; E)] ,\nwhere\nD1 = Bε′(supp(P)) ∩supp(R) −supp(P) ⊆Bε′(supp(P) −supp(P)),\nD2 = D = supp(P) −supp(P) and k = ⌈log Covη,δ(P)⌉. Now since ∥Ax∥≤∥A∥∥x∥≤θ∥x∥,\n∀x ∈Rn, we make take Cabs(ε′, θε′; A, D1) = 0. Moreover, by Lemma B.1, we have\nDshift(θε′, 2σ; E) ≤exp\n\u0012m(4σ + θε′)\n2σ2\nθε′\n\u0013\n= exp\n\u0012\nε\nδ1/pσ +\nε2\n8δ2/pσ2m\n\u0013\n≲1\nwhere we used the facts that m ≥1 and σ ≥ε/δ1/p. Hence\np ≲δ + ek [C−(1/d; A, D2) + C+ (d; A, D2) + C(dσ; E)] .\nFinally, Lemma B.1 implies that\nDupp(dσ; E) ≤\n\u0000de1−d\u0001m/2 ≤exp(−m/16),\nwhere in the final step we used the fact that d ≥2. This gives the result.\n42\n",
  "pages": [
    {
      "page_number": 1,
      "text": "How many measurements are enough? Bayesian\nrecovery in inverse problems with general distributions\nBen Adcock\nDepartment of Mathematics\nSimon Fraser University\nCanada\nNick Huang\nDepartment of Mathematics\nSimon Fraser University\nCanada\nAbstract\nWe study the sample complexity of Bayesian recovery for solving inverse prob-\nlems with general prior, forward operator and noise distributions. We consider\nposterior sampling according to an approximate prior P, and establish sufficient\nconditions for stable and accurate recovery with high probability. Our main result\nis a non-asymptotic bound that shows that the sample complexity depends on (i)\nthe intrinsic complexity of P, quantified by its approximate covering number, and\n(ii) concentration bounds for the forward operator and noise distributions. As a key\napplication, we specialize to generative priors, where P is the pushforward of a\nlatent distribution via a Deep Neural Network (DNN). We show that the sample\ncomplexity scales log-linearly with the latent dimension k, thus establishing the\nefficacy of DNN-based priors. Generalizing existing results on deterministic (i.e.,\nnon-Bayesian) recovery for the important problem of random sampling with an\northogonal matrix U, we show how the sample complexity is determined by the co-\nherence of U with respect to the support of P. Hence, we establish that coherence\nplays a fundamental role in Bayesian recovery as well. Overall, our framework\nunifies and extends prior work, providing rigorous guarantees for the sample\ncomplexity of solving Bayesian inverse problems with arbitrary distributions.\n1\nIntroduction\nInverse problems are of fundamental importance in science, engineering and industry. In a standard\nsetting, the aim is to recover an unknown vector (e.g., an signal or image) x∗∈Rn from measurements\ny = Ax∗+ e ∈Rm.\n(1.1)\nHere e ∈Rm is measurement noise and A ∈Rm×n, often termed the measurement matrix, represents\nthe forwards operator. While simple, the discrete, linear problem (1.1) is sufficient to model many\nimportant applications [5,25,64,68]. It is common to solve (1.1) using a Bayesian approach (see,\ne.g., [30, 71]), where one assumes that x∗is drawn from some prior distribution R. However, in\npractice, R is never known exactly. Especially in modern settings that employ Deep Learning\n(DL) [12,33], it is typical to learn an approximate prior P and then recover x∗from y by approximate\nposterior sampling, i.e., sampling ˆx from the posterior P(·|y, A). An increasingly popular approach\ninvolves using generative models to learn P (see, e.g., [12,21,33,68,70,80] and references therein).\nA major concern in many inverse problems is that the number of measurements m is highly limited,\ndue to physical constraints such as time (e.g., in Magnetic Resonance Imaging (MRI)), power (e.g.,\nin portable sensors), money (e.g., seismic imaging), radiation exposure (e.g., X-Ray CT), or other\nfactors [5,64,68]. Hence, one aims to recover x∗well while keeping the number of measurements m\nas small as possible. With this in mind, in this work we address the following broad question: How\nmany measurements suffice for stable and accurate recovery of x∗∼R via approximate posterior\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n"
    },
    {
      "page_number": 2,
      "text": "sampling ˆx ∼P(·|y, A), and what are conditions on R, P and the distributions A and E of the\nmeasurement matrix A and noise e, respectively, that ensure this recovery?\n1.1\nOverview\nIn this work, we strive to answer to this question in the broadest possible terms, with a theoretical\nframework that allows for very general types of distributions. We now describe the corresponding\nconditions needed and present simplified versions of our main results.\n(i) Closeness of the real and approximate distributions. We assume that Wp(R, P) is small for\nsome 1 ≤p ≤∞, where Wp denotes the Wassertstein p-metric.\n(ii) Low-complexity of P. Since m ≪n in many applications, to have any prospect for accurate\nrecovery we need to impose that P (or equivalently R, in view of the previous assumption) has\nan inherent low complexity. Following [49], we quantify this in terms of its approximate covering\nnumber Covη,δ(P). This is equal to the minimum number of balls of radius η required to cover a\nregion of Rn having P-measure at least 1 −δ. See Definition 2.1 for the full definition.\n(iii) Concentration of A. We consider constants Clow(t) = Clow(t; A, D) ≥0 and Cupp(t) =\nCupp(t; A, D) ≥0 (see Definition 2.3 for the full definition) such that\nPA∼A[∥Ax∥≤t∥x∥] ≤Clow(t),\nPA∼A[∥Ax∥≥t∥x∥] ≤Cupp(t)\nfor all x ∈D := supp(P) −supp(P) and t > 0. Here, and throughout this paper, we write\nS −S = {x1 −x2 : x1, x2 ∈S} ⊆Rn for the difference set associated with a set S ⊆Rn. We\nalso write supp(P) for the support of the measure P (see §2.1 for the definition). Furthermore, we\nwrite ∥·∥for the ℓ2-norm and ∥·∥∞for the ℓ∞-norm. If A is isotropic, i.e., EA∼A∥Ax∥2 = ∥x∥2,\n∀x ∈Rn, as is often the case in practice, these constants measure how fast ∥Ax∥2 concentrates\naround its mean. Notice that this condition is imposed only on D = supp(P) −supp(P), rather\nthan the whole space Rn. As we see later, this is crucial in obtaining meaningful recovery guarantees.\nFinally, in order to present a simplified result in this section, we now make several simplifying\nassumptions. Both of these will be relaxed in our full result, Theorem 3.1.\n(iv) Gaussian noise. Specifically, we assume that E = N(0, σ2\nm I) for some σ > 0.\n(v) Bounded forwards operators. We assume that ∥A∥≤θ a.s. for A ∼A and some θ > 0.\nTheorem 1.1 (Simplified main result). Let 1 ≤p ≤∞, 0 < δ ≤1/4, ε, η > 0 and suppose that\nconditions (i)–(v) hold with Wp(R, P) ≤ε/(2mθ) and σ ≥ε/δ1/p. Suppose that x∗∼R, A ∼A,\ne ∼E independently and ˆx ∼P(·|y, A), where y = Ax∗+ e. Then, for any d ≥2,\nP\n\u0002\n∥x∗−ˆx∥≥(8d2 + 2)(η + σ)\n\u0003\n≲δ + Covη,δ(P)\nh\nClow(1/d) + Cupp(d) + e−m/16i\n.\n(1.2)\nNote that in this and all subsequent results, the term on the left-hand side of (1.2) is the probability\nwith respect to all variables, i.e., x∗∼R, A ∼A, e ∼E and ˆx ∼P(·|y, A). Theorem 1.1 is\nextremely general, in that it allows for essentially arbitrary (real and approximate) signal distributions\nR and P and an essentially arbitrary distribution A for the forwards operator. In broad terms, it\nbounds the probability that the error ∥x∗−ˆx∥of posterior sampling exceeds a constant times the\nnoise level σ plus an arbitrary parameter η. It does so in terms of the approximate covering number\nCovη,δ(P), which measures the complexity of the approximate distribution P, the concentration\nbounds Clow and Cupp for A, which measure how much A elongates or shrinks a fixed vector, and\nan exponentially-decaying term e−m/16, which stems from the (Gaussian) noise. In particular, by\nanalyzing these terms for different classes of distributions P and A, we can derive concrete bounds\nfor various exemplar problems. We next describe two such problems.\n1.2\nExamples\nWe first consider A to be a distribution of subgaussian random matrices. Here A ∼A if its entries\nare i.i.d. subgaussian random variables with mean zero and variance 1/m (see Definition 3.4).\nTheorem 1.2 (Subgaussian measurement matrices, simplified). Consider the setup of Theorem 1.1,\nwhere A is a distribution of subgaussian random matrices. Then there is a constant c > 0 (depending\non the subgaussian parameters β, κ > 0; see Definition 3.4) such that\nP [∥x∗−ˆx∥≥34(η + σ)] ≲δ,\nwhenever m ≥c · [log(Covη,δ(P)) + log(1/δ)] .\n2\n"
    },
    {
      "page_number": 3,
      "text": "This theorem shows the efficacy of Bayesian recovery with subgaussian random matrices: namely,\nthe sample complexity scales linearly in the distribution complexity, i.e., the log of the approximate\ncovering number. Later in Theorem 3.5, we slightly refine and generalize this result.\nGaussian random matrices are very commonly studied, due to their amenability to analysis and tight\ntheoretical bounds [5,22,34,49], with Theorem 1.2 being a case in point. However, they are largely\nirrelevant to practical inverse problems, where physical constraints impose certain structures on the\nforwards operator distribution A [5,64,68]. For instance, in MRI physical constraints mean that the\nmeasurements are samples of the Fourier transform of the image. This has motivated researchers to\nconsider much more practically-relevant distributions, in particular, so-called subsampled orthogonal\ntransforms (see, e.g., [5]). Here U ∈Rn×n is a fixed orthogonal matrix – for example, the matrix\nof the Discrete Fourier Transform (DFT) in the case of MRI – and the distribution A is defined by\nrandomly selecting m rows of U. See Definition 3.7 for the formal definition.\nTheorem 1.3 (Subsampled orthogonal transforms, simplified). Consider the setup of Theorem 1.1,\nwhere A is a distribution of subsampled orthogonal matrices based on a matrix U. Then there is a\nuniversal constant c > 0 such that\nP [∥x∗−ˆx∥≥34(η + σ)] ≲δ,\nwhenever m ≥c · µ(U; D) · [log(Covη,δ(P)) + log(1/δ)] ,\nwhere D = supp(P) −supp(P) and µ(U; D) is the coherence of U relative to D, defined as\nµ(U; D) = n sup\nn\n∥Ux∥2\n∞/∥x∥2 : x ∈D, x ̸= 0\no\n.\nThis result shows that similar stable and accurate recovery to the Gaussian case can be achieved\nusing subsampled orthogonal matrices, provided the number of measurements scales linearly with the\ncoherence. We discuss this term further in §3.2, where we also present the full result, Theorem 3.9.\nIn general, Theorems 1.2 and 1.3 establish a key condition for successful Bayesian recovery in\ninverse problems, in each case relating the number of measurements m to the intrinsic complexity\nlog(Covη,δ(P)) of P. It is therefore informative to see how this complexity behaves for cases of\ninterest. As noted, it is common to use a generative model to learn P. This means that P = G♯γ,\nwhere G : Rk →Rn is a Deep Neural Network (DNN) and γ is some fixed probability measure on\nthe latent space Rk. Typically, γ = N(0, I). If G is L-Lipschitz, we show in Proposition 4.1 that\nlog(Covη,δ(P)) = O(k log[L(\n√\nk + log(1/δ))/η]).\n(1.3)\nThis scales log-linearly in the latent space dimension k, confirming the intrinsic low-complexity of P.\nCombining (1.3) with Theorems 1.2-1.3 we see that posterior sampling achieves stable and accurate\nrecovery, provided the number of measurements m scales near-optimally with k, i.e., as O(k log(k)).\nFurther, in order to compare with deterministic settings such as classical compressed sensing, which\nconcerns the recovery of s-sparse vectors, we also consider distributions P = Ps of s-sparse vectors.\nIn this case, we show in Proposition 4.3 that\nlog(Covη,δ(P)) = O(s log(n/s) + log[(√s + log(1/δ))/η]).\n(1.4)\nHence s measurements, up to log terms, are sufficient for recovery of approximately sparse vectors.\nThis extends a classical result for deterministic compressed sensing to the Bayesian setting.\n1.3\nSignificance\nThe significance of this work is as follows. See §1.4 for additional discussion.\n1. We provide the first results for Bayesian recovery with arbitrary real and approximate prior\ndistributions R, P and forwards operator and noise distributions A, E.\n2. Unlike much of the theory of Bayesian inverse problems, which is asymptotic in nature [12,30,71],\nour results are non-asymptotic. They hold for arbitrary values of the various parameters within given\nranges (e.g., the failure probability δ, the noise level σ, the number of measurements m, and so forth).\n3. For priors defined by Lipschitz generative DNNs, we establish the first result demonstrating that\nthe sample complexity of Bayesian recovery depends log-linearly on the latent dimension k and\nlogarithmically on the Lipschitz constant.\n3\n"
    },
    {
      "page_number": 4,
      "text": "4. For the important class of subsampled orthogonal transforms, we show that the sample complexity\nof Bayesian recovery depends on the coherence, thus resolving several key open problems in the\nliterature (see next).\n5. It is increasingly well-known that DL-based methods for inverse problems are susceptible to\nhallucinations and other undesirable effects [11,15,18,25,29,40,42,45,47,60–63,65,66,79]. This is\na major issue that may limit the uptake of these methods in safety-critical domains such as medical\nimaging [25,55,57,61,74,76,77]. Our results provide theoretical guarantees for stable and accurate,\nand therefore show conditions under which hallucinations provably cannot occur. This is not only\ntheoretically interesting, but it also has practical consequences in the development of robust DL\nmethods for inverse problems – a topic we intend to explore in future work.\n1.4\nRelated work\nBayesian methods for inverse problems have become increasingly popular over the last several\ndecades [30,71], and many state-of-the-art DL methods for inverse problems now follow a Bayesian\napproach (see [7,12,21,46,56,64] and references therein). Learned priors, such as those stemming\nfrom generative models, are now also increasingly used in applications [1,7,12,21,28,33,38,46,48,\n51,53,54,58,64,68,70,78,80].\nThis work is motivated in part by the (non-Bayesian) theory of generative models for solving\ninverse problems (see [22,33,43,44,48,68–70,80] and references therein). This was first developed\nin [22], where compressed sensing techniques were used to show recovery guarantees for a Gaussian\nrandom matrix A when computing an approximate solution to (1.1) in the range Σ := ran(G) of\na Lipschitz map G : Rk →Rn, typically assumed to be a generative DNN. This is a deterministic\napproach. Besides the random forward operator and (potentially) random noise, it recovers a fixed\n(i.e., nonrandom) underlying signal x∗in a deterministic fashion with a point estimator ˆx that is\nobtained as a minimizer of the empirical ℓ2-loss minz∈Σ ∥Az −y∥2. In particular, no information\nabout the latent space distribution γ is used. In this work, following, e.g., [1,21,48,49,70] and others,\nwe consider a Bayesian setting, where x∗∼R is random and where we quantify the number of\nmeasurements that suffice for accurate and stable recovery via posterior sampling ˆx ∼P(·|y, A).\nOur work is a generalization of [49], which considered Bayesian recovery with Gaussian random\nmatrices and standard Gaussian noise. We significantly extend [49] to allow for arbitrary distributions\nA and E for the forward operator and noise, respectively. A key technical step in doing this is the\nintroduction of the concentration bounds Clow and Cupp. In particular, these bounds are imposed\nonly over the subset D = supp(P) −supp(P). This is unnecessary in the Gaussian case considered\nin Theorem 1.2, but crucial to obtain meaningful bounds in, for instance, the case of subsampled\northogonal transforms considered in Theorem 1.3 (see Remarks 3.2 and 3.10 for further discussion).\nAs noted, this case is very relevant to applications. In particular, when U is taken as a DFT matrix our\nwork addresses open problems posed in [48, §3] and [68, §II.F] on recovery guarantees with Fourier\nmeasurements. We also derive bounds for the approximate covering number of distributions given\nby Lipschitz generative DNNs (see (1.3) and Proposition 4.1) and distributions of sparse vectors\n(see (1.4) and Proposition 4.3), addressing an open problem posed in [49, §6]. In particular, we\ndemonstrate stable and accurate recovery in a Bayesian sense with a number of measurements that\nscales linearly in the model complexity, i.e., k in the former case and s in the latter case.\nRecently [16,17] generalized the results of [22] in the non-Bayesian setting from Gaussian random\nmatrices to subsampled orthogonal transforms. Theorem 1.3 provides a Bayesian analogue of this\nwork, as discussed above, where we consider posterior sampling rather than a deterministic point\nestimator. We also extend the setup of [16,17] by allowing for general measurement distributions A\nand priors P. In particular, in [16,17] the quantity Σ, which is the deterministic analogue of the prior\ndistribution P in our work, was assumed to be the range of a ReLU generative DNN. Like in [16],\nwe make use of the concept of coherence (see Theorem 1.3). However, our proof techniques are\ncompletely different to those of [16,17].\nClassical compressed sensing considers the recovery of approximately s-sparse vectors from (1.1).\nHowever, it has been extended to consider much more general types of low-complexity signal\nmodels, such as joint or block sparse vectors, tree sparse vectors, cosparse vectors and many\nothers [4,6,13,23,31,36,73]. However, most recovery guarantees for general model classes consider\nonly (sub)Gaussian measurement matrices (see e.g., [13, 34]). Recently, [3] introduced a general\n4\n"
    },
    {
      "page_number": 5,
      "text": "framework for compressed sensing that allows for general low-complexity models Σ ⊆Rn contained\nwithin a finite union of finite-dimensional subspaces and arbitrary (random) measurement matrices.\nOur work is a Bayesian analogue of this deterministic setting. Similar to [3], a key feature of our work\nis that we consider arbitrary real and approximate signal distributions P, R (analogous to arbitrary\nlow-complexity models Σ) and arbitrary distributions A for the forwards operator. Unsurprisingly, a\nnumber of the conditions in our main result, Theorem 1.1 – namely, the low-complexity condition (ii)\nand concentration condition (iii) – share similarities to those that ensure stable and accurate recovery\nin non-Bayesian compressed sensing. See Remarks 2.2 and 3.3 for further discussion. However, the\nproof techniques used in this work are once more entirely different.\nFinally, while the focus of this work is to establish guarantees for posterior sampling, we mention\nin passing related work on information-theoretically optimal recovery methods. Methods such as\nApproximate Message Passing (AMP) [14,35] are well studied, and asymptotic information-theoretic\nbounds for Gaussian random matrices are known [52]. AMP methods are fast point estimation\nalgorithms, whereas we focus on sampling-based methods and do not consider computational\nimplementations (see §5 for some further discussion). Note that information-theoretic lower bounds\nfor posterior sampling have also been shown for the Gaussian case in [49].\n2\nPreliminaries\n2.1\nNotation\nWe now introduce some further notation. We let Br(x) = {z ∈Rn : ∥z −x∥≤r} and, when x = 0,\nwe write Br := Br(0). Given a set X ⊆Rn, we write Xc = Rn\\X for its complement. We also\nwrite Br(X) = S\nx∈X Br(x) for the r-neighbourhood of X.\nLet (X, F, µ) be a Borel probability space. We write supp(µ) for its support, i.e., the smallest closed\nset A ⊆X for which µ(A) = 1. Given probability spaces (X, F1, µ), (Y, F2, ν), we write Γ = Γµ,ν\nfor the set of couplings, i.e., probability measures on the product space (X × Y, σ(F1 ⊗F2)) whose\nmarginals are µ and ν, respectively. Given a cost function c : X × Y →[0, ∞) and 1 ≤p < ∞, the\nWasserstein-p metric is defined as\nWp(µ, ν) = inf\nγ∈Γ\n\u0012Z\nX×Y\nc(x, y)pdγ(x, y)\n\u00131/p\n.\nIf p = ∞, then W∞(µ, ν) = infγ∈Γ(esssupγc(x, y)). In this paper, unless stated otherwise,\nX = Y = Rn and the cost function c is the Euclidean distance.\n2.2\nApproximate covering numbers\nAs a measure of complexity of measures, we use the concept of approximate covering numbers as\nintroduced in [49].\nDefinition 2.1 (Approximate covering number). Let (X, F, P) be a probability space and δ, η ≥0.\nThe η, δ-approximate covering number of P is defined as\nCovη,δ(P) = min\n(\nk ∈N : ∃{xi}k\ni=1 ⊆supp(P), P\n k[\ni=1\nBη(xi)\n!\n≥1 −δ\n)\n.\nThis quantity measures how many balls of radius η are required to cover at least 1 −δ of the P-\nmass of Rn. See [49] for further discussion. Note that [49] does not require the centres xi of the\napproximate cover belong to supp(P). However, this is useful in our more general setting and\npresents no substantial restriction. At worst, this requirement changes η by a factor of 1/2.\nRemark 2.2 (Relation to non-Bayesian compressed sensing) Note that when δ = 0, the approxi-\nmate covering number Covη,0(P) ≡Covη(supp(P)) is just the classical covering number of the\nset supp(P), i.e., the minimal number of balls of radius η that cover supp(P). Classical covering\nnumbers play a key role in (non-Bayesian) compressed sensing theory. Namely, the covering number\nof (the unit ball of) the model class Σ ⊆Rn directly determines the number of measurements that\nsuffice for stable and accurate recovery. See, e.g., [3,34]. In the Bayesian setting, the approximate\ncovering number plays the same role; see Theorem 1.1.\n5\n"
    },
    {
      "page_number": 6,
      "text": "2.3\nBounds for A and E\nSince our objective is to establish results that hold for arbitrary measurement and noise distributions\nA and E, we require several key definitions. These are variety of (concentration) bounds.\nDefinition 2.3 (Concentration bounds for A). Let A be a distribution on Rm×n, t ≥0 and D ⊆Rn.\nThen a lower concentration bound for A is any constant Clow(t) = Clow(t; A, D) ≥0 such that\nPA∼A{∥Ax∥≤t∥x∥} ≤Clow(t; A, D),\n∀x ∈D.\nSimilarly, an upper concentration bound for A is any constant Cupp(t) = Cupp(t; A, D) ≥0 such\nthat\nPA∼A{∥Ax∥≥t∥x∥} ≤Cupp(t; A, D),\n∀x ∈D.\nFinally, given t, s ≥0 an (upper) absolute concentration bound for A is any constant Cabs(s, t; A, D)\nsuch that\nPA∼A(∥Ax∥> t) ≤Cabs(s, t; A, D),\n∀x ∈D, ∥x∥≤s.\nNotice that if A is isotropic, i.e., E∥Ax∥2 = ∥x∥2, ∀x ∈Rn, then Clow and Cupp determine\nhow well ∥Ax∥2 concentrates around its mean ∥x∥2 for any fixed x ∈D. To obtain desirable\nsample complexity estimates (e.g., Theorems 1.2 and 1.3), we need concentration bounds that decay\nexponentially in m. A crucial component of this analysis is considering concentration bounds over\nsome subset D (related to the support of P), as, in general, one cannot expect fast concentration over\nthe whole of Rn. See Remarks 3.2 and 3.10.\nDefinition 2.4 (Concentration bound for E). Let E be a distribution in Rm and t ≥0. Then an\n(upper) concentration bound for E is any constant Dupp(t) = Dupp(t; E) ≥0 such that\nE(Bc\nt ) = Pe∼E(∥e∥≥t) ≤Dupp(t; E).\nNotice that this bound just measures the probability that the noise is large. We also need the following\nconcept, which estimates how much the density of E changes in a τ-neighbourhood of the origin\nwhen perturbed by an amount ε.\nDefinition 2.5 (Density shift bounds for E). Let E be a distribution in Rm with density pE and\nε, τ ≥0. Then a density shift bound for E is any constant Dshift(ε, τ) = Dshift(ε, τ; E) ≥0 (possibly\n+∞) such that\npE(u) ≤Dshift(ε, τ; E)pE(v),\n∀u, v ∈Rn, ∥u∥≤τ, ∥u −v∥≤ε.\n3\nMain results\nWe now present our main results. The first, an extension of Theorem 1.1, is a general result that holds\nfor arbitrary distributions R, P, A and E.\nTheorem 3.1. Let 1 ≤p ≤∞, 0 ≤δ ≤1/4, ε, η, t > 0, c, c′ ≥1 and σ ≥ε/δ1/p. Let E be a\ndistribution on Rm and R, P be distributions on Rn satisfying Wp(R, P) ≤ε and\nmin(log Covη,δ(R), log Covη,δ(P)) ≤k\n(3.1)\nfor some k ∈N. Suppose that x∗∼R, A ∼A, e ∼E independently and ˆx ∼P(·|y, A), where\ny = Ax∗+ e. Then p := P[∥x∗−ˆx∥≥(c + 2)(η + σ)] satisfies\np ≤2δ + Cabs(ε/δ1/p, tε/δ1/p; A, D1) + Dupp(c′σ; E)\n+ 2Dshift(tε/δ1/p, c′σ; E)ek\n\"\nClow\n \n2\n√\n2\n√c ; A, D2\n!\n+ Cupp\n\u0012 √c\n2\n√\n2; A, D2\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n√\n2; E\n\u0013 #\n,\nwhere\nD1 = Bε/δ1/p(supp(P)) ∩supp(R) −supp(P)\n(3.2)\nand\nD2 =\n\u001asupp(P) −supp(P)\nif P attains the minimum in (3.1)\nsupp(P) −supp(R)\notherwise\n.\n(3.3)\n6\n"
    },
    {
      "page_number": 7,
      "text": "This theorem bounds the probability p of unstable or inaccurate recovery in terms of the various\nparameters using the constants introduced in the previous section and the approximate covering\nnumbers of R, P. This result is powerful in its generality, but as a consequence, rather opaque. In\nparticular, since it considers arbitrary measurement and noise distributions, the number of measure-\nments m does not explicitly enter the bound. For typical distributions, a dependence on m is found\nin the concentration bounds Clow, Cupp, Dupp, as well as the terms Dshift and Cabs. For instance, the\nformer decay exponentially-fast in m for the examples introduced in §1.2, and therefore compensate\nfor the exponentially-large scaling in k in the main bound (see §B for precise estimates, as well as the\ndiscussion in §3.1-3.2). Note that Theorem 3.1 also considers general noise distributions E. While\nGaussian noise is arguably the most important example – and will be used in all our subsequent\nexamples – this additional generality comes at little cost in terms of the technicality of the proofs.\nRemark 3.2 (The concentration bounds in Theorem 3.1) A particularly important facet of this\nresult, for the reasons discussed above, is that the various concentration bounds Cabs, Clow and Cupp\nare taken over sets D1, D2 – given by (3.2) and (3.3), respectively, and related to the support of P\nand R – rather than the whole space Rn. We exploit this fact crucially later in Theorem 3.9.\nRemark 3.3 (Relation to non-Bayesian compressed sensing) The constants Clow and Cupp are\nsimilar, albeit not identical to similar conditions such as the Restricted Isometry Property (RIP)\n(see, e.g., [5, Chpt. 5]) or Restricted Eigenvalue Condition (REC) [19,22] that appear in non-Bayesian\ncompressed sensing. There, one considers a fixed model class Σ ⊆Rn, such as the set Σs of s-sparse\nvectors or, as in [22], the range ran(G) of a generative DNN. Conditions such as the RIP or REC\nimpose that ∥Ax∥is concentrated around ∥x∥for all x belonging to the difference set Σ −Σ. In\nTheorem 3.1, assuming P attains the minimum in (3.1), there is a similar condition with Σ replaced\nby supp(P). Indeed, Clow(2\n√\n2/√c; A, D2) measures how small ∥Ax∥is in relation to ∥x∥and\nCupp(√c/(2\n√\n2); A, D2) measures how large ∥Ax∥is in relation to ∥x∥.\n3.1\nExample: Subgaussian random matrices with Gaussian noise\nWe now apply this theorem to the first example introduced in §1.2. Recall that a random variable X\non R is subgaussian with parameters β, κ > 0 if P(|X| ≥t) ≤βe−κt2 for all t > 0.\nDefinition 3.4 (Subgaussian random matrix). A random matrix A ∈Rm×n is subgaussian with\nparameters β, κ > 0 if A =\n1\n√m eA, where the entries of eA are independent mean-zero sugaussian\nrandom variables with variance 1 and the same subgaussian parameters β, κ.\nNote that 1/√m is a scaling factor that ensures that A is isotropic, i.e., E∥Ax∥2 = ∥x∥2, ∀x ∈Rn.\nTheorem 3.5. Let 1 ≤p ≤∞, 0 ≤δ ≤1/4, ε, η > 0 and σ ≥ε/δ1/p. Let E = N(0, σ2\nm I)\nand A be a distribution of subgaussian random matrices with parameters β, κ > 0. Let R, P be\ndistributions on Rn and suppose that x∗∼R, A ∼A, e ∼E independently and ˆx ∼P(·|y, A),\nwhere y = Ax∗+ e. Then there is a constant c(β, κ) > 0 depending on β, κ only such that\nP[∥x∗−ˆx∥≥34(η + σ)] ≲δ,\nprovided Wp(R, P) ≤ε/c(β, κ) and\nm ≥c(β, κ) · [min(log Covη,δ(R), log Covη,δ(P)) + log(1/δ)] .\n(3.4)\nThis theorem is derived from Theorem 3.1 by showing that the various concentration bounds are\nexponentially small in m for subgaussian random matrices (see §B). It is a direct generalization\nof [49], which considered the Gaussian case only. It shows that subgaussian random matrices are\nnear-optimal for Bayesian recovery, in the sense that m scales linearly with the log of the approximate\ncovering number (3.4). We estimate these covering numbers for several key cases in §4.\nIt is worth at this stage discussing how (3.4) behaves with respect to the various parameters. First,\nsuppose that η decreases so that the error bound becomes smaller. Then Covη,δ(·) increases, meaning,\nas expected, that more measurements are required to meet (3.4). Second, suppose that δ decreases, so\nthat the failure probability shrinks. Then Covη,δ(·) and log(1/δ) both increase, meaning, once again,\nthat more measurements are needed for (3.4) to hold. Both behaviours are as expected.\n7\n"
    },
    {
      "page_number": 8,
      "text": "Remark 3.6 (Relation to Johnson–Lindenstrauss) Suppose that P = R is a sum of d Diracs\nlocated at X = {x1, . . . , xd} ⊆Rn. Since the matrix A ∈Rm×n is a linear dimensionality-reducing\nmap, the Johnson-Lindenstrauss Lemma states that distances in X are preserved under A if and only\nif m = O(log(d)). In this setting, preserving the distances in X is equivalent to being able to stably\nidentify the mode from which a signal is drawn when observing its measurements. In agreement with\nthis argument, Theorem 3.5 also predicts recovery from roughly m = O(log(d)) measurements.\n3.2\nExample: Randomly-subsampled orthogonal transforms with Gaussian noise\nAs discussed, subgaussian random matrices are largely impractical. We now consider the more\npractical case of subsampled orthogonal transforms.\nDefinition 3.7 (Randomly-subsampled orthogonal transform). Let U ∈Rn×n be orthogonal (i.e.,\nU ⊤U = UU ⊤= I) and write u1, . . . , un ∈Rn for its rows. Let X1, . . . , Xn ∼i.i.d. Ber(m/n) be\nindependent Bernoulli random variables with P(Xi = 1) = m/n and P(Xi = 0) = 1 −m/n. Then\nwe define a distribution A as follows. We say that A ∼A if\nA =\nr n\nm\n\n\nu⊤\ni1...\nu⊤\niq\n\n,\nwhere {i1, . . . , iq} ⊆{1, . . . , n} is the set of indices i for which Xi = 1.\nThe factor\np\nn/m ensures that E(A⊤A) = I. Note that the number of measurements q in this model\nis itself a random variable, with E(q) = m. However, q concentrates exponentially around its mean.\nDefinition 3.8. Let U ∈Rn×n and D ⊆Rn. The coherence of U relative to D is\nµ(U; D) = n · sup\nn\n∥Ux∥2\n∞/∥x∥2 : x ∈D, x ̸= 0\no\n.\nCoherence is a well-known concept in classical compressed sensing with sparse vectors. Definition\n3.8 is a generalization that allows for arbitrary model classes D. This definition is similar to that\nof [16], which considered non-Bayesian compressed sensing with generative models. It is also related\nto the more general concept of variation introduced in [3].\nTheorem 3.9. Let 1 ≤p ≤∞, 0 ≤δ ≤1/4, ε, η > 0 and σ ≥ε/δ1/p. Let E = N(0, σ2\nm I) and\nA be a distribution of randomly-subsampled orthogonal matrices based on a matrix U. Let R, P\nbe distributions on Rn and suppose that x∗∼R, A ∼A, e ∼E independently and ˆx ∼P(·|y, A),\nwhere y = Ax∗+ e. Then there is a universal constant c > 0 such that\nP [∥x∗−ˆx∥≥34(η + σ)] ≲δ,\nprovided Wp(R, P) ≤ε/(2√mn) and\nm ≥c · µ(U; D) · [log(Covη,δ(P)) + log(1/δ)] ,\nwhere D = supp(P) −supp(P).\n(3.5)\nThis theorem is a Bayesian analogue of the deterministic results shown in [3, 16]. In [3, 16], the\nmeasurement conditions scale linearly with µ(U; D), where D = Σ −Σ and Σ ⊆Rn is the low-\ncomplexity model class. Similarly, the number of measurements (3.5) scales linearly with respect to\nthe coherence relative to D = supp(P) −supp(P), which, as discussed in Remark 3.3, plays the\nrole of the low-complexity model class in the Bayesian setting. Note that the measurement condition\n(3.5) involves the approximate distribution P only. This is relevant, since the quantities µ(U; D) and\nCovη,δ(P) can be estimated either numerically or analytically in various cases, such as when P is\ngiven by a generative model. Indeed, we estimate Covη,δ(P) analytically for Lipschitz generative\nmodels in Proposition 4.1 below. The coherence µ(U; D) was estimated analytically in [16] for\nReLU DNNs with random weights (see Remark 4.2 below). It can also be estimated numerically for\nmore general types of generative models [2,16]. Overall, by estimating these quantities, one can use\n(3.5) to gauge how well one can recover with a given P. Note that this may not be possible if (3.5)\ninvolved R as well, since this distribution is typically unknown.\nIn classical compressed sensing, coherence determines the sample complexity of recovering sparse\nvectors from randomly-subsampled orthogonal transforms [26]. A similar argument can be made in\nthe Bayesian setting. Notice that µ(U; D) ≤µ(U; Rn) = n. However, we are particularly interested\nin cases where µ(U; D) ≪n, in which case (3.5) may be significantly smaller than the ambient\ndimension n. We discuss this in the context of several examples in the next section.\n8\n"
    },
    {
      "page_number": 9,
      "text": "Remark 3.10 (Concentration over subsets) Theorem 3.9 illustrates why it is important that Theo-\nrem 3.1 involves concentration bounds over subsets of Rn. To derive Theorem 3.9 from Theorem 3.1\n(see §B), we show exponentially-fast concentration in m/µ(U; D). Had we considered the whole of\nRn, then, since µ(U; Rn) = n, this would have lead to an undesirable measurement condition of the\nform m = O(n) scaling linearly in the ambient dimension n.\n4\nCovering number and sample complexity estimates\nWe conclude by applying our results to two different types of approximate prior distributions.\n4.1\nGenerative DNNs\nProposition 4.1 (Approximate covering number for a Lipschitz pushforward of a Gaussian measure).\nLet G : Rk →Rn be Lipschitz with constant L ≥0, i.e., ∥G(x) −G(z)∥≤L∥x −z∥, ∀x, z ∈Rk,\nand define P = G♯γ, where γ = N(0, I) is the standard normal distribution on Rk. Then\nlog(Covη,δ(P)) ≤k log\n\"\n1 + 2\n√\nkL\nη\n \n1 +\nr\n2\nk log(1/δ)\n!#\n.\n(4.1)\nThis result shows that P has low complexity, since log(Covη,δ(P)) scales log-linearly in k. Combined\nwith Theorem 3.5, it shows that accurate and stable Bayesian recovery with such a prior with a sample\ncomplexity that is near-optimal in the latent dimension k, i.e., O(k log(k)).\nNotice that L only appears logarithmically in (4.1). While it is not the main focus of this work, we\nnote that Lipschitz constants of DNNs have been studied quite extensively [37,72,75]. Moreover, it\nis also possible to design and train DNNs with small Lipschitz constants [59].\nRemark 4.2 (Quadratic bottleneck and high coherence) In Theorem 3.9, the measurement con-\ndition (3.5) also depends on the coherence. This quantity has been considered in [16] for the case\nof ReLU DNNs. In particular, if a ReLU DNN G : Rk →Rn has random weights drawn from a\nstandard normal distribution, then its coherence µ(U; D) scales like O(k) up to log factors [16, Thm.\n3]. Combining this with Theorem 3.9 and Proposition 4.1, we see that the overall sample complexity\nfor Bayesian recovery scales like O(k2 log(k)) in this case. This is worse than the subgaussian case,\nwhere there is no coherence factor and the sample complexity, as noted above, is O(k log(k)). Such\na quadratic bottleneck also arises in the non-Bayesian setting [16]. Its removal is an open problem\n(see §5). Note that the coherence is also not guaranteed to be small for general (in particular, trained)\nDNNs. However, [16] also discuss strategies for training generative models to have small coherence.\nNumerically, they show that generative models with smaller coherence achieve better recovery from\nthe same number of measurements than those with larger coherence.\n4.2\nDistributions of sparse vectors\nLet s ∈{1, . . . , n}. We define a distribution P = Ps of s-sparse vectors in Rn as follows. To draw\nx ∼P, we first choose a support set S ⊆{1, . . . , n}, |S| = s, uniformly at random amongst all\npossible\n\u0000n\ns\n\u0001\nsuch subsets. We then define xi = 0, i /∈S, and for each i ∈S we draw xi randomly\nand independently from N(0, 1). Note that there are other ways to define distributions of sparse\nvectors, which can be analyzed similarly. However, for brevity we only consider the above setup.\nProposition 4.3 (Approximate covering number for distributions of sparse vectors). Let P = Ps be\na distribution of s-sparse vectors in Rn. Then\nlog(Covη,δ(Ps)) ≤s\n\"\nlog\n\u0010en\ns\n\u0011\n+ log\n \n1 + 2√s\nη\n \n1 +\nr\n2\ns log(1/δ)\n!!#\n.\n(4.2)\nAs in the previous case, we deduce Bayesian recovery from O(s log(n/s)) subgaussian measurements,\ni.e., near-optimal, log-linear sample complexity. In the case of randomly-subsampled orthogonal\nmatrices, we also need to consider the coherence. For P = Ps as above, one can easily show that\nµ(U; supp(Ps) −supp(Ps)) ≤2sµ∗(U),\nwhere µ∗(U) = n · max\ni,j |uij|2.\n(4.3)\n9\n"
    },
    {
      "page_number": 10,
      "text": "The term µ∗(U) is often referred to as the coherence of U (see, e.g., [5, Defn. 5.8] or [26]). Notice\nthat µ∗(U) ≈1 for DFT matrices, which is one reason why subsampled Fourier transforms are\nparticularly effective in (non-Bayesian) compressed sensing. Our work implies as similar conclusion\nin the Bayesian setting: indeed, substituting (4.2) and (4.3) into (3.5) we immediately deduce that the\nmeasurement condition for Bayesian recovery with Ps behaves like m = O(s2) up to log terms.\nRemark 4.4 (Quadratic bottleneck) Once more we witness a quadratic bottleneck. In the non-\nBayesian setting, one can show stable and accurate recovery of sparse vectors from O(s) measure-\nments, up to log terms (see, e.g., [5, Cor. 13.15]). However, this requires specialized theoretical\ntechniques that heavily leverage the structure of the set of sparse vectors. In the setting of this paper,\nthe bottleneck arises from the generality of the approach considered in this work: specifically, the\nfact that our main results hold for arbitrary probability distributions P.\n5\nLimitations and future work\nWe end by discussing a number of limitations and avenues for future work. First, although our\nmain result Theorem 3.1 is very general, we have only applied it to a number of different cases,\nsuch as Lipschitz pushforward measures and Gaussian random matrices or subsampled orthogonal\ntransforms. We believe many other important problems can be studied as corollaries of our main\nresults. This includes sampling with heavy-tailed vectors [50], sampling with random convolutions\n[67], multi-sensor acquisition problems [27], generative models augmented with sparse deviations\n[32], block sampling [3,20,24], with applications to practical MRI acquisition, sparse tomography [8],\ndeconvolution and inverse source problems [9]. We believe our framework can also be applied to\nvarious types of non-Gaussian noise, as well as problems involving sparsely-corrupted measurements\n[50]. We are actively investigating applying our framework to these problems.\nSecond, as noted in Remarks 4.2 and 4.4 there is a quadratic bottleneck when considering subsampled\northogonal transforms. In the non-Bayesian case, this can be overcome in the case of (structured)\nsparse models using more technical arguments [3]. We believe similar ideas could also be exploited\nin the Bayesian setting. On a related note, both [16, 22] consider ReLU generative models in the\nnon-Bayesian setting, and derive measurement conditions that do not involve the Lipschitz constant\nL of the DNN. It is unclear whether analogous results can be established in the Bayesian setting.\nThird, our main result involves the density shift bound (Definition 2.5). In particular, the noise\ndistribution should have a density. This rather unpleasant technical assumption stems from Lemma\nC.6, which is a key step in proving the main result, Theorem 3.1. This lemma allows one to replace\nthe ‘real’ distribution R in the probability term p in Theorem 3.1 by the approximate distribution P.\nThis is done in order to align the prior and the posterior, which is necessary for the subsequent steps\nof the proof of Theorem 3.1. It would be interesting to see if this assumption on the noise could be\nremoved through a refined analysis.\nFourth, as noted in [16], the coherence µ(U; D) arising in Theorem 3.9 may be high. In the non-\nBayesian setting, this has been addressed in [2,3,17] by using a nonuniform probability distribution\nfor drawing rows of U, with probabilities given in terms of so-called local coherences of U with\nrelative to D. As shown therein, this can lead to significant performance gains over sampling\nuniformly at random. We believe a similar approach can be considered in the Bayesian setting as a\nconsequence of our general framework. We intend to explore this in future work.\nFinally, our results in this paper are theoretical, and strive to study the sample complexity of Bayesian\nrecovery. We do not address the practical problem of sampling from the posterior. This is a key\ncomputational challenge in Bayesian inverse problems [12]. However, efficient techniques for doing\nthis are emerging. See, e.g., [48,54,70] and references therein. We believe an advantage of our results\nis their independence from the choice of posterior sampling algorithm, whose analysis can therefore\nbe performed separately. This is an interesting problem we intend to examine in the future. In future\nwork we also intend present numerical experiments for various practical settings that further support\nthe theory developed in this paper.\n10\n"
    },
    {
      "page_number": 11,
      "text": "Acknowledgments and Disclosure of Funding\nBA acknowledges the support of the Natural Sciences and Engineering Research Council of Canada\nof Canada (NSERC) through grant RGPIN-2021-611675. NH acknowledges support from an NSERC\nCanada Graduate Scholarship. Both authors would like to thank Paul Tupper and Weiran Sun for\nhelpful comments and feedback.\nReferences\n[1] A. Aali, M. Arvinte, S. Kumar, and J. I. Tamir. Solving inverse problems with score-based\ngenerative priors learned from noisy data. In 2023 57th Asilomar Conference on Signals,\nSystems, and Computers, pages 837–843, 2023.\n[2] B. Adcock, J. M. Cardenas, and N. Dexter. CS4ML: A general framework for active learning\nwith arbitrary data based on Christoffel functions. In A. Oh, T. Naumann, A. Globerson,\nK. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing\nSystems, volume 36, pages 19990–20037, 2023.\n[3] B. Adcock, J. M. Cardenas, and N. Dexter. A unified framework for learning with nonlinear\nmodel classes from arbitrary linear samples. In International Conference on Machine Learning,\n2024.\n[4] B. Adcock, A. Gelb, G. Song, and Y. Sui. Joint sparse recovery based on variances. SIAM J.\nSci. Comput., 41(1):A246–A268, 2019.\n[5] B. Adcock and A. C. Hansen. Compressive Imaging: Structure, Sampling, Learning. Cambridge\nUniversity Press, Cambridge, UK, 2021.\n[6] B. Adcock, A. C. Hansen, C. Poon, and B. Roman. Breaking the coherence barrier: a new\ntheory for compressed sensing. Forum Math. Sigma, 5:e4, 2017.\n[7] J. Adler and O. Öktem. Deep Bayesian inversion. In Data-Driven Models in Inverse Problems,\npages 359–412. De Gruyter, Berlin, Boston, 2025.\n[8] G. S. Alberti, A. Felisi, M. Santacesaria, and S. I. Trapasso. Compressed sensing for inverse\nproblems and the sample complexity of the sparse Radon transform. J. Eur. Math. Soc. (in\npress), 2025.\n[9] G. S. Alberti, A. Felisi, M. Santacesaria, and S. I. Trapasso. Compressed sensing for inverse\nproblems ii: applications to deconvolution, source recovery, and MRI. arXiv:2501.01929, 2025.\n[10] L. Ambrosio, E. Brué, and D. Semola. Lectures on Optimal Transport. UNITEXT. Springer,\nCham, Switzerland, 2nd edition, 2024.\n[11] V. Antun, F. Renna, C. Poon, B. Adcock, and A. C. Hansen. On instabilities of deep learning in\nimage reconstruction and the potential costs of AI. Proc. Natl. Acad. Sci. USA, 117(48):30088–\n30095, 2020.\n[12] S. Arridge, P. Maass, O. Öktem, and C.-B. Schönlieb. Solving inverse problems using data-\ndriven models. Acta Numer., 28:1–174, 2019.\n[13] R. G. Baraniuk, V. Cevher, M. F. Duarte, and C. Hedge. Model-based compressive sensing.\nIEEE Trans. Inform. Theory, 56(4):1982–2001, 2010.\n[14] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applica-\ntions to compressed sensing. IEEE Trans. Inform. Theory, 57(2):764–785, 2011.\n[15] C. Belthangady and L. A. Royer. Applications, promises, and pitfalls of deep learning for\nfluorescence image reconstruction. Nature methods, 16(12):1215–1225, 2019.\n[16] A. Berk, S. Brugiapaglia, B. Joshi, Y. Plan, M. Scott, and O. Yilmaz. A coherence parameter\ncharacterizing generative compressed sensing with fourier measurements. IEEE J. Sel. Areas\nInf. Theory, 3(3):502–512, 2022.\n[17] A. Berk, S. Brugiapaglia, Y. Plan, M. Scott, X. Sheng, and O. Yilmaz. Model-adapted Fourier\nsampling for generative compressed sensing. In NeurIPS 2023 Workshop on Deep Learning\nand Inverse Problems, 2023.\n[18] S. Bhadra, V. A. Kelkar, F. J. Brooks, and M. A. Anastasio. On hallucinations in tomographic\nimage reconstruction. IEEE Trans. Med. Imaging, 40(11):3249–3260, 2021.\n11\n"
    },
    {
      "page_number": 12,
      "text": "[19] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.\nAnn. Statist., 37(4):1705–1732, 2009.\n[20] J. Bigot, C. Boyer, and P. Weiss. An analysis of block sampling strategies in compressed sensing.\nIEEE Trans. Inform. Theory, 62(4):2125–2139, 2016.\n[21] P. Bohra, J. Pham, T.-A. Dong, and M. Unser. Bayesian inversion for nonlinear imaging models\nusing deep generative priors. IEEE Trans. Comput. Imag., 8:1237–1249, 2023.\n[22] A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models.\nIn International Conference on Machine Learning, pages 537–546, 2017.\n[23] A. Bourrier, M. E. Davies, T. Peleg, P. Pérez, and R. Gribonval. Fundamental performance\nlimits for ideal decoders in high-dimensional linear inverse problems. IEEE Trans. Inform.\nTheory, 60(12):7928–7946, 2014.\n[24] C. Boyer, J. Bigot, and P. Weiss. Compressed sensing with structured sparsity and structured\nacquisition. Appl. Comput. Harmon. Anal., 46(2):312–350, 2019.\n[25] M. Burger and T. Roith. Learning in image reconstruction: A cautionary tale. SIAM News,\n57(08), Oct 2024.\n[26] E. J. Candès and Y. Plan. A probabilistic and RIPless theory of compressed sensing. IEEE\nTrans. Inform. Theory, 57(11):7235–7254, 2011.\n[27] I.-Y. Chun and B. Adcock. Compressed sensing and parallel acquisition. IEEE Trans. Inform.\nTheory, 63(8):4860–4882, 2017.\n[28] H. Chung and J. C. Ye. Score-based diffusion models for accelerated mri. Medical Image\nAnalysis, 80:102479, 2022.\n[29] M. J. Colbrook, V. Antun, and A. C. Hansen. The difficulty of computing stable and accurate\nneural networks: On the barriers of deep learning and smale’s 18th problem. Proc. Natl. Acad.\nSci. USA, 119(12):e2107151119, 2022.\n[30] M. Dashti and A. M. Stuart. The bayesian approach to inverse problems. In R. Ghanem et al.,\neditor, Handbook of Uncertainty Quantification. Springer, 2017.\n[31] M. A. Davenport, M. F. Duarte, Y. C. Eldar, and G. Kutyniok. Introduction to compressed\nsensing. In Y. C. Eldar and G. Kutyniok, editors, Compressed Sensing: Theory and Applications,\npages 1–64. Cambridge University Press, Cambridge, UK, 2012.\n[32] M. Dhar, A. Grover, and S. Ermon. Modeling sparse deviations for compressed sensing using\ngenerative models. In International Conference on Machine Learning, pages 1214–1223.\nPMLR, 2018.\n[33] A. G. Dimakis. Deep generative models and inverse problems. In P. Grohs and G. Kutyniok, ed-\nitors, Mathematical Aspects of Deep Learning, chapter 9, pages 400–421. Cambridge University\nPress, Cambridge, UK, 2022.\n[34] S. Dirksen. Dimensionality reduction with subgaussian matrices: a unified theory. Found.\nComput. Math., 16:1367–1396, 2016.\n[35] D. L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed\nsensing. Proc. Natl. Acad. Sci. USA, 106(45):18914–18919, 2009.\n[36] M. F. Duarte and Y. C. Eldar. Structured compressed sensing: from theory to applications. IEEE\nTrans. Signal Process., 59(9):4053–4085, 2011.\n[37] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J. Pappas. Efficient and accurate estimation\nof Lipschitz constants for deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d’Alché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 33. Curran Associates, Inc., 2019.\n[38] B. T. Feng, J. Smith, M. Rubinstein, H. Chang, K. L. Bouman, and W. T. Freeman. Score-based\ndiffusion models as principled priors for inverse imaging. In 2023 IEEE/CVF International\nConference on Computer Vision (ICCV), pages 10486–10497, 2023.\n[39] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Appl. Numer.\nHarmon. Anal. Birkhäuser, New York, NY, 2013.\n[40] M. Genzel, J. Macdonald, and M. Marz. Solving inverse problems with deep neural networks –\nrobustness included? IEEE Trans. Pattern Anal. Mach. Intell., 45(1):1119–1134, 2023.\n12\n"
    },
    {
      "page_number": 13,
      "text": "[41] C. R. Givens and R. M. Shortt. A class of Wasserstein metrics for probability distributions.\nMichigan Math. J., 31(2):231–240, 1984.\n[42] N. M. Gottschling, V. Antun, A. C. Hansen, and B. Adcock. The troublesome kernel – on\nhallucinations, no free lunches and the accuracy-stability trade-off in inverse problems. SIAM\nRev., 67(1):73–104, 2025.\n[43] P. Hand, O. Leong, and V. Voroninski. Phase retrieval under a generative prior. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in\nNeural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n[44] P. Hand and V. Voroninski. Global guarantees for enforcing deep generative priors by empirical\nrisk. In S. Bubeck, V. Perchet, and P. Rigollet, editors, Proceedings of the Thirty-First Confer-\nence on Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages\n970–978. PMLR, 2018.\n[45] D. P. Hoffman, I. Slavitt, and C. A. Fitzpatrick. The promise and peril of deep learning in\nmicroscopy. Nature Methods, 18(2):131–132, 2021.\n[46] M. Holden, M. Pereyra, and K. C. Zygalakis. Bayesian imaging with data-driven priors encoded\nby neural networks. SIAM J. Imaging Sci., 15(2):892–924, 2022.\n[47] Y. Huang, T. Würfl, K. Breininger, L. Liu, G. Lauritsch, and A. Maier. Some investigations\non robustness of deep learning in limited angle tomography. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention, pages 145–153, 2018.\n[48] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir. Robust compressed\nsensing mri with deep generative priors. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S.\nLiang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems,\nvolume 34, pages 14938–14954. Curran Associates, Inc., 2021.\n[49] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price. Instance-optimal compressed sensing via\nposterior sampling. In 38th International Conference on Machine Learning, pages 4709–4720,\n2021.\n[50] A. Jalal, L. Liu, A. G. Dimakis, and C. Caramanis. Robust compressed sensing using generative\nmodels. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 713–727. Curran Associates, Inc.,\n2020.\n[51] Z. Kadkhodaie and E. Simoncelli. Stochastic solutions for linear inverse problems using the\nprior implicit in a denoiser. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,\npages 13242–13254. Curran Associates, Inc., 2021.\n[52] A. Karan, K. Shah, S. Chen, and Y. C. Eldar. Unrolled denoising networks provably learn\nto perform optimal Bayesian inference. In A. Globerson, L. Mackey, D. Belgrave, A. Fan,\nU. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing\nSystems, volume 37, pages 135264–135298. Curran Associates, Inc., 2024.\n[53] B. Kawar, M. Elad, S. Ermon, and J. Song. Denoising diffusion restoration models. In\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in\nNeural Information Processing Systems, volume 35, pages 23593–23606. Curran Associates,\nInc., 2022.\n[54] B. Kawar, G. Vaksman, and M. Elad. SNIPS: solving noisy inverse problems stochastically.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, volume 34, pages 21757–21769. Curran\nAssociates, Inc., 2021.\n[55] R. F. Laine, I. Arganda-Carreras, R. Henriques, and G. Jacquemet. Avoiding a replication crisis\nin deep-learning-based bioimage analysis. Nature Methods, 18(10):1136–1144, 2021.\n[56] R. Laumont, V. D. Bortoli, A. Almansa, J. Delon, A. Durmus, and M. Pereyra. Bayesian imaging\nusing plug & play priors: When langevin meets tweedie. SIAM J. Imaging Sci., 15(2):701–737,\n2022.\n[57] X. Liu, B. Glocker, M. M. McCradden, M. Ghassemi, A. K. Denniston, and L. Oakden-Rayner.\nThe medical algorithmic audit. The Lancet Digital Health, 4(5):e384–e397, 2022.\n13\n"
    },
    {
      "page_number": 14,
      "text": "[58] G. Luo, M. Blumenthal, M. Heide, and M. Uecker. Bayesian MRI reconstruction with joint\nuncertainty estimation using diffusion models. Magn. Reson. Med., 90(1):295–311, 2023.\n[59] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative\nadversarial networks. In International Conference on Learning Representations, 2018.\n[60] J. N. Morshuis, S. Gatidis, M. Hein, and C. F. Baumgartner. Adversarial robustness of MR\nimage reconstruction under realistic perturbations. arXiv:2208.03161, 2022.\n[61] Matthew J Muckley, Bruno Riemenschneider, Alireza Radmanesh, Sunwoo Kim, Geunu Jeong,\nJingyu Ko, Yohan Jun, Hyungseob Shin, Dosik Hwang, Mahmoud Mostapha, et al. Results of\nthe 2020 fastMRI challenge for machine learning MR image reconstruction. IEEE Trans. Med.\nImaging, 2021.\n[62] M. Neyra-Nesterenko and B. Adcock. NESTANets: stable, accurate and efficient neural\nnetworks for analysis-sparse inverse problems. Sampl. Theory Signal Process. Data Anal., 21:4,\n2023.\n[63] C. R. Noordman, D. Yakar, J. Bosma, F. F. J. Simonis, and H. Huisman. Complexities of deep\nlearning-based undersampled MR image reconstruction. Eur. Radiol. Exp., 7:58, 2023.\n[64] G. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and R. Willett. Deep learning\ntechniques for inverse problems in imaging. IEEE J. Sel. Areas Inf. Theory, 1(1):39–56, 2020.\n[65] A. Raj, Y. Bresler, and B. Li. Improving robustness of deep-learning-based image reconstruction.\nIn Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages\n7932–7942. PMLR, 13–18 Jul 2020.\n[66] A. J. Reader and B. Pan. AI for PET image reconstruction. Brit. J. Radiol., 96(1150):20230292,\n2023.\n[67] J. Romberg. Compressive sensing by random convolution. SIAM J. Imaging Sci., 2(4):1098–\n1128, 2009.\n[68] J. Scarlett, R. Heckel, M. R. D. Rodrigues, P. Hand, and Y. C. Eldar. Theoretical perspectives on\ndeep learning methods in inverse problems. IEEE J. Sel. Areas Inf. Theory, 3(3):433–453, 2022.\n[69] V. Shah and C. Hegde. Solving linear inverse problems using GAN priors: An algorithm with\nprovable guarantees. In 2018 IEEE international conference on Acoustics, Speech and Signal\nProcessing (ICASSP) conference on acoustics, speech and signal processing (ICASSP), pages\n4609–4613. IEEE, 2018.\n[70] Y. Song, L. Shen, L. Xing, and S. Ermon. Solving inverse problems in medical imaging with\nscore-based generative models. In International Conference on Learning Representations, 2022.\n[71] A. M. Stuart. Inverse problems: a Bayesian perspective. Acta Numer., 19:451–559, 2010.\n[72] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, J. Ian Goodfellow, and R. Fergus.\nIntriguing properties of neural networks. In Proceedings of the International Conference on\nLearning Representations, 2014.\n[73] Y. Traonmilin and R. Gribonval. Stable recovery of low-dimensional cones in Hilbert spaces:\none RIP to rule them all. Appl. Comput. Harmon. Anal., 45(1):170–205, 2018.\n[74] G. Varoquaux and V. Cheplygina. Machine learning for medical imaging: methodological\nfailures and recommendations for the future. NPJ digital medicine, 5(1):1–8, 2022.\n[75] A. Virmaux and K. Scaman. Lipschitz regularity of deep neural networks: analysis and efficient\nestimation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran\nAssociates, Inc., 2018.\n[76] E. Wu, K. Wu, R. Daneshjou, D. Ouyang, D. E. Ho, and J. Zou. How medical AI devices\nare evaluated: limitations and recommendations from an analysis of FDA approvals. Nature\nMedicine, 27(4):582–584, 2021.\n[77] T. Yu, T. Hilbert, G. G. Piredda, A. Joseph, G. Bonanno, S. Zenkhri, P. Omoumi, M. B. Cuadra,\nE. J. Canales-Rodríguez, T. Kober, et al. Validation and generalizability of self-supervised\nimage reconstruction methods for undersampled MRI. arXiv:2201.12535, 2022.\n[78] M. Zach, F. Knoll, and T. Pock. Stable deep MRI reconstructions using generative priors. IEEE\nTrans. Med. Imag., 42(12):3817–3831, 2023.\n14\n"
    },
    {
      "page_number": 15,
      "text": "[79] C. Zhang, J. Jia, B. Yaman, S. Moeller, S. Liu, M. Hong, and M. Akçakaya. Instabilities in\nconventional multi-coil MRI reconstruction with small adversarial perturbations. In 2021 55th\nAsilomar Conference on Signals, Systems, and Computers, pages 895–899, 2021.\n[80] Z. Zhao, J. C. Ye, and Y. Bresler. Generative models for inverse imaging problems: from\nmathematical foundations to physics-driven applications. IEEE Signal Process. Mag., 40(1):148–\n163, 2023.\n15\n"
    },
    {
      "page_number": 16,
      "text": "NeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: We thoroughly discuss the main claims made in the abstract and introduction\nand the necessary assumptions to show them. Our main theoretical contributions directly\naddress these claims. We also have several further remarks after these results to provide\nadditional context for our work.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We discuss limitations at the end of the paper in §5.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n16\n"
    },
    {
      "page_number": 17,
      "text": "Answer: [Yes]\nJustification: We have a detailed discussion of the assumptions needed to show our theoreti-\ncal results in §2. We provide further discussion and present the results themselves in §3-4.\nWe provide full proofs of our results in the supplemental material.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\n17\n"
    },
    {
      "page_number": 18,
      "text": "Question: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not\nbe possible, so No is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n18\n"
    },
    {
      "page_number": 19,
      "text": "• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [NA]\nJustification: There are no experiments in the paper.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We have complied with the NeurIPS Code of Ethics in the preparation of this\nmanuscript.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: This work is primarily foundational, and the examples considered do not\ndirectly impact society.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n19\n"
    },
    {
      "page_number": 20,
      "text": "• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This paper is theoretical, and does not involve any data or models.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: This paper is theoretical, and does not use any existing assets.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n20\n"
    },
    {
      "page_number": 21,
      "text": "• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: This paper is theoretical, and does not introduce any new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: No experiments were conducted in this paper.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: No experiments were conducted in this paper.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\n21\n"
    },
    {
      "page_number": 22,
      "text": "Question: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: The core method development in this paper does not involve LLMs as an\nimportant, original, or non-standard component.\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n22\n"
    },
    {
      "page_number": 23,
      "text": "A\nCovering number estimates and the proofs of Propositions 4.1 and 4.3\nThe proof of Proposition 4.1 relies on the following two lemmas.\nLemma A.1 (Approximate covering number under a Lipschitz pushforward map). Let G : Rk →Rn\nbe Lipschitz with constant L ≥0, i.e.,\n∥G(x) −G(z)∥≤L∥x −z∥,\n∀x, z ∈Rk,\nand define P = G♯γ, where γ is any probability distribution on Rk. Then\nCovη,δ(P) ≤Covη/L,δ(γ),\n∀δ, η ≥0.\nProof. Let {xi}k\ni=1 ⊆supp(γ) and define zi = G(xi) ∈supp(G♯γ) for i = 1, . . . , k. Let\nz ∈G(Bη/L(xi)) and write z = G(x) for some x ∈Bη/L(xi). Then\n∥z −zi∥= ∥G(x) −G(xi)∥≤L∥x −xi∥≤η.\nHence z ∈Bη(zi). Since z was arbitrary, we deduce that G(Bη/L(xi)) ⊆Bη(zi). It follows that\nBη/L(xi) ⊆G−1(Bη(zi)). Now suppose that γ\nhSk\ni=1 Bη/L(xi)\ni\n≥1 −δ. Then, by definition of\nthe pushforward measure\nG♯γ\n\" k[\ni=1\nBη(zi)\n#\n= γ\n\" k[\ni=1\nG−1(Bη(zi))\n#\n≥γ\n\" k[\ni=1\nBη/L(xi)\n#\n≥1 −δ.\nThis gives the result.\nLemma A.2 (Approximate covering number of a normal distribution). Let P = N(0, σ2I) on Rn.\nThen its approximate covering number (Definition 2.1) satisfies\nCovη,δ(P) ≤\n\u0012\n1 + 2√nσt\nη\n\u0013n\n,\nwhere t = 1 +\nr\n2\nn log(1/δ).\nProof. Observe that, for t ≥0,\nP(Bc√nσt) = P(X ≥nt2) ≤(t2e1−t2)n/2.\nwhere X ∼χ2\nn is a chi-squared random variable, and the inequality follows from a standard Chernoff\nbound. Now t2 ≤e2t gives that\nP(Bc√nσt) ≤(e−(t−1)2)n/2.\nNow set t = 1 +\nq\n2\nn log(1/δ) so that P(Bc√nσt) ≤δ. Hence, we have shown that\nCovη,δ(P) ≤Covη(B√nσt),\nwhere Covη is the classical covering number of a set, i.e.,\nCovη(A) = min\n(\nk : ∃{xi}k\ni=1 ⊆A, A ⊆\nk[\ni=1\nBη(xi)\n)\n.\nUsing standard properties of covering numbers (see, e.g., [5, Lem. 13.22], we get\nCovη(B√nσt) = Covη/(√nσt)(B1) ≤\n\u0012\n1 + 2√nσt\nη\n\u0013n\n,\nas required.\nProof of Proposition 4.1. By Lemma A.1,\nCovη,δ(P) ≤Covη/L,δ(N(0, I)).\nThe result now follows from Lemma A.2.\n23\n"
    },
    {
      "page_number": 24,
      "text": "To prove Proposition 4.3, we first require the following lemma.\nLemma A.3 (Approximate covering number of a mixture). Let P = Pr\ni=1 piPi be a mixture of\nprobability distributions Pi on Rn, where pi ≥0, ∀i, and Pr\ni=1 pi = 1. Then\nCovη,δ(P) ≤\nr\nX\ni=1\nCovη,δ(Pi).\nProof. For each i = 1, . . . , r, let {x(i)\nj }ki\nj=1 ⊆Rn, and, in particular, 3x(i)\nj\n∈supp(Pi), be such that\nPi\n\n\nki\n[\nj=1\nBη(x(i)\nj )\n\n≥1 −δ.\nThen\nP\n\n\nr[\ni=1\nki\n[\nj=1\nBη(x(i)\nj )\n\n=\nr\nX\ni=1\npiPi\n\n\nr[\ni=1\nki\n[\nj=1\nBη(x(i)\nj )\n\n\n≥\nr\nX\ni=1\npiPi\n\n\nki\n[\nj=1\nBη(x(i)\nj )\n\n\n≥\nr\nX\ni=1\npi(1 −δ) = 1 −δ.\nNotice that supp(Pi) ⊆supp(P), therefore, x(i)\nj\n∈supp(P). The result now follows.\nProof of Proposition 4.3. Let S = {S : S ⊆{1, . . . , n}, |S| = s}. Then we can write Ps as the\nmixture\nPs =\n|S|\nX\ni=1\n1\n|S|PS,\nwhere PS is defined as follows: x ∼PS if xi = 0 for i /∈S and, for i ∈S, xi is drawn independently\nfrom the standard normal distribution on R. Notice that PS = G♯γ, where γ = N(0, I) is the\nstandard, multivariate normal distribution on Rs and G : Rs →Rn is a zero-padding map. The map\nG is Lipschitz with constant L = 1. Hence, by Lemmas A.1 and A.2,\nCovη,δ(PS) ≤\n\u0012\n1 + 2√st\nη\n\u0013s\n,\nt = 1 +\nr\n2\ns log(1/δ)\nWe now apply Lemma A.3 and the fact that\n|S| =\n\u0012n\ns\n\u0013\n≤\n\u0010en\ns\n\u0011s\n,\nthe latter being a standard bound, to obtain\nCovη,δ(Ps) ≤\n\u0010en\ns\n\u0011s \u0012\n1 + 2√st\nη\n\u0013s\n,\nt = 1 +\nr\n2\ns log(1/δ).\nTaking logarithms gives the result.\n24\n"
    },
    {
      "page_number": 25,
      "text": "B\nConcentration inequalities and the proofs of Theorems 3.5 and 3.9\nWe now aim to prove Theorems 3.5 and 3.9. To do this, we first derive concentration inequalities for\nsubsgaussian random matrices and subsampled orthogonal transforms.\nB.1\nGaussian concentration and density shift bounds\nLemma B.1 (Concentration and density shift bounds for Gaussian noise). Let E = N(0, σ2\nm I). Then\nthe upper concentration bound Dupp(t; E) (Definition 2.4) can be taken as\nDupp(t; E) =\n\u0012 t2\nσ2 e1−t2\nσ2\n\u0013m/2\n,\n∀t > σ.\nand the density shift bound Dshift(ε, τ; E) (Definition 2.5) can be taken as\nDshift(ε, τ; E) = exp\n\u0012m(2τ + ε)\n2σ2\nε\n\u0013\n,\n∀ε, τ ≥0.\nProof. Write e ∼E as e =\nσ\n√mn, where n ∼N(0, I). Then\nP(∥e∥≥t) = P(∥n∥2 ≥t2m/σ2) = P(X ≥t2m/σ2),\nwhere X = ∥n∥2 ∼χ2\nm is a chi-squared random variable with m degrees of freedom. Using a\nstandard Chernoff bound once more, we have\nP(X ≥zm) ≤(ze1−z)m/2,\nfor any z > 1. Setting z = t2\nσ2 , we have\nP(∥e∥≥t) ≤\n\u0012 t2\nσ2 e1−t2\nσ2\n\u0013m/2\n,\nwhich gives the first result.\nFor the second result, we recall that E has density\npE(e) = (2πσ2/m)−m/2 exp\n\u0010\n−m\n2σ2 ∥e∥2\u0011\n.\nTherefore\npE(u)\npE(v) = exp\n\u0010 m\n2σ2 (∥v∥−∥u∥) (∥v∥+ ∥u∥)\n\u0011\n.\nNow suppose that ∥u∥≤τ and ∥u −v∥≤ε. Then\npE(u)\npE(v) ≤exp\n\u0012m(2τ + ε)\n2σ2\nε\n\u0013\n.\nHence\nDshift(ε, τ; E) ≤exp\n\u0012m(2τ + ε)\n2σ2\nε\n\u0013\n,\nwhich gives the second result.\nB.2\nSubgaussian concentration inequalities\nLemma B.2 (Lower and upper concentration bounds for subgaussian random matrices). Let A be a\ndistribution of subgaussian random matrices with parameters β, κ > 0 (Definition 3.4). Then the\nlower ad upper concentration bounds for A (Definition 2.3) can be taken as\nCupp(t; A, Rn) = Clow(1/t; A, Rn) = 2 exp(−c(t, β, κ)m)\nfor any t > 1, where c(t, β, κ) > 0 depends on β, κ only.\n25\n"
    },
    {
      "page_number": 26,
      "text": "Proof. Let x ∈Rn and observe that\nP(∥Ax∥≥t∥x∥) ≤P\n\u0010\f\f\f∥Ax∥2 −∥x∥2\f\f\f ≥(t2 −1)∥x∥2\u0011\nP(∥Ax∥≤t−1∥x∥) ≤P\n\u0010\f\f\f∥Ax∥2 −∥x∥2\f\f\f ≥(1 −t−2)∥x∥2\u0011.\n(B.1)\nWe now use [39, Lem. 9.8]. Note that this result only considers a bound of the form P(|∥Ax∥2 −\n∥x∥2| ≥s∥x∥2) for s ∈(0, 1). But the proof straightforwardly extends to s > 0.\nB.3\nConcentration inequalities for randomly-subsampled orthogonal transforms\nLemma B.3 (Concentration bounds for randomly-subsampled orthogonal transforms). Let D ⊆\nRn and A be a distribution of randomly-subsampled orthognal transforms based on a matrix U\n(Definition 3.7). Then the lower and upper concentration bounds for A (Definition 2.3) can be taken\nas\nCupp(t; A, D) = Clow(1/t; A, D) = 2 exp\n\u0012\n−mc(t)\nµ(U; D)\n\u0013\nfor any t > 1, where µ(U; D) is a in Definition 3.8 and c(t) > 0 depend on t only.\nProof. Due to (B.1), it suffices to bound\nP\n\u0010\f\f\f∥Ax∥2 −∥x∥2\f\f\f ≥s∥x∥2\u0011\nfor s > 0. The result uses Bernstein’s inequality for bounded random variables (see, e.g., [5, Thm.\n12.18]). Let x ∈D. By definition of A and the fact that U is orthogonal, we can write\n∥Ax∥2 −∥x∥2 =\nn\nX\ni=1\n\u0010 n\nmIXi=1 −1\n\u0011\n|⟨ui, x⟩|2 =:\nN\nX\ni=1\nZi.\nNotice that the random variables Zi are independent, with E(Zi) = 0. We also have\n|Zi| ≤n\nm|⟨ui, x⟩|2 ≤µ(U; D)\nm\n∥x∥2 =: K\nand\nn\nX\ni=1\nE|Zi|2 ≤\nn\nX\ni=1\nn2|⟨ui, x⟩|4\nm2\nE(I2\nXi=1) ≤K\nn\nX\ni=1\nn|⟨ui, x⟩|2\nm\nm\nn = K∥x∥2 =: σ2.\nTherefore, by Bernstein’s inequality,\nP(|∥Ax∥2 −∥x∥2| ≥s∥x∥2) ≤2 exp\n \n−\ns2∥x∥4/2\nσ2 + Ks∥x∥2/3)\n!\n= 2 exp\n\u0012\n−\nms2/2\nµ(U; D)(1 + s/3)\n\u0013\nfor any s > 0 and x ∈D. The result now follows.\nLemma B.4 (Absolute concentration bounds for subsampled orthogonal transforms). Let D ⊆Rn\nand A be a distribution of randomly-subsampled orthogonal transforms based on a matrix U\n(Definition 3.7). Then ∥A∥≤\np\nn/m a.s. for A ∼A, and consequently the absolute concentration\nbound for A (Definition 2.3) can be taken as Cabs(s, t; A, D) = 0 for any s ≥0 and t ≥s\np\nn/m.\nProof. Recall that A consists of q rows of an orthogonal matrix U multiplied by the scalar\np\nn/m.\nHence ∥A∥≤\np\nn/m∥U∥=\np\nn/m. Now let x ∈D with ∥x∥≤s. Then ∥Ax∥≤∥A∥∥x∥≤\n∥A∥s. Therefore ∥Ax∥\np\nn/ms ≤t, meaning that P(∥Ax∥> t) = 0. This gives the result.\n26\n"
    },
    {
      "page_number": 27,
      "text": "B.4\nProofs of Theorems 3.5 and 3.9\nProof of Theorem 3.5. Let p = P [∥x∗−ˆx∥≥34(η + σ)]. We use Theorem 3.1 with c = 32, c′ = 2,\nt = 2 and ε replaced by ε/d, where d ≥1 is a constant that will be chosen later. Let ε′ = ε/(dδ1/p).\nThen Theorem 3.1 gives\np ≲δ + Cabs(ε′, 2ε′; A, Rn) + Dupp(2σ; E)\n+ 2Dshift(2ε′, 2σ; E)ek [Clow (1/2; A, Rn) + Cupp (2; A, Rn) + 2Dupp(2σ; E)] .\nConsider Cabs(ε′, 2ε′; A, Rn). If x ∈Rn with ∥x∥≤s, then\nP(∥Ax∥> t) ≤P(∥Ax∥> (t/s)∥x∥).\nHence, in this case, we may take\nCabs(ε′, 2ε′; A, Rn) = Cupp(2; A, Rn).\n(B.2)\nNow by Lemma B.2, we have that\nClow(1/2; A, Rn) = Cupp(2; A, Rn) = exp(−c(β, κ)m),\nwhere c(β, κ) > 0 depends on β, κ only. Also, by Lemma B.1, we have\nDupp(2σ; E) =\n\u00002e−1\u0001m/2 = exp(−cm),\nfor some universal constant c > 0 and\nDshift(2ε′, 2σ; E) = exp\n\u0012m(4σ + 2ε′)\n2σ2\n2ε′\n\u0013\n≤exp\n\u00126m\nd\n\u0013\n,\nwhere we used the facts that σ ≥ε/δ1/p = dϵ′ and d ≥1. We deduce that\np ≲δ + exp(k + 6m/d −c(β, κ)m)\nfor a possibly different constant c(β, κ) > 0. We now choose d = d(β, κ) = 12/c(β, κ). Up to\nanother possible change in c(β, κ), the condition (3.4) on m and (3.1) now give that p ≲δ, as\nrequired.\nProof of Theorem 3.9. Let p = P [∥x∗−ˆx∥≥34(η + σ)]. In this case, the forwards operator A\nsatisfies ∥A∥≤\np\nn/m (Lemma B.4). Hence we may apply Theorem 1.1 with θ =\np\nn/m and\nd = 2 to obtain\np ≲δ + Covη,δ(P) [Clow(1/2; A, D) + Cupp(2; A, D) + exp(−cm)]\nfor some universal constant c > 0, where D = supp(P) −supp(P). Lemma B.3 now gives that\np ≲δ + Covη,δ(P)\n\u0014\nexp\n\u0012\n−\ncm\nµ(U; D)\n\u0013\n+ exp(−cm)\n\u0015\nfor a possibly different constant c > 0. The result now follows from the condition (3.5) on m.\n27\n"
    },
    {
      "page_number": 28,
      "text": "C\nProofs of Theorems 1.1 and 3.1\nWe finally consider the proofs of the two general results, Theorems 1.1 and 3.1. Our main effort will\nbe in establishing the latter, from which the former will follow after a short argument.\nTo prove Theorem 3.1, we first require some additional background on couplings, along with several\nlemmas. This is given in §C.1. We then establish a series of key technical lemmas, presented in §C.2,\nwhich are used in the main proof. Having shown these, the proof of Theorem 3.1 proceeds in §C.3\nvia a series of step. We now briefly describe these steps, and by doing so explain how the sets D1, D2\ndefined in (3.2)-(3.3) arise.\n(i) First, using Lemma C.7, we decompose P, R into distributions P′, R′ that are supported in\nballs of a given radius, plus remainder terms. The distributions P′, R′ are close (in W∞)\nto a discrete distribution Q supported at the centres of the balls that give the approximate\ncover satisfying (3.1).\n(ii) Next, we replace x∗∼R in the definition of the probability p in Theorem 3.1 by z∗∼P′.\nThe is done to align the prior with the posterior P(·|y, A), which is needed later in the proof.\nWe do this using Lemma C.6. Here we have to consider the action of A on vectors of the\nform x −z, where x ∈supp(R′) and z ∈supp(P′). After determining the supports of\nR′, P′, we see that x −z ∈D1, where D1 is the set defined in (3.2).\n(iii) We now decompose P′ into a mixture over the balls mentioned in (i). After a series of\narguments, we reduce the task to that of considering the probability that the conditional\ndistribution is drawn from one ball when the prior is drawn from another. Lemmas C.4 and\nC.5 handle this. They involve estimating the action of A on vectors x −z, where z is the\ncentre of one of the balls and x comes from another ball. Since the balls are supported in\nsupp(P) we have x ∈supp(P), and since the centres come from the approximate covering\nnumber bound (3.1), we have that z ∈supp(P) if P attains the minimum and z ∈supp(R)\notherwise. Hence x −z ∈D2, with D2 as in (3.3).\nC.1\nBackground on couplings\nFor a number of our results, we require some background on couplings. We first recall some notation.\nGiven probability spaces (X, F1, µ), (Y, F2, ν), we write Γ = Γµ,ν for the set of couplings, i.e.,\nprobability measures on the product space (X × Y, σ(F1 ⊗F2)) whose marginals are µ and ν,\nrespectively. For convenience, we write π1 : X × Y →X and π2 : X × Y →Y for the projections\nπ1(x, y) = x and likewise π2(x, y) = y. In particular, for any coupling γ we have π1♯γ = µ and\nπ2♯γ = ν, where ♯denotes the pushforward operation. As an immediate consequence, we observe\nthat for any measurable function φ : X →R,\nZ\nX\nφ(x) dµ(x) =\nZ\nX\nφ(x) dπ1♯γ(x) =\nZ\nX×Y\nφ(x) dγ(x, y).\n(C.1)\nGiven a cost function c : X × Y →[0, ∞), the Wasserstein-p metric is defined as\nWp(µ, ν) = inf\nγ∈Γ\n\u0012Z\nX×Y\nc(x, y)p dγ(x, y)\n\u00131/p\nfor 1 ≤p < ∞and\nW∞(µ, ν) = inf\nγ∈Γ\n\u0000esssupγc(x, y)\n\u0001\n.\nWe say that γ ∈Γ is a Wp-optimal coupling of µ and ν if\n\u0012Z\nX×Y\nc(x, y)p dγ(x, y)\n\u00131/p\n= Wp(µ, ν)\nfor 1 ≤p < ∞or\nesssupγc(x, y) = W∞(µ, ν)\nwhen p = ∞. Note that such a coupling exists whenever X, Y are Polish spaces, and when the cost\nfunction is lower semicontinuous [41]. In our case, we generally work with Euclidean spaces with\nthe cost function being the Euclidean norm, hence both conditions are satisfied.\n28\n"
    },
    {
      "page_number": 29,
      "text": "For convenience, if γ is probability measure on the product space (X ×Y, σ(F1 ⊗F2)), we will often\nwrite γ(E1, E2) instead of γ(E1 × E2) for Ei ∈Fi, i = 1, 2. Moreover, if x ∈X is a singleton, we\nwrite γ(x, E2) for γ({x} × E2) and likewise for γ(E1, y).\nWe now need several lemmas on couplings.\nLemma C.1. Suppose that (X, F1, µ), (Y, F2, ν) are Borel probability spaces, and let γ be a\ncoupling of µ, ν on the space (X × Y, σ(F1 ⊗F2)). Then supp(γ) ⊆supp(µ) × supp(ν).\nProof. Let (x, y) ∈supp(γ). Then γ(Ux,y) > 0 for every open set Ux,y ⊆X × Y that contains\n(x, y). Now, to show that (x, y) ∈supp(µ) × supp(ν), we show that x ∈supp(µ) and y ∈supp(ν).\nLet Ux ⊆X be open with x ∈Ux. By definition, µ(Ux) = γ(Ux × Rn). Since Ux × Rn is open and\ncontains (x, y), it follows that γ(Ux ×Rn) > 0. Since Ux was arbitrary, we deduce that x ∈supp(µ).\nThe argument that y ∈supp(ν) is identical.\nLemma C.2. Let X be a Polish space with a complete metric d. Let µ, ν be Borel probability\nmeasures on X. Let dH be the Hausdorff metric with respect to d and W∞be the Wasserstein-∞\nmetric with cost function d. Then\ndH(supp(µ), supp(ν)) ≤W∞(µ, ν).\nIn particular, supp(µ) ⊆Bη(supp(ν)) for any η ≥W∞(µ, ν).\nProof. Since\ndH(supp(µ), supp(ν)) = max\n(\nsup\nx∈supp(ν)\nd(x, supp(µ)),\nsup\ny∈supp(µ)\nd(y, supp(ν))\n)\nwe\nmay,\nwithout\nloss\nof\ngenerality,\nassume\nthat\nthe\nmaximum\nis\nachieved\nby\nsupx∈supp(ν) d(x, supp(µ))\n=:\nD.\nTake a sequence {xn}n∈N\n⊆\nsupp(ν) such that\nDn := d(xn, supp(µ)) →D. Since xn ∈supp(ν), for any ε > 0, we have ν(Bε(xn)) > 0. Note\nthat Bε(xn) is measurable as we assume X is Borel. For each n ∈N, define εn = 1\nnDn. We show\nthat for all x ∈Bεn(xn), y ∈supp(µ), d(x, y) > Dn(1 −1\nn). By triangle inequality, we have\nd(x, y) ≥d(xn, y) −d(xn, x) ≥Dn −Dn/n = Dn(1 −1/n).\nNotice also that Dn(1 −1\nn) ≤D and converges to D as n →∞. This implies that\nA := {(x′, y′) : d(x′, y′) > Dn(1 −1/n)} ⊇Bεn(xn) × supp(µ).\nNow consider any coupling γ ∈Γµ,ν. We have\nγ(A) ≥γ(Bεn(xn) × supp(µ)) = γ(Bεn(xn) × X) = ν(Bεn(xn)) > 0.\nTherefore ess supγ d(x, y) > Dn(1−1/n). Now since Dn(1−1\nn) →D we have ess supγ d(x, y) ≥\nD. This is holds for any coupling, therefore the result follows.\nWhen working with a coupling between a finitely-supported distribution and a continuous distribution,\nthe following lemma is often useful.\nLemma C.3. Let (X, F1, µ), (Y, F2, ν) be probability spaces, such that ν is finitely supported on a set\nS ⊆Y . Let γ be a coupling of µ, ν and E ⊆X × Y be γ-measurable. Write Ey = {x : (x, y) ∈E}\nfor the slice of E at y ∈Y . Then\nγ(E) =\nX\ns∈S,s∈π2(E)\nγ(Es × {s}).\nProof. Write\nγ(E) =\nX\ns∈S,s∈π2(E)\nγ(Es × {s}) + γ( ˆE),\nwhere ˆE = E\\ S\ns∈S,s∈π2(E)(Es × {s}). It suffices to show that γ( ˆE) = 0. Since F ⊆π−1\n2 (π2(F))\nfor any set F, we have\nγ( ˆE) ≤γ(π−1\n2 (π2( ˆE)) = ν(π2( ˆE)) = ν(π2( ˆE) ∩S).\nBut\nˆE = {(x, y) ∈E : y /∈S}\nand therefore π2( ˆE) ∩S = ∅. The result now follows.\n29\n"
    },
    {
      "page_number": 30,
      "text": "C.2\nTechnical lemmas\nC.2.1\nSeparation lemma\nThe lemma considers a scenario where two random variables are drawn for a mixture of k probability\ndistributions. The second random variable is conditioned on the draw of the first. It then considers\nthe probability that the two random variables are drawn from different distributions in the mixture,\nbounding this in terms of their Total Variation (TV) distance. It generalizes [49, Lem. 3.1].\nLemma C.4 (Separation lemma). Let H1, . . . , Hk be Borel probability measures and consider the\nmixture H = Pk\ni=1 aiHi. Let y∗∼H and ˆy ∼Pk\ni=1 P(y∗∼Hi|y∗)Hi(·|y∗) where P(y∗∼\nHi|y∗) are the posterior weights. Then\nP[y∗∼Hi, ˆy ∼Hj(·|y∗)] ≤1 −TV(Hi, Hj).\nTo clarify, in this lemma and elsewhere we use the notation y∗∼Hi (and similar) to mean the event\nthat y∗is drawn from the ith distribution Hi.\nProof. Note that if the Hi have densities hi with respect to some measure, these weights are given by\nP(y∗∼Hi|y∗) =\naihi(y∗)\nPk\nj=1 ajhj(y∗)\n.\n(C.2)\nWe now write\np := P[y∗∼Hi, ˆy ∼Hj(·|y∗)] = P[y∗∼Hi]P[ˆy ∼Hj(·|y∗)|y∗∼Hj]\n= P[y∗∼Hi]E[P(ˆy ∼Hj(·|y∗)|y∗∼Hi]\n= aiE[P(ˆy ∼Hj(·|y∗))|y∗∼Hi].\nSince E[y∗|y∗∼Hi] ∼Hi, we have\np = aiE[P(ˆy ∼Hj(·|y∗))|y∗∼Hi] = ai\nZ\nP(ˆy ∼Hj(·|y∗)) dHi(y∗).\nNow, because of the mixture property, Hi ≪H and therefore its Radon-Nikodym derivative\nhi = dHi\ndH exists. This means we may write\np = ai\nZ\nP(ˆy ∼Hj(·|y∗))hi(y∗) dH(y∗).\nBy definition, we have P(ˆy ∼Hj(·|y∗)) = P(y∗∼Hj|y∗) and using (C.2), we deduce that\np =\nZ aiajhi(y∗)hj(y∗)\nPk\nl=1 alhl(y∗)\ndH(y∗)\nWe now write\np =\nZ\naihi(y∗)hj(y∗)aj\naihi(y∗) + ajhj(y∗) + P\nl̸=i,j alhl(y∗) dH(y∗)\n≤\nZ\naihi(y∗)hj(y∗)aj\naihi(y∗) + ajhj(y∗) dH(y∗)\n=\nZ\naihi(y∗)hj(y∗)aj\n(aihi(y∗) + ajhj(y∗)) dH(y∗)\n=\nZ\naihi(y∗)hj(y∗)aj\naihi(y∗) + ajhj(y∗) dH(y∗)\n≤\nZ\naihi(y∗)hj(y∗)aj\nmax{aihi(y∗), ajhj(y∗)} dH(y∗)\n=\nZ\nmin{aihi(y∗), ajhj(y∗)} dH(y∗)\n= 1 −\nZ 1\n2(hi(y∗) + hj(y∗)) −min{aihi(y∗), ajhj(y∗)} dH(y∗)\n= 1 −\nZ 1\n2|hi(y∗) −hj(y∗)| dH(y∗)\n= 1 −TV(Hi, Hj),\n30\n"
    },
    {
      "page_number": 31,
      "text": "as required.\nC.2.2\nDisjointly-supported measures induce well-separated measurement distributions\nThe following lemma pertains to the pushforwards of measures supported in Rn via the forward\noperator A and noise e. Specifically, it states that if two distributions Pint and Pext are disjointly\nsupported then their corresponding pushforwards Hint,A and Hext,A are, on average with respect\nto A ∼A, well-separated, in the sense of their TV-distance. It is generalization of [49, Lem. 3.2]\nthat allows for arbitrary distributions A of the forward operators, as opposed to just distributions of\nGaussian random matrices.\nLemma C.5 (Disjointly-supported measures induce well-separated measurement distributions). Let\n˜x ∈Rn, σ ≥0, η ≥0, c ≥1, Pext be a distribution supported in the set\nS˜x,ext = {x ∈Rn : ∥x −˜x∥≥c(η + σ)}\nand Pint be a distribution supported in the set\nS˜x,int = {x ∈Rn : ∥x −˜x∥≤η}.\nGiven A ∈Rm×n, let Hint,A be the distribution of y = Ax∗+ e where x∗∼Pint and e ∼E\nindependently, and define Hext,A in a similar way. Then\nEA∼A[TV(Hint,A, Hext,A)] ≥1−\n\u0014\nClow\n\u0012 2\n√c; A, Dext\n\u0013\n+ Cupp\n\u0012√c\n2 ; A, Dint\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n; E\n\u0013\u0015\n,\nwhere Dext = {x −˜x : x ∈supp(Pext)}, Dint = {x −˜x : x ∈supp(Pint)} and Cupp(·; A),\nClow(·; A) and Dupp(·; E) are as in Definitions 2.3 and 2.4, respectively.\nNotice that the average TV-distance is bounded below by the concentration bounds Clow and Cupp\nfor A (Definition 2.3) and the concentration bound Dupp for E (Definition 2.4). This is unsurprising.\nThe pushforward measures are expected to be well-separated if, firstly, the action of A approximately\npreserves the lengths of vectors (which explains the appearance of Clow and Cupp) and, secondly,\nadding noise by E does not, with high probability, cause well-separated vectors to become close\nto each other (which explains the appearance of Dupp). Also as expected, as c increases, i.e., the\ndistributions Pint and Pext become further separated, the average TV-distance increases.\nProof. Given A ∈Rm×n, let\nBA = {y ∈Rm : ∥y −A˜x∥≤√c(η + σ)}.\nWe claim that\nEA[Hext,A(BA)] ≤Clow\n\u0012 2\n√c; A, Dext\n\u0013\n+ Dupp(σ√c; E),\n(C.3)\nEA[Hint,A(BA)] ≥1 −\n\u0014\nCupp\n\u0012√c\n2 ; A, Dint\n\u0013\n+ Dupp\n\u0012√c\n2 σ; E\n\u0013\u0015\n.\n(C.4)\nNotice that these claims immediately imply the result, since\nEA∼ATV(Hext,A, Hint,A) ≥EA∼A[Hint,A(BA)] −EA∼A[Hext,A(BA)].\nTherefore, the rest of the proof is devoted to showing (C.3) and (C.4). For the former, we write\nEA∼A[Hext,A(BA)] = EA∼A\n\u0014Z Z\n1BA(Ax + e) dPext(x) dE(e)\n\u0015\n= EA∼A\n\u0014Z Z\n1BA(Ax + e) dE(e) dPext(x)\n\u0015\n= EA∼A[Ex∼Pext[E(BA −Ax)]]\n= Ex∼Pext[EA∼A[E(BA −Ax)]],\n(C.5)\nwhere BA −Ax = {b−Ax : b ∈BA}. We now bound Ex∼Pext[EA∼AE(BA −Ax)]. Given x ∈Rn,\nlet Cx = {A : ∥Ax −A˜x∥< 2√c(η + σ)} ⊆Rm×n and write\nI1 = Ex∼Pext[EA∼AE(BA −Ax)1Cx],\nI2 = Ex∼Pext[EA∼AE(BA −Ax)1Cc\nx]\n31\n"
    },
    {
      "page_number": 32,
      "text": "so that\nEx∼Pext[EA∼A[E(BA −Ax)]] = I1 + I2.\n(C.6)\nWe will bound I1, I2 separately. For I1, we first write\nI1 = Ex∼Pext[EA∼A[E(BA −Ax)1Cx]]\n≤Ex∼Pext[EA∼A[1Cx]]\n= Ex∼Pext[PA∼A(∥Ax −A˜x∥< 2√c(η + σ))],\nwhere the inequality follows from the fact that E(BA −Ax) ≤1. Now since x ∼Pext, we have\nx ∈S˜x,ext and therefore ∥x −˜x∥≥c(η + σ). Hence\nEx∼Pext[PA∼A(∥Ax −A˜x∥< 2√c(η + σ)] ≤Ex∼Pext\n\u0014\nPA∼A\n\u0012\n∥Ax −A˜x∥< 2\n√c∥x −˜x∥\n\u0013\u0015\n.\nSince the outer expectation term has x ∼Pext, we have that x ∈supp(Pext) with probability one.\nUsing Definition 2.3, we deduce that\nI1 ≤Clow\n\u0012 2\n√c; A, Dext\n\u0013\n.\n(C.7)\nWe now bound I2. Let x ∈S˜x,ext and A ∈Cc\nx, i.e., ∥A(x −˜x)∥> 2√c(η + σ). We now show\nthat BA ⊆BA,x, where BA,x = {y ∈Rm : ∥y −Ax∥≥√c(η + σ)}. Suppose that y ∈BA, i.e.,\n∥y −A˜x∥≤√c(η + σ). We have\n∥y −Ax∥= ∥y −A˜x + A˜x −Ax∥\n≥∥A(˜x −x)∥−∥y −A˜x∥\n> 2√c(η + σ) −√c(η + σ)\n= √c(η + σ),\nand therefore y ∈BA,x, as required. Using this, we have\nE(BA −Ax) ≤E(BA,x −Ax),\n∀A ∈Cc\nx, x ∈S˜x,ext,\nand therefore\nI2 = Ex∼Pext[EA∼A[E(BA −Ax)1Ccx]] ≤Ex∼Pext[EA∼A[E(BA,x −Ax)1Ccx]].\nBut we notice that BA,x −Ax = Bc√c(η+σ). Now since η ≥0, we have Bc√c(η+σ) ⊆Bc\nσ√c. Hence\nE(BA,x −Ax) = E(Bc√c(η+σ)) ≤E(Bc\nσ√c) ≤Dupp(σ√c; E),\nand therefore I2 ≤Dupp(σ√c; E). Combining this with (C.5), (C.6) and (C.7), we deduce that\nEA[Hext,A(BA)] ≤Ex∼Pext[EA∼A[E(BA−Ax)]] = I1+I2 ≤Clow(2/√c; A, Dext)+Cupp(σ√c; E),\nwhich shows (C.3).\nWe will now establish (C.4). With similar reasoning to (C.5), we have\nEA∼A[Hint,A(Bc\nA)] = Ex∼Pint[EA∼A[E(Bc\nA −Ax)]]\nProceeding as before, let Dx = {A : ∥Ax −A˜x∥<\n√c\n2 (η + σ)}, I1 = Ex∼Pint[EA∼A[E(Bc\nA −\nAx)]1Dcx], and I2 = Ex∼Pint[EA∼A[E(Bc\nA −Ax)]1Dx] so that\nEA∼A[Hint,A(Bc\nA)] = Ex∼Pint[EA∼A[E(Bc\nA −Ax)]] = I1 + I2.\n(C.8)\nThe terms I1, I2 are similar to those considered in the previous case. We bound them similarly. For\nI1, we have, by dropping the inner probability terms,\nI1 ≤Ex∼Pint[EA∼A[1Dcx]] = Ex∼Pint\n\u0014\nPA∼A(∥A(x −˜x)∥≥\n√c\n2 (η + σ))\n\u0015\n.\nSince x ∈S˜x,int, we have ∥x −˜x∥≤η ≤η + σ which gives\nEx∼Pint[PA∼A(∥A(x −˜x)∥≥\n√c\n2 (η + σ))] ≤Ex∼Pint\n\u0014\nPA∼A(∥A(x −˜x)∥≥\n√c\n2 ∥x −˜x∥)\n\u0015\n32\n"
    },
    {
      "page_number": 33,
      "text": "and therefore\nI1 ≤Cupp\n\u0012√c\n2 ; A, Dint\n\u0013\n.\n(C.9)\nWe now bound I2. Let x ∈S˜x,int and suppose that A ∈Dx, i.e., ∥x −˜x∥≤η and ∥A(x −˜x)∥<\n√c\n2 (η + σ). Define ˆBA,x = {y ∈Rm : ∥y −Ax∥<\n√c\n2 (η + σ)}. We will show ˆBA,x ⊆BA in this\ncase. Let y ∈BA,x. Then\n∥y −A˜x∥≤∥y −Ax∥+ ∥Ax −A˜x∥<\n√c\n2 (η + σ) +\n√c\n2 (η + σ) = √c(η + σ),\nas required. This implies that Bc\nA ⊆ˆBc\nA,x. Hence\nE(Bc\nA −Ax) ≤E( ˆBc\nA,x −Ax) = E(Bc√c\n2 (η+σ)) ≤E(Bc√c\n2 σ)\nwhich implies that I2 ≤Dupp(\n√c\n2 σ; E). Combining with (C.8) and (C.9) we get\nEA[Hint,A(Bc\nA)] ≤Ex∼Pint[EA∼A[E((BA −Ax)c)]] ≤Cupp\n\u0012√c\n2 ; A, Dint\n\u0013\n+ Dupp\n\u0012√c\n2 σ; E\n\u0013\n,\nwhich implies (C.4). This completes the proof.\nC.2.3\nReplacing the real distribution with the approximate distribution\nWe next establish a result that allows one to upper bound the failure probability based on draws from\nthe real distribution R with the failure probability based on draws from the approximate distribution\nP. This lemma is a key technical step that aligns the prior distribution with the posterior. The specific\nbound is given in terms of the Wasserstein distance between R and P and several of the concentration\nbounds defined in §2. This is a significant generalization of [49, Lem. 3.3] that allows for arbitrary\ndistributions A, E for the forwards operator and noise.\nLemma C.6 (Replacing the real distribution with the approximate distribution). Let ε, σ, d, t ≥0,\nc ≥1, E be a distribution on Rm and R, P be distributions on Rn such that W∞(R, P) ≤ε. Let Π\nbe an W∞-optimal coupling of R and P and define the set D = {x∗−z∗: (x∗, z∗) ∈supp(Π)}.\nLet\np = Px∗∼R,A∼A,e∼E,ˆx∼P(·|Ax∗+e,A)[∥x∗−ˆx∥≥d + ε]\nand\nq = Pz∗∼P,A∼A,e∼E,ˆz∼P(·|Az∗+e,A)[∥z∗−ˆz∥≥d].\nThen\np ≤Cabs(ε, tε; A, D) + Dupp(cσ; E) + Dshift(tε, cσ; E)q,\nwhere Cabs(ε, tε; A, D), Dupp(cσ; E) and Dshift(tε, cσ; E) are as in Definitions 2.3, 2.4 and 2.5,\nrespectively.\nAs expected, this lemma involves a trade-off. The constant Cabs(ε, tε; A, D) is made smaller (for fixed\nε) by making the constant t larger. However, this increases Dshift(tε, cσ; E), which is compensated\nby making c smaller. However, this in turn increases the constant Dupp(cσ; E).\nProof. Define the events\nB1,ˆx = {x∗: ∥x∗−ˆx∥≥d + ε},\nB2,ˆz = {z∗: ∥z∗−ˆz∥≥d}\nso that\np = Px∗∼R,A∼A,e∼E,ˆx∼P(·|Ax∗+e,A)[x∗∈B1,ˆx]\nq = Pz∗∼P,A∼A,e∼E,ˆz∼P(·|Az∗+e,A) [z∗∈B2,ˆz].\n(C.10)\nObserve that\np = Ex∗∼R[EA∼AEy|A,x∗[Eˆx∼P(·|Ax∗+e,A)[1B1,ˆx]]]\n=\nZ Z Z Z\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx)dE(e)dA(A)dR(x∗)\n33\n"
    },
    {
      "page_number": 34,
      "text": "and similarly\nq =\nZ Z Z Z\n1B2,ˆz(z∗)dP(·|Az∗+ e, A)(ˆz)dE(e)dA(A)dP(z∗).\nTherefore, to obtain the result, it suffices to replace samples from the real distribution R with samples\nfrom the approximate distribution P and to replace the indicator function of B1,ˆx by the indicator\nfunction over B2,ˆz. For the first task, we use couplings. Since W∞(R, P) ≤ε, there exists a coupling\nΠ between R, P with Π(∥x∗−z∗∥≤ε) = 1. By (C.1), we can write\np =\nZ Z Z Z\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx)dE(e)dA(A)dΠ(x∗, z∗).\nDefine E = {(x∗, z∗) : ∥x∗−z∗∥≤ε} and observe that Π(E) = 1. Then, for fixed A, e, we have\nZ Z\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx)dΠ(x∗, z∗) =\nZ\nE\nZ\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx)dΠ(x∗, z∗)\nZ Z\n1B2,ˆx(z∗)dP(·|Ax∗+ e, A)(ˆx)dΠ(x∗, z∗) =\nZ\nE\nZ\n1B2,ˆx(z∗)dP(·|Ax∗+ e, A)(ˆx)dΠ(x∗, z∗).\nWe now show 1B1,ˆx(x∗) ≤1B2,ˆx(z∗) for (x∗, z∗) ∈E. Let (x∗, z∗) ∈E and suppose that\nx∗∈B1,ˆx. Then ∥x∗−ˆx∥≥d + ε and, since ∥x∗−z∗∥≤ε, we also have that ∥z∗−ˆx∥≥d and\ntherefore z∗∈B2,ˆx, as required. Hence\nZ\n1B1,ˆx(x∗)dP(·|Ax∗+ e, A)(ˆx) ≤\nZ\n1B2,ˆx(z∗)dP(·|Ax∗+ e, A)(ˆx)\nfor (x∗, z∗) ∈E. Now, since indicator functions are non-negative, Fubini’s theorem immediately\nimplies that\np =\nZ Z Z Z\n1B1,ˆx(x∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\n≤\nZ Z Z Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗).\nHaving introduced the coupling Π and replaced 1B1,ˆx by 1B2,ˆx, to establish the result it remains to\nreplace the conditional distribution P(·|Ax∗+ e, A) by P(·|Az∗+ e, A). With a similar technique\nto that used in the proof of Lemma C.5, we define Cx∗,z∗= {A : ∥A(x∗−z∗)∥> tε} and\nI1 =\nZ Z\n1Cx∗,z∗(A)\nZ Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\nI2 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\nso that\np ≤\nZ Z Z Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗) = I1 + I2.\n(C.11)\nWe first bound I1. As before, we write\nI1 =\nZ Z\n1Cx∗,z∗(A)\nZ Z\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\n≤\nZ Z\n1Cx∗,z∗(A) dA(A) dΠ(x∗, z∗).\nRecalling the definition of the set E above, we get\nZ Z\n1Cx∗,z∗(A) dA(A) dΠ(x∗, z∗) ≤\nZ\nE\nPA∼A{∥A(x∗−z∗)∥> tε} dΠ(x∗, z∗).\nUsing the definition of C0, E and D, we deduce that\nI1 ≤Cabs(ε, tε; A, D).\n(C.12)\n34\n"
    },
    {
      "page_number": 35,
      "text": "Now we bound I2. We further split the integral I2 as follows:\nI2 = I21 + I22,\n(C.13)\nwhere\nI21 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bc\ncσ(e)\nZ\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\nI22 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bcσ(e)\nZ\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\nLet us first find an upper bound for I21. We have\nI21 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bccσ(e)\nZ\n1B2,ˆx(z∗) dP(·|Ax∗+ e, A)(ˆx) dE(e) dA(A) dΠ(x∗, z∗)\n≤\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bccσ(e) dE(e) dA(A) dΠ(x∗, z∗)\n≤\nZ\n1Bccσ(e) dE(e),\nand therefore, by Definition 2.4,\nI21 = E(Bc\ncσ) ≤Dupp(cσ; E).\n(C.14)\nWe now find a bound for I22. We first use Definition 2.5 to write\nI22 =\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1Bcσ(e)pE(e)\nZ\n1B2,ˆx(z∗)dP(·|Ax∗+ e, A)(ˆx) de dA(A) dΠ(x∗, z∗).\nNow define the new variable e′ = e + A(x∗−z∗). Since, in the integrand, ∥e∥≤cσ (due to\nthe indicator function 1Bcσ(e)) and ∥A(x∗−z∗)∥≤tε (due to the indicator function 1Cc\nx∗,z∗(A)),\nDefinition 2.5 yields the bound\nI22 ≤Dshift(tε, cσ; E)\nZ Z\n1Cc\nx∗,z∗(A)\nZ\n1B2σ(e′ −A(x∗−z∗))pE(e′)\n×\nZ\n1B2,ˆx(z∗) dP(·|Az∗+ e′, A)(ˆx) de′ dA(A) dΠ(x∗, z∗).\nWe now drop the first two indicator functions and relabel the variables e′ and ˆx as e and ˆz, respectively,\nto obtain\nI22 ≤Dshift(tε, cσ; E)\nZ Z Z Z\n1B2,ˆz(z∗) dP(·|Az∗+ e, A)(ˆz) dE(e) dA(A) dΠ(x∗, z∗).\nThis gives\nI22 ≤Dshift(tε, cσ; E)q,\nwhere q is as in (C.10). Combining this with (C.13), we deduce that\nI2 ≤Dupp(cσ, ε) + Dshift(tε, cσ; E)q.\nTo complete the proof, we combine this with (C.11) and (C.12), and then recall (C.10) once more.\nC.2.4\nDecomposing distributions\nThe following lemma is in large part similar to [49, Lem. A.1]. However, we streamline and rewrite\nits proof for clarity and completeness, fix a number of small issues and make an addition to the\nstatement (see item (v) below) that is important for proving our main result.\nLemma C.7 (Decomposing distributions). Let R, P be arbitrary distributions on Rn, p ≥1 and\nη, ρ, δ > 0. If Wp(R, P) ≤ρ and k ∈N is such that\nmin{log Covη,δ(P), log Covη,δ(R)} ≤k,\n(C.15)\nthen there exist distributions R′, R′′, P′, P′′, a constant 0 < δ′ ≤δ and a discrete distribution Q\nwith supp(Q) = S satisfying\n35\n"
    },
    {
      "page_number": 36,
      "text": "(i) min{W∞(P′, Q), W∞(R′, Q)} ≤η,\n(ii) W∞(R′, P′) ≤\nρ\nδ1/p ,\n(iii) P = (1 −2δ′)P′ + (2δ′)P′′ and R = (1 −2δ′)R′ + (2δ′)R′′,\n(iv) |S| ≤ek,\n(v) and S ⊆supp(P) if P attains the minimum in (C.15) with S ⊆supp(R) otherwise.\nThis lemma states that two distributions that are close in Wasserstein p-distance, and for which at\nleast one has small approximate covering number (C.15), can be decomposed into mixtures (iii) of\ndistributions, where the following holds. One of the distributions, say P′, is close (i) in Wasserstein-∞\ndistance to a discrete distribution Q with the cardinality of its support (iv) bounded by the approximate\ncovering number. The other R′ is close in Wasserstein-∞distance to P′. Moreover, both mixtures\n(iii) are dominated by these distributions: the ‘remainder’ terms P′′ and R′′ are associated with a\nsmall constant δ′ ≤δ, meaning they are sampled with probability ≤δ when drawing from either P\nor R. Note that if p < ∞then the Wasserstein-∞distance between R′ and P′ may get larger as\nδ shrinks, i.e., as the remainder gets smaller. However, this does not occur when p = ∞, as (ii) is\nindependent of δ in this case.\nProof. Without loss of generality, we assume that log Covη,δ(P) ≤k. Then Covη,δ(P) ≤ek and\nhence there is a set S = {ui}l\ni=1 ⊆supp(P) with l ≤ek, where the ui are the centres of the balls\nused to cover at least 1 −δ of the measure of P. That is,\nP\n\" l[\ni=1\nB(ui, η)\n#\n= Px∼P\n\"\nx ∈\nl[\ni=1\nB(ui, η)\n#\n=: 1 −c∗≥1 −δ.\nWe now define f : Rn →R so that f(x) = 0 if x lies outside these balls, and otherwise, f(x) is the\nequal to the reciprocal of the number of balls in which x is contained. Namely,\nf(x) =\n(\n1\nPl\ni=1 1B(ui,η)(x)\nif x ∈Sl\ni=1 B(ui, η)\n0\notherwise\n.\nWe divide the remainder of the proof into a series of steps.\n1. Construction of Q′. We will now define a finite measure Q′. The point of Q′ is to, concentrate the\nmass of the measure P into the centres of the balls ui. If the sets B(ui, η) are disjoint, then this is\nstraightforward. However, to ensure that Q′ is indeed a probability measure, we need to normalize\nand account for any non-trivial intersections. This is done via the function f. Pick some arbitrary\nˆu /∈{u1, . . . , ul} and define\nQ′ =\nl\nX\ni=1\n Z\nB(ui,η)\nf(x) dP(x)\n!\nδui + c∗δˆu.\nObserve that\nZ\ndQ′(x) =\nl\nX\ni=1\nZ\nB(ui,η)\nf(x) dP(x) + c∗= P\n l[\ni=1\nB(ui, η)\n!\n+ c∗= (1 −c∗) + c∗= 1,\nand therefore Q′ is a probability distribution supported on the finite set S ∪{ˆu}.\n2. Coupling Q′, P. Now that we have associated all the mass of P with the points ui, we can define a\ncoupling Π between Q′ and P that associates the mass of P and ui with a single measure. Moreover,\nthis measure will keep points within η distance of each other with high probability. We define Π as\nfollows for measurable sets E, F ⊆Rn:\nΠ(E, F) =\nl\nX\ni=1\n1F (ui)\nZ\nB(ui,η)∩E\nf(x) dP(x) + 1F (ˆu)P\n \nE\\\nl[\ni=1\nB(ui, η)\n!\n.\nTo see that this is a coupling, we first observe that\nΠ(Rn, F) =\nl\nX\ni=1\n1F (ui)\nZ\nB(ui,η)\nf(x) dP(x) + 1F (ˆu)(1 −c∗) ≡Q′(F),\n36\n"
    },
    {
      "page_number": 37,
      "text": "which gives the result for the first marginal. For the other, we have\nΠ(E, Rn) =\nl\nX\ni=1\nZ\nB(ui,η)∩E\nf(x) dP(x) + P\n \nE\\\nl[\ni=1\nB(ui, η)\n!\n.\nBy definition of f, this is precisely\nΠ(E, Rn) = P\n \nE ∩\nl[\ni=1\nB(ui, η)\n!\n+ P\n \nE\\\nl[\ni=1\nB(ui, η)\n!\n≡P(E),\nwhich gives the result for the second marginal. Note that Π was only defined for product sets,\nbut, since Q′ is finitely supported, it follows directly from Lemma C.3 that it extends to arbitrary\nmeasurable sets in the product sigma-algebra. We now show that Π[∥x1 −x2∥> η] ≤c∗≤δ. That\nis, we show that most points drawn from Π are within η distance of each other. By law of total\nprobability we have\nΠ(∥x1 −x2∥> η) =\nl\nX\ni=1\nΠ(∥x1 −x2∥> η|x2 = ui)Π(x2 = ui)\n+ Π(∥x1 −x2∥> η|x2 = ˆu)Π(x2 = ˆu)\n=\nl\nX\ni=1\nΠ(Ui, ui)Q′(ui) + Π( ˆU, ˆu)Q′(ˆu),\nwhere Ui = {x : ∥x −ui∥> η} and ˆU = {x : ∥x −ˆu∥> η}. Notice that Ui ∩B(ui, η) = ∅and\ntherefore\nΠ(Ui, ui) =\nZ\n{x:∥x−ui∥>η}∩B(ui,η)\nf dP = 0.\nHence\nl\nX\ni=1\nΠ(Ui, ui)Q′(ui) + Π( ˆU, ˆu)Q′(ˆu) = Π( ˆU, ˆu)Q′(ˆu)\nand, since Q′(ˆu) = c∗, we have Π( ˆU, ˆu)Q′(ˆu) ≤c∗≤δ. This gives\nΠ(∥x1 −x2∥> η) ≤δ,\n(C.16)\nas required.\n3. Coupling P, R. The next step is to introduce R. With the assumption that Wp(R, P) ≤ρ, by\ndefinition there exists a coupling Γ between P and R such that EΓ[∥x1 −x2∥p] ≤ρp. Markov’s\ninequality then gives that\nΓ\n\u0010\n∥x1 −x2∥≥\nρ\nδ1/p\n\u0011\n≤EΓ[∥x1 −x2∥p]\nρp\nδ\n≤δ.\n(C.17)\n4. Coupling P, Q′, R. We next couple P, Q′ and R. Before doing so, we first discuss the goal of our\nfinal coupling. Recall that we have the distribution P, the distribution Π that couples P, Q′ closely\nexcept for up to δ mass of P, and Γ which keeps P, R close again except for up to δ of the mass of\nΓ. We want to decompose P into the portions that are η close to Q′, and points that are not. These\nwill become P′ and P′′, respectively. At the same time, we want to decompose R to points that are\nρ\nδ1/p close to P′, and points that are not. Naturally this will become R′ and R′′. To achieve this, we\ncouple P, Q′ and R in this step and then use this to construct the final decomposition in the next step.\nWe have measures P, Q′ and R and couplings Π of P, Q′ and Γ of P, R. We will in a sense, couple\nΠ, Γ. Since (Rn)3 is a Polish space, by [10, Lem. 8.4], there exists a coupling Ωwith\nπ1,2♯Ω= Π,\nπ1,3♯Ω= Γ,\nwhere π1,2(x1, x2, x3) = (x1, x2) and likewise for π1,3. One should intuitively think of the x1\ncomponent as samples from P, the x2 component as samples from Q, and the x3 component as\nsamples from R. With the base measure defined, we still want to ensure that x1, x3 are sampled\n37\n"
    },
    {
      "page_number": 38,
      "text": "closely, and x1, x2 are as well. Consider the event such that x1, x3 are\nρ\nδ1/p close and x1, x2 are η\nclose: namely,\nE := {(x1, x2, x3) : ∥x1 −x3∥≤ρ/δ1/p and ∥x1 −x2∥≤η}.\nSplit up the negation of the two events of ∥x1 −x3∥≤ρ/δ1/p and ∥x1 −x2∥≤η into the events\nE1 := {(x1, x2, x3) : ∥x1 −x2∥> η, z ∈Rn},\nE2 := {(x1, x2, x3) : ∥x1 −x3∥> ρ/δ1/p, y ∈Rn},\nso that Ec = E1 ∪E2. We will now show Ω(E1) ≤δ. Write E1 = E′\n1 ×Rn where E′\n1 = {(x1, x2) :\n∥x1 −x2∥> η} satisfies Π(E′\n1) ≤δ′ by (C.16). Then\nδ′ ≥Π(E′\n1) =\nZ\n1E′\n1(x1, x2) dπ1,2♯Ω(x1, x2)\n=\nZ\n1E′\n1(p1,2(x1, x2, x3)) dΩ(x1, x2, x3)\n=\nZ\n1E1(x1, x2, x3) dΩ(x1, x2, x3)\n= Ω(E1),\nas required. Using (C.17), we also have the analogous result for E2. Hence Ω(Ec) = Ω(E1 ∪E2) ≤\nΩ(E1) + Ω(E2) =: 2δ′ ≤2δ′, and consequently,\nΩ(E) = 1 −2δ′ ≥1 −2δ,\nwhere E := {(x1, x2, x3) : ∥x1 −x3∥≤ρ/δ1/p and ∥x1 −x2∥≤η}.\n(C.18)\n4. Decomposing P, R. Finally, we define P′, P′′, R′, R′′ and Q by conditioning on the events E\nand Ec, as follows:\nP′(A) = Ω(A, Rn, Rn|E),\nR′(A) = Ω(Rn, Rn, A|E),\nP′′(A) = Ω(A, Rn, Rn|Ec),\nR′′(A) = Ω(Rn, Rn, A|Ec),\nQ(A) = Ω(Rn, A, Rn|E).\nThis gives\nP(A) = Ω(A, Rn, Rn) = Ω(E)Ω(A, Rn, Rn|E) + Ω(Ec)Ω(A, Rn, Rn|Ec)\n= (1 −2δ′)P′(A) + 2δ′P′′(A)\nand similarly\nR(A) = Ω(Rn, A, Rn) = Ω(E)Ω(Rn, A, Rn|E) + Ω(Ec)Ω(Rn, A, Rn|Ec)\n= (1 −2δ′)R′(A) + 2δ′R′′(A).\nWe now claim that these distributions satisfy (i)-(v) in the statement of the lemma. We have already\nshown that (iii) holds. To show (i), we define a coupling γ of P′, Q by γ(B) = Ω(B, Rn|E)\nfor any B ⊆(Rn)2. Observe that γ(A, Rn) = Ω(A, Rn, Rn|E) = P′(A) and γ(Rn, A) =\nΩ(Rn, A, Rn|E) = Q(A). Hence this is indeed a coupling of P′, Q. Therefore it suffices to\nshow that γ(B) = 0, where B is the event {∥x1 −x2∥> η}. We have γ(B) = Ω(B, Rn|E).\nRecall that for (x1, x2, x3) ∈E, we have ∥x1 −x2∥≤η. Hence Ω(B, Rn|E) = 0. Therefore,\nW∞(P′, Q) ≤η, which gives (i).\nSimilarly, for (ii) we define a coupling γ′ of R′, P′ by γ′(B) = Ω(B|E) where B := {(x1, x2, x3) :\n(x1x3) ∈B, x2 ∈Rn}. With similar reasoning as the previous case, γ′ is a coupling of R′, P′ and,\nfor (x1, x2, x3) ∈E we have ∥x1 −x3∥≤ρ/δ1/p, so letting B be the event {∥x1 −x3∥> ρ/δ1/p},\nwe conclude that W∞(R′, P′) ≤ρ/δ1/p. This gives (ii).\nFinally we verify (iv) and (v). First recall that both properties hold for Q′ by construction. The results\nnow follow from the fact that Q′(·) = Ω(Rn, ·, Rn) and Q(·) = Ω(Rn, ·, Rn|E).\n38\n"
    },
    {
      "page_number": 39,
      "text": "C.3\nProof of Theorem 3.1\nWe now prove Theorem 3.1. This follows a similar approach to that of [49, Thm. 3.4], but with a\nseries of significant modifications to account for the substantially more general setup considered in\nthis work. We also streamline the proof and clarify a number of key steps.\nProof of Theorem 3.1. By Lemma C.7, we can decompose P, R into measures P′, P′′ and R′, R′′,\nand construct a finite distribution Q supported on a finite set S such that\n(i) min{W∞(P′, Q), W∞(R′, Q)} ≤η,\n(ii) W∞(R′, P′) ≤ε′ :=\nε\nδ1/p ,\n(iii) P = (1 −2δ′)P′ + (2δ′)P′′ and R = (1 −2δ′)R′ + (2δ′)R′′ for some 0 ≤δ′ ≤δ,\n(iv) |S| ≤ek,\n(v) and S ⊆supp(P) if P attains the minimum in (C.15) with S ⊆supp(R) otherwise.\nIt is helpful to briefly recall the construction of these sets. Beginning with δ, η as parameters for\nthe approximate covering numbers, the distribution Q concentrates 1 −δ of the mass of P into the\ncentres of the η-radius balls used. Then the distributions P′, R′ are the measures P, R within the\nballs. We now write\np := Px∗∼R,A∼A,e∼E,ˆx∼P(·|y,A)[∥x∗−ˆx∥≥(c + 2)η + (c + 2)σ]\n≤Px∗∼R,A∼A,e∼E,ˆx∼P(·|y,A)[∥x∗−ˆx∥≥(c + 2)η + (c + 1)σ + ε′]\n≤2δ′ + (1 −2δ′)Px∗∼R′,A∼A,e∼E,ˆx∼P(·|y,A)[∥x∗−ˆx∥≥(c + 1)(η + σ) + ε′]\n=: 2δ′ + (1 −2δ′)q.\n(C.19)\nHere, in the first inequality we used the fact that σ ≥ε′, and in the second, we used the decomposition\nR = (1 −2δ′)R′ + 2δ′R′′ and the fact that δ′ ≤δ. We now bound q by using Lemma C.6 to replace\nthe distribution R′ by the distribution P′. Writing u = Az∗+ e, this lemma and (ii) give that\nq ≤Cabs(ε′, tε′; A, D) + Dupp(c′σ; E) + Dshift(tε′, c′σ; E)r,\nwhere r = Pz∗∼P′,A∼A,e∼E,ˆz∼P(·|u,A)[∥z∗−ˆz∥≥(c + 1)(η + σ)]\n(C.20)\nand D = {x∗−z∗: (x∗, z∗) ∈supp(Π)}, for Π being the W∞-optimal coupling of R′, P′\nguaranteed by (ii). Lemma C.1 implies that supp(Π) ⊆supp(R′) × supp(P′) and therefore\nD ⊆supp(R′) −supp(P′).\nNow (iii) implies that supp(P′) ⊆supp(P). Similarly, (iii) implies that supp(R′) ⊆supp(R). But\nLemma C.2 and (ii) imply that supp(R′) ⊆Bε′(supp(P′)). Therefore\nD ⊆Bε′(supp(P)) ∩supp(R) −supp(P) = D1,\nwhere D1 as in (3.2).\nWe now bound r. Observe first that\nW∞(P′, Q) ≤η′ := η + ε′.\nIndeed, from (i) either W∞(P′, Q) ≤η or W∞(R′, Q) ≤η. In the former case, the inequality\ntrivially holds. In the latter case, we can use the triangle inequality and (ii) to obtain the desired\nbound. This implies that there is a coupling Γ of P′, Q with esssupΓ∥x −y∥≤η′. Fix ˜z ∈S and,\nfor any Borel set E ⊆Rn, define\nΓ˜z(E) = Γ(E, ˜z)\nQ(˜z) .\nThen it is readily checked that Γ˜z(·) defines a probability measure. Note also that Γ˜z is supported on\na ball of radius η′ around ˜z, since esssupΓ(∥x −y∥) ≤η′. Recall that Γ is a coupling between P′\nand Q. Let E ⊆Rn be a Borel set. Then Lemma C.3 gives that\nP′(E) = Γ(E, Rn) =\nX\n˜z∈S\nΓ((E, Rn)˜z, ˜z) =\nX\n˜z∈S\nΓ(E, ˜z) =\nX\n˜z∈S\nΓ˜z(E)Q(˜z).\n39\n"
    },
    {
      "page_number": 40,
      "text": "Therefore, we can express P′ as the mixture\nP′(·) =\nX\n˜z∈S\nΓ˜z(·)Q(˜z).\nDefine the event E = {∥z∗−ˆz∥≥(c + 1)(η + σ)} ⊆Rn × Rn so that the probability r defined in\n(C.20) can be expressed as\nr = Ez∗∼P′,A∼A,e∼E,ˆz∼P(·|A,u)[1E].\nUsing the above expression for P′ we now write\nr =\nZ Z Z Z\n1E(z∗, ˆz) dP(·|A, u)(ˆz) dE(e) dA(A) dP′(z∗)\n=\nZ Z Z Z\n1E(z∗, ˆz) dP(·|A, u)(ˆz) dE(e) dA(A) d\n X\n˜z∈S\nQ(˜z)Γ˜z(·)\n!\n(z∗)\n=\nX\n˜z∈S\nQ(˜z)\n\u0012Z Z Z Z\n1E(z∗, ˆz) dP(·|A, u)(ˆz) dE(e) dA(A) dΓ˜z(z∗)\n\u0013\n,\nwhere the last line holds as Q(˜z) is a constant. Hence\nr =\nX\n˜z∈S\nQ(˜z)Pz∗∼Γ˜z,A∼A,e∼E,ˆz∼P(·|A,u)[E].\n(C.21)\nNow we bound each term in this sum. We do this by decomposing P into a mixture of three\nprobability measures depending on ˜z ∈S. To do this, let θ = c(η + σ) and observe that, for any\nBorel set E ⊆Rn,\nP(E) = P(E ∩Bθ(˜z)) + P(E ∩Bc\nθ(˜z))\n= P(E ∩Bθ(˜z)) + P(E ∩Bc\nθ(˜z)) + (1 −2δ′)Q(˜z)Γ˜z(E) −(1 −2δ′)Q(˜z)Γ˜z(E)\n= P(E ∩Bθ(˜z)) −(1 −2δ′)Q(˜z)Γ˜z(E ∩Bθ(˜z))\n+ P(E ∩Bc\nθ(˜z)) −(1 −2δ′)Q(˜z)Γ˜z(E ∩Bc\nθ(˜z))\n+ (1 −2δ′)Q(˜z)Γ˜z(E).\nNow define the constants\nc˜z,mid = P(Bθ(˜z)) −(1 −2δ′)Q(˜z)Γ˜z(Bθ(˜z)),\nc˜z,ext = P(Bc\nθ(˜z)) −(1 −2δ′)Q(˜z)Γ˜z(Bc\nθ(˜z)).\nand let\nP˜z,int(E) = Γ˜z(E)\nP˜z,mid(E) =\n1\nc˜z,mid\n(P(E ∩Bθ(z∗)) −(1 −2δ′)Q(z∗)Γ˜z(E ∩Bθ(z∗))) ,\nP˜z,ext(E) =\n1\nc˜z,ext\n(P(E ∩Bc\nθ(z∗)) −(1 −2δ′)Q(z∗)Γ˜z(E ∩Bc\nθ(z∗))).\nThen P can be expressed as the mixture\nP = (1 −2δ′)Q(˜z)P˜z,int + c˜z,midP˜z,mid + c˜z,extP˜z,ext.\n(C.22)\nTo ensure this is a well-defined mixture, we need to show that P˜z,mid and P˜z,ext are probability\nmeasures. However, by (iii) we have, for any Borel set E ⊆Rn,\nP(E) ≥(1 −2δ′)P′(E) = (1 −2δ′)\nX\n˜z∈S\nΓ˜z(E)Q(˜z) ≥(1 −2δ′)Γ˜z(E)Q(˜z).\nTherefore, P˜z,mid and P˜z,ext are well-defined, provided the constants c˜z,int, c˜z,ext > 0. However, if\none of these constants is zero, then we can simply exclude this term from the mixture (C.22). For the\nrest of the theorem, we will assume that, at least, c˜z,ext > 0.\nIt is now useful to note that\nsupp(P˜z,mid) ⊆Bθ(˜z)\nand\nsupp(P˜z,ext) ⊆Bc\nθ(˜z),\n40\n"
    },
    {
      "page_number": 41,
      "text": "which follows immediately from their definitions, and also that\nsupp(P˜z,int) ⊆Bη′(˜z) ⊆Bθ(˜z).\nwhere in the second inclusion we used the fact that η′ = η + ε/δ1/p ≤η + σ ≤c(η + σ) = θ, as\nσ ≥ε/δ1/p and c ≥1.\nWe now return to the sum (C.21). Consider an arbitrary term. First, observe that, for z∗∼P, we\nhave P(z∗∼Γ˜z) = Q(˜z)(1 −2δ′) by (C.22). Hence\nQ(˜z)Ez∗∼Γ˜z,ˆz∼P(·|A,u)[1E] = P(z∗∼Γ˜z)\n1 −2δ′\nEz∗∼P,ˆz∼P(·|A,u)[1E|z∗∼Γ˜z]\n= P(z∗∼Γ˜z)\n(1 −2δ′)\n1\nP(z∗∼Γ˜z)\nZ Z\n1E1z∗∼Γ˜z dP(z∗) dP(·|A, u)(ˆz).\nRecall that z∗∼Γ˜z is supported in Bη′(˜z). Therefore, for the event E to occur, i.e., ∥z∗−ˆz∥>\n(c + 1)(η + σ), it must be that ˆz ∈Bc\nθ(˜z), which means that ˆz ∼P˜z,ext(·|A, u). Hence\nQ(˜z)Ez∗∼Γ˜z,ˆz∼P(·|A,u)[1E] ≤\n1\n1 −2δ′\nZ Z\n1ˆz∼P˜z,ext(·|A,u)1z∗∼Γ˜z dP(z∗) dP(·|A, u)(ˆz)\n=\n1\n1 −2δ′ P[z∗∼P˜z,int, ˆz ∼P˜z,ext(·|A, u)].\nNow fix A ∈Rm×n. Let H˜z,int,A be the distribution of y∗= Az∗+ e for z∗∼P˜z,int and e ∼E\nindependently, and define H˜z,ext,A similarly. Then, by Fubini’s theorem, we have\nQ(˜z)Ez∗∼Γ˜z,A∼A,e∼E,ˆz∼P(·|A,u)[1E] ≤\n1\n1 −2δ′ EA∼A,e∼EP[y∗∼H˜z,int,A, ˆy ∼H˜z,ext,A(·|y∗)].\nNow let H˜z,A be the distribution of y = Az + e for z ∼P and e ∼E independently. Then Lemma\nC.4 (with H = H˜z,A, H1 = H˜z,int,A, H2 = H˜z,mid,A, H3 = H˜z,ext,A and a1 = (1 −2δ′)Q(˜z),\na2 = c˜z,mid, a3 = c˜z,ext) gives\nQ(˜z)Ez∗∼Γ˜z,A∼A,e∼E,ˆz∼P(·|A,u)[1E] ≤\n1\n1 −2δ′ EA∼A [1 −TV(H˜z,int,A, H˜z,ext,A)] .\nFinally, summing over all ˜z we deduce that\nr =\nX\n˜z∈S\nQ(˜z)Ez∗∼Γ˜z,A∼A,e∼E,ˆz∼P(·|A,u)[1E] ≤\n1\n1 −2δ′\nX\n˜z∈S\nEA∼A[1 −TV(H˜z,int,A, H˜z,ext,A)].\n(C.23)\nNow recall that H˜z,int,A is the pushforward of a measure P˜z,int supported in Bη′(˜z), where η′ =\nη + ε′ ≤η + σ and H˜z,ext,A is the pushforward of a measure P˜z,ext supported in Bc\nθ(˜z), where\nθ = c(η + σ) ≥c\n2(η′ + σ). Therefore, Lemma C.5 (with c replaced by c/2) gives that\nEA∼A[1 −TV(H˜z,int,A, H˜z,ext,A)] ≤Clow\n \n2\n√\n2\n√c ; A, D˜z,ext\n!\n+ Cupp\n\u0012 √c\n2\n√\n2; A, D˜z,int\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n√\n2; E\n\u0013\n,\nwhere D˜z,ext = {x −˜z : x ∈supp(P˜z,ext)} and D˜z,int = {x −˜z : x ∈supp(P˜z,int)}. It follows\nimmediately from (C.22) that\nsupp(P˜z,int), supp(P˜z,ext) ⊆supp(P).\nMoreover, ˜z ∈S and therefore\nD˜z,ext, D˜z,int ⊆D2,\nwhere D2 is as in (3.3). Using this, the previous bound and (C.23), we deduce that\nr ≤\n|S|\n1 −2δ′\n\"\nClow\n \n2\n√\n2\n√c ; A, D2\n!\n+ Cupp\n\u0012 √c\n2\n√\n2; A, D2\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n√\n2; E\n\u0013#\n.\nTo complete the proof, now substitute this into (C.19) and (C.20), to obtain\np ≤2δ′ + [Cabs(ε′, tε′; A, D1) + Dupp(c′σ; E)]\n+ 2Dshift(tε′, c′σ; E)|S|\n\"\nClow\n \n2\n√\n2\n√c ; A, D2\n!\n+ Cupp\n\u0012 √c\n2\n√\n2; A, D2\n\u0013\n+ 2Dupp\n\u0012√cσ\n2\n√\n2; E\n\u0013#\n.\nThe result now follows after recalling (iv), i.e., |S| ≤ek, and the fact that δ′ ≤δ ≤1/4.\n41\n"
    },
    {
      "page_number": 42,
      "text": "C.4\nProof of Theorem 1.1\nFinally, we now show how Theorem 3.1 implies the simplified result, Theorem 1.1.\nProof of Theorem 1.1. Let p = P\n\u0002\n∥x∗−ˆx∥≥(8d2 + 2)(η + σ)\n\u0003\n. We use Theorem 3.1 with ε\nreplaced by ε/(2mθ). Let c = 8d2, c′ = 2 , t = θ and ε′ = ε/(2δ1/pmθ). Then Theorem 3.1 gives\nthat\np ≲δ + Cabs(ε′, θε′; A, D1) + Dupp(2σ; E)\n+ 2Dshift(θε′, 2σ; E)ek [Clow (1/d; A, D2) + Cupp (d; A, D2) + 2Dupp(dσ; E)] ,\nwhere\nD1 = Bε′(supp(P)) ∩supp(R) −supp(P) ⊆Bε′(supp(P) −supp(P)),\nD2 = D = supp(P) −supp(P) and k = ⌈log Covη,δ(P)⌉. Now since ∥Ax∥≤∥A∥∥x∥≤θ∥x∥,\n∀x ∈Rn, we make take Cabs(ε′, θε′; A, D1) = 0. Moreover, by Lemma B.1, we have\nDshift(θε′, 2σ; E) ≤exp\n\u0012m(4σ + θε′)\n2σ2\nθε′\n\u0013\n= exp\n\u0012\nε\nδ1/pσ +\nε2\n8δ2/pσ2m\n\u0013\n≲1\nwhere we used the facts that m ≥1 and σ ≥ε/δ1/p. Hence\np ≲δ + ek [C−(1/d; A, D2) + C+ (d; A, D2) + C(dσ; E)] .\nFinally, Lemma B.1 implies that\nDupp(dσ; E) ≤\n\u0000de1−d\u0001m/2 ≤exp(−m/16),\nwhere in the final step we used the fact that d ≥2. This gives the result.\n42\n"
    }
  ]
}