{
  "filename": "k03mB41vyM_Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning.pdf",
  "total_pages": 28,
  "full_text": "Published as a conference paper at ICLR 2025\nIDENTIFIABLE EXCHANGEABLE MECHANISMS FOR\nCAUSAL STRUCTURE AND REPRESENTATION LEARN-\nING\nPatrik Reizinger∗1,3, Siyuan Guo∗1,2, Ferenc Huszár2, Bernhard Schölkopf † 1,3, and\nWieland Brendel †1,3,4\n1Max Planck Institute for Intelligent Systems, Tübingen, Germany\n2University of Cambridge, Cambridge, United Kingdom\n3ELLIS Institute Tübingen, Tübingen, Germany\n4Tübingen AI Center, Tübingen, Germany\nABSTRACT\nIdentifying latent representations or causal structures is important for good gen-\neralization and downstream task performance. However, both fields developed\nrather independently. We observe that several structure and representation iden-\ntifiability methods, particularly those that require multiple environments, rely on\nexchangeable non–i.i.d. (independent and identically distributed) data. To formal-\nize this connection, we propose the Identifiable Exchangeable Mechanisms (IEM)\nframework to unify key representation and causal structure learning methods. IEM\nprovides a unified probabilistic graphical model encompassing causal discovery,\nIndependent Component Analysis, and Causal Representation Learning. With the\nhelp of the IEM model, we generalize the Causal de Finetti theorem of Guo et al.\n(2024a) by relaxing the necessary conditions for causal structure identification in\nexchangeable data. We term these conditions cause and mechanism variability, and\nshow how they imply a duality condition in identifiable representation learning,\nleading to new identifiability results.\n1\nINTRODUCTION\nProvably identifying latent representations and causal structures has been a central problem in machine\nlearning, as such guarantees promise good generalization and downstream task performance (Richens\n& Everitt, 2024; Perry et al., 2022; Zimmermann et al., 2021; Arjovsky et al., 2020; Arjovsky, 2021;\nBrady et al., 2023; Wiedemer et al., 2023b;a; Lachapelle et al., 2023; Rusak et al., 2024). Causal\nstructure identification, also known as Causal Discovery (CD), aims to infer cause-effect relationships,\nwhereas identifiable representation learning aims to infer ground-truth sources. Due to their different\nlearning objectives, such problems have been treated separately.\nRecent works on Causal Representation Learning (CRL) (Schölkopf et al., 2021) propose to learn\nlatent representations with causal structures that allow efficient generalization in downstream tasks.\nYet despite progress (Zeˇcevi´c et al., 2021; Reizinger et al., 2023; Xi & Bloem-Reddy, 2023), our\nunderstanding is still limited regarding the question of\nwhat enables structure and representation identifiability?\nGuo et al. (2024a) formalize causality for exchangeable data generating processes (DGPs), showing\nthat unique structure identification is feasible under exchangeable non–i.i.d. data, assuming Indepen-\ndent Causal Mechanisms (ICMs) (Schölkopf et al., 2012). Such unique structure identification was\nclassically deemed impossible (Pearl, 2009a). The present work makes the observation that exchange-\nable non–i.i.d. data is the driving force in identification for both representation and structure identifi-\ncation. We introduce a unified framework for CD, Independent Component Analysis (ICA), and CRL\n(Fig. 1) and show that relaxed exchangeability conditions, termed cause and mechanism variability\n(Fig. 2), are sufficient for both representation and structure identifiability. Our contributions are:\n• Unifying structure and representation learning under the lens of exchangeability (§ 3, also\ncf. Fig. 1): We develop a probabilistic model, Identifiable Exchangeable Mechanisms (IEM), that\nsubsumes key methods in CD, ICA, and CRL.\n• Relaxing causal discovery assumptions in exchangeable non–i.i.d. data (§ 3.2): we show\nhow exchangeable non–i.i.d. cause or effect-given-cause mechanisms, termed cause and mech-\n∗Equal contribution. Correspondence to patrik.reizinger@tuebingen.mpg.de\n†Equal supervision\n1\nPublished as a conference paper at ICLR 2025\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\nO2\n(a) IEM\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nθ1\nθ2\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\n(b) CD\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nS2\n1\nS2\n2\nO2\n(c) ICA\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\nO2\n(d) CRL\nFigure 1: Identifiable Exchangeable Mechanisms (IEM)–A unified model for structure and rep-\nresentation identifiability: Here we show that exchangeable but non-i.i.d. data enables identification\nin key methods across Causal Discovery (CD), Independent Component Analysis (ICA), and Causal\nRepresentation Learning (CRL). Fig. 1a shows the graphical model for IEM, which subsumes Causal\nDiscovery (CD) (§ 3.2), Independent Component Analysis (ICA) (§ 3.3), and Causal Representation\nLearning (CRL) (§ 3.4). S denotes latent, Z causal, and O observed variables with corresponding\nlatent parameters θ, ψ, superscripts denote different samples. Red denotes observed/known quantities,\nblue stands for target quantities, and gray illustrates components that are not explicitly modeled in a\nparticular paradigm. θi are latent variables controlling separate probabilistic mechanisms, indicated\nby dotted vertical lines. CD (Fig. 1b) corresponds to the left-most layer of IEM, focusing on the\nstudy of cause-effect relationships between observed causal variables; ICA (Fig. 1c) infers source\nvariables from observations, but without causal connections in the left-most layer of IEM; CRL\n(Fig. 1d) shares the most similar structure with IEM, as it has both layers, including the intermediate\ncausal representations. See Fig. 4 for an enlarged view\nanism variability, provide sufficient and necessary conditions for bivariate CD, generalizing the\nidentification theorem in (Guo et al., 2024a).\n• Providing dual identifiability results in Time-Contrastive Learning (TCL) (§ 3.3): we show\nhow an auxiliary-variable ICA method, TCL, is a special case of cause variability—we discuss\nGeneralized Contrastive Learning (GCL) in Appx. A.3. Using insights from the duality in cause\nand mechanism variability, we prove the identifiability of TCL under mechanism variability.\n2\nPRELIMINARIES\nThe impossibility of bivariate CD (Pearl, 2009b) and representation identifiability (Hyvärinen &\nPajunen, 1999; Locatello et al., 2019) from i.i.d. data is well known (cf. Appx. C for examples). Thus,\nwe focus on non–i.i.d., particularly, exchangeable data (Defn. 1) and discuss a causal framework\nfrom (Guo et al., 2024a) building on exchangeability. An example of exchangeable non–i.i.d. data is\nwhen training samples come from different distributions, e.g., Gaussians with different means and/or\nvariances, where the different means and/or variances are modeled as (causal) de Finetti parameters.\nNotation.\nCapital letters denote random variables (RVs), lowercase letters their realizations, and\nbold letters sets/vectors of RVs. S are the latent sources in representation learning or, equivalently,\nthe set of exogenous variables in a Structural Equation Model (SEM); Z are causal variables, and O\nare observations in (causal) representation learning. Data generated by a DGP is a sequence of RVs\nX1, X2, . . . where superscripts index samples and subscripts the vector components (RVs), i.e., X1\ni\nspecifies the ith random variable in X1. f is the mixing function between latents to observations,\ni.e., f : s →o for representation learning, and f : z →o for CRL. Structural assignments from\nexogenous to causal variables are denoted as Z := g(Pa(Z)), where Pa(Z) are the parents or causes\nof Z and Pa(Z) includes the corresponding exogenous variable S. Whenever the RV sequence\ncontains a single variable or bivariate pairs per position, we use Xn or (Xn, Y n). Uppercase P is a\nprobability distribution, and lowercase p is a probability density function. δθ0(θ) is a shorthand for\nthe delta-distribution δ(θ = θ0).\nCausal de Finetti (CdF) and Exchangeability.\nDefinition 1 (Exchangeable sequence). An infinite sequence of random variables X1, X2, . . . is\nexchangeable if for any finite permutation π on the position indices, the joint distribution satisfies:\nP(X1, . . . , Xn) = P(Xπ(1), . . . , Xπ(n)).\n(1)\n2\nPublished as a conference paper at ICLR 2025\nAn important result to characterize any exchangeable sequence is the theorem of de Finetti (1931). It\nstates that for any exchangeable sequence, there exists a latent variable θ such that the sequence’s\njoint distribution can be represented as a mixture of conditionally i.i.d. distributions:\nP(x1, . . . , xn) =\nZ\nn\nY\ni=1\np(xi|θ)p(θ)dθ.\n(2)\nAny i.i.d. sequence is exchangeable since p(x1, . . . , xn) = Qn\ni=1 p(xi) and the joint distribution\nremains identical when changing the order of observations. Alternatively, the right-hand side of Eq.\n(2) collapses to an i.i.d. sequence whenever p(θ) = δ(θ = θ0) for some constant θ0. Though i.i.d. is\na special case of exchangeable sequences, not all exchangeable sequences are i.i.d. Examples include,\nbut are not limited to: the Pólya urn model (Hoppe, 1984), Chinese restaurant processes (Aldous\net al., 1985), or Dirichlet processes (Ferguson, 1973).\nCausality.\nCausality infers the ground-truth causal structure from the observed joint distribution\nP to enable efficient generalization in novel scenarios. It studies interventional and counterfactual\nqueries beyond purely associational relationships in observational data. The ICM principle (Schölkopf\net al., 2021) hypothesizes that distinct causal mechanisms neither inform nor influence each other.\nGuo et al. (2024a) proves that for exchangeable sequences, the ICM principle implies the existence\nof statistically independent latent variables governing each causal mechanism. Thus, establishing a\nmathematical framework to study causality in exchangeable data. We state their bivariate result.\nTheorem 1 (Causal de Finetti (Guo et al., 2024a)). Let {(Xn, Y n)}n∈N be an infinite sequence\nof binary random variable pairs and denote the set {1, 2, . . . , n} as [n]. The sequence is infinitely\nexchangeable, and satisfies Y [n] ⊥Xn+1 | X[n] for all n ∈N if and only if there exists random\nvariables θ ∈[0, 1] and ψ ∈[0, 1]2 such that the joint probability can be represented as\nP(x1, y1, . . . , xn, yn) =\nZ\nn\nY\ni=1\np(yi | xi, ψ)p(xi | θ)p(θ)p(ψ)dθdψ\n(3)\nThm. 1 shows that for any exchangeable sequence with paired random variables that satisfy certain\nconditional independences, there exist statistically independent latent variables θ, ψ governing each\n(causal) mechanism P(Y | X, ψ), P(X|θ). Guo et al. (2024a) further shows that unique causal\nstructure identification is possible in exchangeable non–i.i.d. data, contrary to the common belief that\nstructure identification is infeasible in i.i.d. data (Pearl, 2009a).\nOur work further observes that exchangeable non–i.i.d. data is again the key for representation\nidentifiability. We thus propose our unifying model, IEM, to allow understanding the driving forces\nbehind general identifiability.\n3\nIDENTIFIABLE EXCHANGEABLE MECHANISMS: A UNIFYING FRAMEWORK\nFOR STRUCTURAL AND REPRESENTATIONAL IDENTIFIABILITY\nThis section demonstrates how non–i.i.d., particularly, exchangeable data (Defn. 1) enables several\nstructure and representation identifiability results. We introduce Identifiable Exchangeable Mech-\nanisms (IEM) (cf. Fig. 1 and § 3.1) to illustrate how exchangeability is the common principle for\nmultiple identifiability results across Causal Discovery (CD), Independent Component Analysis (ICA),\nand Causal Representation Learning (CRL). Furthermore, we relax the exchangeability condition\ninto what we call cause and mechanism variability, which provides novel and relaxed identifiability\nconditions (Thm. 2 and Lem. 4). We derive a probabilistic model from IEM for CD, ICA, and CRL\n(see the graphical relationship in Fig. 1). Then, we show how the bivariate Causal de Finetti (CdF)\ntheorem (Guo et al., 2024a) (§ 3.2), TCL (Hyvarinen & Morioka, 2016) (§ 3.3), and CauCA (Wendong\net al., 2023) (§ 3.4) all leverage exchangeable data, and, thus, are special cases of IEM.\n3.1\nIDENTIFIABLE EXCHANGEABLE MECHANISMS (IEM)\nIEM encompasses three types of variables: exogenous (source) variables S for (disentangled) latent\nrepresentations, causal variables Z for representations that contain cause–effect relationships, and\nobserved variables O for observed (high-dimensional) quantities.\nA probabilistic model for IEM.\nWith all three variable types, assuming that there is an intermediate\ncausal layer, the joint distribution of source, causal, and observed variables is:\np(s, z, o)=\nZ\nθs,θg,ψ\np(o|z, ψ)\nY\nj\n\u0002\np(zj|Pa(zj); θg\nj )p(θg\nj )\n\u0003Y\ni\n[p(si|θs\ni )p(θs\ni )] p(ψ)dψdθgdθs,\n(4)\n3\nPublished as a conference paper at ICLR 2025\nwhere j indexes causal, i source variables (we omit the sample superscript for brevity), Pa(zj) denotes\nthe parents of zj (including sj) and we integrate over all θg\nj and θs\ni —the superscripts g and s denote\nseparate parameters controlling structural assignments gj and the source distributions, respectively.\nAn intuition for IEM.\nConsider multi-environment data where each environment has a distinct\ndistribution, while observations within the same environment are assumed to be exchangeable, i.e.,\nthe observations’ order is irrelevant. IEM models such multi-environment data by treating each\nenvironment as an i.i.d. copy of the model in (4). Across-environment variability is ensured by choos-\ning non-delta parameter priors p(ψ), p(θg\nj ), p(θs\ni ), while exchangeability within the environment is\nensured by the conditional independence of observations given these parameters. i.i.d. data, or a\nsingle environment in this context, is a special case of exchangeable data with delta priors (see § 2).\nWe introduce IEM to elucidate the relationship of CD, ICA, and CRL: despite distinct learning\nobjectives, they often rely on the same exchangeable non–i.i.d. data structure to allow structure or\nrepresentation identification.\nFurther, IEM can model both the passive (observation) and active (intervention) view of data. For\nexample, both a passive distribution shift and an active hard intervention can be modelled with\nexchangeability as a switch between binary variables. Tab. 1 in Appx. D illustrates the similarity of\nthe (passive) variability and (active) interventional assumptions.\nThe graphical model of IEM illustrates the relationship of source, causal and observed variables\n(Fig. 1). We connect the seemingly unrelated methods of CD, ICA, and CRL by deriving their\nmodel from IEM via omission (cf. Figs. 1b to 1d). Namely, CD does not handle high-dimensional\nobservations, ICA does not model causal variables, and CRL does not aim to recover source variables.\nWe detail these connections in the following case studies.\nCase study: Identifiable Latent Neural Causal Models (Liu et al., 2024) in the unified model.\nLiu et al. (2024) proposed to learn source (exogenous) variables, causal variables, and the correspond-\ning Directed Acyclic Graph (DAG) together, i.e., all target quantities from Fig. 1. We show how\nthis is possible via exchangeable sources and mechanisms (Lem. 1). For the sources, they assume\nnon-stationary, conditionally exponential source variables. Thus, they can use TCL (Hyvarinen &\nMorioka, 2016) to identify S from O (details in § 3.3). For the causal variables, they require diverse\ninterventions, quantified by a derivative-based condition (Assum. 1) on the structural assignments gi\n(the authors generalize to post-nonlinear models; we focus on Additive Noise Models (ANMs)).\nAssumption 1 (Structural assignment assumption (Liu et al., 2024)). Assume that the structural\nassignments gi between causal variables zi form an ANM such that zi := gi(Pa(zi); θg\ni (u)) + si,\nwhere θg\ni (u) are the parameters of the structural assignments, and they depend on the auxiliary-\nvariable u. Then, to identify the causal structure and causal variables, there exists a value u = u0\nsuch that (denoting θg\ni0 := θg\ni (u0))\n∀zj ∈Pa(zi) :\n∂gi(Pa(zi), θg\ni = θg\ni0)\n∂zj\n= 0.\n(5)\nAssum. 1 requires for a specific value u = u0, the path Zj →Zi for each Zj ∈Pa(Zi) is blocked—\nthis can be thought of as emulating perfect interventions, for which structure identifiability results\nexists (Pearl, 2009b). We rephrase the identifiability result of Liu et al. (2024), showing how it relies\non exchangeability conditions (see Appx. A.7 for proof):\nLemma 1. [Identifiable Latent Neural Causal Models are identifiable with exchangeable sources\nand mechanisms] The model of Liu et al. (2024) (Fig. 1a) identifies both the latent sources s and the\ncausal variables z (including the graph), by the variability of s via a non-delta prior over θs and by\nthe variability of the structural assignments via θg.\nThe identifiability result of (Liu et al., 2024) requires two separate variability conditions: one for the\nsources and one for the mechanisms. We show how these separate conditions, when the SEM is an\nANM, disentangle the CdF parameters into separate (independent) parameters controlling sources\nand structural assignments respectively (see proof in Appx. A.8):\nLemma 2. [Independent source and structural assignment CdF parameters for ANMs] In the setting\nof Liu et al. (2024), where the SEM is an ANM, the CdF parameters for the sources, θs, and the\nstructural assignments, θg, are independent, i.e. p(θg, θs) = p(θg)p(θs).\nLem. 2 says that the representation learning (TCL) part relies on the exchangeability of the source\n(exogenous) variables, whereas the CRL part requires exchangeability in the SEM. The connection\nbetween Gaussian LTI systems and CdF (Rajendran et al., 2023, Sec. 3.5) can be seen as a special\n4\nPublished as a conference paper at ICLR 2025\nX1\nY 1\nθ\nψ\nX2\nY 2\n(a) Causal de Finetti\nX1\nY 1\nθ\nψ\nX2\nY 2\n(b) Cause variability\nX1\nY 1\nθ\nψ\nX2\nY 2\n(c) Mechanism variability\nFigure 2: non–i.i.d. conditions for bivariate CD: (a) Exchangeable non–i.i.d. DGP for both cause\nP(X) and mechanism P(Y|X) (Guo et al., 2024a); (b): exchangeable non–i.i.d. DGP for cause\nP(X) and i.i.d. DGP for mechanism P(Y|X) (c): exchangeable non–i.i.d. DGP for mechanism\nP(Y|X) and i.i.d. DGP for cause P(X). Thm. 2 shows that identifying the unique bivariate causal\nstructure is possible if either the cause or the mechanism follows an exchangeable non–i.i.d. DGP\ncase of Lem. 2, where the sources and the mechanism (the LTI dynamical system) have independent\nmatrix parameters. Our result also conceptually resemble mechanized SEMs (Kenton et al., 2023),\nwhere the structural assignments are modeled by distinct nodes.\nNext, we show how the probabilistic models for CD, ICA, and CRL can be derived from IEM (Fig. 1),\ndepending on whether we model cause-effect relationships and/or source variables.\n3.2\nEXCHANGEABILITY IN CAUSAL DISCOVERY: EXTENDING CAUSAL DE FINETTI\nCausal Discovery (CD) infers the causal graph between observed causal variables (Fig. 1b). SEMs\n(Pearl, 2009a) are classic causal models, where deterministic causal mechanisms and stochastic noise\n(exogenous/latent) variables determine each causal variable’s value. For i.i.d. observational data\nalone, causal structure is identifiable only up to its Markov equivalence class (Defn. 5). In the present\nwork, we introduce a relaxed set of conditions, termed cause and mechanism variability, and show in\nthe bivariate case how these non–i.i.d., specifically a mixture of i.i.d. and exchangeable, data, are\nnecessary and sufficient for uniquely identifying causal structures.\nA probabilistic model for CD.\nWe consider the bivariate case with exchangeable sequences\n(Xn, Y n) that adheres to the ICM principle (Peters et al., 2018). Thm. 1 states there exist statistically\nindependent CdF parameters θ, ψ such that the joint distribution can be represented as:\np(x1, y1, . . . , xn, yn) =\nZ\nθ\nZ\nψ\nn\nY\ni=1\np(yi|xi, ψ)p(xi|θ)dψdθ\nwhere\nψ ⊥θ.\n(6)\nCD with unique structure identification is possible when the parameter priors p(θ), p(ψ) are not delta\ndistributions, i.e., when the data pairs are from exchangeable non–i.i.d. sequences. Fig. 2a shows a\nMarkov graph compatible with (6).\nCase study: CdF in the unified model.\nCD in general, and CdF in particular, focuses on the\nstudy of observed causal variables (denoted by Z in Fig. 1 and (4)). CD aims to learn cause-effect\nrelationships among the observed causal variables Zi, rather than reconstructing the Zi or uncovering\nthe true mixing function f. Bivariate CdF fits into the IEM probabilistic model by relabeling Y = Zi\nand X = Pa(Zi). We use our insights from IEM to relax the assumptions for bivariate CD from\nexchangeable pairs, generalizing CdF.\nRelaxing CdF: cause and mechanism variability.\nWe show that it is not necessary for CD that\nboth p(θ) and p(ψ) differ from a delta distribution—equivalently, the presence of both graphical\nsubstructures X1 ←θ →X2 and Y 1 ←ψ →Y 2 are not required to distinguish the causal direction\nbetween X and Y . We distinguish two cases: “cause variability,\" when only the cause mechanism\nchanges (Fig. 2b), i.e. p(ψ) = δψ0(ψ), p(θ) ̸= δθ0(θ); and “mechanism variability,\" when only the\neffect-given-the-cause mechanism changes (Fig. 2c), i.e. p(θ) = δθ0(θ) and p(ψ) ̸= δψ0(ψ)—we\nmotivate these assumptions by the Sparse Mechanism Shift (SMS) hypothesis (Perry et al., 2022) and\nprovide real-world examples for both in Appx. E. When p(θ) is sufficiently different from a delta\ndistribution, then each cause distribution sampled from p(x|θ) will have a different distribution with\nhigh probability. This is similar for p(ψ) when the effect-given-the-cause mechanism p(y|x, ψ) is\nshifted. Formally (the proof is in Appx. A.1):\n5\nPublished as a conference paper at ICLR 2025\nTheorem 2. [Cause/mechanism variability is necessary and sufficient for bivariate CD] Given a\nsequence of bivariate pairs {Xn, Y n}n∈N such that for any N ∈N, the joint distribution can be\nrepresented as:\n• X →Y : p(x1, y1, . . . , xN, yN) =\nR\nθ\nR\nψ\nQ\nn p(yn|xn, ψ)p(xn|θ)p(θ)p(ψ)dθdψ\n• X ←Y : p(x1, y1, . . . , xN, yN) =\nR\nθ\nR\nψ\nQ\nn p(xn|yn, θ)p(yn|ψ)p(ψ)p(θ)dθdψ\nThen the causal direction between variables X, Y can still be distinguished when:\n1. either only p(θ) = δθ0(θ) for some constant θ0 or only p(ψ) = δψ0(ψ) for some constant\nψ0 (but not both). Fig. 2b and Fig. 2c show the Markov structure of such factorizations.\n2. the distribution of P is faithful (Defn. 4) w.r.t. Fig. 2b or Fig. 2c.\nThm. 2 relaxes Thm. 1 and states that the causal structure can be identified even if only one mecha-\nnisms varies. That is, if the X, Y pairs are a mixture of i.i.d. and exchangeable data such that either\ncause variability (Fig. 2b) or mechanism variability (Fig. 2c) holds; then we can distinguish X →Y\nfrom X ←Y —which we empirically verify in synthetic experiments in Appx. F. Thm. 2 focuses on\nthe bivariate case, though we expect similar results can be extended to multivariate cases. Thm. 2\naligns with well-known results stating that assuming no confounders, single-node interventions are\nsufficient to identify the causal structure (Pearl, 2009b). The contribution of Thm. 2 lies in taking the\npassive view, similar to (Guo et al., 2024a).\n3.3\nEXCHANGEABILITY IN REPRESENTATION LEARNING\nRepresentation learning aims to infer latent sources s from observations o, which are generated via\na mixing function f : s →o. ICA1 (Comon, 1994; Hyvarinen et al., 2001; Shimizu et al., 2006)\nassumes component-wise independent latent sources s where p(s) = Q\ni pi(si) and aims to learn an\nunmixing function that maps to independent components. Recent methods (Hyvarinen & Morioka,\n2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Morioka et al., 2021; Zimmermann et al.,\n2021) focus on auxiliary-variable ICA, which assumes the existence and observation2 of an auxiliary\nvariable u such that p(s|u) = Q\ni pi(si|u) holds—thus, providing identifiability results for a much\nbroader model class. Here, we show that representation identifiability in (auxiliary-variable) ICA,\nparticularly TCL (Hyvarinen & Morioka, 2016) (and GCL with conditionally exponential-family\nsources, cf. Appx. A.3), relies on the latent sources to be an exchangeable non–i.i.d. sequence.\nA probabilistic model for (auxiliary-variable) ICA.\nAuxiliary variables can represent many\nforms of additional information (Hyvarinen et al., 2019). Our focus is when u represents segment\nindices, i.e., it enumerates multiple environments. This is equivalent to a draw from a categorical prior\np(u), thus, sources are a marginal copy of an exchangeable sequence p(s) =\nR\nu\nQ\ni p(si|u)p(u)du.\nIn auxiliary-variable ICA (Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al.,\n2020a), there is a separate parameter θi := θ(u) for each si. Conditioned on observing the segment\nindex u, the joint probability distribution w.r.t. latent sources s and observations o factorizes as (i\nindexes source variables, we omit the sample superscript for brevity):\npu(o, s) =\nZ\nθ\nZ\nψ\np(o|s, ψ)\nY\ni\n[p(si|θi)pu(θi)] dψdθ\nwhere\np(ψ)=δψ0(ψ).\n(7)\nCompared to (4), Eq. (7) does not have a “causal layer\", expressing the (conditional) independence\nbetween the sources in ICA. Compared to CdF, representation learning with ICA additionally\nrestricts the joint probability between sources and observations to extract more information (the latent\nvariables), compared to only the DAG. This relation was demonstrated by Reizinger et al. (2023),\nshowing that representation identifiability in some cases implies causal structure identification.\nCase study: TCL in the unified model.\nWe next present how auxiliary-variable ICA, particularly\nTCL (Hyvarinen & Morioka, 2016) (cf. Appx. A.3 for the generalization), fits into IEM (Fig. 1c)\nand present a duality result on cause and mechanism variability. The TCL model assumes that\nthe conditional log-density log p(s|u) is a sum of components qi(si, u), where qi belongs to the\nexponential family of order one, i.e.:\nqi (si, u) = ˜qi (si) θi(u) −log Ni(u) + log Qi(si),\n(8)\n1Though the literature is referred to as the nonlinear ICA literature, it often uses conditionally independent\nlatents, but expressions such as Independently Modulated Component Analysis (IMCA) are not widely used\n2There is a variant of auxiliary-variable ICA for Hidden Markov Models, which does not require observing\nu (Morioka et al., 2021); we focus on the case when u is observed\n6\nPublished as a conference paper at ICLR 2025\nwhere Ni is the normalizing constant, Qi the base measure, ˜qi the sufficient statistics, and the\nmodulation parameters θi := θi(u) depend on u. The identifiability of TCL requires multiple\nsegments (i.e., realizations of u with different values) such that for environment j, the modulation\nparameters fulfill a sufficient variability condition, defined via a rank condition:\nAssumption 2 (Sufficient variability). A DGP is called sufficiently variable if there exists (d + 1)\ndistinct realizations of u for d−dimensional source variables and modulation parameter vectors such\nthat the modulation parameter matrix L ∈R(E−1)×d has full column rank. For E environments and\nmodulation parameter vectors θj =\nh\nθj\n1, . . . , θj\nd\ni\n, the jth row of L is:\n[L]j: = (θj −θ0).\n(9)\nHere θi are the de Finetti parameters for the exchangeable sources. We show in Appx. A.2 that pu(θi)\ncannot be a delta distribution; otherwise, the variability condition of TCL is violated. Thus, the\nidentifiability of TCL hinges on exchangeable non–i.i.d. sources (we prove the same for conditionally-\nexponential sources in GCL (Hyvarinen et al., 2019), cf. Cor. 1 in Appx. A.3):\nLemma 3. [TCL is identifiable due to exchangeable non–i.i.d. sources] The sufficient variability\ncondition in TCL corresponds to cause variability, i.e., exchangeable non–i.i.d. source variables with\na fixed mixing function, which leads to the identifiability of the latent sources.\nS1\nO1\nf\nθ(u)\nS2\nO2\nf\nS\nO1\nˆf(u1)\nO2\nˆf(u2)\nFigure 3: The duality of cause and mechanism\nvariability in TCL: Lem. 4 shows that the same\nidentifiability result holds in (Left): the original\nTCL setting with exchangeable non–i.i.d. sources\nS with deterministic f mixing (cause variability),\nand the matching (Right): i.i.d. sources S with a\nstochastic ˆf(u) mixing (mechanism variability)\nExtending TCL via the cause–mechanism vari-\nability duality.\nWe next demonstrate the flexi-\nbility of the IEM framework as it relates the proba-\nbilistic model for TCL to that of bivariate CdF (6).\nTreating the observations o as the “effect\", and the\nsource vector s = [s1, . . . , sd] as the “cause\", (7)\nbecomes equal to (6) when X = S and Y = O.\nAs in auxiliary-variable ICA the mixing function\nf is deterministic, it constitutes “cause variabil-\nity\" (Fig. 2b). Our extension of the CdF theorem\nin Thm. 2 shows a symmetry between cause and\nmechanism variability: flipping the arrows and\nrelabeling X/Y and θ/ψ transforms one case into\nthe other (cf. Fig. 5 in Appx. A.1). Our insight\nis that identification can be achieved both with\ncause variability or mechanism variability. This\nnot only holds for CD, leading to a dual formu-\nlation of TCL with mechanism variability. We\nillustrate this in an example, then state our result\n(cf. Appx. A.4 for the proof):\nExample 1 (Duality of cause and mechanism variability for Gaussian models). Assume condi-\ntionally independent latent sources with variance-modulated Gaussian components, i.e., p(s|u) =\nQ\ni pi(si|u), where each pi(si|u) = N\n\u0000µi; σ2\ni (u)\n\u0001\n, depending on auxiliary variable u. In this case,\nthe observation distribution is the pushforward of p(s|u) by f, denoted as f∗p(s|u). For given σ2\ni (u)\nand f, where Σ2(u) = diag\n\u0000σ2\n1(u), . . . , σ2\nn(u)\n\u0001\n, we can find stochastic functions ˆf = f ◦Σ(u) such\nthat the pushforward f∗p(s|u) = f∗N\n\u0000µ; Σ2(u)\n\u0001\nequals to ˆf∗N (µ; I). By construction, ˆf varies\nwith u and satisfies mechanism variability.\nLemma 4. [Duality of cause and mechanism variability for TCL] For a given deterministic mix-\ning function f : s →o and conditionally factorizing (non-stationary) latent sources p(s|u) =\nQ\ni pi(si|u) fulfilling the sufficient variability of TCL, there exists an equivalent setup with stationary\n(i.i.d.) sources p(s) = Q\ni pi(si) with stochastic functions ˆf = f ◦g : s →o, where g = g(u) and\neach component gi is defined as an element-wise function such that the pushforward of pi(si) by gi\nequals pi(si|u), i.e., gi∗pi(si) = pi(si|u). Then, gi∗pi(si) fulfils the same variability condition; thus,\nthe same identifiability result applies.\nLem. 4 shows that both cause and mechanism variability lead to representation identification in TCL,\nvisualized in Fig. 3. We illustrate the practical differences between cause and mechanism variability\nin the medical example of learning representations from fMRI data (Hyvarinen & Morioka, 2016;\nKhemakhem et al., 2020a). Cause variability means having access to data from patients with different\n7\nPublished as a conference paper at ICLR 2025\nunderlying conditions; mechanism variability corresponds to measuring a single patient’s condition\nwith multiple diagnostic methods.\n3.4\nEXCHANGEABILITY IN CAUSAL REPRESENTATION LEARNING\nCausal Representation Learning (CRL) aims to learn the causal representations Z and their graphical\nstructure from high-dimensional observations O. That is, CRL can be considered as performing\nrepresentation learning for the latent causal variables and CD between those learned latent variables\nsimultaneously.\nA probabilistic model for CRL.\nAuxiliary-variable ICA considers the source distribution as\nQ\ni p(si|θi) , where si and sj are not causally related. CRL takes one step further and studies how to\nfind causal dependencies between the latent causal variables. We show that CdF theorems apply just\nas de Finetti applies in exchangeable ICA. In CRL the joint distribution factorizes (j indexes causal\nvariables, we omit the sample superscript for brevity):\np(z, o) =\nZ\nθ\nZ\nψ\np(o|z, ψ)\nY\nj\n[p(zj|Pa(zj); θj)p(θj)] dψdθ,\n(10)\nwhere θj are the CdF parameters controlling each latent causal mechanism, leading to exchangeable\ncausal variables that adhere to the ICM principle. Compared to exchangeable ICA (7), eq. (10) allows\nthat the modeled latent causal variables zi can depend on each other, whereas ICA does not model\ncause–effect relationships (Fig. 1d).\nCase study: CauCA in the unified model.\nCausal Component Analysis (CauCA) (Wendong et al.,\n2023) defines a subproblem of CRL by assuming that the DAG between the zi is known. Wendong\net al. (2023) show that identifying causal representations zi requires single-node (soft) interventions\nthat change the probabilistic mechanisms p(zj|Pa(zj)) almost everywhere, which they quantify with\nthe interventional discrepancy:\nAssumption 3 (Interventional discrepancy condition (Wendong et al., 2023)). Pairs of observational\nand single-node (soft) interventional densities p, ˜p need to differ almost everywhere, i.e.:\n∂\n∂zj\nlog ˜p(zj|Pa(zj))\np(zj|Pa(zj)) ̸= 0\na.e.\n(11)\nSatisfying Assum. 3 means an intervention on the parameters θj of the causal mechanisms\np(zj|Pa(zj); θj) (we compare to Assum. 1 in Appx. A.7). The following lemma follows from\nhaving interventions on the value of θj that fulfill Assum. 3 (proof in Appx. A.5):\nLemma 5. [Non-delta priors in the causal mechanisms can enable identifiable CRL]\nIf the interventional discrepancy condition Assum. 3 holds, then the parameter priors in (10) cannot\nequal a delta distribution, i.e., p(θj) ̸= δθj0(θj); thus, if the other conditions of CauCA hold, then,\nthe causal variables zi are identifiable. For real-valued θj, non-delta priors also imply Assum. 3\nalmost everywhere.\nLem. 5 says that when the interventional discrepancy condition is satisfied, then a change in p(θ)\nmust have occurred. This provides a sufficient criterion to determine when multi-environment data\nenables representation identification. However, as Assum. 3 is formulated as an almost everywhere\ncondition, the reverse does not necessarily hold—e.g., for discrete RVs such as when θj follows a\nBernoulli distribution (Rem. 1). Thus, we prove the reverse for real-valued θj.\nTowards the simultaneous identifiability of S, Z, and the DAG.\nWe finish our discussion of IEM\nby illustrating how the joint treatment of structure and representation identifiability can be possible\nwith less environments than the two separate problems. As we have shown in § 3.1, it is possible to\nidentify both sources and causal variables by two separate variability conditions (Liu et al., 2024).\nHowever, as Assum. 1 requires variability of the structural assignments gi, it cannot be fulfilled by\nexchangeable sources, at least not for an ANM. Thus, we consider the most general identifiability\nresult in CRL by (Jin & Syrgkanis, 2023), which requires dim Z single-node non-degenerate (in the\nsense of Assum. 3) soft interventions for generic nonparametric SEMs—i.e., when interventions on\nthe exogenous variables change the observational density almost everywhere. Further restricting the\nsources to first-order conditional exponential family distributions, adding one more intervention can\nsatisfy Assum. 2. Thus, we sidestep the requirement of having (dim Z + 1) different environments\nfor ICA, and another dim Z for CRL. Namely, by Lem. 5, we know that when Assum. 3 holds, then\nthe parameter priors are non-degenerate. Then, by Lem. 3, Assum. 2 also holds. Thus (proof is in\nAppx. A.6):\n8\nPublished as a conference paper at ICLR 2025\nLemma 6. [Simultaneous identifiability via generic non-degenerate source priors] Provided the\nassumptions of (Jin & Syrgkanis, 2023, Thm. 4) hold with the restriction of the source variables’ den-\nsity belonging to the exponential family of order one, and assuming that the nonparametric structural\nassignments are generic such that single-node soft interventions on each Si satisfy Assum. 3, then\n(dim Z+1) interventions can provide exchangeable data sufficient for the simultaneous identification\nof both exogenous and causal variables (and also the DAG)—as opposed to (2 dim Z + 1), where\ndim Z separate environments are used for CRL and another (dim Z + 1) for ICA.\n4\nDISCUSSION AND FUTURE DIRECTIONS\nOur work unifies several Causal Discovery (CD), Independent Component Analysis (ICA), and\nCausal Representation Learning (CRL) methods with the lens of exchangeability. Next, we answer\nthe question:\nWhat do we gain from the Identifiable Exchangeable Mechanisms (IEM) framework?\nThe motivation of introducing IEM is to provide a unified model that eases understanding and\ndiscovery of the synergies between representation and structure identifiability. Our work leverages\nIEM to relax conditions of general exchangeability to cause and mechanism variability for enabling\nboth structure and representation identifiability. Exchangeability can also model both the passive\nnotion of data variability posited in the ICA literature (e.g., Assum. 2) and the active, agency-\nbased notion of diverse interventions (e.g., Assum. 3). We provide a detailed comparison of the\nassumptions in both fields in Tab. 1 in Appx. D. By interpreting the variability in ICA as coming from\ninterventions on the exogenous variables, IEM explains why ICA can allow for causal inferences.\nNamely, assuming that the observations correspond to the causal variables and using ICA to recover\nthe source (exogenous) variables, we can infer the causal graph depending on the identifiability class,\nas shown by Reizinger et al. (2023).\nIn the following, we show how exchangeability can model i.i.d., out-of-distribution (OOD) and\ninterventional distributions (§ 4.1), discuss the general conditions that allow for identifiability in the\nIEM setting (§ 4.2), and detail three additional directions where we believe IEM can open up new\npossibilities (§§ 4.3 to 4.5).\n4.1\nEXCHANGEABILITY FOR MODELING I.I.D., OOD, AND INTERVENTIONAL DATA\nBy de Finetti’s theorem (de Finetti, 1931), the joint distribution of exchangeable data can be repre-\nsented as a mixture of i.i.d. distributions p(xi|θ), where θ is drawn from a prior distribution (2). In\nthe special case of p(θ) = δθ0 we get i.i.d. samples. Guo et al. (2024b) studies how intervention\npropagates in an exchangeable sequence. Here we note that exchangeability may be a natural choice\nfor modelling OOD and interventional data. For example, when we assume access to multiple\nenvironments—where each environment has a distinct parameter drawn from p(θ): OOD and inter-\nventions can be analogusly modelled as a shift in θ, i.e., data in a novel or intervened environment is\ndrawn from p(x | θ1) instead of p(x | θ0), where θ1 ̸= θ0 (cf. the intution in § 3.1 for an example).\n4.2\nGENERAL CONDITIONS FOR IDENTIFIABILITY IN THE IEM SETTING\nWhen introducing IEM, we focused on exchangeability as a driving factor for structure and represen-\ntation identifiability. However, theoretical guarantees usually require further assumptions. Here we\ndiscuss the general set of assumptions required for identifiabiltiy of the causal structure, the causal\nvariables, and the exogenous (source) variables. We review such assumptions in Tab. 1 in Appx. D.\nIn the case of exchangeable data, we can characterize the best achievable identifiability results as:\nCausal structure (DAG).\nObserved causal variables under faithfulness and cause or mechanism\nvariability are necessary and sufficient to identify the DAG (Thm. 2).\nCausal variables (Z).\nAssuming independent exogenous variables and a diffeomorphic mixing\nfunction is sufficient to identify the causal variables up to elementwise nonlinear transformations when\nwe have access to dim Z single-node soft interventions with unknown targets (Jin & Syrgkanis, 2023).\nExogenous (source) variables (S).\nHaving exchangeable sources and a surjective feature extractor\nare sufficient to achieve identifiability up to element-wise nonlinear transformations if the feature\nextractor is either positive or is augmented by squared features (Khemakhem et al., 2020b).\n4.3\nIDENTIFYING COMPONENTS OF CAUSAL MECHANISMS\nCausal mechanisms are composed of exogenous variables and structural assignments. CdF proves the\nexistence of a statistically independent latent variable per causal mechanism. Lem. 2 shows that for\nANMs, such latent RVs can be decomposed into separate variables controlling exogenous variables\nand structural assignments. Wang et al. (2018), for example, performs multi-environment CD via\nchanging only the weights in the linear SEM across environments, which corresponds to changing\n9\nPublished as a conference paper at ICLR 2025\nthe mechanism parameters θg. Liu et al. (2024) showed how changing both parameters leads to latent\nsource and causal structure identification. This suggests that partitioning the CdF parameters into\nmechanism and source parameters can be beneficial to identifying individual components of causal\nmechanisms.\n4.4\nCAUSE AND MECHANISM VARIABILITY: POTENTIAL GAPS AND FUTURE DIRECTIONS\nWe relax the assumptions for bivariate CD (§ 3.2) by noticing that changing only either cause or\nmechanism leads to identifiability, which we term cause and mechanism variability. We further\nshowed with TCL how ICA methods—which usually belong to the cause variability category—\ncan be equivalently extended to mechanism variability (Lem. 4). This dual formulation, though\nmathematically equivalent, presents new opportunities in practice. Existing work in the ICA literature\nhave focused on identification through variation in the sources with a single deterministic mixing\nfunction f : s →o, where functional constraints are used for identifiability (Gresele et al., 2021;\nLachapelle et al., 2023; Brady et al., 2023; Wiedemer et al., 2023a;b). Multi-view ICA (Gresele et al.,\n2019), on the other hand, might be related to mechanism variability—we leave investigating this\nconnection to future work.\n4.5\nCHARACTERIZING DEGREES OF NON–I.I.D. DATA\nExisting work have developed multiple criteria to characterize non-i.i.d. data from out-of-distribution\n(Quionero-Candela et al., 2009; Schölkopf et al., 2012; Arjovsky et al., 2020) to out-of-variable (Guo\net al.) generalization. Here we assay common criteria for identifiability and highlight potential gaps.\nOften identification conditions are descriptive with no clear practical guidance in quantifying how\nand when to induce non-i.i.d. data. Metric computation is also difficult in practice.\nRank conditions\nsuch as Assum. 2 in TCL (Hyvarinen & Morioka, 2016), our running example for\nICA, uses a rank-condition to prove identifiability. Assum. 2 expresses that the multi-environment\ndata is non–i.i.d. However, full-rank matrices can differ to a large extent, e.g., by their condition\nnumber, which affects numerical stability, thus, matters in practice (Rajendran et al., 2023). We\nexpect that the condition number could be used to develop bounds for the required sample sizes in\npractice—an aspect generally missing from the identifiability literature, as most works assume access\nto infinite samples, with the exception of (Lyu & Fu, 2022).\nDerivative conditions\nsuch as interventional discrepancy (Wendong et al., 2023) require that\nbetween environments, there is a non-trivial (i.e., non-zero measure) shift between the causal\nmechanisms—i.e., the data is not i.i.d. The similarity between interventional discrepancy and\nthe derivative-based condition on the structural assignments in (Liu et al., 2024) (Assum. 1) also\nhas an interesting interpretation: Liu et al. (2024) does not require interventional data per se, only\nnon–i.i.d. data that is akin to being generated by a SEM that was intervened on. This assumption is\nsimilar to the concepts of Mendelian randomization (Didelez & Sheehan, 2007) or natural experi-\nments (Angrist & Krueger, 1991; Imbens & Angrist, 1994), which assume that an intervention not\ncontrolled by the experimenter (but by, e.g., genetic mutations) provides sufficiently diverse data.\nMechanism shift-based conditions\nquantify the number of shifted causal mechanisms. The\ndistribution shift perspective was already present in, e.g., (Zhang et al., 2015; Arjovsky et al., 2020).\nPerry et al. (2022) explore the SMS hypothesis (Schölkopf, 2019), postulating that domain shifts\nare due to a sparse change in the set of mechanisms. Their Mechanism Shift Score (MSS) counts\nthe number of changing conditionals, which is minimal for the true DAG. Richens & Everitt (2024)\ncharacterize mechanism shifts for causal agents solving decision tasks. Their condition posits that\nthe agent’s optimal policy should change when the causal mechanisms shift.\n5\nCONCLUSION\nWe introduced Identifiable Exchangeable Mechanisms (IEM), a unifying framework that captures a\ncommon theme between causal discovery, representation learning, and causal representation learning:\naccess to exchangeable non–i.i.d. data. We showed how particular causal structure and representation\nidentifiability results can be reframed in IEM as exchangeability conditions, from the Causal de\nFinetti theorem through auxiliary-variable Independent Component Analysis and Causal Component\nAnalysis. Our unified model also led to new insights: we introduced cause and mechanism variability\nas a special case of exchangeable but not-i.i.d. data, which led us to provide relaxed necessary and\nsufficient conditions for causal structure identification (Thm. 2), and to formulate identifiability\nresults for mechanism variability-based time-contrastive learning (Lem. 4) We acknowledge that our\nunified framework might not incorporate all identifiable methods. However, by providing a formal\nconnection between the mostly separately advancing fields of causality and representation learning,\nmore synergies and new results can be developed, just as Thm. 2 and Lem. 4. This, we hope, will\ninspire further research to investigate the formal connection between these fields.\n10\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGMENTS\nThe authors thank Luigi Gresele and anonymous reviewers for insightful comments and discussions.\nThis work was supported by a Turing AI World-Leading Researcher Fellowship G111021. Patrik\nReizinger acknowledges his membership in the European Laboratory for Learning and Intelligent\nSystems (ELLIS) PhD program and thanks the International Max Planck Research School for\nIntelligent Systems (IMPRS-IS) for its support. This work was supported by the German Federal\nMinistry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A. Wieland\nBrendel acknowledges financial support via an Emmy Noether Grant funded by the German Research\nFoundation (DFG) under grant no. BR 6382/1-1 and via the Open Philantropy Foundation funded\nby the Good Ventures Foundation. Wieland Brendel is a member of the Machine Learning Cluster\nof Excellence, EXC number 2064/1 – Project number 390727645. This research utilized compute\nresources at the Tübingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG.\nREFERENCES\nKartik Ahuja, Jason Hartford, and Yoshua Bengio. Weakly Supervised Representation Learning\nwith Sparse Perturbations. October 2022a. URL https://openreview.net/forum?id=\n6ZI4iF_T7t. 26\nKartik Ahuja, Yixin Wang, Divyat Mahajan, and Yoshua Bengio.\nInterventional Causal Rep-\nresentation Learning, September 2022b.\nURL http://arxiv.org/abs/2209.11924.\narXiv:2209.11924 [cs, stat]. 26\nDavid J Aldous, Illdar A Ibragimov, Jean Jacod, and David J Aldous. Exchangeability and related\ntopics. Springer, 1985. 3\nJoshua D. Angrist and Alan B. Krueger. Does compulsory school attendance affect schooling\nand earnings? The Quarterly Journal of Economics, 106(4):979–1014, November 1991. ISSN\n0033-5533. doi: 10.2307/2937954. URL http://dx.doi.org/10.2307/2937954. 10,\n26\nYashas Annadani, Jonas Rothfuss, Alexandre Lacoste, Nino Scherrer, Anirudh Goyal, Yoshua Bengio,\nand Stefan Bauer. Variational Causal Networks: Approximate Bayesian Inference over Causal\nStructures. arXiv:2106.07635 [cs, stat], June 2021. URL http://arxiv.org/abs/2106.\n07635. arXiv: 2106.07635. 24\nMartin Arjovsky. Out of Distribution Generalization in Machine Learning. arXiv:2103.02667 [cs,\nstat], March 2021. URL http://arxiv.org/abs/2103.02667. arXiv: 2103.02667. 1\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization.\narXiv:1907.02893 [cs, stat], March 2020. URL http://arxiv.org/abs/1907.02893.\narXiv: 1907.02893. 1, 10\nAlice Bizeul, Bernhard Schölkopf, and Carl Allen. A Probabilistic Model to explain Self-Supervised\nRepresentation Learning, February 2024. URL http://arxiv.org/abs/2402.01399.\narXiv:2402.01399 [cs, stat]. 24\nJack Brady, Roland S. Zimmermann, Yash Sharma, Bernhard Schölkopf, Julius von Kügelgen,\nand Wieland Brendel. Provably Learning Object-Centric Representations, May 2023. URL\nhttp://arxiv.org/abs/2305.14229. arXiv:2305.14229 [cs]. 1, 10\nDavid Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine\nlearning research, 3(Nov):507–554, 2002. 26\nPierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287–314,\n1994. 6, 24\nGeorge Darmois. Analyse des liaisons de probabilité. In Proc. Int. Stat. Conferences 1947, pp. 231,\n1951. 24\nB. de Finetti. Funzione caratteristica di un fenomeno aleatorio. Atti della R. Accademia Nazionale\ndei Lincei, Ser. 6. Memorie, Classe di Scienze Fisiche, Matematiche e Naturali 4, pp. 251–299,\n1931. 3, 9\n11\nPublished as a conference paper at ICLR 2025\nVanessa Didelez and Nuala Sheehan. Mendelian randomization as an instrumental variable approach\nto causal inference. Statistical Methods in Medical Research, 16(4):309–330, August 2007.\nISSN 1477-0334. doi: 10.1177/0962280206077743. URL http://dx.doi.org/10.1177/\n0962280206077743. 10\nCian Eastwood, Julius von Kügelgen, Linus Ericsson, Diane Bouchacourt, Pascal Vincent, Bern-\nhard Schölkopf, and Mark Ibrahim. Self-Supervised Disentanglement by Leveraging Structure\nin Data Augmentations, November 2023. URL http://arxiv.org/abs/2311.08815.\narXiv:2311.08815 [cs, stat]. 24\nThomas S Ferguson. A bayesian analysis of some nonparametric problems. The annals of statistics,\npp. 209–230, 1973. 3\nMarco Fumero, Florian Wenzel, Luca Zancato, Alessandro Achille, Emanuele Rodolà, Stefano Soatto,\nBernhard Schölkopf, and Francesco Locatello. Leveraging sparse and shared feature activations\nfor disentangled representation learning, April 2023. URL http://arxiv.org/abs/2304.\n07939. arXiv:2304.07939 [cs]. 26\nGaël Gendron, Michael Witbrock, and Gillian Dobbie. Disentanglement of Latent Representations via\nSparse Causal Interventions, February 2023. URL http://arxiv.org/abs/2302.00869.\narXiv:2302.00869 [cs, stat]. 26\nLuigi Gresele, Paul K. Rubenstein, Arash Mehrjou, Francesco Locatello, and Bernhard Schölkopf.\nThe Incomplete Rosetta Stone Problem: Identifiability Results for Multi-View Nonlinear ICA.\narXiv:1905.06642 [cs, stat], August 2019. URL http://arxiv.org/abs/1905.06642.\narXiv: 1905.06642. 10, 24\nLuigi Gresele, Julius von Kügelgen, Vincent Stimper, Bernhard Schölkopf, and Michel Besserve.\nIndependent mechanism analysis, a new concept? arXiv:2106.05200 [cs, stat], June 2021. URL\nhttp://arxiv.org/abs/2106.05200. arXiv: 2106.05200. 10, 24\nSiyuan Guo, Jonas Bernhard Wildberger, and Bernhard Schölkopf. Out-of-variable generalisation for\ndiscriminative models. In The Twelfth International Conference on Learning Representations. 10\nSiyuan Guo, Viktor Tóth, Bernhard Schölkopf, and Ferenc Huszár. Causal de finetti: On the\nidentification of invariant causal structure in exchangeable data. Advances in Neural Information\nProcessing Systems, 36, 2024a. 1, 2, 3, 5, 6, 19, 25, 26, 27\nSiyuan Guo, Chi Zhang, Karthika Mohan, Ferenc Huszár, and Bernhard Schölkopf. Do Finetti: On\nCausal Effects for Exchangeable Data, May 2024b. URL http://arxiv.org/abs/2405.\n18836. arXiv:2405.18836 [cs, stat]. 9\nFred M. Hoppe. Pólya-like urns and the Ewens’ sampling formula. Journal of Mathematical\nBiology, 20(1):91–94, August 1984. ISSN 1432-1416. doi: 10.1007/BF00275863. URL https:\n//doi.org/10.1007/BF00275863. 3\nPatrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear\ncausal discovery with additive noise models. In Advances in Neural Information Processing Sys-\ntems, volume 21. Curran Associates, Inc., 2008. URL https://proceedings.neurips.\ncc/paper/2008/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.\nhtml. 24\nBiwei Huang, Kun Zhang, Jiji Zhang, Ruben Sanchez-Romero, Clark Glymour, and Bernhard\nSchölkopf. Behind distribution shift: Mining driving forces of changes and causal arrows. In 2017\nIEEE International Conference on Data Mining (ICDM), pp. 913–918. IEEE, 2017. 26\nAapo Hyvarinen and Hiroshi Morioka. Unsupervised Feature Extraction by Time-Contrastive\nLearning and Nonlinear ICA. arXiv:1605.06336 [cs, stat], May 2016. URL http://arxiv.\norg/abs/1605.06336. arXiv: 1605.06336. 3, 4, 6, 7, 10, 24, 25\nAapo Hyvarinen and Hiroshi Morioka.\nNonlinear ICA of Temporally Dependent Stationary\nSources.\nIn Artificial Intelligence and Statistics, pp. 460–469. PMLR, April 2017.\nURL\nhttp://proceedings.mlr.press/v54/hyvarinen17a.html.\nISSN: 2640-3498.\n24\n12\nPublished as a conference paper at ICLR 2025\nAapo Hyvarinen, Juha Karhunen, and Erkki Oja. Independent component analysis. J. Wiley, New\nYork, 2001. ISBN 978-0-471-40540-5. 6, 24\nAapo Hyvarinen, Kun Zhang, Shohei Shimizu, and Patrik O Hoyer. Estimation of a Structural Vector\nAutoregression Model Using Non-Gaussianity. pp. 23, 2010. URL https://www.jmlr.\norg/papers/volume11/hyvarinen10a/hyvarinen10a.pdf. 24\nAapo Hyvarinen, Hiroaki Sasaki, and Richard E. Turner. Nonlinear ICA Using Auxiliary Variables\nand Generalized Contrastive Learning. arXiv:1805.08651 [cs, stat], February 2019. URL http:\n//arxiv.org/abs/1805.08651. arXiv: 1805.08651. 6, 7, 20, 24, 25\nAapo Hyvärinen and Petteri Pajunen.\nNonlinear independent component analysis: Existence\nand uniqueness results. Neural Networks, 12(3):429–439, April 1999. ISSN 0893-6080. doi:\n10.1016/S0893-6080(98)00140-3. URL https://www.sciencedirect.com/science/\narticle/pii/S0893608098001403. 2, 23, 24\nAapo Hyvärinen, Ilyes Khemakhem, and Ricardo Monti. Identifiability of latent-variable and\nstructural-equation models: from linear to nonlinear, February 2023. URL http://arxiv.\norg/abs/2302.02672. arXiv:2302.02672 [cs, stat]. 24\nHermanni Hälvä and Aapo Hyvärinen. Hidden Markov Nonlinear ICA: Unsupervised Learning\nfrom Nonstationary Time Series. arXiv:2006.12107 [cs, stat], June 2020. URL http://arxiv.\norg/abs/2006.12107. arXiv: 2006.12107. 24\nHermanni Hälvä, Sylvain Le Corff, Luc Lehéricy, Jonathan So, Yongjie Zhu, Elisabeth Gassiat, and\nAapo Hyvarinen. Disentangling Identifiable Features from Noisy Data with Structured Nonlinear\nICA. arXiv:2106.09620 [cs, stat], June 2021. URL http://arxiv.org/abs/2106.09620.\narXiv: 2106.09620. 24\nGuido W. Imbens and Joshua D. Angrist. Identification and estimation of local average treatment\neffects. Econometrica, 62(2):467, March 1994. ISSN 0012-9682. doi: 10.2307/2951620. URL\nhttp://dx.doi.org/10.2307/2951620. 10, 26\nJikai Jin and Vasilis Syrgkanis. Learning Causal Representations from General Environments:\nIdentifiability and Intrinsic Ambiguity, November 2023. URL http://arxiv.org/abs/\n2311.12267. arXiv:2311.12267 [cs, econ, stat]. 8, 9, 21, 22, 25\nDiviyan Kalainathan, Olivier Goudet, Isabelle Guyon, David Lopez-Paz, and Michèle Sebag. Struc-\ntural Agnostic Modeling: Adversarial Learning of Causal Graphs. arXiv:1803.04929 [stat],\nOctober 2020. URL http://arxiv.org/abs/1803.04929. arXiv: 1803.04929. 24\nZachary Kenton, Ramana Kumar, Sebastian Farquhar, Jonathan Richens, Matt MacDermott, and\nTom Everitt. Discovering agents. Artificial Intelligence, 322:103963, September 2023. ISSN\n0004-3702. doi: 10.1016/j.artint.2023.103963. URL https://www.sciencedirect.com/\nscience/article/pii/S0004370223001091. 5\nIlyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational Autoen-\ncoders and Nonlinear ICA: A Unifying Framework. In International Conference on Artificial\nIntelligence and Statistics, pp. 2207–2217. PMLR, June 2020a. URL http://proceedings.\nmlr.press/v108/khemakhem20a.html. ISSN: 2640-3498. 6, 7, 24, 25\nIlyes Khemakhem, Ricardo Pio Monti, Diederik P. Kingma, and Aapo Hyvärinen. ICE-BeeM: Identi-\nfiable Conditional Energy-Based Deep Models Based on Nonlinear ICA. arXiv:2002.11537 [cs,\nstat], October 2020b. URL http://arxiv.org/abs/2002.11537. arXiv: 2002.11537. 9,\n22, 25\nMichael Kirchhof, Enkelejda Kasneci, and Seong Joon Oh. Probabilistic Contrastive Learning\nRecovers the Correct Aleatoric Uncertainty of Ambiguous Inputs, February 2023. URL http:\n//arxiv.org/abs/2302.02865. arXiv:2302.02865 [cs, stat]. 24\nDavid Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge,\nand Dylan Paiton. Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse\nCoding. arXiv:2007.10930 [cs, stat], March 2021. URL http://arxiv.org/abs/2007.\n10930. arXiv: 2007.10930. 24\n13\nPublished as a conference paper at ICLR 2025\nSébastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-Based\nNeural DAG Learning. arXiv:1906.02226 [cs, stat], February 2020. URL http://arxiv.\norg/abs/1906.02226. arXiv: 1906.02226. 24\nSébastien Lachapelle, Pau Rodríguez López, Rémi Le Priol, Alexandre Lacoste, and Simon Lacoste-\nJulien. Discovering Latent Causal Variables via Mechanism Sparsity: A New Principle for\nNonlinear ICA. arXiv:2107.10098 [cs, stat], July 2021a. URL http://arxiv.org/abs/\n2107.10098. arXiv: 2107.10098. 26\nSébastien Lachapelle, Pau Rodríguez López, Yash Sharma, Katie Everett, Rémi Le Priol, Alexandre\nLacoste, and Simon Lacoste-Julien. Disentanglement via Mechanism Sparsity Regularization:\nA New Principle for Nonlinear ICA. arXiv:2107.10098 [cs, stat], November 2021b. URL\nhttp://arxiv.org/abs/2107.10098. arXiv: 2107.10098. 26\nSébastien Lachapelle, Tristan Deleu, Divyat Mahajan, Ioannis Mitliagkas, Yoshua Bengio, Simon\nLacoste-Julien, and Quentin Bertrand. Synergies Between Disentanglement and Sparsity: a Multi-\nTask Learning Perspective, November 2022. URL http://arxiv.org/abs/2211.14666.\narXiv:2211.14666 [cs, stat]. 26\nSébastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, and Simon Lacoste-Julien. Additive\nDecoders for Latent Variables Identification and Cartesian-Product Extrapolation, July 2023. URL\nhttp://arxiv.org/abs/2307.02598. arXiv:2307.02598 [cs, stat]. 1, 10\nYuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun\nZhang, and Javen Qinfeng Shi. Identifiable Latent Neural Causal Models, March 2024. URL\nhttp://arxiv.org/abs/2403.15711. arXiv:2403.15711 [cs, stat] version: 1. 4, 8, 10,\n22, 23, 25\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf,\nand Olivier Bachem. Challenging Common Assumptions in the Unsupervised Learning of Disen-\ntangled Representations. In International Conference on Machine Learning, pp. 4114–4124.\nPMLR, May 2019.\nURL http://proceedings.mlr.press/v97/locatello19a.\nhtml. ISSN: 2640-3498. 2, 23, 24\nQi Lyu and Xiao Fu. On Finite-Sample Identifiability of Contrastive Learning-Based Nonlinear\nIndependent Component Analysis. In Proceedings of the 39th International Conference on\nMachine Learning, pp. 14582–14600. PMLR, June 2022. URL https://proceedings.\nmlr.press/v162/lyu22a.html. ISSN: 2640-3498. 10\nAmin Mansouri, Jason Hartford, Yan Zhang, and Yoshua Bengio. Object-centric architectures enable\nefficient causal representation learning, October 2023. URL http://arxiv.org/abs/2310.\n19054. arXiv:2310.19054 null. 26\nFrancesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.\nCausal Discovery with Score Matching on Additive Models with Arbitrary Noise, April 2023a.\nURL http://arxiv.org/abs/2304.03265. arXiv:2304.03265 [cs, stat]. 24\nFrancesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.\nScalable Causal Discovery with Score Matching, April 2023b. URL http://arxiv.org/\nabs/2304.03382. arXiv:2304.03382 [cs, stat]. 24\nRicardo Pio Monti, Kun Zhang, and Aapo Hyvärinen. Causal Discovery with General Non-Linear\nRelationships using Non-Linear ICA. In Uncertainty in Artificial Intelligence, pp. 186–195. PMLR,\nAugust 2020. URL http://proceedings.mlr.press/v115/monti20a.html. ISSN:\n2640-3498. 24\nHiroshi Morioka and Aapo Hyvarinen.\nConnectivity-contrastive learning: Combining causal\ndiscovery and representation learning for multimodal data.\nIn Proceedings of The 26th In-\nternational Conference on Artificial Intelligence and Statistics, pp. 3399–3426. PMLR, April\n2023. URL https://proceedings.mlr.press/v206/morioka23a.html. ISSN:\n2640-3498. 24\n14\nPublished as a conference paper at ICLR 2025\nHiroshi Morioka, Hermanni Hälvä, and Aapo Hyvärinen. Independent Innovation Analysis for\nNonlinear Vector Autoregressive Process. arXiv:2006.10944 [cs, stat], February 2021. URL\nhttps://arxiv.org/abs/2006.10944. arXiv: 2006.10944. 6, 24\nIgnavier Ng, Zhuangyan Fang, Shengyu Zhu, Zhitang Chen, and Jun Wang. Masked Gradient-\nBased Causal Structure Learning. arXiv:1910.08527 [cs, stat], February 2020. URL http:\n//arxiv.org/abs/1910.08527. arXiv: 1910.08527. 24\nJudea Pearl.\nCausal inference in statistics:\nAn overview.\nStatistics Surveys,\n3\n(none),\nJanuary\n2009a.\nISSN\n1935-7516.\ndoi:\n10.1214/09-SS057.\nURL\nhttps://projecteuclid.org/journals/statistics-surveys/volume-3/\nissue-none/Causal-inference-in-statistics-An-overview/10.1214/\n09-SS057.full. 1, 3, 5, 19\nJudea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, Cambridge,\n2 edition, 2009b. ISBN 978-0-511-80316-1. doi: 10.1017/CBO9780511803161. URL http:\n//ebooks.cambridge.org/ref/id/CBO9780511803161. 2, 4, 6, 23, 24\nRonan Perry, Julius von Kügelgen, and Bernhard Schölkopf. Causal Discovery in Heterogeneous\nEnvironments Under the Sparse Mechanism Shift Hypothesis, October 2022. URL http://\narxiv.org/abs/2206.02013. arXiv:2206.02013 [cs, stat]. 1, 5, 10, 25, 26\nJonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foun-\ndations and learning algorithms. Journal of Statistical Computation and Simulation, 88(16):\n3248–3248, November 2018.\nISSN 0094-9655, 1563-5163.\ndoi: 10.1080/00949655.2018.\n1505197. URL https://www.tandfonline.com/doi/full/10.1080/00949655.\n2018.1505197. 5, 23\nJoaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil Lawrence. Dataset\nshift in machine learning. 01 2009. 10\nGoutham Rajendran, Patrik Reizinger, Wieland Brendel, and Pradeep Ravikumar. An Interventional\nPerspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis,\nNovember 2023. URL http://arxiv.org/abs/2311.18048. arXiv:2311.18048 [cs, eess,\nstat]. 4, 10, 26\nPatrik Reizinger, Yash Sharma, Matthias Bethge, Bernhard Schölkopf, Ferenc Huszár, and Wieland\nBrendel. Jacobian-based Causal Discovery with Nonlinear ICA. Transactions on Machine Learning\nResearch, April 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=\n2Yo9xqR6Ab. 1, 6, 9, 24, 25\nJonathan Richens and Tom Everitt. Robust agents learn causal world models, February 2024. URL\nhttp://arxiv.org/abs/2402.10877. arXiv:2402.10877 [cs]. 1, 10, 21, 24\nEvgenia Rusak, Patrik Reizinger, Attila Juhos, Oliver Bringmann, Roland S. Zimmermann, and\nWieland Brendel. InfoNCE: Identifying the Gap Between Theory and Practice, June 2024. URL\nhttp://arxiv.org/abs/2407.00143. arXiv:2407.00143 [cs, stat]. 1, 25\nBernhard Schölkopf. Causality for machine learning. 2019. doi: 10.1145/3501714.3501755. 10\nBernhard Schölkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij.\nOn Causal and Anticausal Learning. arXiv:1206.6471 [cs, stat], June 2012. URL http://\narxiv.org/abs/1206.6471. arXiv: 1206.6471. 1, 10, 26\nBernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,\nAnirudh Goyal, and Yoshua Bengio. Towards Causal Representation Learning. arXiv:2102.11107\n[cs], February 2021. URL http://arxiv.org/abs/2102.11107. arXiv: 2102.11107\nversion: 1. 1, 3, 24\nShohei Shimizu, Patrik O Hoyer, Aapo Hyvarinen, and Antti Kerminen. A Linear Non-Gaussian\nAcyclic Model for Causal Discovery. pp. 28, 2006. 6, 24\n15\nPublished as a conference paper at ICLR 2025\nShohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara,\nTakashi Washio, Patrik O Hoyer, Kenneth Bollen, and Patrik Hoyer. Directlingam: A direct\nmethod for learning a linear non-gaussian structural equation model. Journal of Machine Learning\nResearch-JMLR, 12(Apr):1225–1248, 2011. 26\nPeter L Spirtes, Christopher Meek, and Thomas S Richardson. Causal inference in the presence of\nlatent variables and selection bias. arXiv preprint arXiv:1302.4983, 2013. 26\nChandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler.\nLinear Causal Disentangle-\nment via Interventions, February 2023.\nURL http://arxiv.org/abs/2211.16467.\narXiv:2211.16467 [cs, stat]. 24\nJulius von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf, Michel\nBesserve, and Francesco Locatello. Self-Supervised Learning with Data Augmentations Provably\nIsolates Content from Style, June 2021. URL http://arxiv.org/abs/2106.04619.\narXiv: 2106.04619. 24, 26\nJulius von Kügelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Keki´c, Elias Barein-\nboim, David M. Blei, and Bernhard Schölkopf. Nonparametric Identifiability of Causal Represen-\ntations from Unknown Interventions, October 2023. URL http://arxiv.org/abs/2306.\n00542. arXiv:2306.00542 [cs, stat]. 21, 22, 25\nYuhao Wang, Chandler Squires, Anastasiya Belyaeva, and Caroline Uhler. Direct Estimation of Dif-\nferences in Causal Graphs. In Advances in Neural Information Processing Systems, volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/\nhash/e1314fc026da60d837353d20aefaf054-Abstract.html. 9\nLiang Wendong, Armin Keki´c, Julius von Kügelgen, Simon Buchholz, Michel Besserve, Luigi\nGresele, and Bernhard Schölkopf. Causal Component Analysis, October 2023. URL http:\n//arxiv.org/abs/2305.17225. arXiv:2305.17225 [cs, stat]. 3, 8, 10, 21, 22, 25\nThaddäus Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, and Wieland\nBrendel. Provable Compositional Generalization for Object-Centric Learning, October 2023a.\nURL http://arxiv.org/abs/2310.05327. arXiv:2310.05327 [cs]. 1, 10\nThaddäus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, and Wieland Brendel. Compositional\nGeneralization from First Principles, July 2023b. URL http://arxiv.org/abs/2307.\n05596. arXiv:2307.05596 [cs, stat]. 1, 10\nQuanhan Xi and Benjamin Bloem-Reddy. Indeterminacy and Strong Identifiability in Generative\nModels, February 2023. URL http://arxiv.org/abs/2206.00801. arXiv:2206.00801\n[cs, stat]. 1\nMengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. CausalVAE:\nDisentangled Representation Learning via Neural Structural Causal Models. In 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 9588–9597, Nashville, TN,\nUSA, June 2021. IEEE. ISBN 978-1-66544-509-2. doi: 10.1109/CVPR46437.2021.00947. URL\nhttps://ieeexplore.ieee.org/document/9578520/. 24\nDingling Yao, Dario Rancati, Riccardo Cadei, Marco Fumero, and Francesco Locatello. Unifying\nCausal Representation Learning with the Invariance Principle, September 2024. URL http:\n//arxiv.org/abs/2409.02772. arXiv:2409.02772 [cs, stat]. 24\nMatej Zeˇcevi´c, Devendra Singh Dhami, Petar Veliˇckovi´c, and Kristian Kersting. Relating Graph\nNeural Networks to Structural Causal Models. arXiv:2109.04173 [cs, stat], October 2021. URL\nhttp://arxiv.org/abs/2109.04173. arXiv: 2109.04173. 1, 24\nK. Zhang, M. Gong, and B. Schölkopf.\nMulti-source domain adaptation: A causal view.\nIn\nProceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 3150–3157. AAAI\nPress, 2015. URL http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/\nview/10052/9994. 10\n16\nPublished as a conference paper at ICLR 2025\nKun Zhang and Aapo Hyvarinen.\nOn the Identifiability of the Post-Nonlinear Causal Model.\narXiv:1205.2599 [cs, stat], May 2012. URL http://arxiv.org/abs/1205.2599. arXiv:\n1205.2599. 24\nXun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing.\nDAGs with NO TEARS:\nContinuous Optimization for Structure Learning. March 2018. URL https://arxiv.org/\nabs/1803.01422v2. 24, 26\nRoland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel.\nContrastive Learning Inverts the Data Generating Process. arXiv:2102.08850 [cs], February 2021.\nURL http://arxiv.org/abs/2102.08850. arXiv: 2102.08850. 1, 6, 24, 25\n17\nPublished as a conference paper at ICLR 2025\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nθ1\nθ2\nO1\nψ\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\nO2\n(a) Identifiable Exchangeable Mechanisms\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nθ1\nθ2\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\n(b) Causal Discovery\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nS2\n1\nS2\n2\nO2\n(c) Independent Component Analysis\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\nO2\n(d) Causal Representation Learning\nFigure 4: Identifiable Exchangeable Mechanisms (IEM)–A unified model for structure and\nrepresentation identifiability: Here we show that exchangeable but non-i.i.d. data enables iden-\ntification in key methods across Causal Discovery (CD), Independent Component Analysis (ICA),\nand Causal Representation Learning (CRL). The graphical model in Fig. 1a shows the IEM model,\nwhich subsumes Causal Discovery (CD) (§ 3.2), Independent Component Analysis (ICA) (§ 3.3),\nand Causal Representation Learning (CRL) (§ 3.4). S denotes latent, Z causal, and O observed\nvariables with corresponding latent parameters θ, ψ, superscripts denote different samples. Red\ndenotes observed/known quantities, blue stands for target quantities, and gray illustrates components\nthat are not explicitly modeled in a particular paradigm. θi are latent variables controlling sepa-\nrate probabilistic mechanisms, indicated by dotted vertical lines. CD (Fig. 1b) corresponds to the\nleft-most layer of IEM, focusing on the study of cause-effect relationships between observed causal\nvariables; ICA (Fig. 1c) infers source variables from observations, but without causal connections in\nthe left-most layer of IEM; CRL (Fig. 1d) shares the most similar structure with IEM, as it has both\nlayers, including the intermediate causal representations\n18\nPublished as a conference paper at ICLR 2025\nA\nPROOFS AND EXTENDED THEORY\nA.1\nCAUSE/MECHANISM VARIABILITY FOR BIVARIATE CD: THM. 2\nTheorem 2. [Cause/mechanism variability is necessary and sufficient for bivariate CD] Given a\nsequence of bivariate pairs {Xn, Y n}n∈N such that for any N ∈N, the joint distribution can be\nrepresented as:\n• X →Y : p(x1, y1, . . . , xN, yN) =\nR\nθ\nR\nψ\nQ\nn p(yn|xn, ψ)p(xn|θ)p(θ)p(ψ)dθdψ\n• X ←Y : p(x1, y1, . . . , xN, yN) =\nR\nθ\nR\nψ\nQ\nn p(xn|yn, θ)p(yn|ψ)p(ψ)p(θ)dθdψ\nThen the causal direction between variables X, Y can still be distinguished when:\n1. either only p(θ) = δθ0(θ) for some constant θ0 or only p(ψ) = δψ0(ψ) for some constant\nψ0 (but not both). Fig. 2b and Fig. 2c show the Markov structure of such factorizations.\n2. the distribution of P is faithful (Defn. 4) w.r.t. Fig. 2b or Fig. 2c.\nProof. The impossibility of both mechanisms being degenerate (i.e., the i.i.d. case) is well-known\n(Pearl, 2009a). For distributions that are Markov and faithful to Fig. 2a, Fig. 2b and Fig. 2c, one can\ndifferentiate the causal direction through checking Y 1 ⊥X2|X1, X1 ⊥Y 2|Y 1 and X1 ⊥Y 1. One\ncan observe Y 1 ⊥X2|X1 only holds in Fig. 5a and fails at Fig. 5b.\nX1\nY 1\nθ\nψ\nX2\nY 2\nX1\nY 1\nθ\nψ\nX2\nY 2\nX2 ̸⊥Y 1|Y 2\nX1 ⊥Y 2|X2\nX2 ⊥Y 1|Y 2\nX1 ̸⊥Y 2|X2\n(a) Cause variability\nX1\nY 1\nθ\nψ\nX2\nY 2\nX1\nY 1\nθ\nψ\nX2\nY 2\nY 1 ⊥X2|X1\nX1 ̸⊥Y 2|Y 1\nY 1 ̸⊥X2|X1\nX1 ⊥Y 2|Y 1\n(b) Mechanism variability\nFigure 5: We show that the richness argument in CdF (Guo et al., 2024a) can be realized, in the\nbivariate case, via either only varying the prior of the causes’ parameters θ (Fig. 5a) or the prior of\nthe mechanism’ parameters ψ (Fig. 5b). That is, it is not necessary to have rich priors for both θ, ψ\nA.2\nEXCHANGEABILITY IN TCL: LEM. 3\nLemma 3. [TCL is identifiable due to exchangeable non–i.i.d. sources] The sufficient variability\ncondition in TCL corresponds to cause variability, i.e., exchangeable non–i.i.d. source variables with\na fixed mixing function, which leads to the identifiability of the latent sources.\nProof. Latent variables violating the sufficient variability (Assum. 2) condition in nonlinear ICA\nimply that those variables are i.i.d.; thus, if more than one latent variables violate this condition, then\nthey become non-identifiable; thus, making non-delta priors a necessary condition for identifiability\nof factorizing priors (assuming that no further constraints can be applied, e.g., on the function class).\nAn important fact for the proof is that the parameters θi are continuous RVs, as they parametrize an\nexponential family distribution. That is, their support has infinitely many distinct values, each with a\nprobability of zero—this will be important to reason about the zero-measure of edge cases where two\nparameters happen to be “tuned\" to each other, violating the sufficient variability condition.\nThe full rank condition of the matrix L means that ∀i ̸= j environment indices for two rows in matrix\nL\nθi −θ0 ̸= c ·\n\u0000θj −θ0\u0001\n; c > 0\n(12)\n⇐= : Non-delta priors imply sufficient variability. Assume a real-valued RV θi with corresponding\nnon-delta parameter prior p(θi). Then, outside of a zero-measure set, the sufficient variability\ncondition holds. Assume that ∀i : p(θi) ̸= δ(θi −θi0). Then, as all θi are real RVs, their support has\n19\nPublished as a conference paper at ICLR 2025\ninfinitely many values. Thus, the probability of (12) being violated (by setting both sides to be equal\nand solving for θi) is zero:\nPr\n\u0002\nθi = c · θj −(c −1)θ0\u0003\n= 0,\n(13)\nfrom which it follows that there exists such θi, θj where (12) holds, implying that L has full rank.\nThus, Assum. 2 holds.\n=⇒: Sufficient variability implies non-delta priors. When rank (L) = n, then none of p(θi) is\ndelta. rankL = n = dim s means that ∀i ̸= j environment indices for two rows in matrix L (12)\nholds. That is, we can construct L such that any two rows are linearly independent3. If for coordinate\nk, θe\nk = θk = const, then L cannot have full column rank. Since θk cannot be constant for all k, this\nrequires that p(θk) is non-delta.\nA.3\nEXCHANGEABILITY IN GCL (HYVARINEN ET AL., 2019): EXTENDING LEM. 3\nHyvarinen et al. (2019) proposed a generalization of TCL (and other auxiliary-variable ICA methods),\ncalled GCL. GCL uses a more general conditional distribution, it only assumes that assumes that the\nconditional log-density log p(s|u) is a sum of components qi(si, u):\nlog p(s|u) =\nX\ni\nqi(si, u)\n(14)\nFor this generalized model, they define the following variability condition:\nAssumption 4 (Assumption of Variability). For any y ∈Rn (used as a drop-in replacement for the\nsources s), there exist 2n + 1 values for the auxiliary variable u, denoted by uj, j = 0 . . . 2n such\nthat the 2n vectors in R2n given by\n(w (y, u1) −w (y, u0)) , (w (y, u2) −w (y, u0)) . . . , (w (y, u2n) −w (y, u0))\nwith\nw(y, u) =\n\u0012∂q1 (y1, u)\n∂y1\n, . . . , ∂qn (yn, u)\n∂yn\n,\n∂2q1 (y1, u)\n∂y2\n1\n, . . . , ∂2qn (yn, u)\n∂y2n\n\u0013\nare linearly independent.\nAssum. 4 puts a constraint on the components of the first- and second derivatives of the functions\nconstituting the conditional log-density of the source/latent variables, conditioned on the auxiliary\nvariable u. As the authors write: “[Assum. 4] is basically saying that the auxiliary variable must\nhave a sufficiently strong and diverse effect on the distributions of the independent components.\"\nWe focus on a special case, which assumes that the source conditional log-densities qi(si, u) are\nconditionally exponential, i.e.:\nqi (si, u) =\nk\nX\nj=1\n[˜qij (si) θij(u)] −log Ni(u) + log Qi(si),\n(15)\nwhere k is the order of the exponential family, Ni is the normalizing constant, Qi the base measure,\n˜qi is the sufficient statistics, and the modulation parameters θi := θi(u) depend on u. In this case,\nAssum. 4 becomes similar to Assum. 2, but the modulation parameter matrix now has (E −1) × nk\ndimensions, where the rows are:\n[L]j: = (θj −θ0)⊤\n(16)\nθj =\nh\nθj\n11, . . . , θj\nnk\ni\n.\n(17)\nIn this case, we can generalize Lem. 3 to GCL:\nCorollary 1 (GCL with conditionally exponential family sources is identifiable due to exchangeable\nnon–i.i.d. sources). The sufficient variability condition in GCL with conditionally exponential family\nsources corresponds to cause variability, i.e., exchangeable non–i.i.d. source variables with fixed\nmixing function, which leads to the identifiability of the latent sources.\nProof. The proof follows from the proof of Lem. 3, the only difference is that for each source si,\nthere are k sufficient statistics ˜qik and modulation parameters θik(u). Thus, the modulation parameter\nmatrix L (Assum. 2) will be [(E −1) × nk]−dimensional where n = dim s.\n3Note that θi can be correlated, as Hyvarinen et al. (2019) pointed out in the proof of their Thm. 2\n20\nPublished as a conference paper at ICLR 2025\nA.4\nDUALITY OF CAUSE AND MECHANISM VARIABILITY FOR TCL: LEM. 4\nLemma 4. [Duality of cause and mechanism variability for TCL] For a given deterministic mix-\ning function f : s →o and conditionally factorizing (non-stationary) latent sources p(s|u) =\nQ\ni pi(si|u) fulfilling the sufficient variability of TCL, there exists an equivalent setup with stationary\n(i.i.d.) sources p(s) = Q\ni pi(si) with stochastic functions ˆf = f ◦g : s →o, where g = g(u) and\neach component gi is defined as an element-wise function such that the pushforward of pi(si) by gi\nequals pi(si|u), i.e., gi∗pi(si) = pi(si|u). Then, gi∗pi(si) fulfils the same variability condition; thus,\nthe same identifiability result applies.\nProof. The proof follows from the observation that it is a modelling choice which component provides\nthe source of non-stationarity. That is, we can incorporate the transformation of the source variables\ninto the source distribution (cause variability, as TCL does) or we can think of that as stochasticity in\nthe mixing function (mechanism variability).\nA.5\nEXCHANGEABILITY IN CAUCA: LEM. 5\nLemma 5. [Non-delta priors in the causal mechanisms can enable identifiable CRL]\nIf the interventional discrepancy condition Assum. 3 holds, then the parameter priors in (10) cannot\nequal a delta distribution, i.e., p(θj) ̸= δθj0(θj); thus, if the other conditions of CauCA hold, then,\nthe causal variables zi are identifiable. For real-valued θj, non-delta priors also imply Assum. 3\nalmost everywhere.\nProof. We define each mechanism p(zi|Pa(zi)) as\np(zi|Pa(zi)) =\nZ\nθi\np(zi|Pa(zi), θi)p(θi)dθi\n(18)\nThus, the observational and interventional mechanisms, respectively, are:\npi = p(zi|Pa(zi), θi = θ0\ni )\n(19)\n˜pi = p(zi|Pa(zi), θi = ˜θi)\n(20)\nThat is, each intervention corresponds to a specific parameter value θi (which exist by the CdF\ntheorem (Thm. 1). Thus, (18) is akin to mixtures of interventions (Richens & Everitt, 2024, Defn. 3).\n=⇒: Interventional discrepancy implies non-delta priors.\nWe prove this direction by contradiction. Assume that Assum. 3 is fulfilled and p(θi) = δθi0. Then\n˜θi = θi, so Assum. 3 cannot hold.\n⇐= : Non-delta priors for real-valued parameters imply interventional discrepancy.\nIf θi is real-valued, then the following probability is zero:\nPr\nh\n˜θi = θ0\ni\ni\n= 0,\n(21)\nthus, there must exist ˜θi ̸= θ0\ni , and this inequality holds almost everywhere since:\nPr\nh\n˜θi ̸= θ0\ni\ni\n= 1 −Pr\nh\n˜θi = θ0\ni\ni\n= 1 −0 = 1,\n(22)\nRemark 1 (Non-delta priors do not always imply interventional discrepancy almost everywhere).\nWhen the parameter priors p(θi) are not a delta distribution, then, barring the case when sampling\nthe interventional mechanism parameter ˜θi yields the same θ0\ni , then the distributions pi and ˜pi\nwould differ. However, this is not necessarily a zero-measure event, e.g., when p(θ) has a Bernoulli\ndistribution with parameter 1/2. Thus, Assum. 3 cannot hold almost everywhere without further\nrestrictions.\nA.6\nEXCHANGEABILITY AND THE GENERICITY CONDITION FROM VON KÜGELGEN ET AL.\n(2023) AND JIN & SYRGKANIS (2023)\nvon Kügelgen et al. (2023) extends CauCA Wendong et al. (2023) by providing identifiability proofs\nfor CRL without parametric assumptions on the function class. Their assumption (von Kügelgen et al.,\n2023, (A3’) in Thm. 3.4) is stronger than Assum. 3 as it requires that two interventional densities\ndiffer everywhere—or that the observational and one interventional density differ (von Kügelgen\n21\nPublished as a conference paper at ICLR 2025\net al., 2023, (A3) in Thm. 3.2). Furthermore, (von Kügelgen et al., 2023, (A4) in Thm. 3.2) excludes\nthe pathogological case of fine-tuned densities (thus the name genericity condition)—this might be\nthought to be an analog of faithfulness Defn. 4.\nJin & Syrgkanis (2023) leverages a similar assumption as Assum. 3 in their (Jin & Syrgkanis, 2023,\nDef. 6). They strengthen the nonparametric identifiability results of von Kügelgen et al. (2023) by\nshowing that dim Z single-node soft interventions with unknown targets are sufficient to identify the\ncausal variables.\nTowards identifying the exogenous variables in CRL.\nBoth von Kügelgen et al. (2023); Jin\n& Syrgkanis (2023) derive identifiability results for the causal variables from interventional data.\nHowever, they do not make claims about the exogenous variables. Based on the insights of our unified\nmodel, IEM, provides, we investigate whether there is an identifiability proof that encompasses the\nwhole hierarchy.\nConsider that the mechanism p(Zi|Pa(Zi)) can be intervened upon by changing how the other causal\nvariables Zj ∈Pa(Zi) affect Zi. Alternatively, p(Zi|Pa(Zi)) can also change by modifying the\ndistribution of the corresponding exogenous variable Si—note that this corresponds to a one-node\nsoft intervention. Thus, if the other assumptions of Jin & Syrgkanis (2023) holds, we can say that:\nsingle-node soft interventions on Si can satisfy the genericity condition of Jin & Syrgkanis (2023).\nTo reason about the identifiability of the exogenous variables, we need the variability of their\ndistribution across the available environments. Our summary of the assumptions in Tab. 1 suggests\nthat with sufficiently many environments, it should be possible to identify the exogenous variables as\nwell. If the only assumption we make is exchangeability, then, following the reasoning of Khemakhem\net al. (2020b), we might need dim Z + 1 additional environments (2 dim Z + 1 in total). If we further\nrestrict the source distributions to belong to the exponential family, then we can apply TCL (and\nlinear ICA on top) to identify the source variables. Thus, we can state:\nLemma 6. [Simultaneous identifiability via generic non-degenerate source priors] Provided the\nassumptions of (Jin & Syrgkanis, 2023, Thm. 4) hold with the restriction of the source variables’ den-\nsity belonging to the exponential family of order one, and assuming that the nonparametric structural\nassignments are generic such that single-node soft interventions on each Si satisfy Assum. 3, then\n(dim Z+1) interventions can provide exchangeable data sufficient for the simultaneous identification\nof both exogenous and causal variables (and also the DAG)—as opposed to (2 dim Z + 1), where\ndim Z separate environments are used for CRL and another (dim Z + 1) for ICA.\nProof. Our goal is to prove that performing CRL can lead to the additional identifiability of the\nexogenous sources, with a negligible overhead in terms of assumptions on the data, compared to\nperforming only CRL. Also, we aim to show that the joint identifiability requires less data (less\nenvironments) than performing both tasks separately. We start by assuming that (Jin & Syrgkanis,\n2023, Thm. 4) holds, which implies Assum. 3. By Lem. 5, we know that when Assum. 3 holds,\nthen the parameter priors are non-degenerate. We assumed that the single-node soft interventions\nonly affect Si. Following the reasonings of von Kügelgen et al. (2023); Jin & Syrgkanis (2023),\nw.l.o.g., if the structural assignments in the nonparametric SEM are not fine-tuned (i.e., they are\ngeneric), then Assum. 3 should hold. Then, as the source distributions are exchangeable, we can\napply Lem. 3, which states that Assum. 2 also holds. Thus, we can identify the exogenous variables\nas well, concluding the proof.\nWe leave it to future work to investigate whether identifying both causal and exogenous variables is\npossible from fewer environments. Nonetheless, we believe that this example shows the potential\nadvantage of the IEM framework for providing new identifiability results.\nA.7\nEXCHANGEABILITY IN THE UNIFIED MODEL: LEM. 1\nInterventional discrepancy and the derivative condition of (Liu et al., 2024)\nThe identifiability\nresult of Liu et al. (2024) combines the results from ICA and CRL. As they use TCL to learn the latent\nsources, we can apply Lem. 3. To see how the causal variables and the edges between them can also\nbe learned, we first relate the derivative condition on the structural assignments to the interventional\ndiscrepancy condition of Wendong et al. (2023) (Assum. 3).\nAssum. 1 requires access to a set of environments (indexed by auxiliary variable u), such that for\neach parent zj ∈Pa(zi) node, there is an environment, where the edge zj →zi is blocked. To relate\nAssum. 1 to the interventional discrepancy Assum. 3, we recall that Wendong et al. (2023) note that\nfor perfect interventions, the conditioning on the parents for the interventional density in Assum. 3\ndisappears. Thus, we interpret Assum. 1 as “emulating\" perfect interventions for each zi. By this, we\n22\nPublished as a conference paper at ICLR 2025\nmean that we need data from such environments, where the structural assignments change as if a\nperfect intervention is carried out to remove the zj →zi edge.\nLemma 1. [Identifiable Latent Neural Causal Models are identifiable with exchangeable sources\nand mechanisms] The model of Liu et al. (2024) (Fig. 1a) identifies both the latent sources s and the\ncausal variables z (including the graph), by the variability of s via a non-delta prior over θs and by\nthe variability of the structural assignments via θg.\nProof. As the authors rely on TCL and a form of the interventional discrepancy Assum. 3, the proof\nfollows from Lem. 3 and Lem. 5.\nA.8\nINDEPENDENT SOURCE AND STRUCTURAL ASSIGNMENT CDF PARAMETERS IN ANMS:\nLEM. 2\nLemma 2. [Independent source and structural assignment CdF parameters for ANMs] In the setting\nof Liu et al. (2024), where the SEM is an ANM, the CdF parameters for the sources, θs, and the\nstructural assignments, θg, are independent, i.e. p(θg, θs) = p(θg)p(θs).\nProof. In the model of Liu et al. (2024), the two identifiability results impose two non-i.i.d. require-\nments: to identify the latent sources, a sufficient variability condition from TCL is required (cf. the\ngeneralized version in Assum. 4), whereas for CRL, a derivative-based condition on the mechanisms\n(akin to Assum. 3) is required. As the SEM is an ANM in this case, defined by zi := gi(Pa(zi)) + si,\nand the exogenous variables are assumed to be independent from gi(Pa(zi)). Thus, it is impossible\n(assuming faithfulness) that a change in θs\ni would change θg\ni ; otherwise, gi(Pa(zi)) and si would be\ndependent. That is, their parameters are independent.\nB\nDEFINITIONS\nDefinition 2 (d-separation (adapted from Defn. 6.1 in (Peters et al., 2018))). Given a DAG G, the\ndisjoint subsets of nodes A and B are d-separated by a third (also disjoint) subset S if every path\nbetween nodes in A and B is blocked by S. We then write\nA ⊥G B | S.\nDefinition 3 (Global Markov property (adapted from Defn. 6.21(i) in (Peters et al., 2018))). Given a\nDAG G and a joint distribution P, P satisfies the global Markov property w.r.t. the G if\nA ⊥G B|C ⇒A ⊥B|C\n(23)\nfor all disjoint vertex sets A, B, C (the symbol ⊥G denotes d-separation, cf. Defn. 2).\nDefinition 4 (Faithfulness (adapted from Defn. 6.33 in (Peters et al., 2018))). Consider a distribution\nP and a DAG G. Then, P is faithful to G if for all disjoint node sets A, B, C:\nA ⊥B |C ⇒A ⊥G B| C.\n(24)\nThat is, if a conditional independence relationship holds in P, then the corresponding node sets are\nd-separated in G.\nDefinition 5 (Markov equivalence class of graphs (adapted from Defn. 6.24 in (Peters et al., 2018)).\nWe denote by M(G) the set of distributions that are Markovian w.r.t. G : M(G) := {P : P\nsatisfies the global Markov property w.r.t. G}. Two DAGs G1 and G2 are Markov equivalent if\nM (G1) = M (G2), i.e.. if and only if G1 and G2 satisfy the same set of d-separations, which means\nthe Markov condition entails the same set of (conditional) independence conditions. The set of all\nDAGs that are Markov equivalent to some DAG is called Markov equivalence class of G.\nC\nWHY DOES I.I.D. DATA FAIL?\nWe next assay key results and provide concrete examples to illustrate why i.i.d. data fails to enable\nidentification for both structure and representation learning.\nExample 2 (Bivariate CD is impossible from i.i.d. data (Pearl, 2009b)). One cannot distinguish\nX →Y from X ←Y from i.i.d. data as both structures imply identical graphical conditional\nindependence, i.e., ∅. Thus, bivariate CD is impossible in i.i.d. data without further parametric\nassumptions.\nLearning disentangled latent factors is also impossible without further parametric assumptions in i.i.d.\ndata (Hyvärinen & Pajunen, 1999; Locatello et al., 2019):\n23\nPublished as a conference paper at ICLR 2025\nExample 3 (Gaussian latent factors are not identifiable from i.i.d. data). Assume independent\nlatents with Gaussian components, i.e. p(s) = Q\ni pi(si), where pi(si) = N\n\u0000µi; σ2\ni\n\u0001\n. Even if\n∀i, j : σ2\ni ̸= σ2\nj̸=i, Gaussian sources are not identifiable to their rotational symmetry and the\nscale-invariance of ICA.\nWe show in § 3 how exchangeability unifies the non-i.i.d. conditions (often termed, weak supervision\nor auxiliary information) in many causal structure and representation identifiability methods.\nD\nRELATED WORK\nIdentifiable representation learning and ICA.\nIdentifiable representation learning aims to learn\n(low-dimensional) latent variable models (LVMs) from (high-dimensional) observations. The most\nprevalent family of models is that of Independent Component Analysis (ICA) (Comon, 1994; Hy-\nvarinen et al., 2001), which assumes that the observations are a mixture of independent variables\nvia a deterministic mixing function. Identifiability means that the latents can be can be recovered\nup to indeterminacies (e.g., permutation, element-wise transformations). As this is provably im-\npossible in the nonlinear case without further assumptions (Darmois, 1951; Hyvärinen & Pajunen,\n1999; Locatello et al., 2019), recent work has focused auxiliary-variable ICA, where the latents\nare conditionally independent given the auxiliary variable u (Hyvarinen et al., 2019; Gresele et al.,\n2019; Khemakhem et al., 2020a; Hälvä et al., 2021; Hyvarinen & Morioka, 2017; 2016; Hälvä &\nHyvärinen, 2020; Morioka et al., 2021; Monti et al., 2020; Hyvarinen et al., 2010; Klindt et al., 2021;\nZimmermann et al., 2021)—despite the latents are not marginally independent, the literature still\nrefers to these models are ICA. Several such methods model multiple environments with an auxiliary\nvariable, which is also known as using ensembles (Eastwood et al., 2023; Kirchhof et al., 2023).\nWe note that some methods make functional assumptions (Shimizu et al., 2006; Hoyer et al., 2008;\nZhang & Hyvarinen, 2012; Gresele et al., 2021), but our focus is on the auxiliary-variable methods.\nRecently, Bizeul et al. (2024) developed a probabilistic model for self-supervised representation\nlearning, including auxiliary-variable ICA methods.\nCausality.\nSEMs model cause-effect relationships between causal variables zi, where each zi is\ndetermined by a deterministic function gi(Pa(zi)), where Pa(zi) includes all the zj<i causal variables\nthat cause zi and also the stochastic exogenous variable xi. Learning causal models enables to make\nmore fine-grained (interventional, counterfactual) queries compared to observational data (Pearl,\n2009b). CD aims to uncover the graph between the zi from observing zi. This admits interventional\nqueries. CRL also learns that zi from high-dimensional observations (Schölkopf et al., 2021). Causal\nmethods need to rely on certain assumptions, either restricting the distribution of the exogenous\nvariables (Kalainathan et al., 2020; Lachapelle et al., 2020; Shimizu et al., 2006; Monti et al., 2020),\nand/or the function class of the SEM (Shimizu et al., 2006; Zheng et al., 2018; Squires et al., 2023;\nMontagna et al., 2023b;a; Gresele et al., 2021; Hoyer et al., 2008; Ng et al., 2020; Lachapelle et al.,\n2020; Annadani et al., 2021; Yang et al., 2021). Concurrently with our work, Yao et al. (2024)\nprovides an invariance-based framework to unify CRL. Their framework can encompass multi-view,\nmulti-environment, and also temporal settings—ouw work focuses on the multi-environment case,\nbut it also includes representation learning and CD.\nConnections between representation learning and causality.\nCausality and identifiability both\naim to recover some ground truth structures (latent factors, DAGs, or functional relationships), thus,\nseveral works explored possible connections (Reizinger et al., 2023; Morioka & Hyvarinen, 2023;\nHyvärinen et al., 2023; Zeˇcevi´c et al., 2021; Richens & Everitt, 2024; Monti et al., 2020). Several\nmethods connected ICA to the SEM model in causality (Gresele et al., 2021; Monti et al., 2020;\nShimizu et al., 2006; von Kügelgen et al., 2021; Hyvärinen et al., 2023). An important observation\nwe rely on is that identifiability guarantees from require a notion of non-i.i.d.ness, e.g., both the ICA\nand the causal literature often relies on the multi-environmental setting.\n24\nPublished as a conference paper at ICLR 2025\nTable 1: Mixing assumptions: p(s) stands for assumptions on the source distribution, f on the\nmixing function, ⊥stands for independence (the superscript u denotes conditional independence\ngiven u), CEF for conditional exponential family (the superscript + denotes monotonicity, 2 denotes\na CEF of order two), ING for independent non-Gaussian (in Jin & Syrgkanis (2023) maximum one\nGaussian component allowed, the distributions need to be different; in Zimmermann et al. (2021),\nthe Lα metric is such that α ≥1, α ̸= 2), exg. stands for exchangeability, AG for an anistropic\nGaussian on the hypersphere, EnAG for an ensemble of such Gaussians, inj. for injectivity, surj. for\nsubjectivity, C2 for diffeomorphism; SEM assumptions: Z stands for assumptions on the causal\nvariables, g on the SEM, M denotes the Markov assumption, F faithfulness (or lack thereof), NP\nstands for non-parametric; Interventional (variability) assumptions: # denotes the number of\nnodes affected by the intervention, P/S denotes perfect or soft interventions, the target column\nwhether the intervention targets are known, |E| stands for the number of environments (d = dim Z),\nk is the order of the exponential family; Identifiability ambiguities: DAG denotes identifiability\nof the causal graph (✓means the DAG is known; ✓’s come from the result of Reizinger et al.\n(2023)), h denotes identifiability up to elementwise (non-)linear transformations, D denotes scaling,\nπ permutations, c a constant shift, O an orthogonal, {O} a block-orthogonal, A an invertible matrix.\nMixing\nSEM\nInterventions\nIdent. Z\nIdent. S\nMethod\nS\nf\nZ\ng\n# type target\n|E|\nDAG h D π c\nA\nh D π c\nGuo et al. (2024a)\nF\nexg.\n0\n✓\nHyvarinen & Morioka (2016) CEF\nC2\n-\nS\n✗\nd+1\n✓\n✓✓✓✓\nHyvarinen & Morioka (2016) CEF+\nC2\n-\nS\n✗\nd+1\n✓\n✗\n✓✓✓✓\nHyvarinen et al. (2019)\n⊥u\nC2\n-\nS\n✗\n2d+1\n✓\n✗\n✓✓✓✗\nHyvarinen et al. (2019)\nCEF\nC2\n-\nS\n✗\ndk+1\n✓\n✓✓✓✓\nZimmermann et al. (2021)\nvMF\nC2\nd\nS\n✗\n1\nO\n✗✗✓✗\nZimmermann et al. (2021)\nR\nC2\nd\nS\n✗\n1\n✓\n✗✗✓✓\nZimmermann et al. (2021)\nING\nC2\nd\nS\n✗\n1\n✓\n✗\n✗✓✓✗\nRusak et al. (2024)\nAG\nC2\nd\nS\n✗\n1\n{O} ✗✗✓✗\nRusak et al. (2024)\nEnAG\nC2\n-\nS\n✗\n1<\n✓\n✗\n✗✗✓✗\nKhemakhem et al. (2020a)\nCEF2\ninj.\n-\nS\n✗\ndk+1\n✓\n✗\n✓✓✓✓\nKhemakhem et al. (2020b)\nexg.\nsurj.4\n-\nS\n✗\n2d+1\n✓\n✗✓✓✓\nKhemakhem et al. (2020b)\nexg.\nsurj.+\n-\nS\n✗\n2d+1\n✓\n✗\n✗✓✓✓\nKhemakhem et al. (2020b)\nexg.\nsurj.2\n-\nS\n✗\n2d+1\n✓\n✗\n✗✓✓✓\nReizinger et al. (2023)\n✓\n✗\n✓✓✓✓\nWendong et al. (2023)\n⊥\nC2\nM\n1\nS\n✓\nd\n✓\n✓✓✗✓\nWendong et al. (2023)\n⊥\nC2\nM\n1\nP\n✓\nd\n✓\n✗✓✗✗\nvon Kügelgen et al. (2023)\n⊥\nC2\nF\nNP\n1\nP\n✗\n2d\n✓\n✓✓✓✓\nJin & Syrgkanis (2023)\nING\nC2\n̸ F\nlin\n1\nS\n✗\nd2\n✓\n✓✓✓✓\nJin & Syrgkanis (2023)\nING\nC2\n̸ F\nlin\n-\nS\n✗\nd\n✓\n✓✓✓✓\nJin & Syrgkanis (2023)\n⊥\nC2\n̸ F\nNP\n1\nS\n✗\nd\n✓\n✓✓✓✓\nLiu et al. (2024)\nCEF2\ninj.\nF ANM 1\nP\n✗\n2d+1\n✓\n✗✓✗✓\n✗\n✗✓✓✓\nE\nINTUITION AND EXAMPLES FOR CAUSE AND MECHANISM VARIABILITY\nThe Sparse Mechanism Shift hypothesis motivates cause and mechanism variability.\nIn § 3,\nwe relaxed exchangeability into cause and mechanism variability. In this section, we show that both\ncause and mechanism variability can be used to describe many real-world scenarios. Intuitively,\nCause and mechanism variability can be seen as particular realizations of the Sparse Mechanism\nShift (SMS) hypothesis (Perry et al., 2022).\nThe SMS posits that the causal mechanisms (the factors in the causal Markov factorization) tend to\nchange sparsely, i.e., interventions or distribution shifts can be described by changing a (strict) subset\nof mechanisms. This is one main argument for the efficiency of causal modelling, as the modularity\nimplies that only parts of the model need to be adapted in case of a distribution shift—in contrast to a\nnon-causal factorization, where the whole learned model needs to be fine-tuned.\n4In ICE-BeeM (Khemakhem et al., 2020b), the assumption is on the feature extractor\n25\nPublished as a conference paper at ICLR 2025\nIndeed, the SMS hypothesis captures the reasoning behind many works in causality (Gendron et al.,\n2023; Perry et al., 2022; Lachapelle et al., 2021b; 2022; Schölkopf et al., 2012; Lachapelle et al.,\n2021a; Ahuja et al., 2022b). Sparse changes have been also connected to causal modeling (Rajendran\net al., 2023; von Kügelgen et al., 2021; Fumero et al., 2023; Mansouri et al., 2023; Ahuja et al.,\n2022a).\nE.1\nREAL-WORLD EXAMPLES\nIn this section, we draw on prior works to provide real-world examples of cause and mechanism\nvariability—for examples of the exchangeable case, we refer the reader to (Guo et al., 2024a). As\nwith any model, we will make certain simplifications, though we aim to convey that the principle of\ncause and mechanism variability still applies. We will restrict ourselves to the bivariate case, as in\nFig. 5. The causal factorization for X →Y is p(Y |X, ψ)p(X|θ), where the CdF parameters are θ, ψ.\nCause variability means that p(ψ) is a delta distribution, whereas mechanism variability means that\np(θ) is a delta distribution.\nE.1.1\nCAUSE VARIABILITY.\nExample 4 (Lung cancer). Assume that θ parametrizes the lifestyle, socioeconomic, and environmen-\ntal factors of people, whereas ψ parametrizes how lung cancer develops. In this case, we can assume\nthat p(X|θ) differs across cities, whereas the mechanism for developing lung cancer, p(Y |X, ψ) is\nthe same. That is, only p(ψ) is a delta distribution.\nExample 5 (Altitude and temperature). Assume that θ parametrizes the altitude distribution of\ncountries, whereas ψ parametrizes how altitude affects temperature. In this case, we can assume that\np(X|θ) differs across countries, whereas the effect of altitude on temperature p(Y |X, ψ) is the same.\nThat is, only p(ψ) is a delta distribution.\nE.1.2\nMECHANISM VARIABILITY\nExample 6 (Natural experiments). In natural experiments in economics (Angrist & Krueger, 1991;\nImbens & Angrist, 1994), it is possible to select two populations such that we can assume that their\ndistributions are the same, i.e., the corresponding θ parameter has a delta distribution, whereas the\neconomic situation, parametrized by ψ, differs, e.g., by the two cities having different local taxes.\nExample 7 (Medical diagnoses). Assume that several people having the same lifestyle, socioeconomic,\nand environmental status are admitted to the same hospital after food poisoning at a local restaurant.\nThen, the probability distribution describing the symptoms, parametrized by θ, will have a delta\nprior, as each person suffers from the same disease. If we assume that multiple doctors are required\nto diagnose and treat all patients, then we can posit that there will be (slight) differences in their\ndecisions and prescribed treatments, which means that the corresponding parametric mechanism\np(Y |X, ψ) for the treatment has a non-delta prior for ψ.\nF\nEXPERIMENTAL RESULTS: CAUSE AND MECHANISM VARIABILITY FOR\nCAUSAL DISCOVERY\nSetup.\nTo demonstrate that both cause and mechanism variability enable causal structure iden-\ntification, we ran synthetic experiments based on the publicly available repository of the Causal\nde Finetti paper5. We focus on the continuous case, as problems can arise for discrete RVs (e.g.,\nin Lem. 5)—i.e., we follow the protocol described in the “Bivariate Causal Discovery\" paragraph\nin (Guo et al., 2024a, Sec. 6). The continuous experiments used in the original CdF paper consider the\nbivariate case, which we follow to be comparable. The only change in the evaluation protocol is not\nevaluating the CD-NOD method (Huang et al., 2017), as we do not have access to a MatLab license.\nThat is, we compare against FCI (Spirtes et al., 2013), GES (Chickering, 2002), NOTEARS (Zheng\net al., 2018), DirectLinGAM (Shimizu et al., 2011), plus a random baseline.\nFollowing (Guo et al., 2024a, Sec. 6), we describe the DGP in detail. The CdF parameters N = [ψ, θ]\nwere randomly generated with distinct and independent elements in each environment. Samples\nwithin each environment have the noise variables S generated via Laplace distributions conditioned\non the corresponding CdF parameters—i.e., the CdF parameter is the location (mean) of the Laplace\ndistribution. We observe a bivariate vector X = [X1, X2] ∈R2 and aim to uncover the causal\ndirection between X1 and X2. Let the superscript (·)e denote variables contained in environment e.\n5https://github.com/syguo96/Causal-de-Finetti. Our code is available at https://\ngithub.com/rpatrik96/IEM\n26\nPublished as a conference paper at ICLR 2025\n(a) Causal de Finetti\n(b) Cause variability\n(c) Mechanism variability\nFigure 6: Bivariate causal discovery is possible with cause and mechanism variability: Com-\nparison of the CdF protocol with FCI, GES, NOTEARS, DirectLinGAM, and a random baseline for\ncausal structure discovery in the bivariate case with continuous random variables. The proportion of\ncorrectly identified causal structures is shown against a different number of environments, chosen\nfrom {100, 200, 300, 400, 500}. Shading shows the standard deviation across 100 seeds. (a): the\noriginal CdF setting, reproducing (Guo et al., 2024a, Fig. 3(a)) with non-delta priors for both CdF\nparameters; (b): cause variability with a delta parameter prior for the effect-given-the-cause parameter\nψ; (c): mechanism variability with a delta parameter prior for the cause parameter θ; For details, cf.\nAppx. F\nThen, the data is generated as follows:\nNe ∼Uniform[−1, 1]\n(25)\nSe ∼Laplace(N, 1)\n(26)\nXe = AeSe + Be (Ne)o2 1nonlinear (e),\n(27)\nwhere ◦2 denotes elementwise squaring. Ae ∈R2×2 is a randomly sampled triangular matrix and\nBe = Ae −I, where I is the identity matrix. The causal direction is randomly sampled from\nX1 →X2, X2 →X1, X1 ⊥X2—this ensures that A is either a lower triangular, upper triangular or\ndiagonal matrix. 1nonlinear (e) is an environment-dependent, randomly sampled nonlinear-dependence\nindicator, which models the realistic scenario of invariant causal structure but changing functional\nrelationships.\nWe implement cause and mechanism variability in the above synthetic DGP, which by changing\nthe scm_bivariate_continuous function in the original GitHub repository. There, we set\nthe noise variables for the delta-distributed CdF parameter (θ for mechanism and ψ for cause\nvariability) to be equal to the corresponding parameter value (as that is used as the location of the\nLaplace distribution). This means collapsing the Laplace distribution to a delta distribution for the\ncorresponding CdF parameter in (26).\nFor comparison, we evaluate three settings: the original scenario (with non-delta priors for both\nparameters), cause variability, and mechanism variability. We use, as in the original code, two\nsamples per environment and ablate over {100, 200, 300, 400, 500} environments. Each experiment\nis repeated 100 times. We measure causal structure identification by three conditional independence\ntests with a significance level of α = 0.05. We choose the estimated causal structure to be the one\ncorresponding to the test with the highest p-value.\nResults.\nFig. 6 shows the proportion of correctly identified causal structures for different numbers\nof environments. The Causal-de-Finetti algorithm outperforms all the other methods with an accuracy\nclose to 100%. This holds not just in the original scenario proposed by Guo et al. (2024a) (Fig. 6a),\nbut also in the case of cause and mechanism variability (Figs. 6b and 6c), corroborating our Thm. 2.\n27\nPublished as a conference paper at ICLR 2025\nG\nACRONYMS\nANM Additive Noise Model\nCD Causal Discovery\nCdF Causal de Finetti\nCRL Causal Representation Learning\nDAG Directed Acyclic Graph\nDGP data generating process\nGCL Generalized Contrastive Learning\ni.i.d. independent and identically distributed\nICA Independent Component Analysis\nICM Independent Causal Mechanisms\nIEM Identifiable Exchangeable Mechanisms\nLVM latent variable model\nMSS Mechanism Shift Score\nOOD out-of-distribution\nRV random variable\nSEM Structural Equation Model\nSMS Sparse Mechanism Shift\nTCL Time-Contrastive Learning\n28\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Published as a conference paper at ICLR 2025\nIDENTIFIABLE EXCHANGEABLE MECHANISMS FOR\nCAUSAL STRUCTURE AND REPRESENTATION LEARN-\nING\nPatrik Reizinger∗1,3, Siyuan Guo∗1,2, Ferenc Huszár2, Bernhard Schölkopf † 1,3, and\nWieland Brendel †1,3,4\n1Max Planck Institute for Intelligent Systems, Tübingen, Germany\n2University of Cambridge, Cambridge, United Kingdom\n3ELLIS Institute Tübingen, Tübingen, Germany\n4Tübingen AI Center, Tübingen, Germany\nABSTRACT\nIdentifying latent representations or causal structures is important for good gen-\neralization and downstream task performance. However, both fields developed\nrather independently. We observe that several structure and representation iden-\ntifiability methods, particularly those that require multiple environments, rely on\nexchangeable non–i.i.d. (independent and identically distributed) data. To formal-\nize this connection, we propose the Identifiable Exchangeable Mechanisms (IEM)\nframework to unify key representation and causal structure learning methods. IEM\nprovides a unified probabilistic graphical model encompassing causal discovery,\nIndependent Component Analysis, and Causal Representation Learning. With the\nhelp of the IEM model, we generalize the Causal de Finetti theorem of Guo et al.\n(2024a) by relaxing the necessary conditions for causal structure identification in\nexchangeable data. We term these conditions cause and mechanism variability, and\nshow how they imply a duality condition in identifiable representation learning,\nleading to new identifiability results.\n1\nINTRODUCTION\nProvably identifying latent representations and causal structures has been a central problem in machine\nlearning, as such guarantees promise good generalization and downstream task performance (Richens\n& Everitt, 2024; Perry et al., 2022; Zimmermann et al., 2021; Arjovsky et al., 2020; Arjovsky, 2021;\nBrady et al., 2023; Wiedemer et al., 2023b;a; Lachapelle et al., 2023; Rusak et al., 2024). Causal\nstructure identification, also known as Causal Discovery (CD), aims to infer cause-effect relationships,\nwhereas identifiable representation learning aims to infer ground-truth sources. Due to their different\nlearning objectives, such problems have been treated separately.\nRecent works on Causal Representation Learning (CRL) (Schölkopf et al., 2021) propose to learn\nlatent representations with causal structures that allow efficient generalization in downstream tasks.\nYet despite progress (Zeˇcevi´c et al., 2021; Reizinger et al., 2023; Xi & Bloem-Reddy, 2023), our\nunderstanding is still limited regarding the question of\nwhat enables structure and representation identifiability?\nGuo et al. (2024a) formalize causality for exchangeable data generating processes (DGPs), showing\nthat unique structure identification is feasible under exchangeable non–i.i.d. data, assuming Indepen-\ndent Causal Mechanisms (ICMs) (Schölkopf et al., 2012). Such unique structure identification was\nclassically deemed impossible (Pearl, 2009a). The present work makes the observation that exchange-\nable non–i.i.d. data is the driving force in identification for both representation and structure identifi-\ncation. We introduce a unified framework for CD, Independent Component Analysis (ICA), and CRL\n(Fig. 1) and show that relaxed exchangeability conditions, termed cause and mechanism variability\n(Fig. 2), are sufficient for both representation and structure identifiability. Our contributions are:\n• Unifying structure and representation learning under the lens of exchangeability (§ 3, also\ncf. Fig. 1): We develop a probabilistic model, Identifiable Exchangeable Mechanisms (IEM), that\nsubsumes key methods in CD, ICA, and CRL.\n• Relaxing causal discovery assumptions in exchangeable non–i.i.d. data (§ 3.2): we show\nhow exchangeable non–i.i.d. cause or effect-given-cause mechanisms, termed cause and mech-\n∗Equal contribution. Correspondence to patrik.reizinger@tuebingen.mpg.de\n†Equal supervision\n1\n"
    },
    {
      "page_number": 2,
      "text": "Published as a conference paper at ICLR 2025\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\nO2\n(a) IEM\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nθ1\nθ2\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\n(b) CD\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nS2\n1\nS2\n2\nO2\n(c) ICA\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\nO2\n(d) CRL\nFigure 1: Identifiable Exchangeable Mechanisms (IEM)–A unified model for structure and rep-\nresentation identifiability: Here we show that exchangeable but non-i.i.d. data enables identification\nin key methods across Causal Discovery (CD), Independent Component Analysis (ICA), and Causal\nRepresentation Learning (CRL). Fig. 1a shows the graphical model for IEM, which subsumes Causal\nDiscovery (CD) (§ 3.2), Independent Component Analysis (ICA) (§ 3.3), and Causal Representation\nLearning (CRL) (§ 3.4). S denotes latent, Z causal, and O observed variables with corresponding\nlatent parameters θ, ψ, superscripts denote different samples. Red denotes observed/known quantities,\nblue stands for target quantities, and gray illustrates components that are not explicitly modeled in a\nparticular paradigm. θi are latent variables controlling separate probabilistic mechanisms, indicated\nby dotted vertical lines. CD (Fig. 1b) corresponds to the left-most layer of IEM, focusing on the\nstudy of cause-effect relationships between observed causal variables; ICA (Fig. 1c) infers source\nvariables from observations, but without causal connections in the left-most layer of IEM; CRL\n(Fig. 1d) shares the most similar structure with IEM, as it has both layers, including the intermediate\ncausal representations. See Fig. 4 for an enlarged view\nanism variability, provide sufficient and necessary conditions for bivariate CD, generalizing the\nidentification theorem in (Guo et al., 2024a).\n• Providing dual identifiability results in Time-Contrastive Learning (TCL) (§ 3.3): we show\nhow an auxiliary-variable ICA method, TCL, is a special case of cause variability—we discuss\nGeneralized Contrastive Learning (GCL) in Appx. A.3. Using insights from the duality in cause\nand mechanism variability, we prove the identifiability of TCL under mechanism variability.\n2\nPRELIMINARIES\nThe impossibility of bivariate CD (Pearl, 2009b) and representation identifiability (Hyvärinen &\nPajunen, 1999; Locatello et al., 2019) from i.i.d. data is well known (cf. Appx. C for examples). Thus,\nwe focus on non–i.i.d., particularly, exchangeable data (Defn. 1) and discuss a causal framework\nfrom (Guo et al., 2024a) building on exchangeability. An example of exchangeable non–i.i.d. data is\nwhen training samples come from different distributions, e.g., Gaussians with different means and/or\nvariances, where the different means and/or variances are modeled as (causal) de Finetti parameters.\nNotation.\nCapital letters denote random variables (RVs), lowercase letters their realizations, and\nbold letters sets/vectors of RVs. S are the latent sources in representation learning or, equivalently,\nthe set of exogenous variables in a Structural Equation Model (SEM); Z are causal variables, and O\nare observations in (causal) representation learning. Data generated by a DGP is a sequence of RVs\nX1, X2, . . . where superscripts index samples and subscripts the vector components (RVs), i.e., X1\ni\nspecifies the ith random variable in X1. f is the mixing function between latents to observations,\ni.e., f : s →o for representation learning, and f : z →o for CRL. Structural assignments from\nexogenous to causal variables are denoted as Z := g(Pa(Z)), where Pa(Z) are the parents or causes\nof Z and Pa(Z) includes the corresponding exogenous variable S. Whenever the RV sequence\ncontains a single variable or bivariate pairs per position, we use Xn or (Xn, Y n). Uppercase P is a\nprobability distribution, and lowercase p is a probability density function. δθ0(θ) is a shorthand for\nthe delta-distribution δ(θ = θ0).\nCausal de Finetti (CdF) and Exchangeability.\nDefinition 1 (Exchangeable sequence). An infinite sequence of random variables X1, X2, . . . is\nexchangeable if for any finite permutation π on the position indices, the joint distribution satisfies:\nP(X1, . . . , Xn) = P(Xπ(1), . . . , Xπ(n)).\n(1)\n2\n"
    },
    {
      "page_number": 3,
      "text": "Published as a conference paper at ICLR 2025\nAn important result to characterize any exchangeable sequence is the theorem of de Finetti (1931). It\nstates that for any exchangeable sequence, there exists a latent variable θ such that the sequence’s\njoint distribution can be represented as a mixture of conditionally i.i.d. distributions:\nP(x1, . . . , xn) =\nZ\nn\nY\ni=1\np(xi|θ)p(θ)dθ.\n(2)\nAny i.i.d. sequence is exchangeable since p(x1, . . . , xn) = Qn\ni=1 p(xi) and the joint distribution\nremains identical when changing the order of observations. Alternatively, the right-hand side of Eq.\n(2) collapses to an i.i.d. sequence whenever p(θ) = δ(θ = θ0) for some constant θ0. Though i.i.d. is\na special case of exchangeable sequences, not all exchangeable sequences are i.i.d. Examples include,\nbut are not limited to: the Pólya urn model (Hoppe, 1984), Chinese restaurant processes (Aldous\net al., 1985), or Dirichlet processes (Ferguson, 1973).\nCausality.\nCausality infers the ground-truth causal structure from the observed joint distribution\nP to enable efficient generalization in novel scenarios. It studies interventional and counterfactual\nqueries beyond purely associational relationships in observational data. The ICM principle (Schölkopf\net al., 2021) hypothesizes that distinct causal mechanisms neither inform nor influence each other.\nGuo et al. (2024a) proves that for exchangeable sequences, the ICM principle implies the existence\nof statistically independent latent variables governing each causal mechanism. Thus, establishing a\nmathematical framework to study causality in exchangeable data. We state their bivariate result.\nTheorem 1 (Causal de Finetti (Guo et al., 2024a)). Let {(Xn, Y n)}n∈N be an infinite sequence\nof binary random variable pairs and denote the set {1, 2, . . . , n} as [n]. The sequence is infinitely\nexchangeable, and satisfies Y [n] ⊥Xn+1 | X[n] for all n ∈N if and only if there exists random\nvariables θ ∈[0, 1] and ψ ∈[0, 1]2 such that the joint probability can be represented as\nP(x1, y1, . . . , xn, yn) =\nZ\nn\nY\ni=1\np(yi | xi, ψ)p(xi | θ)p(θ)p(ψ)dθdψ\n(3)\nThm. 1 shows that for any exchangeable sequence with paired random variables that satisfy certain\nconditional independences, there exist statistically independent latent variables θ, ψ governing each\n(causal) mechanism P(Y | X, ψ), P(X|θ). Guo et al. (2024a) further shows that unique causal\nstructure identification is possible in exchangeable non–i.i.d. data, contrary to the common belief that\nstructure identification is infeasible in i.i.d. data (Pearl, 2009a).\nOur work further observes that exchangeable non–i.i.d. data is again the key for representation\nidentifiability. We thus propose our unifying model, IEM, to allow understanding the driving forces\nbehind general identifiability.\n3\nIDENTIFIABLE EXCHANGEABLE MECHANISMS: A UNIFYING FRAMEWORK\nFOR STRUCTURAL AND REPRESENTATIONAL IDENTIFIABILITY\nThis section demonstrates how non–i.i.d., particularly, exchangeable data (Defn. 1) enables several\nstructure and representation identifiability results. We introduce Identifiable Exchangeable Mech-\nanisms (IEM) (cf. Fig. 1 and § 3.1) to illustrate how exchangeability is the common principle for\nmultiple identifiability results across Causal Discovery (CD), Independent Component Analysis (ICA),\nand Causal Representation Learning (CRL). Furthermore, we relax the exchangeability condition\ninto what we call cause and mechanism variability, which provides novel and relaxed identifiability\nconditions (Thm. 2 and Lem. 4). We derive a probabilistic model from IEM for CD, ICA, and CRL\n(see the graphical relationship in Fig. 1). Then, we show how the bivariate Causal de Finetti (CdF)\ntheorem (Guo et al., 2024a) (§ 3.2), TCL (Hyvarinen & Morioka, 2016) (§ 3.3), and CauCA (Wendong\net al., 2023) (§ 3.4) all leverage exchangeable data, and, thus, are special cases of IEM.\n3.1\nIDENTIFIABLE EXCHANGEABLE MECHANISMS (IEM)\nIEM encompasses three types of variables: exogenous (source) variables S for (disentangled) latent\nrepresentations, causal variables Z for representations that contain cause–effect relationships, and\nobserved variables O for observed (high-dimensional) quantities.\nA probabilistic model for IEM.\nWith all three variable types, assuming that there is an intermediate\ncausal layer, the joint distribution of source, causal, and observed variables is:\np(s, z, o)=\nZ\nθs,θg,ψ\np(o|z, ψ)\nY\nj\n\u0002\np(zj|Pa(zj); θg\nj )p(θg\nj )\n\u0003Y\ni\n[p(si|θs\ni )p(θs\ni )] p(ψ)dψdθgdθs,\n(4)\n3\n"
    },
    {
      "page_number": 4,
      "text": "Published as a conference paper at ICLR 2025\nwhere j indexes causal, i source variables (we omit the sample superscript for brevity), Pa(zj) denotes\nthe parents of zj (including sj) and we integrate over all θg\nj and θs\ni —the superscripts g and s denote\nseparate parameters controlling structural assignments gj and the source distributions, respectively.\nAn intuition for IEM.\nConsider multi-environment data where each environment has a distinct\ndistribution, while observations within the same environment are assumed to be exchangeable, i.e.,\nthe observations’ order is irrelevant. IEM models such multi-environment data by treating each\nenvironment as an i.i.d. copy of the model in (4). Across-environment variability is ensured by choos-\ning non-delta parameter priors p(ψ), p(θg\nj ), p(θs\ni ), while exchangeability within the environment is\nensured by the conditional independence of observations given these parameters. i.i.d. data, or a\nsingle environment in this context, is a special case of exchangeable data with delta priors (see § 2).\nWe introduce IEM to elucidate the relationship of CD, ICA, and CRL: despite distinct learning\nobjectives, they often rely on the same exchangeable non–i.i.d. data structure to allow structure or\nrepresentation identification.\nFurther, IEM can model both the passive (observation) and active (intervention) view of data. For\nexample, both a passive distribution shift and an active hard intervention can be modelled with\nexchangeability as a switch between binary variables. Tab. 1 in Appx. D illustrates the similarity of\nthe (passive) variability and (active) interventional assumptions.\nThe graphical model of IEM illustrates the relationship of source, causal and observed variables\n(Fig. 1). We connect the seemingly unrelated methods of CD, ICA, and CRL by deriving their\nmodel from IEM via omission (cf. Figs. 1b to 1d). Namely, CD does not handle high-dimensional\nobservations, ICA does not model causal variables, and CRL does not aim to recover source variables.\nWe detail these connections in the following case studies.\nCase study: Identifiable Latent Neural Causal Models (Liu et al., 2024) in the unified model.\nLiu et al. (2024) proposed to learn source (exogenous) variables, causal variables, and the correspond-\ning Directed Acyclic Graph (DAG) together, i.e., all target quantities from Fig. 1. We show how\nthis is possible via exchangeable sources and mechanisms (Lem. 1). For the sources, they assume\nnon-stationary, conditionally exponential source variables. Thus, they can use TCL (Hyvarinen &\nMorioka, 2016) to identify S from O (details in § 3.3). For the causal variables, they require diverse\ninterventions, quantified by a derivative-based condition (Assum. 1) on the structural assignments gi\n(the authors generalize to post-nonlinear models; we focus on Additive Noise Models (ANMs)).\nAssumption 1 (Structural assignment assumption (Liu et al., 2024)). Assume that the structural\nassignments gi between causal variables zi form an ANM such that zi := gi(Pa(zi); θg\ni (u)) + si,\nwhere θg\ni (u) are the parameters of the structural assignments, and they depend on the auxiliary-\nvariable u. Then, to identify the causal structure and causal variables, there exists a value u = u0\nsuch that (denoting θg\ni0 := θg\ni (u0))\n∀zj ∈Pa(zi) :\n∂gi(Pa(zi), θg\ni = θg\ni0)\n∂zj\n= 0.\n(5)\nAssum. 1 requires for a specific value u = u0, the path Zj →Zi for each Zj ∈Pa(Zi) is blocked—\nthis can be thought of as emulating perfect interventions, for which structure identifiability results\nexists (Pearl, 2009b). We rephrase the identifiability result of Liu et al. (2024), showing how it relies\non exchangeability conditions (see Appx. A.7 for proof):\nLemma 1. [Identifiable Latent Neural Causal Models are identifiable with exchangeable sources\nand mechanisms] The model of Liu et al. (2024) (Fig. 1a) identifies both the latent sources s and the\ncausal variables z (including the graph), by the variability of s via a non-delta prior over θs and by\nthe variability of the structural assignments via θg.\nThe identifiability result of (Liu et al., 2024) requires two separate variability conditions: one for the\nsources and one for the mechanisms. We show how these separate conditions, when the SEM is an\nANM, disentangle the CdF parameters into separate (independent) parameters controlling sources\nand structural assignments respectively (see proof in Appx. A.8):\nLemma 2. [Independent source and structural assignment CdF parameters for ANMs] In the setting\nof Liu et al. (2024), where the SEM is an ANM, the CdF parameters for the sources, θs, and the\nstructural assignments, θg, are independent, i.e. p(θg, θs) = p(θg)p(θs).\nLem. 2 says that the representation learning (TCL) part relies on the exchangeability of the source\n(exogenous) variables, whereas the CRL part requires exchangeability in the SEM. The connection\nbetween Gaussian LTI systems and CdF (Rajendran et al., 2023, Sec. 3.5) can be seen as a special\n4\n"
    },
    {
      "page_number": 5,
      "text": "Published as a conference paper at ICLR 2025\nX1\nY 1\nθ\nψ\nX2\nY 2\n(a) Causal de Finetti\nX1\nY 1\nθ\nψ\nX2\nY 2\n(b) Cause variability\nX1\nY 1\nθ\nψ\nX2\nY 2\n(c) Mechanism variability\nFigure 2: non–i.i.d. conditions for bivariate CD: (a) Exchangeable non–i.i.d. DGP for both cause\nP(X) and mechanism P(Y|X) (Guo et al., 2024a); (b): exchangeable non–i.i.d. DGP for cause\nP(X) and i.i.d. DGP for mechanism P(Y|X) (c): exchangeable non–i.i.d. DGP for mechanism\nP(Y|X) and i.i.d. DGP for cause P(X). Thm. 2 shows that identifying the unique bivariate causal\nstructure is possible if either the cause or the mechanism follows an exchangeable non–i.i.d. DGP\ncase of Lem. 2, where the sources and the mechanism (the LTI dynamical system) have independent\nmatrix parameters. Our result also conceptually resemble mechanized SEMs (Kenton et al., 2023),\nwhere the structural assignments are modeled by distinct nodes.\nNext, we show how the probabilistic models for CD, ICA, and CRL can be derived from IEM (Fig. 1),\ndepending on whether we model cause-effect relationships and/or source variables.\n3.2\nEXCHANGEABILITY IN CAUSAL DISCOVERY: EXTENDING CAUSAL DE FINETTI\nCausal Discovery (CD) infers the causal graph between observed causal variables (Fig. 1b). SEMs\n(Pearl, 2009a) are classic causal models, where deterministic causal mechanisms and stochastic noise\n(exogenous/latent) variables determine each causal variable’s value. For i.i.d. observational data\nalone, causal structure is identifiable only up to its Markov equivalence class (Defn. 5). In the present\nwork, we introduce a relaxed set of conditions, termed cause and mechanism variability, and show in\nthe bivariate case how these non–i.i.d., specifically a mixture of i.i.d. and exchangeable, data, are\nnecessary and sufficient for uniquely identifying causal structures.\nA probabilistic model for CD.\nWe consider the bivariate case with exchangeable sequences\n(Xn, Y n) that adheres to the ICM principle (Peters et al., 2018). Thm. 1 states there exist statistically\nindependent CdF parameters θ, ψ such that the joint distribution can be represented as:\np(x1, y1, . . . , xn, yn) =\nZ\nθ\nZ\nψ\nn\nY\ni=1\np(yi|xi, ψ)p(xi|θ)dψdθ\nwhere\nψ ⊥θ.\n(6)\nCD with unique structure identification is possible when the parameter priors p(θ), p(ψ) are not delta\ndistributions, i.e., when the data pairs are from exchangeable non–i.i.d. sequences. Fig. 2a shows a\nMarkov graph compatible with (6).\nCase study: CdF in the unified model.\nCD in general, and CdF in particular, focuses on the\nstudy of observed causal variables (denoted by Z in Fig. 1 and (4)). CD aims to learn cause-effect\nrelationships among the observed causal variables Zi, rather than reconstructing the Zi or uncovering\nthe true mixing function f. Bivariate CdF fits into the IEM probabilistic model by relabeling Y = Zi\nand X = Pa(Zi). We use our insights from IEM to relax the assumptions for bivariate CD from\nexchangeable pairs, generalizing CdF.\nRelaxing CdF: cause and mechanism variability.\nWe show that it is not necessary for CD that\nboth p(θ) and p(ψ) differ from a delta distribution—equivalently, the presence of both graphical\nsubstructures X1 ←θ →X2 and Y 1 ←ψ →Y 2 are not required to distinguish the causal direction\nbetween X and Y . We distinguish two cases: “cause variability,\" when only the cause mechanism\nchanges (Fig. 2b), i.e. p(ψ) = δψ0(ψ), p(θ) ̸= δθ0(θ); and “mechanism variability,\" when only the\neffect-given-the-cause mechanism changes (Fig. 2c), i.e. p(θ) = δθ0(θ) and p(ψ) ̸= δψ0(ψ)—we\nmotivate these assumptions by the Sparse Mechanism Shift (SMS) hypothesis (Perry et al., 2022) and\nprovide real-world examples for both in Appx. E. When p(θ) is sufficiently different from a delta\ndistribution, then each cause distribution sampled from p(x|θ) will have a different distribution with\nhigh probability. This is similar for p(ψ) when the effect-given-the-cause mechanism p(y|x, ψ) is\nshifted. Formally (the proof is in Appx. A.1):\n5\n"
    },
    {
      "page_number": 6,
      "text": "Published as a conference paper at ICLR 2025\nTheorem 2. [Cause/mechanism variability is necessary and sufficient for bivariate CD] Given a\nsequence of bivariate pairs {Xn, Y n}n∈N such that for any N ∈N, the joint distribution can be\nrepresented as:\n• X →Y : p(x1, y1, . . . , xN, yN) =\nR\nθ\nR\nψ\nQ\nn p(yn|xn, ψ)p(xn|θ)p(θ)p(ψ)dθdψ\n• X ←Y : p(x1, y1, . . . , xN, yN) =\nR\nθ\nR\nψ\nQ\nn p(xn|yn, θ)p(yn|ψ)p(ψ)p(θ)dθdψ\nThen the causal direction between variables X, Y can still be distinguished when:\n1. either only p(θ) = δθ0(θ) for some constant θ0 or only p(ψ) = δψ0(ψ) for some constant\nψ0 (but not both). Fig. 2b and Fig. 2c show the Markov structure of such factorizations.\n2. the distribution of P is faithful (Defn. 4) w.r.t. Fig. 2b or Fig. 2c.\nThm. 2 relaxes Thm. 1 and states that the causal structure can be identified even if only one mecha-\nnisms varies. That is, if the X, Y pairs are a mixture of i.i.d. and exchangeable data such that either\ncause variability (Fig. 2b) or mechanism variability (Fig. 2c) holds; then we can distinguish X →Y\nfrom X ←Y —which we empirically verify in synthetic experiments in Appx. F. Thm. 2 focuses on\nthe bivariate case, though we expect similar results can be extended to multivariate cases. Thm. 2\naligns with well-known results stating that assuming no confounders, single-node interventions are\nsufficient to identify the causal structure (Pearl, 2009b). The contribution of Thm. 2 lies in taking the\npassive view, similar to (Guo et al., 2024a).\n3.3\nEXCHANGEABILITY IN REPRESENTATION LEARNING\nRepresentation learning aims to infer latent sources s from observations o, which are generated via\na mixing function f : s →o. ICA1 (Comon, 1994; Hyvarinen et al., 2001; Shimizu et al., 2006)\nassumes component-wise independent latent sources s where p(s) = Q\ni pi(si) and aims to learn an\nunmixing function that maps to independent components. Recent methods (Hyvarinen & Morioka,\n2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Morioka et al., 2021; Zimmermann et al.,\n2021) focus on auxiliary-variable ICA, which assumes the existence and observation2 of an auxiliary\nvariable u such that p(s|u) = Q\ni pi(si|u) holds—thus, providing identifiability results for a much\nbroader model class. Here, we show that representation identifiability in (auxiliary-variable) ICA,\nparticularly TCL (Hyvarinen & Morioka, 2016) (and GCL with conditionally exponential-family\nsources, cf. Appx. A.3), relies on the latent sources to be an exchangeable non–i.i.d. sequence.\nA probabilistic model for (auxiliary-variable) ICA.\nAuxiliary variables can represent many\nforms of additional information (Hyvarinen et al., 2019). Our focus is when u represents segment\nindices, i.e., it enumerates multiple environments. This is equivalent to a draw from a categorical prior\np(u), thus, sources are a marginal copy of an exchangeable sequence p(s) =\nR\nu\nQ\ni p(si|u)p(u)du.\nIn auxiliary-variable ICA (Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al.,\n2020a), there is a separate parameter θi := θ(u) for each si. Conditioned on observing the segment\nindex u, the joint probability distribution w.r.t. latent sources s and observations o factorizes as (i\nindexes source variables, we omit the sample superscript for brevity):\npu(o, s) =\nZ\nθ\nZ\nψ\np(o|s, ψ)\nY\ni\n[p(si|θi)pu(θi)] dψdθ\nwhere\np(ψ)=δψ0(ψ).\n(7)\nCompared to (4), Eq. (7) does not have a “causal layer\", expressing the (conditional) independence\nbetween the sources in ICA. Compared to CdF, representation learning with ICA additionally\nrestricts the joint probability between sources and observations to extract more information (the latent\nvariables), compared to only the DAG. This relation was demonstrated by Reizinger et al. (2023),\nshowing that representation identifiability in some cases implies causal structure identification.\nCase study: TCL in the unified model.\nWe next present how auxiliary-variable ICA, particularly\nTCL (Hyvarinen & Morioka, 2016) (cf. Appx. A.3 for the generalization), fits into IEM (Fig. 1c)\nand present a duality result on cause and mechanism variability. The TCL model assumes that\nthe conditional log-density log p(s|u) is a sum of components qi(si, u), where qi belongs to the\nexponential family of order one, i.e.:\nqi (si, u) = ˜qi (si) θi(u) −log Ni(u) + log Qi(si),\n(8)\n1Though the literature is referred to as the nonlinear ICA literature, it often uses conditionally independent\nlatents, but expressions such as Independently Modulated Component Analysis (IMCA) are not widely used\n2There is a variant of auxiliary-variable ICA for Hidden Markov Models, which does not require observing\nu (Morioka et al., 2021); we focus on the case when u is observed\n6\n"
    },
    {
      "page_number": 7,
      "text": "Published as a conference paper at ICLR 2025\nwhere Ni is the normalizing constant, Qi the base measure, ˜qi the sufficient statistics, and the\nmodulation parameters θi := θi(u) depend on u. The identifiability of TCL requires multiple\nsegments (i.e., realizations of u with different values) such that for environment j, the modulation\nparameters fulfill a sufficient variability condition, defined via a rank condition:\nAssumption 2 (Sufficient variability). A DGP is called sufficiently variable if there exists (d + 1)\ndistinct realizations of u for d−dimensional source variables and modulation parameter vectors such\nthat the modulation parameter matrix L ∈R(E−1)×d has full column rank. For E environments and\nmodulation parameter vectors θj =\nh\nθj\n1, . . . , θj\nd\ni\n, the jth row of L is:\n[L]j: = (θj −θ0).\n(9)\nHere θi are the de Finetti parameters for the exchangeable sources. We show in Appx. A.2 that pu(θi)\ncannot be a delta distribution; otherwise, the variability condition of TCL is violated. Thus, the\nidentifiability of TCL hinges on exchangeable non–i.i.d. sources (we prove the same for conditionally-\nexponential sources in GCL (Hyvarinen et al., 2019), cf. Cor. 1 in Appx. A.3):\nLemma 3. [TCL is identifiable due to exchangeable non–i.i.d. sources] The sufficient variability\ncondition in TCL corresponds to cause variability, i.e., exchangeable non–i.i.d. source variables with\na fixed mixing function, which leads to the identifiability of the latent sources.\nS1\nO1\nf\nθ(u)\nS2\nO2\nf\nS\nO1\nˆf(u1)\nO2\nˆf(u2)\nFigure 3: The duality of cause and mechanism\nvariability in TCL: Lem. 4 shows that the same\nidentifiability result holds in (Left): the original\nTCL setting with exchangeable non–i.i.d. sources\nS with deterministic f mixing (cause variability),\nand the matching (Right): i.i.d. sources S with a\nstochastic ˆf(u) mixing (mechanism variability)\nExtending TCL via the cause–mechanism vari-\nability duality.\nWe next demonstrate the flexi-\nbility of the IEM framework as it relates the proba-\nbilistic model for TCL to that of bivariate CdF (6).\nTreating the observations o as the “effect\", and the\nsource vector s = [s1, . . . , sd] as the “cause\", (7)\nbecomes equal to (6) when X = S and Y = O.\nAs in auxiliary-variable ICA the mixing function\nf is deterministic, it constitutes “cause variabil-\nity\" (Fig. 2b). Our extension of the CdF theorem\nin Thm. 2 shows a symmetry between cause and\nmechanism variability: flipping the arrows and\nrelabeling X/Y and θ/ψ transforms one case into\nthe other (cf. Fig. 5 in Appx. A.1). Our insight\nis that identification can be achieved both with\ncause variability or mechanism variability. This\nnot only holds for CD, leading to a dual formu-\nlation of TCL with mechanism variability. We\nillustrate this in an example, then state our result\n(cf. Appx. A.4 for the proof):\nExample 1 (Duality of cause and mechanism variability for Gaussian models). Assume condi-\ntionally independent latent sources with variance-modulated Gaussian components, i.e., p(s|u) =\nQ\ni pi(si|u), where each pi(si|u) = N\n\u0000µi; σ2\ni (u)\n\u0001\n, depending on auxiliary variable u. In this case,\nthe observation distribution is the pushforward of p(s|u) by f, denoted as f∗p(s|u). For given σ2\ni (u)\nand f, where Σ2(u) = diag\n\u0000σ2\n1(u), . . . , σ2\nn(u)\n\u0001\n, we can find stochastic functions ˆf = f ◦Σ(u) such\nthat the pushforward f∗p(s|u) = f∗N\n\u0000µ; Σ2(u)\n\u0001\nequals to ˆf∗N (µ; I). By construction, ˆf varies\nwith u and satisfies mechanism variability.\nLemma 4. [Duality of cause and mechanism variability for TCL] For a given deterministic mix-\ning function f : s →o and conditionally factorizing (non-stationary) latent sources p(s|u) =\nQ\ni pi(si|u) fulfilling the sufficient variability of TCL, there exists an equivalent setup with stationary\n(i.i.d.) sources p(s) = Q\ni pi(si) with stochastic functions ˆf = f ◦g : s →o, where g = g(u) and\neach component gi is defined as an element-wise function such that the pushforward of pi(si) by gi\nequals pi(si|u), i.e., gi∗pi(si) = pi(si|u). Then, gi∗pi(si) fulfils the same variability condition; thus,\nthe same identifiability result applies.\nLem. 4 shows that both cause and mechanism variability lead to representation identification in TCL,\nvisualized in Fig. 3. We illustrate the practical differences between cause and mechanism variability\nin the medical example of learning representations from fMRI data (Hyvarinen & Morioka, 2016;\nKhemakhem et al., 2020a). Cause variability means having access to data from patients with different\n7\n"
    },
    {
      "page_number": 8,
      "text": "Published as a conference paper at ICLR 2025\nunderlying conditions; mechanism variability corresponds to measuring a single patient’s condition\nwith multiple diagnostic methods.\n3.4\nEXCHANGEABILITY IN CAUSAL REPRESENTATION LEARNING\nCausal Representation Learning (CRL) aims to learn the causal representations Z and their graphical\nstructure from high-dimensional observations O. That is, CRL can be considered as performing\nrepresentation learning for the latent causal variables and CD between those learned latent variables\nsimultaneously.\nA probabilistic model for CRL.\nAuxiliary-variable ICA considers the source distribution as\nQ\ni p(si|θi) , where si and sj are not causally related. CRL takes one step further and studies how to\nfind causal dependencies between the latent causal variables. We show that CdF theorems apply just\nas de Finetti applies in exchangeable ICA. In CRL the joint distribution factorizes (j indexes causal\nvariables, we omit the sample superscript for brevity):\np(z, o) =\nZ\nθ\nZ\nψ\np(o|z, ψ)\nY\nj\n[p(zj|Pa(zj); θj)p(θj)] dψdθ,\n(10)\nwhere θj are the CdF parameters controlling each latent causal mechanism, leading to exchangeable\ncausal variables that adhere to the ICM principle. Compared to exchangeable ICA (7), eq. (10) allows\nthat the modeled latent causal variables zi can depend on each other, whereas ICA does not model\ncause–effect relationships (Fig. 1d).\nCase study: CauCA in the unified model.\nCausal Component Analysis (CauCA) (Wendong et al.,\n2023) defines a subproblem of CRL by assuming that the DAG between the zi is known. Wendong\net al. (2023) show that identifying causal representations zi requires single-node (soft) interventions\nthat change the probabilistic mechanisms p(zj|Pa(zj)) almost everywhere, which they quantify with\nthe interventional discrepancy:\nAssumption 3 (Interventional discrepancy condition (Wendong et al., 2023)). Pairs of observational\nand single-node (soft) interventional densities p, ˜p need to differ almost everywhere, i.e.:\n∂\n∂zj\nlog ˜p(zj|Pa(zj))\np(zj|Pa(zj)) ̸= 0\na.e.\n(11)\nSatisfying Assum. 3 means an intervention on the parameters θj of the causal mechanisms\np(zj|Pa(zj); θj) (we compare to Assum. 1 in Appx. A.7). The following lemma follows from\nhaving interventions on the value of θj that fulfill Assum. 3 (proof in Appx. A.5):\nLemma 5. [Non-delta priors in the causal mechanisms can enable identifiable CRL]\nIf the interventional discrepancy condition Assum. 3 holds, then the parameter priors in (10) cannot\nequal a delta distribution, i.e., p(θj) ̸= δθj0(θj); thus, if the other conditions of CauCA hold, then,\nthe causal variables zi are identifiable. For real-valued θj, non-delta priors also imply Assum. 3\nalmost everywhere.\nLem. 5 says that when the interventional discrepancy condition is satisfied, then a change in p(θ)\nmust have occurred. This provides a sufficient criterion to determine when multi-environment data\nenables representation identification. However, as Assum. 3 is formulated as an almost everywhere\ncondition, the reverse does not necessarily hold—e.g., for discrete RVs such as when θj follows a\nBernoulli distribution (Rem. 1). Thus, we prove the reverse for real-valued θj.\nTowards the simultaneous identifiability of S, Z, and the DAG.\nWe finish our discussion of IEM\nby illustrating how the joint treatment of structure and representation identifiability can be possible\nwith less environments than the two separate problems. As we have shown in § 3.1, it is possible to\nidentify both sources and causal variables by two separate variability conditions (Liu et al., 2024).\nHowever, as Assum. 1 requires variability of the structural assignments gi, it cannot be fulfilled by\nexchangeable sources, at least not for an ANM. Thus, we consider the most general identifiability\nresult in CRL by (Jin & Syrgkanis, 2023), which requires dim Z single-node non-degenerate (in the\nsense of Assum. 3) soft interventions for generic nonparametric SEMs—i.e., when interventions on\nthe exogenous variables change the observational density almost everywhere. Further restricting the\nsources to first-order conditional exponential family distributions, adding one more intervention can\nsatisfy Assum. 2. Thus, we sidestep the requirement of having (dim Z + 1) different environments\nfor ICA, and another dim Z for CRL. Namely, by Lem. 5, we know that when Assum. 3 holds, then\nthe parameter priors are non-degenerate. Then, by Lem. 3, Assum. 2 also holds. Thus (proof is in\nAppx. A.6):\n8\n"
    },
    {
      "page_number": 9,
      "text": "Published as a conference paper at ICLR 2025\nLemma 6. [Simultaneous identifiability via generic non-degenerate source priors] Provided the\nassumptions of (Jin & Syrgkanis, 2023, Thm. 4) hold with the restriction of the source variables’ den-\nsity belonging to the exponential family of order one, and assuming that the nonparametric structural\nassignments are generic such that single-node soft interventions on each Si satisfy Assum. 3, then\n(dim Z+1) interventions can provide exchangeable data sufficient for the simultaneous identification\nof both exogenous and causal variables (and also the DAG)—as opposed to (2 dim Z + 1), where\ndim Z separate environments are used for CRL and another (dim Z + 1) for ICA.\n4\nDISCUSSION AND FUTURE DIRECTIONS\nOur work unifies several Causal Discovery (CD), Independent Component Analysis (ICA), and\nCausal Representation Learning (CRL) methods with the lens of exchangeability. Next, we answer\nthe question:\nWhat do we gain from the Identifiable Exchangeable Mechanisms (IEM) framework?\nThe motivation of introducing IEM is to provide a unified model that eases understanding and\ndiscovery of the synergies between representation and structure identifiability. Our work leverages\nIEM to relax conditions of general exchangeability to cause and mechanism variability for enabling\nboth structure and representation identifiability. Exchangeability can also model both the passive\nnotion of data variability posited in the ICA literature (e.g., Assum. 2) and the active, agency-\nbased notion of diverse interventions (e.g., Assum. 3). We provide a detailed comparison of the\nassumptions in both fields in Tab. 1 in Appx. D. By interpreting the variability in ICA as coming from\ninterventions on the exogenous variables, IEM explains why ICA can allow for causal inferences.\nNamely, assuming that the observations correspond to the causal variables and using ICA to recover\nthe source (exogenous) variables, we can infer the causal graph depending on the identifiability class,\nas shown by Reizinger et al. (2023).\nIn the following, we show how exchangeability can model i.i.d., out-of-distribution (OOD) and\ninterventional distributions (§ 4.1), discuss the general conditions that allow for identifiability in the\nIEM setting (§ 4.2), and detail three additional directions where we believe IEM can open up new\npossibilities (§§ 4.3 to 4.5).\n4.1\nEXCHANGEABILITY FOR MODELING I.I.D., OOD, AND INTERVENTIONAL DATA\nBy de Finetti’s theorem (de Finetti, 1931), the joint distribution of exchangeable data can be repre-\nsented as a mixture of i.i.d. distributions p(xi|θ), where θ is drawn from a prior distribution (2). In\nthe special case of p(θ) = δθ0 we get i.i.d. samples. Guo et al. (2024b) studies how intervention\npropagates in an exchangeable sequence. Here we note that exchangeability may be a natural choice\nfor modelling OOD and interventional data. For example, when we assume access to multiple\nenvironments—where each environment has a distinct parameter drawn from p(θ): OOD and inter-\nventions can be analogusly modelled as a shift in θ, i.e., data in a novel or intervened environment is\ndrawn from p(x | θ1) instead of p(x | θ0), where θ1 ̸= θ0 (cf. the intution in § 3.1 for an example).\n4.2\nGENERAL CONDITIONS FOR IDENTIFIABILITY IN THE IEM SETTING\nWhen introducing IEM, we focused on exchangeability as a driving factor for structure and represen-\ntation identifiability. However, theoretical guarantees usually require further assumptions. Here we\ndiscuss the general set of assumptions required for identifiabiltiy of the causal structure, the causal\nvariables, and the exogenous (source) variables. We review such assumptions in Tab. 1 in Appx. D.\nIn the case of exchangeable data, we can characterize the best achievable identifiability results as:\nCausal structure (DAG).\nObserved causal variables under faithfulness and cause or mechanism\nvariability are necessary and sufficient to identify the DAG (Thm. 2).\nCausal variables (Z).\nAssuming independent exogenous variables and a diffeomorphic mixing\nfunction is sufficient to identify the causal variables up to elementwise nonlinear transformations when\nwe have access to dim Z single-node soft interventions with unknown targets (Jin & Syrgkanis, 2023).\nExogenous (source) variables (S).\nHaving exchangeable sources and a surjective feature extractor\nare sufficient to achieve identifiability up to element-wise nonlinear transformations if the feature\nextractor is either positive or is augmented by squared features (Khemakhem et al., 2020b).\n4.3\nIDENTIFYING COMPONENTS OF CAUSAL MECHANISMS\nCausal mechanisms are composed of exogenous variables and structural assignments. CdF proves the\nexistence of a statistically independent latent variable per causal mechanism. Lem. 2 shows that for\nANMs, such latent RVs can be decomposed into separate variables controlling exogenous variables\nand structural assignments. Wang et al. (2018), for example, performs multi-environment CD via\nchanging only the weights in the linear SEM across environments, which corresponds to changing\n9\n"
    },
    {
      "page_number": 10,
      "text": "Published as a conference paper at ICLR 2025\nthe mechanism parameters θg. Liu et al. (2024) showed how changing both parameters leads to latent\nsource and causal structure identification. This suggests that partitioning the CdF parameters into\nmechanism and source parameters can be beneficial to identifying individual components of causal\nmechanisms.\n4.4\nCAUSE AND MECHANISM VARIABILITY: POTENTIAL GAPS AND FUTURE DIRECTIONS\nWe relax the assumptions for bivariate CD (§ 3.2) by noticing that changing only either cause or\nmechanism leads to identifiability, which we term cause and mechanism variability. We further\nshowed with TCL how ICA methods—which usually belong to the cause variability category—\ncan be equivalently extended to mechanism variability (Lem. 4). This dual formulation, though\nmathematically equivalent, presents new opportunities in practice. Existing work in the ICA literature\nhave focused on identification through variation in the sources with a single deterministic mixing\nfunction f : s →o, where functional constraints are used for identifiability (Gresele et al., 2021;\nLachapelle et al., 2023; Brady et al., 2023; Wiedemer et al., 2023a;b). Multi-view ICA (Gresele et al.,\n2019), on the other hand, might be related to mechanism variability—we leave investigating this\nconnection to future work.\n4.5\nCHARACTERIZING DEGREES OF NON–I.I.D. DATA\nExisting work have developed multiple criteria to characterize non-i.i.d. data from out-of-distribution\n(Quionero-Candela et al., 2009; Schölkopf et al., 2012; Arjovsky et al., 2020) to out-of-variable (Guo\net al.) generalization. Here we assay common criteria for identifiability and highlight potential gaps.\nOften identification conditions are descriptive with no clear practical guidance in quantifying how\nand when to induce non-i.i.d. data. Metric computation is also difficult in practice.\nRank conditions\nsuch as Assum. 2 in TCL (Hyvarinen & Morioka, 2016), our running example for\nICA, uses a rank-condition to prove identifiability. Assum. 2 expresses that the multi-environment\ndata is non–i.i.d. However, full-rank matrices can differ to a large extent, e.g., by their condition\nnumber, which affects numerical stability, thus, matters in practice (Rajendran et al., 2023). We\nexpect that the condition number could be used to develop bounds for the required sample sizes in\npractice—an aspect generally missing from the identifiability literature, as most works assume access\nto infinite samples, with the exception of (Lyu & Fu, 2022).\nDerivative conditions\nsuch as interventional discrepancy (Wendong et al., 2023) require that\nbetween environments, there is a non-trivial (i.e., non-zero measure) shift between the causal\nmechanisms—i.e., the data is not i.i.d. The similarity between interventional discrepancy and\nthe derivative-based condition on the structural assignments in (Liu et al., 2024) (Assum. 1) also\nhas an interesting interpretation: Liu et al. (2024) does not require interventional data per se, only\nnon–i.i.d. data that is akin to being generated by a SEM that was intervened on. This assumption is\nsimilar to the concepts of Mendelian randomization (Didelez & Sheehan, 2007) or natural experi-\nments (Angrist & Krueger, 1991; Imbens & Angrist, 1994), which assume that an intervention not\ncontrolled by the experimenter (but by, e.g., genetic mutations) provides sufficiently diverse data.\nMechanism shift-based conditions\nquantify the number of shifted causal mechanisms. The\ndistribution shift perspective was already present in, e.g., (Zhang et al., 2015; Arjovsky et al., 2020).\nPerry et al. (2022) explore the SMS hypothesis (Schölkopf, 2019), postulating that domain shifts\nare due to a sparse change in the set of mechanisms. Their Mechanism Shift Score (MSS) counts\nthe number of changing conditionals, which is minimal for the true DAG. Richens & Everitt (2024)\ncharacterize mechanism shifts for causal agents solving decision tasks. Their condition posits that\nthe agent’s optimal policy should change when the causal mechanisms shift.\n5\nCONCLUSION\nWe introduced Identifiable Exchangeable Mechanisms (IEM), a unifying framework that captures a\ncommon theme between causal discovery, representation learning, and causal representation learning:\naccess to exchangeable non–i.i.d. data. We showed how particular causal structure and representation\nidentifiability results can be reframed in IEM as exchangeability conditions, from the Causal de\nFinetti theorem through auxiliary-variable Independent Component Analysis and Causal Component\nAnalysis. Our unified model also led to new insights: we introduced cause and mechanism variability\nas a special case of exchangeable but not-i.i.d. data, which led us to provide relaxed necessary and\nsufficient conditions for causal structure identification (Thm. 2), and to formulate identifiability\nresults for mechanism variability-based time-contrastive learning (Lem. 4) We acknowledge that our\nunified framework might not incorporate all identifiable methods. However, by providing a formal\nconnection between the mostly separately advancing fields of causality and representation learning,\nmore synergies and new results can be developed, just as Thm. 2 and Lem. 4. This, we hope, will\ninspire further research to investigate the formal connection between these fields.\n10\n"
    },
    {
      "page_number": 11,
      "text": "Published as a conference paper at ICLR 2025\nACKNOWLEDGMENTS\nThe authors thank Luigi Gresele and anonymous reviewers for insightful comments and discussions.\nThis work was supported by a Turing AI World-Leading Researcher Fellowship G111021. Patrik\nReizinger acknowledges his membership in the European Laboratory for Learning and Intelligent\nSystems (ELLIS) PhD program and thanks the International Max Planck Research School for\nIntelligent Systems (IMPRS-IS) for its support. This work was supported by the German Federal\nMinistry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A. Wieland\nBrendel acknowledges financial support via an Emmy Noether Grant funded by the German Research\nFoundation (DFG) under grant no. BR 6382/1-1 and via the Open Philantropy Foundation funded\nby the Good Ventures Foundation. Wieland Brendel is a member of the Machine Learning Cluster\nof Excellence, EXC number 2064/1 – Project number 390727645. This research utilized compute\nresources at the Tübingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG.\nREFERENCES\nKartik Ahuja, Jason Hartford, and Yoshua Bengio. Weakly Supervised Representation Learning\nwith Sparse Perturbations. October 2022a. URL https://openreview.net/forum?id=\n6ZI4iF_T7t. 26\nKartik Ahuja, Yixin Wang, Divyat Mahajan, and Yoshua Bengio.\nInterventional Causal Rep-\nresentation Learning, September 2022b.\nURL http://arxiv.org/abs/2209.11924.\narXiv:2209.11924 [cs, stat]. 26\nDavid J Aldous, Illdar A Ibragimov, Jean Jacod, and David J Aldous. Exchangeability and related\ntopics. Springer, 1985. 3\nJoshua D. Angrist and Alan B. Krueger. Does compulsory school attendance affect schooling\nand earnings? The Quarterly Journal of Economics, 106(4):979–1014, November 1991. ISSN\n0033-5533. doi: 10.2307/2937954. URL http://dx.doi.org/10.2307/2937954. 10,\n26\nYashas Annadani, Jonas Rothfuss, Alexandre Lacoste, Nino Scherrer, Anirudh Goyal, Yoshua Bengio,\nand Stefan Bauer. Variational Causal Networks: Approximate Bayesian Inference over Causal\nStructures. arXiv:2106.07635 [cs, stat], June 2021. URL http://arxiv.org/abs/2106.\n07635. arXiv: 2106.07635. 24\nMartin Arjovsky. Out of Distribution Generalization in Machine Learning. arXiv:2103.02667 [cs,\nstat], March 2021. URL http://arxiv.org/abs/2103.02667. arXiv: 2103.02667. 1\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization.\narXiv:1907.02893 [cs, stat], March 2020. URL http://arxiv.org/abs/1907.02893.\narXiv: 1907.02893. 1, 10\nAlice Bizeul, Bernhard Schölkopf, and Carl Allen. A Probabilistic Model to explain Self-Supervised\nRepresentation Learning, February 2024. URL http://arxiv.org/abs/2402.01399.\narXiv:2402.01399 [cs, stat]. 24\nJack Brady, Roland S. Zimmermann, Yash Sharma, Bernhard Schölkopf, Julius von Kügelgen,\nand Wieland Brendel. Provably Learning Object-Centric Representations, May 2023. URL\nhttp://arxiv.org/abs/2305.14229. arXiv:2305.14229 [cs]. 1, 10\nDavid Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine\nlearning research, 3(Nov):507–554, 2002. 26\nPierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287–314,\n1994. 6, 24\nGeorge Darmois. Analyse des liaisons de probabilité. In Proc. Int. Stat. Conferences 1947, pp. 231,\n1951. 24\nB. de Finetti. Funzione caratteristica di un fenomeno aleatorio. Atti della R. Accademia Nazionale\ndei Lincei, Ser. 6. Memorie, Classe di Scienze Fisiche, Matematiche e Naturali 4, pp. 251–299,\n1931. 3, 9\n11\n"
    },
    {
      "page_number": 12,
      "text": "Published as a conference paper at ICLR 2025\nVanessa Didelez and Nuala Sheehan. Mendelian randomization as an instrumental variable approach\nto causal inference. Statistical Methods in Medical Research, 16(4):309–330, August 2007.\nISSN 1477-0334. doi: 10.1177/0962280206077743. URL http://dx.doi.org/10.1177/\n0962280206077743. 10\nCian Eastwood, Julius von Kügelgen, Linus Ericsson, Diane Bouchacourt, Pascal Vincent, Bern-\nhard Schölkopf, and Mark Ibrahim. Self-Supervised Disentanglement by Leveraging Structure\nin Data Augmentations, November 2023. URL http://arxiv.org/abs/2311.08815.\narXiv:2311.08815 [cs, stat]. 24\nThomas S Ferguson. A bayesian analysis of some nonparametric problems. The annals of statistics,\npp. 209–230, 1973. 3\nMarco Fumero, Florian Wenzel, Luca Zancato, Alessandro Achille, Emanuele Rodolà, Stefano Soatto,\nBernhard Schölkopf, and Francesco Locatello. Leveraging sparse and shared feature activations\nfor disentangled representation learning, April 2023. URL http://arxiv.org/abs/2304.\n07939. arXiv:2304.07939 [cs]. 26\nGaël Gendron, Michael Witbrock, and Gillian Dobbie. Disentanglement of Latent Representations via\nSparse Causal Interventions, February 2023. URL http://arxiv.org/abs/2302.00869.\narXiv:2302.00869 [cs, stat]. 26\nLuigi Gresele, Paul K. Rubenstein, Arash Mehrjou, Francesco Locatello, and Bernhard Schölkopf.\nThe Incomplete Rosetta Stone Problem: Identifiability Results for Multi-View Nonlinear ICA.\narXiv:1905.06642 [cs, stat], August 2019. URL http://arxiv.org/abs/1905.06642.\narXiv: 1905.06642. 10, 24\nLuigi Gresele, Julius von Kügelgen, Vincent Stimper, Bernhard Schölkopf, and Michel Besserve.\nIndependent mechanism analysis, a new concept? arXiv:2106.05200 [cs, stat], June 2021. URL\nhttp://arxiv.org/abs/2106.05200. arXiv: 2106.05200. 10, 24\nSiyuan Guo, Jonas Bernhard Wildberger, and Bernhard Schölkopf. Out-of-variable generalisation for\ndiscriminative models. In The Twelfth International Conference on Learning Representations. 10\nSiyuan Guo, Viktor Tóth, Bernhard Schölkopf, and Ferenc Huszár. Causal de finetti: On the\nidentification of invariant causal structure in exchangeable data. Advances in Neural Information\nProcessing Systems, 36, 2024a. 1, 2, 3, 5, 6, 19, 25, 26, 27\nSiyuan Guo, Chi Zhang, Karthika Mohan, Ferenc Huszár, and Bernhard Schölkopf. Do Finetti: On\nCausal Effects for Exchangeable Data, May 2024b. URL http://arxiv.org/abs/2405.\n18836. arXiv:2405.18836 [cs, stat]. 9\nFred M. Hoppe. Pólya-like urns and the Ewens’ sampling formula. Journal of Mathematical\nBiology, 20(1):91–94, August 1984. ISSN 1432-1416. doi: 10.1007/BF00275863. URL https:\n//doi.org/10.1007/BF00275863. 3\nPatrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear\ncausal discovery with additive noise models. In Advances in Neural Information Processing Sys-\ntems, volume 21. Curran Associates, Inc., 2008. URL https://proceedings.neurips.\ncc/paper/2008/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.\nhtml. 24\nBiwei Huang, Kun Zhang, Jiji Zhang, Ruben Sanchez-Romero, Clark Glymour, and Bernhard\nSchölkopf. Behind distribution shift: Mining driving forces of changes and causal arrows. In 2017\nIEEE International Conference on Data Mining (ICDM), pp. 913–918. IEEE, 2017. 26\nAapo Hyvarinen and Hiroshi Morioka. Unsupervised Feature Extraction by Time-Contrastive\nLearning and Nonlinear ICA. arXiv:1605.06336 [cs, stat], May 2016. URL http://arxiv.\norg/abs/1605.06336. arXiv: 1605.06336. 3, 4, 6, 7, 10, 24, 25\nAapo Hyvarinen and Hiroshi Morioka.\nNonlinear ICA of Temporally Dependent Stationary\nSources.\nIn Artificial Intelligence and Statistics, pp. 460–469. PMLR, April 2017.\nURL\nhttp://proceedings.mlr.press/v54/hyvarinen17a.html.\nISSN: 2640-3498.\n24\n12\n"
    },
    {
      "page_number": 13,
      "text": "Published as a conference paper at ICLR 2025\nAapo Hyvarinen, Juha Karhunen, and Erkki Oja. Independent component analysis. J. Wiley, New\nYork, 2001. ISBN 978-0-471-40540-5. 6, 24\nAapo Hyvarinen, Kun Zhang, Shohei Shimizu, and Patrik O Hoyer. Estimation of a Structural Vector\nAutoregression Model Using Non-Gaussianity. pp. 23, 2010. URL https://www.jmlr.\norg/papers/volume11/hyvarinen10a/hyvarinen10a.pdf. 24\nAapo Hyvarinen, Hiroaki Sasaki, and Richard E. Turner. Nonlinear ICA Using Auxiliary Variables\nand Generalized Contrastive Learning. arXiv:1805.08651 [cs, stat], February 2019. URL http:\n//arxiv.org/abs/1805.08651. arXiv: 1805.08651. 6, 7, 20, 24, 25\nAapo Hyvärinen and Petteri Pajunen.\nNonlinear independent component analysis: Existence\nand uniqueness results. Neural Networks, 12(3):429–439, April 1999. ISSN 0893-6080. doi:\n10.1016/S0893-6080(98)00140-3. URL https://www.sciencedirect.com/science/\narticle/pii/S0893608098001403. 2, 23, 24\nAapo Hyvärinen, Ilyes Khemakhem, and Ricardo Monti. Identifiability of latent-variable and\nstructural-equation models: from linear to nonlinear, February 2023. URL http://arxiv.\norg/abs/2302.02672. arXiv:2302.02672 [cs, stat]. 24\nHermanni Hälvä and Aapo Hyvärinen. Hidden Markov Nonlinear ICA: Unsupervised Learning\nfrom Nonstationary Time Series. arXiv:2006.12107 [cs, stat], June 2020. URL http://arxiv.\norg/abs/2006.12107. arXiv: 2006.12107. 24\nHermanni Hälvä, Sylvain Le Corff, Luc Lehéricy, Jonathan So, Yongjie Zhu, Elisabeth Gassiat, and\nAapo Hyvarinen. Disentangling Identifiable Features from Noisy Data with Structured Nonlinear\nICA. arXiv:2106.09620 [cs, stat], June 2021. URL http://arxiv.org/abs/2106.09620.\narXiv: 2106.09620. 24\nGuido W. Imbens and Joshua D. Angrist. Identification and estimation of local average treatment\neffects. Econometrica, 62(2):467, March 1994. ISSN 0012-9682. doi: 10.2307/2951620. URL\nhttp://dx.doi.org/10.2307/2951620. 10, 26\nJikai Jin and Vasilis Syrgkanis. Learning Causal Representations from General Environments:\nIdentifiability and Intrinsic Ambiguity, November 2023. URL http://arxiv.org/abs/\n2311.12267. arXiv:2311.12267 [cs, econ, stat]. 8, 9, 21, 22, 25\nDiviyan Kalainathan, Olivier Goudet, Isabelle Guyon, David Lopez-Paz, and Michèle Sebag. Struc-\ntural Agnostic Modeling: Adversarial Learning of Causal Graphs. arXiv:1803.04929 [stat],\nOctober 2020. URL http://arxiv.org/abs/1803.04929. arXiv: 1803.04929. 24\nZachary Kenton, Ramana Kumar, Sebastian Farquhar, Jonathan Richens, Matt MacDermott, and\nTom Everitt. Discovering agents. Artificial Intelligence, 322:103963, September 2023. ISSN\n0004-3702. doi: 10.1016/j.artint.2023.103963. URL https://www.sciencedirect.com/\nscience/article/pii/S0004370223001091. 5\nIlyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational Autoen-\ncoders and Nonlinear ICA: A Unifying Framework. In International Conference on Artificial\nIntelligence and Statistics, pp. 2207–2217. PMLR, June 2020a. URL http://proceedings.\nmlr.press/v108/khemakhem20a.html. ISSN: 2640-3498. 6, 7, 24, 25\nIlyes Khemakhem, Ricardo Pio Monti, Diederik P. Kingma, and Aapo Hyvärinen. ICE-BeeM: Identi-\nfiable Conditional Energy-Based Deep Models Based on Nonlinear ICA. arXiv:2002.11537 [cs,\nstat], October 2020b. URL http://arxiv.org/abs/2002.11537. arXiv: 2002.11537. 9,\n22, 25\nMichael Kirchhof, Enkelejda Kasneci, and Seong Joon Oh. Probabilistic Contrastive Learning\nRecovers the Correct Aleatoric Uncertainty of Ambiguous Inputs, February 2023. URL http:\n//arxiv.org/abs/2302.02865. arXiv:2302.02865 [cs, stat]. 24\nDavid Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge,\nand Dylan Paiton. Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse\nCoding. arXiv:2007.10930 [cs, stat], March 2021. URL http://arxiv.org/abs/2007.\n10930. arXiv: 2007.10930. 24\n13\n"
    },
    {
      "page_number": 14,
      "text": "Published as a conference paper at ICLR 2025\nSébastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-Based\nNeural DAG Learning. arXiv:1906.02226 [cs, stat], February 2020. URL http://arxiv.\norg/abs/1906.02226. arXiv: 1906.02226. 24\nSébastien Lachapelle, Pau Rodríguez López, Rémi Le Priol, Alexandre Lacoste, and Simon Lacoste-\nJulien. Discovering Latent Causal Variables via Mechanism Sparsity: A New Principle for\nNonlinear ICA. arXiv:2107.10098 [cs, stat], July 2021a. URL http://arxiv.org/abs/\n2107.10098. arXiv: 2107.10098. 26\nSébastien Lachapelle, Pau Rodríguez López, Yash Sharma, Katie Everett, Rémi Le Priol, Alexandre\nLacoste, and Simon Lacoste-Julien. Disentanglement via Mechanism Sparsity Regularization:\nA New Principle for Nonlinear ICA. arXiv:2107.10098 [cs, stat], November 2021b. URL\nhttp://arxiv.org/abs/2107.10098. arXiv: 2107.10098. 26\nSébastien Lachapelle, Tristan Deleu, Divyat Mahajan, Ioannis Mitliagkas, Yoshua Bengio, Simon\nLacoste-Julien, and Quentin Bertrand. Synergies Between Disentanglement and Sparsity: a Multi-\nTask Learning Perspective, November 2022. URL http://arxiv.org/abs/2211.14666.\narXiv:2211.14666 [cs, stat]. 26\nSébastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, and Simon Lacoste-Julien. Additive\nDecoders for Latent Variables Identification and Cartesian-Product Extrapolation, July 2023. URL\nhttp://arxiv.org/abs/2307.02598. arXiv:2307.02598 [cs, stat]. 1, 10\nYuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun\nZhang, and Javen Qinfeng Shi. Identifiable Latent Neural Causal Models, March 2024. URL\nhttp://arxiv.org/abs/2403.15711. arXiv:2403.15711 [cs, stat] version: 1. 4, 8, 10,\n22, 23, 25\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf,\nand Olivier Bachem. Challenging Common Assumptions in the Unsupervised Learning of Disen-\ntangled Representations. In International Conference on Machine Learning, pp. 4114–4124.\nPMLR, May 2019.\nURL http://proceedings.mlr.press/v97/locatello19a.\nhtml. ISSN: 2640-3498. 2, 23, 24\nQi Lyu and Xiao Fu. On Finite-Sample Identifiability of Contrastive Learning-Based Nonlinear\nIndependent Component Analysis. In Proceedings of the 39th International Conference on\nMachine Learning, pp. 14582–14600. PMLR, June 2022. URL https://proceedings.\nmlr.press/v162/lyu22a.html. ISSN: 2640-3498. 10\nAmin Mansouri, Jason Hartford, Yan Zhang, and Yoshua Bengio. Object-centric architectures enable\nefficient causal representation learning, October 2023. URL http://arxiv.org/abs/2310.\n19054. arXiv:2310.19054 null. 26\nFrancesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.\nCausal Discovery with Score Matching on Additive Models with Arbitrary Noise, April 2023a.\nURL http://arxiv.org/abs/2304.03265. arXiv:2304.03265 [cs, stat]. 24\nFrancesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.\nScalable Causal Discovery with Score Matching, April 2023b. URL http://arxiv.org/\nabs/2304.03382. arXiv:2304.03382 [cs, stat]. 24\nRicardo Pio Monti, Kun Zhang, and Aapo Hyvärinen. Causal Discovery with General Non-Linear\nRelationships using Non-Linear ICA. In Uncertainty in Artificial Intelligence, pp. 186–195. PMLR,\nAugust 2020. URL http://proceedings.mlr.press/v115/monti20a.html. ISSN:\n2640-3498. 24\nHiroshi Morioka and Aapo Hyvarinen.\nConnectivity-contrastive learning: Combining causal\ndiscovery and representation learning for multimodal data.\nIn Proceedings of The 26th In-\nternational Conference on Artificial Intelligence and Statistics, pp. 3399–3426. PMLR, April\n2023. URL https://proceedings.mlr.press/v206/morioka23a.html. ISSN:\n2640-3498. 24\n14\n"
    },
    {
      "page_number": 15,
      "text": "Published as a conference paper at ICLR 2025\nHiroshi Morioka, Hermanni Hälvä, and Aapo Hyvärinen. Independent Innovation Analysis for\nNonlinear Vector Autoregressive Process. arXiv:2006.10944 [cs, stat], February 2021. URL\nhttps://arxiv.org/abs/2006.10944. arXiv: 2006.10944. 6, 24\nIgnavier Ng, Zhuangyan Fang, Shengyu Zhu, Zhitang Chen, and Jun Wang. Masked Gradient-\nBased Causal Structure Learning. arXiv:1910.08527 [cs, stat], February 2020. URL http:\n//arxiv.org/abs/1910.08527. arXiv: 1910.08527. 24\nJudea Pearl.\nCausal inference in statistics:\nAn overview.\nStatistics Surveys,\n3\n(none),\nJanuary\n2009a.\nISSN\n1935-7516.\ndoi:\n10.1214/09-SS057.\nURL\nhttps://projecteuclid.org/journals/statistics-surveys/volume-3/\nissue-none/Causal-inference-in-statistics-An-overview/10.1214/\n09-SS057.full. 1, 3, 5, 19\nJudea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, Cambridge,\n2 edition, 2009b. ISBN 978-0-511-80316-1. doi: 10.1017/CBO9780511803161. URL http:\n//ebooks.cambridge.org/ref/id/CBO9780511803161. 2, 4, 6, 23, 24\nRonan Perry, Julius von Kügelgen, and Bernhard Schölkopf. Causal Discovery in Heterogeneous\nEnvironments Under the Sparse Mechanism Shift Hypothesis, October 2022. URL http://\narxiv.org/abs/2206.02013. arXiv:2206.02013 [cs, stat]. 1, 5, 10, 25, 26\nJonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foun-\ndations and learning algorithms. Journal of Statistical Computation and Simulation, 88(16):\n3248–3248, November 2018.\nISSN 0094-9655, 1563-5163.\ndoi: 10.1080/00949655.2018.\n1505197. URL https://www.tandfonline.com/doi/full/10.1080/00949655.\n2018.1505197. 5, 23\nJoaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil Lawrence. Dataset\nshift in machine learning. 01 2009. 10\nGoutham Rajendran, Patrik Reizinger, Wieland Brendel, and Pradeep Ravikumar. An Interventional\nPerspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis,\nNovember 2023. URL http://arxiv.org/abs/2311.18048. arXiv:2311.18048 [cs, eess,\nstat]. 4, 10, 26\nPatrik Reizinger, Yash Sharma, Matthias Bethge, Bernhard Schölkopf, Ferenc Huszár, and Wieland\nBrendel. Jacobian-based Causal Discovery with Nonlinear ICA. Transactions on Machine Learning\nResearch, April 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=\n2Yo9xqR6Ab. 1, 6, 9, 24, 25\nJonathan Richens and Tom Everitt. Robust agents learn causal world models, February 2024. URL\nhttp://arxiv.org/abs/2402.10877. arXiv:2402.10877 [cs]. 1, 10, 21, 24\nEvgenia Rusak, Patrik Reizinger, Attila Juhos, Oliver Bringmann, Roland S. Zimmermann, and\nWieland Brendel. InfoNCE: Identifying the Gap Between Theory and Practice, June 2024. URL\nhttp://arxiv.org/abs/2407.00143. arXiv:2407.00143 [cs, stat]. 1, 25\nBernhard Schölkopf. Causality for machine learning. 2019. doi: 10.1145/3501714.3501755. 10\nBernhard Schölkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij.\nOn Causal and Anticausal Learning. arXiv:1206.6471 [cs, stat], June 2012. URL http://\narxiv.org/abs/1206.6471. arXiv: 1206.6471. 1, 10, 26\nBernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,\nAnirudh Goyal, and Yoshua Bengio. Towards Causal Representation Learning. arXiv:2102.11107\n[cs], February 2021. URL http://arxiv.org/abs/2102.11107. arXiv: 2102.11107\nversion: 1. 1, 3, 24\nShohei Shimizu, Patrik O Hoyer, Aapo Hyvarinen, and Antti Kerminen. A Linear Non-Gaussian\nAcyclic Model for Causal Discovery. pp. 28, 2006. 6, 24\n15\n"
    },
    {
      "page_number": 16,
      "text": "Published as a conference paper at ICLR 2025\nShohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara,\nTakashi Washio, Patrik O Hoyer, Kenneth Bollen, and Patrik Hoyer. Directlingam: A direct\nmethod for learning a linear non-gaussian structural equation model. Journal of Machine Learning\nResearch-JMLR, 12(Apr):1225–1248, 2011. 26\nPeter L Spirtes, Christopher Meek, and Thomas S Richardson. Causal inference in the presence of\nlatent variables and selection bias. arXiv preprint arXiv:1302.4983, 2013. 26\nChandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler.\nLinear Causal Disentangle-\nment via Interventions, February 2023.\nURL http://arxiv.org/abs/2211.16467.\narXiv:2211.16467 [cs, stat]. 24\nJulius von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf, Michel\nBesserve, and Francesco Locatello. Self-Supervised Learning with Data Augmentations Provably\nIsolates Content from Style, June 2021. URL http://arxiv.org/abs/2106.04619.\narXiv: 2106.04619. 24, 26\nJulius von Kügelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Keki´c, Elias Barein-\nboim, David M. Blei, and Bernhard Schölkopf. Nonparametric Identifiability of Causal Represen-\ntations from Unknown Interventions, October 2023. URL http://arxiv.org/abs/2306.\n00542. arXiv:2306.00542 [cs, stat]. 21, 22, 25\nYuhao Wang, Chandler Squires, Anastasiya Belyaeva, and Caroline Uhler. Direct Estimation of Dif-\nferences in Causal Graphs. In Advances in Neural Information Processing Systems, volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/\nhash/e1314fc026da60d837353d20aefaf054-Abstract.html. 9\nLiang Wendong, Armin Keki´c, Julius von Kügelgen, Simon Buchholz, Michel Besserve, Luigi\nGresele, and Bernhard Schölkopf. Causal Component Analysis, October 2023. URL http:\n//arxiv.org/abs/2305.17225. arXiv:2305.17225 [cs, stat]. 3, 8, 10, 21, 22, 25\nThaddäus Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, and Wieland\nBrendel. Provable Compositional Generalization for Object-Centric Learning, October 2023a.\nURL http://arxiv.org/abs/2310.05327. arXiv:2310.05327 [cs]. 1, 10\nThaddäus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, and Wieland Brendel. Compositional\nGeneralization from First Principles, July 2023b. URL http://arxiv.org/abs/2307.\n05596. arXiv:2307.05596 [cs, stat]. 1, 10\nQuanhan Xi and Benjamin Bloem-Reddy. Indeterminacy and Strong Identifiability in Generative\nModels, February 2023. URL http://arxiv.org/abs/2206.00801. arXiv:2206.00801\n[cs, stat]. 1\nMengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. CausalVAE:\nDisentangled Representation Learning via Neural Structural Causal Models. In 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 9588–9597, Nashville, TN,\nUSA, June 2021. IEEE. ISBN 978-1-66544-509-2. doi: 10.1109/CVPR46437.2021.00947. URL\nhttps://ieeexplore.ieee.org/document/9578520/. 24\nDingling Yao, Dario Rancati, Riccardo Cadei, Marco Fumero, and Francesco Locatello. Unifying\nCausal Representation Learning with the Invariance Principle, September 2024. URL http:\n//arxiv.org/abs/2409.02772. arXiv:2409.02772 [cs, stat]. 24\nMatej Zeˇcevi´c, Devendra Singh Dhami, Petar Veliˇckovi´c, and Kristian Kersting. Relating Graph\nNeural Networks to Structural Causal Models. arXiv:2109.04173 [cs, stat], October 2021. URL\nhttp://arxiv.org/abs/2109.04173. arXiv: 2109.04173. 1, 24\nK. Zhang, M. Gong, and B. Schölkopf.\nMulti-source domain adaptation: A causal view.\nIn\nProceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 3150–3157. AAAI\nPress, 2015. URL http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/\nview/10052/9994. 10\n16\n"
    },
    {
      "page_number": 17,
      "text": "Published as a conference paper at ICLR 2025\nKun Zhang and Aapo Hyvarinen.\nOn the Identifiability of the Post-Nonlinear Causal Model.\narXiv:1205.2599 [cs, stat], May 2012. URL http://arxiv.org/abs/1205.2599. arXiv:\n1205.2599. 24\nXun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing.\nDAGs with NO TEARS:\nContinuous Optimization for Structure Learning. March 2018. URL https://arxiv.org/\nabs/1803.01422v2. 24, 26\nRoland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel.\nContrastive Learning Inverts the Data Generating Process. arXiv:2102.08850 [cs], February 2021.\nURL http://arxiv.org/abs/2102.08850. arXiv: 2102.08850. 1, 6, 24, 25\n17\n"
    },
    {
      "page_number": 18,
      "text": "Published as a conference paper at ICLR 2025\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nθ1\nθ2\nO1\nψ\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\nO2\n(a) Identifiable Exchangeable Mechanisms\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nθ1\nθ2\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\n(b) Causal Discovery\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nS2\n1\nS2\n2\nO2\n(c) Independent Component Analysis\nZ1\n1\nZ1\n2\nS1\n1\nS1\n2\nO1\nθ1\nθ2\nψ\nZ2\n1\nZ2\n2\nS2\n1\nS2\n2\nO2\n(d) Causal Representation Learning\nFigure 4: Identifiable Exchangeable Mechanisms (IEM)–A unified model for structure and\nrepresentation identifiability: Here we show that exchangeable but non-i.i.d. data enables iden-\ntification in key methods across Causal Discovery (CD), Independent Component Analysis (ICA),\nand Causal Representation Learning (CRL). The graphical model in Fig. 1a shows the IEM model,\nwhich subsumes Causal Discovery (CD) (§ 3.2), Independent Component Analysis (ICA) (§ 3.3),\nand Causal Representation Learning (CRL) (§ 3.4). S denotes latent, Z causal, and O observed\nvariables with corresponding latent parameters θ, ψ, superscripts denote different samples. Red\ndenotes observed/known quantities, blue stands for target quantities, and gray illustrates components\nthat are not explicitly modeled in a particular paradigm. θi are latent variables controlling sepa-\nrate probabilistic mechanisms, indicated by dotted vertical lines. CD (Fig. 1b) corresponds to the\nleft-most layer of IEM, focusing on the study of cause-effect relationships between observed causal\nvariables; ICA (Fig. 1c) infers source variables from observations, but without causal connections in\nthe left-most layer of IEM; CRL (Fig. 1d) shares the most similar structure with IEM, as it has both\nlayers, including the intermediate causal representations\n18\n"
    },
    {
      "page_number": 19,
      "text": "Published as a conference paper at ICLR 2025\nA\nPROOFS AND EXTENDED THEORY\nA.1\nCAUSE/MECHANISM VARIABILITY FOR BIVARIATE CD: THM. 2\nTheorem 2. [Cause/mechanism variability is necessary and sufficient for bivariate CD] Given a\nsequence of bivariate pairs {Xn, Y n}n∈N such that for any N ∈N, the joint distribution can be\nrepresented as:\n• X →Y : p(x1, y1, . . . , xN, yN) =\nR\nθ\nR\nψ\nQ\nn p(yn|xn, ψ)p(xn|θ)p(θ)p(ψ)dθdψ\n• X ←Y : p(x1, y1, . . . , xN, yN) =\nR\nθ\nR\nψ\nQ\nn p(xn|yn, θ)p(yn|ψ)p(ψ)p(θ)dθdψ\nThen the causal direction between variables X, Y can still be distinguished when:\n1. either only p(θ) = δθ0(θ) for some constant θ0 or only p(ψ) = δψ0(ψ) for some constant\nψ0 (but not both). Fig. 2b and Fig. 2c show the Markov structure of such factorizations.\n2. the distribution of P is faithful (Defn. 4) w.r.t. Fig. 2b or Fig. 2c.\nProof. The impossibility of both mechanisms being degenerate (i.e., the i.i.d. case) is well-known\n(Pearl, 2009a). For distributions that are Markov and faithful to Fig. 2a, Fig. 2b and Fig. 2c, one can\ndifferentiate the causal direction through checking Y 1 ⊥X2|X1, X1 ⊥Y 2|Y 1 and X1 ⊥Y 1. One\ncan observe Y 1 ⊥X2|X1 only holds in Fig. 5a and fails at Fig. 5b.\nX1\nY 1\nθ\nψ\nX2\nY 2\nX1\nY 1\nθ\nψ\nX2\nY 2\nX2 ̸⊥Y 1|Y 2\nX1 ⊥Y 2|X2\nX2 ⊥Y 1|Y 2\nX1 ̸⊥Y 2|X2\n(a) Cause variability\nX1\nY 1\nθ\nψ\nX2\nY 2\nX1\nY 1\nθ\nψ\nX2\nY 2\nY 1 ⊥X2|X1\nX1 ̸⊥Y 2|Y 1\nY 1 ̸⊥X2|X1\nX1 ⊥Y 2|Y 1\n(b) Mechanism variability\nFigure 5: We show that the richness argument in CdF (Guo et al., 2024a) can be realized, in the\nbivariate case, via either only varying the prior of the causes’ parameters θ (Fig. 5a) or the prior of\nthe mechanism’ parameters ψ (Fig. 5b). That is, it is not necessary to have rich priors for both θ, ψ\nA.2\nEXCHANGEABILITY IN TCL: LEM. 3\nLemma 3. [TCL is identifiable due to exchangeable non–i.i.d. sources] The sufficient variability\ncondition in TCL corresponds to cause variability, i.e., exchangeable non–i.i.d. source variables with\na fixed mixing function, which leads to the identifiability of the latent sources.\nProof. Latent variables violating the sufficient variability (Assum. 2) condition in nonlinear ICA\nimply that those variables are i.i.d.; thus, if more than one latent variables violate this condition, then\nthey become non-identifiable; thus, making non-delta priors a necessary condition for identifiability\nof factorizing priors (assuming that no further constraints can be applied, e.g., on the function class).\nAn important fact for the proof is that the parameters θi are continuous RVs, as they parametrize an\nexponential family distribution. That is, their support has infinitely many distinct values, each with a\nprobability of zero—this will be important to reason about the zero-measure of edge cases where two\nparameters happen to be “tuned\" to each other, violating the sufficient variability condition.\nThe full rank condition of the matrix L means that ∀i ̸= j environment indices for two rows in matrix\nL\nθi −θ0 ̸= c ·\n\u0000θj −θ0\u0001\n; c > 0\n(12)\n⇐= : Non-delta priors imply sufficient variability. Assume a real-valued RV θi with corresponding\nnon-delta parameter prior p(θi). Then, outside of a zero-measure set, the sufficient variability\ncondition holds. Assume that ∀i : p(θi) ̸= δ(θi −θi0). Then, as all θi are real RVs, their support has\n19\n"
    },
    {
      "page_number": 20,
      "text": "Published as a conference paper at ICLR 2025\ninfinitely many values. Thus, the probability of (12) being violated (by setting both sides to be equal\nand solving for θi) is zero:\nPr\n\u0002\nθi = c · θj −(c −1)θ0\u0003\n= 0,\n(13)\nfrom which it follows that there exists such θi, θj where (12) holds, implying that L has full rank.\nThus, Assum. 2 holds.\n=⇒: Sufficient variability implies non-delta priors. When rank (L) = n, then none of p(θi) is\ndelta. rankL = n = dim s means that ∀i ̸= j environment indices for two rows in matrix L (12)\nholds. That is, we can construct L such that any two rows are linearly independent3. If for coordinate\nk, θe\nk = θk = const, then L cannot have full column rank. Since θk cannot be constant for all k, this\nrequires that p(θk) is non-delta.\nA.3\nEXCHANGEABILITY IN GCL (HYVARINEN ET AL., 2019): EXTENDING LEM. 3\nHyvarinen et al. (2019) proposed a generalization of TCL (and other auxiliary-variable ICA methods),\ncalled GCL. GCL uses a more general conditional distribution, it only assumes that assumes that the\nconditional log-density log p(s|u) is a sum of components qi(si, u):\nlog p(s|u) =\nX\ni\nqi(si, u)\n(14)\nFor this generalized model, they define the following variability condition:\nAssumption 4 (Assumption of Variability). For any y ∈Rn (used as a drop-in replacement for the\nsources s), there exist 2n + 1 values for the auxiliary variable u, denoted by uj, j = 0 . . . 2n such\nthat the 2n vectors in R2n given by\n(w (y, u1) −w (y, u0)) , (w (y, u2) −w (y, u0)) . . . , (w (y, u2n) −w (y, u0))\nwith\nw(y, u) =\n\u0012∂q1 (y1, u)\n∂y1\n, . . . , ∂qn (yn, u)\n∂yn\n,\n∂2q1 (y1, u)\n∂y2\n1\n, . . . , ∂2qn (yn, u)\n∂y2n\n\u0013\nare linearly independent.\nAssum. 4 puts a constraint on the components of the first- and second derivatives of the functions\nconstituting the conditional log-density of the source/latent variables, conditioned on the auxiliary\nvariable u. As the authors write: “[Assum. 4] is basically saying that the auxiliary variable must\nhave a sufficiently strong and diverse effect on the distributions of the independent components.\"\nWe focus on a special case, which assumes that the source conditional log-densities qi(si, u) are\nconditionally exponential, i.e.:\nqi (si, u) =\nk\nX\nj=1\n[˜qij (si) θij(u)] −log Ni(u) + log Qi(si),\n(15)\nwhere k is the order of the exponential family, Ni is the normalizing constant, Qi the base measure,\n˜qi is the sufficient statistics, and the modulation parameters θi := θi(u) depend on u. In this case,\nAssum. 4 becomes similar to Assum. 2, but the modulation parameter matrix now has (E −1) × nk\ndimensions, where the rows are:\n[L]j: = (θj −θ0)⊤\n(16)\nθj =\nh\nθj\n11, . . . , θj\nnk\ni\n.\n(17)\nIn this case, we can generalize Lem. 3 to GCL:\nCorollary 1 (GCL with conditionally exponential family sources is identifiable due to exchangeable\nnon–i.i.d. sources). The sufficient variability condition in GCL with conditionally exponential family\nsources corresponds to cause variability, i.e., exchangeable non–i.i.d. source variables with fixed\nmixing function, which leads to the identifiability of the latent sources.\nProof. The proof follows from the proof of Lem. 3, the only difference is that for each source si,\nthere are k sufficient statistics ˜qik and modulation parameters θik(u). Thus, the modulation parameter\nmatrix L (Assum. 2) will be [(E −1) × nk]−dimensional where n = dim s.\n3Note that θi can be correlated, as Hyvarinen et al. (2019) pointed out in the proof of their Thm. 2\n20\n"
    },
    {
      "page_number": 21,
      "text": "Published as a conference paper at ICLR 2025\nA.4\nDUALITY OF CAUSE AND MECHANISM VARIABILITY FOR TCL: LEM. 4\nLemma 4. [Duality of cause and mechanism variability for TCL] For a given deterministic mix-\ning function f : s →o and conditionally factorizing (non-stationary) latent sources p(s|u) =\nQ\ni pi(si|u) fulfilling the sufficient variability of TCL, there exists an equivalent setup with stationary\n(i.i.d.) sources p(s) = Q\ni pi(si) with stochastic functions ˆf = f ◦g : s →o, where g = g(u) and\neach component gi is defined as an element-wise function such that the pushforward of pi(si) by gi\nequals pi(si|u), i.e., gi∗pi(si) = pi(si|u). Then, gi∗pi(si) fulfils the same variability condition; thus,\nthe same identifiability result applies.\nProof. The proof follows from the observation that it is a modelling choice which component provides\nthe source of non-stationarity. That is, we can incorporate the transformation of the source variables\ninto the source distribution (cause variability, as TCL does) or we can think of that as stochasticity in\nthe mixing function (mechanism variability).\nA.5\nEXCHANGEABILITY IN CAUCA: LEM. 5\nLemma 5. [Non-delta priors in the causal mechanisms can enable identifiable CRL]\nIf the interventional discrepancy condition Assum. 3 holds, then the parameter priors in (10) cannot\nequal a delta distribution, i.e., p(θj) ̸= δθj0(θj); thus, if the other conditions of CauCA hold, then,\nthe causal variables zi are identifiable. For real-valued θj, non-delta priors also imply Assum. 3\nalmost everywhere.\nProof. We define each mechanism p(zi|Pa(zi)) as\np(zi|Pa(zi)) =\nZ\nθi\np(zi|Pa(zi), θi)p(θi)dθi\n(18)\nThus, the observational and interventional mechanisms, respectively, are:\npi = p(zi|Pa(zi), θi = θ0\ni )\n(19)\n˜pi = p(zi|Pa(zi), θi = ˜θi)\n(20)\nThat is, each intervention corresponds to a specific parameter value θi (which exist by the CdF\ntheorem (Thm. 1). Thus, (18) is akin to mixtures of interventions (Richens & Everitt, 2024, Defn. 3).\n=⇒: Interventional discrepancy implies non-delta priors.\nWe prove this direction by contradiction. Assume that Assum. 3 is fulfilled and p(θi) = δθi0. Then\n˜θi = θi, so Assum. 3 cannot hold.\n⇐= : Non-delta priors for real-valued parameters imply interventional discrepancy.\nIf θi is real-valued, then the following probability is zero:\nPr\nh\n˜θi = θ0\ni\ni\n= 0,\n(21)\nthus, there must exist ˜θi ̸= θ0\ni , and this inequality holds almost everywhere since:\nPr\nh\n˜θi ̸= θ0\ni\ni\n= 1 −Pr\nh\n˜θi = θ0\ni\ni\n= 1 −0 = 1,\n(22)\nRemark 1 (Non-delta priors do not always imply interventional discrepancy almost everywhere).\nWhen the parameter priors p(θi) are not a delta distribution, then, barring the case when sampling\nthe interventional mechanism parameter ˜θi yields the same θ0\ni , then the distributions pi and ˜pi\nwould differ. However, this is not necessarily a zero-measure event, e.g., when p(θ) has a Bernoulli\ndistribution with parameter 1/2. Thus, Assum. 3 cannot hold almost everywhere without further\nrestrictions.\nA.6\nEXCHANGEABILITY AND THE GENERICITY CONDITION FROM VON KÜGELGEN ET AL.\n(2023) AND JIN & SYRGKANIS (2023)\nvon Kügelgen et al. (2023) extends CauCA Wendong et al. (2023) by providing identifiability proofs\nfor CRL without parametric assumptions on the function class. Their assumption (von Kügelgen et al.,\n2023, (A3’) in Thm. 3.4) is stronger than Assum. 3 as it requires that two interventional densities\ndiffer everywhere—or that the observational and one interventional density differ (von Kügelgen\n21\n"
    },
    {
      "page_number": 22,
      "text": "Published as a conference paper at ICLR 2025\net al., 2023, (A3) in Thm. 3.2). Furthermore, (von Kügelgen et al., 2023, (A4) in Thm. 3.2) excludes\nthe pathogological case of fine-tuned densities (thus the name genericity condition)—this might be\nthought to be an analog of faithfulness Defn. 4.\nJin & Syrgkanis (2023) leverages a similar assumption as Assum. 3 in their (Jin & Syrgkanis, 2023,\nDef. 6). They strengthen the nonparametric identifiability results of von Kügelgen et al. (2023) by\nshowing that dim Z single-node soft interventions with unknown targets are sufficient to identify the\ncausal variables.\nTowards identifying the exogenous variables in CRL.\nBoth von Kügelgen et al. (2023); Jin\n& Syrgkanis (2023) derive identifiability results for the causal variables from interventional data.\nHowever, they do not make claims about the exogenous variables. Based on the insights of our unified\nmodel, IEM, provides, we investigate whether there is an identifiability proof that encompasses the\nwhole hierarchy.\nConsider that the mechanism p(Zi|Pa(Zi)) can be intervened upon by changing how the other causal\nvariables Zj ∈Pa(Zi) affect Zi. Alternatively, p(Zi|Pa(Zi)) can also change by modifying the\ndistribution of the corresponding exogenous variable Si—note that this corresponds to a one-node\nsoft intervention. Thus, if the other assumptions of Jin & Syrgkanis (2023) holds, we can say that:\nsingle-node soft interventions on Si can satisfy the genericity condition of Jin & Syrgkanis (2023).\nTo reason about the identifiability of the exogenous variables, we need the variability of their\ndistribution across the available environments. Our summary of the assumptions in Tab. 1 suggests\nthat with sufficiently many environments, it should be possible to identify the exogenous variables as\nwell. If the only assumption we make is exchangeability, then, following the reasoning of Khemakhem\net al. (2020b), we might need dim Z + 1 additional environments (2 dim Z + 1 in total). If we further\nrestrict the source distributions to belong to the exponential family, then we can apply TCL (and\nlinear ICA on top) to identify the source variables. Thus, we can state:\nLemma 6. [Simultaneous identifiability via generic non-degenerate source priors] Provided the\nassumptions of (Jin & Syrgkanis, 2023, Thm. 4) hold with the restriction of the source variables’ den-\nsity belonging to the exponential family of order one, and assuming that the nonparametric structural\nassignments are generic such that single-node soft interventions on each Si satisfy Assum. 3, then\n(dim Z+1) interventions can provide exchangeable data sufficient for the simultaneous identification\nof both exogenous and causal variables (and also the DAG)—as opposed to (2 dim Z + 1), where\ndim Z separate environments are used for CRL and another (dim Z + 1) for ICA.\nProof. Our goal is to prove that performing CRL can lead to the additional identifiability of the\nexogenous sources, with a negligible overhead in terms of assumptions on the data, compared to\nperforming only CRL. Also, we aim to show that the joint identifiability requires less data (less\nenvironments) than performing both tasks separately. We start by assuming that (Jin & Syrgkanis,\n2023, Thm. 4) holds, which implies Assum. 3. By Lem. 5, we know that when Assum. 3 holds,\nthen the parameter priors are non-degenerate. We assumed that the single-node soft interventions\nonly affect Si. Following the reasonings of von Kügelgen et al. (2023); Jin & Syrgkanis (2023),\nw.l.o.g., if the structural assignments in the nonparametric SEM are not fine-tuned (i.e., they are\ngeneric), then Assum. 3 should hold. Then, as the source distributions are exchangeable, we can\napply Lem. 3, which states that Assum. 2 also holds. Thus, we can identify the exogenous variables\nas well, concluding the proof.\nWe leave it to future work to investigate whether identifying both causal and exogenous variables is\npossible from fewer environments. Nonetheless, we believe that this example shows the potential\nadvantage of the IEM framework for providing new identifiability results.\nA.7\nEXCHANGEABILITY IN THE UNIFIED MODEL: LEM. 1\nInterventional discrepancy and the derivative condition of (Liu et al., 2024)\nThe identifiability\nresult of Liu et al. (2024) combines the results from ICA and CRL. As they use TCL to learn the latent\nsources, we can apply Lem. 3. To see how the causal variables and the edges between them can also\nbe learned, we first relate the derivative condition on the structural assignments to the interventional\ndiscrepancy condition of Wendong et al. (2023) (Assum. 3).\nAssum. 1 requires access to a set of environments (indexed by auxiliary variable u), such that for\neach parent zj ∈Pa(zi) node, there is an environment, where the edge zj →zi is blocked. To relate\nAssum. 1 to the interventional discrepancy Assum. 3, we recall that Wendong et al. (2023) note that\nfor perfect interventions, the conditioning on the parents for the interventional density in Assum. 3\ndisappears. Thus, we interpret Assum. 1 as “emulating\" perfect interventions for each zi. By this, we\n22\n"
    },
    {
      "page_number": 23,
      "text": "Published as a conference paper at ICLR 2025\nmean that we need data from such environments, where the structural assignments change as if a\nperfect intervention is carried out to remove the zj →zi edge.\nLemma 1. [Identifiable Latent Neural Causal Models are identifiable with exchangeable sources\nand mechanisms] The model of Liu et al. (2024) (Fig. 1a) identifies both the latent sources s and the\ncausal variables z (including the graph), by the variability of s via a non-delta prior over θs and by\nthe variability of the structural assignments via θg.\nProof. As the authors rely on TCL and a form of the interventional discrepancy Assum. 3, the proof\nfollows from Lem. 3 and Lem. 5.\nA.8\nINDEPENDENT SOURCE AND STRUCTURAL ASSIGNMENT CDF PARAMETERS IN ANMS:\nLEM. 2\nLemma 2. [Independent source and structural assignment CdF parameters for ANMs] In the setting\nof Liu et al. (2024), where the SEM is an ANM, the CdF parameters for the sources, θs, and the\nstructural assignments, θg, are independent, i.e. p(θg, θs) = p(θg)p(θs).\nProof. In the model of Liu et al. (2024), the two identifiability results impose two non-i.i.d. require-\nments: to identify the latent sources, a sufficient variability condition from TCL is required (cf. the\ngeneralized version in Assum. 4), whereas for CRL, a derivative-based condition on the mechanisms\n(akin to Assum. 3) is required. As the SEM is an ANM in this case, defined by zi := gi(Pa(zi)) + si,\nand the exogenous variables are assumed to be independent from gi(Pa(zi)). Thus, it is impossible\n(assuming faithfulness) that a change in θs\ni would change θg\ni ; otherwise, gi(Pa(zi)) and si would be\ndependent. That is, their parameters are independent.\nB\nDEFINITIONS\nDefinition 2 (d-separation (adapted from Defn. 6.1 in (Peters et al., 2018))). Given a DAG G, the\ndisjoint subsets of nodes A and B are d-separated by a third (also disjoint) subset S if every path\nbetween nodes in A and B is blocked by S. We then write\nA ⊥G B | S.\nDefinition 3 (Global Markov property (adapted from Defn. 6.21(i) in (Peters et al., 2018))). Given a\nDAG G and a joint distribution P, P satisfies the global Markov property w.r.t. the G if\nA ⊥G B|C ⇒A ⊥B|C\n(23)\nfor all disjoint vertex sets A, B, C (the symbol ⊥G denotes d-separation, cf. Defn. 2).\nDefinition 4 (Faithfulness (adapted from Defn. 6.33 in (Peters et al., 2018))). Consider a distribution\nP and a DAG G. Then, P is faithful to G if for all disjoint node sets A, B, C:\nA ⊥B |C ⇒A ⊥G B| C.\n(24)\nThat is, if a conditional independence relationship holds in P, then the corresponding node sets are\nd-separated in G.\nDefinition 5 (Markov equivalence class of graphs (adapted from Defn. 6.24 in (Peters et al., 2018)).\nWe denote by M(G) the set of distributions that are Markovian w.r.t. G : M(G) := {P : P\nsatisfies the global Markov property w.r.t. G}. Two DAGs G1 and G2 are Markov equivalent if\nM (G1) = M (G2), i.e.. if and only if G1 and G2 satisfy the same set of d-separations, which means\nthe Markov condition entails the same set of (conditional) independence conditions. The set of all\nDAGs that are Markov equivalent to some DAG is called Markov equivalence class of G.\nC\nWHY DOES I.I.D. DATA FAIL?\nWe next assay key results and provide concrete examples to illustrate why i.i.d. data fails to enable\nidentification for both structure and representation learning.\nExample 2 (Bivariate CD is impossible from i.i.d. data (Pearl, 2009b)). One cannot distinguish\nX →Y from X ←Y from i.i.d. data as both structures imply identical graphical conditional\nindependence, i.e., ∅. Thus, bivariate CD is impossible in i.i.d. data without further parametric\nassumptions.\nLearning disentangled latent factors is also impossible without further parametric assumptions in i.i.d.\ndata (Hyvärinen & Pajunen, 1999; Locatello et al., 2019):\n23\n"
    },
    {
      "page_number": 24,
      "text": "Published as a conference paper at ICLR 2025\nExample 3 (Gaussian latent factors are not identifiable from i.i.d. data). Assume independent\nlatents with Gaussian components, i.e. p(s) = Q\ni pi(si), where pi(si) = N\n\u0000µi; σ2\ni\n\u0001\n. Even if\n∀i, j : σ2\ni ̸= σ2\nj̸=i, Gaussian sources are not identifiable to their rotational symmetry and the\nscale-invariance of ICA.\nWe show in § 3 how exchangeability unifies the non-i.i.d. conditions (often termed, weak supervision\nor auxiliary information) in many causal structure and representation identifiability methods.\nD\nRELATED WORK\nIdentifiable representation learning and ICA.\nIdentifiable representation learning aims to learn\n(low-dimensional) latent variable models (LVMs) from (high-dimensional) observations. The most\nprevalent family of models is that of Independent Component Analysis (ICA) (Comon, 1994; Hy-\nvarinen et al., 2001), which assumes that the observations are a mixture of independent variables\nvia a deterministic mixing function. Identifiability means that the latents can be can be recovered\nup to indeterminacies (e.g., permutation, element-wise transformations). As this is provably im-\npossible in the nonlinear case without further assumptions (Darmois, 1951; Hyvärinen & Pajunen,\n1999; Locatello et al., 2019), recent work has focused auxiliary-variable ICA, where the latents\nare conditionally independent given the auxiliary variable u (Hyvarinen et al., 2019; Gresele et al.,\n2019; Khemakhem et al., 2020a; Hälvä et al., 2021; Hyvarinen & Morioka, 2017; 2016; Hälvä &\nHyvärinen, 2020; Morioka et al., 2021; Monti et al., 2020; Hyvarinen et al., 2010; Klindt et al., 2021;\nZimmermann et al., 2021)—despite the latents are not marginally independent, the literature still\nrefers to these models are ICA. Several such methods model multiple environments with an auxiliary\nvariable, which is also known as using ensembles (Eastwood et al., 2023; Kirchhof et al., 2023).\nWe note that some methods make functional assumptions (Shimizu et al., 2006; Hoyer et al., 2008;\nZhang & Hyvarinen, 2012; Gresele et al., 2021), but our focus is on the auxiliary-variable methods.\nRecently, Bizeul et al. (2024) developed a probabilistic model for self-supervised representation\nlearning, including auxiliary-variable ICA methods.\nCausality.\nSEMs model cause-effect relationships between causal variables zi, where each zi is\ndetermined by a deterministic function gi(Pa(zi)), where Pa(zi) includes all the zj<i causal variables\nthat cause zi and also the stochastic exogenous variable xi. Learning causal models enables to make\nmore fine-grained (interventional, counterfactual) queries compared to observational data (Pearl,\n2009b). CD aims to uncover the graph between the zi from observing zi. This admits interventional\nqueries. CRL also learns that zi from high-dimensional observations (Schölkopf et al., 2021). Causal\nmethods need to rely on certain assumptions, either restricting the distribution of the exogenous\nvariables (Kalainathan et al., 2020; Lachapelle et al., 2020; Shimizu et al., 2006; Monti et al., 2020),\nand/or the function class of the SEM (Shimizu et al., 2006; Zheng et al., 2018; Squires et al., 2023;\nMontagna et al., 2023b;a; Gresele et al., 2021; Hoyer et al., 2008; Ng et al., 2020; Lachapelle et al.,\n2020; Annadani et al., 2021; Yang et al., 2021). Concurrently with our work, Yao et al. (2024)\nprovides an invariance-based framework to unify CRL. Their framework can encompass multi-view,\nmulti-environment, and also temporal settings—ouw work focuses on the multi-environment case,\nbut it also includes representation learning and CD.\nConnections between representation learning and causality.\nCausality and identifiability both\naim to recover some ground truth structures (latent factors, DAGs, or functional relationships), thus,\nseveral works explored possible connections (Reizinger et al., 2023; Morioka & Hyvarinen, 2023;\nHyvärinen et al., 2023; Zeˇcevi´c et al., 2021; Richens & Everitt, 2024; Monti et al., 2020). Several\nmethods connected ICA to the SEM model in causality (Gresele et al., 2021; Monti et al., 2020;\nShimizu et al., 2006; von Kügelgen et al., 2021; Hyvärinen et al., 2023). An important observation\nwe rely on is that identifiability guarantees from require a notion of non-i.i.d.ness, e.g., both the ICA\nand the causal literature often relies on the multi-environmental setting.\n24\n"
    },
    {
      "page_number": 25,
      "text": "Published as a conference paper at ICLR 2025\nTable 1: Mixing assumptions: p(s) stands for assumptions on the source distribution, f on the\nmixing function, ⊥stands for independence (the superscript u denotes conditional independence\ngiven u), CEF for conditional exponential family (the superscript + denotes monotonicity, 2 denotes\na CEF of order two), ING for independent non-Gaussian (in Jin & Syrgkanis (2023) maximum one\nGaussian component allowed, the distributions need to be different; in Zimmermann et al. (2021),\nthe Lα metric is such that α ≥1, α ̸= 2), exg. stands for exchangeability, AG for an anistropic\nGaussian on the hypersphere, EnAG for an ensemble of such Gaussians, inj. for injectivity, surj. for\nsubjectivity, C2 for diffeomorphism; SEM assumptions: Z stands for assumptions on the causal\nvariables, g on the SEM, M denotes the Markov assumption, F faithfulness (or lack thereof), NP\nstands for non-parametric; Interventional (variability) assumptions: # denotes the number of\nnodes affected by the intervention, P/S denotes perfect or soft interventions, the target column\nwhether the intervention targets are known, |E| stands for the number of environments (d = dim Z),\nk is the order of the exponential family; Identifiability ambiguities: DAG denotes identifiability\nof the causal graph (✓means the DAG is known; ✓’s come from the result of Reizinger et al.\n(2023)), h denotes identifiability up to elementwise (non-)linear transformations, D denotes scaling,\nπ permutations, c a constant shift, O an orthogonal, {O} a block-orthogonal, A an invertible matrix.\nMixing\nSEM\nInterventions\nIdent. Z\nIdent. S\nMethod\nS\nf\nZ\ng\n# type target\n|E|\nDAG h D π c\nA\nh D π c\nGuo et al. (2024a)\nF\nexg.\n0\n✓\nHyvarinen & Morioka (2016) CEF\nC2\n-\nS\n✗\nd+1\n✓\n✓✓✓✓\nHyvarinen & Morioka (2016) CEF+\nC2\n-\nS\n✗\nd+1\n✓\n✗\n✓✓✓✓\nHyvarinen et al. (2019)\n⊥u\nC2\n-\nS\n✗\n2d+1\n✓\n✗\n✓✓✓✗\nHyvarinen et al. (2019)\nCEF\nC2\n-\nS\n✗\ndk+1\n✓\n✓✓✓✓\nZimmermann et al. (2021)\nvMF\nC2\nd\nS\n✗\n1\nO\n✗✗✓✗\nZimmermann et al. (2021)\nR\nC2\nd\nS\n✗\n1\n✓\n✗✗✓✓\nZimmermann et al. (2021)\nING\nC2\nd\nS\n✗\n1\n✓\n✗\n✗✓✓✗\nRusak et al. (2024)\nAG\nC2\nd\nS\n✗\n1\n{O} ✗✗✓✗\nRusak et al. (2024)\nEnAG\nC2\n-\nS\n✗\n1<\n✓\n✗\n✗✗✓✗\nKhemakhem et al. (2020a)\nCEF2\ninj.\n-\nS\n✗\ndk+1\n✓\n✗\n✓✓✓✓\nKhemakhem et al. (2020b)\nexg.\nsurj.4\n-\nS\n✗\n2d+1\n✓\n✗✓✓✓\nKhemakhem et al. (2020b)\nexg.\nsurj.+\n-\nS\n✗\n2d+1\n✓\n✗\n✗✓✓✓\nKhemakhem et al. (2020b)\nexg.\nsurj.2\n-\nS\n✗\n2d+1\n✓\n✗\n✗✓✓✓\nReizinger et al. (2023)\n✓\n✗\n✓✓✓✓\nWendong et al. (2023)\n⊥\nC2\nM\n1\nS\n✓\nd\n✓\n✓✓✗✓\nWendong et al. (2023)\n⊥\nC2\nM\n1\nP\n✓\nd\n✓\n✗✓✗✗\nvon Kügelgen et al. (2023)\n⊥\nC2\nF\nNP\n1\nP\n✗\n2d\n✓\n✓✓✓✓\nJin & Syrgkanis (2023)\nING\nC2\n̸ F\nlin\n1\nS\n✗\nd2\n✓\n✓✓✓✓\nJin & Syrgkanis (2023)\nING\nC2\n̸ F\nlin\n-\nS\n✗\nd\n✓\n✓✓✓✓\nJin & Syrgkanis (2023)\n⊥\nC2\n̸ F\nNP\n1\nS\n✗\nd\n✓\n✓✓✓✓\nLiu et al. (2024)\nCEF2\ninj.\nF ANM 1\nP\n✗\n2d+1\n✓\n✗✓✗✓\n✗\n✗✓✓✓\nE\nINTUITION AND EXAMPLES FOR CAUSE AND MECHANISM VARIABILITY\nThe Sparse Mechanism Shift hypothesis motivates cause and mechanism variability.\nIn § 3,\nwe relaxed exchangeability into cause and mechanism variability. In this section, we show that both\ncause and mechanism variability can be used to describe many real-world scenarios. Intuitively,\nCause and mechanism variability can be seen as particular realizations of the Sparse Mechanism\nShift (SMS) hypothesis (Perry et al., 2022).\nThe SMS posits that the causal mechanisms (the factors in the causal Markov factorization) tend to\nchange sparsely, i.e., interventions or distribution shifts can be described by changing a (strict) subset\nof mechanisms. This is one main argument for the efficiency of causal modelling, as the modularity\nimplies that only parts of the model need to be adapted in case of a distribution shift—in contrast to a\nnon-causal factorization, where the whole learned model needs to be fine-tuned.\n4In ICE-BeeM (Khemakhem et al., 2020b), the assumption is on the feature extractor\n25\n"
    },
    {
      "page_number": 26,
      "text": "Published as a conference paper at ICLR 2025\nIndeed, the SMS hypothesis captures the reasoning behind many works in causality (Gendron et al.,\n2023; Perry et al., 2022; Lachapelle et al., 2021b; 2022; Schölkopf et al., 2012; Lachapelle et al.,\n2021a; Ahuja et al., 2022b). Sparse changes have been also connected to causal modeling (Rajendran\net al., 2023; von Kügelgen et al., 2021; Fumero et al., 2023; Mansouri et al., 2023; Ahuja et al.,\n2022a).\nE.1\nREAL-WORLD EXAMPLES\nIn this section, we draw on prior works to provide real-world examples of cause and mechanism\nvariability—for examples of the exchangeable case, we refer the reader to (Guo et al., 2024a). As\nwith any model, we will make certain simplifications, though we aim to convey that the principle of\ncause and mechanism variability still applies. We will restrict ourselves to the bivariate case, as in\nFig. 5. The causal factorization for X →Y is p(Y |X, ψ)p(X|θ), where the CdF parameters are θ, ψ.\nCause variability means that p(ψ) is a delta distribution, whereas mechanism variability means that\np(θ) is a delta distribution.\nE.1.1\nCAUSE VARIABILITY.\nExample 4 (Lung cancer). Assume that θ parametrizes the lifestyle, socioeconomic, and environmen-\ntal factors of people, whereas ψ parametrizes how lung cancer develops. In this case, we can assume\nthat p(X|θ) differs across cities, whereas the mechanism for developing lung cancer, p(Y |X, ψ) is\nthe same. That is, only p(ψ) is a delta distribution.\nExample 5 (Altitude and temperature). Assume that θ parametrizes the altitude distribution of\ncountries, whereas ψ parametrizes how altitude affects temperature. In this case, we can assume that\np(X|θ) differs across countries, whereas the effect of altitude on temperature p(Y |X, ψ) is the same.\nThat is, only p(ψ) is a delta distribution.\nE.1.2\nMECHANISM VARIABILITY\nExample 6 (Natural experiments). In natural experiments in economics (Angrist & Krueger, 1991;\nImbens & Angrist, 1994), it is possible to select two populations such that we can assume that their\ndistributions are the same, i.e., the corresponding θ parameter has a delta distribution, whereas the\neconomic situation, parametrized by ψ, differs, e.g., by the two cities having different local taxes.\nExample 7 (Medical diagnoses). Assume that several people having the same lifestyle, socioeconomic,\nand environmental status are admitted to the same hospital after food poisoning at a local restaurant.\nThen, the probability distribution describing the symptoms, parametrized by θ, will have a delta\nprior, as each person suffers from the same disease. If we assume that multiple doctors are required\nto diagnose and treat all patients, then we can posit that there will be (slight) differences in their\ndecisions and prescribed treatments, which means that the corresponding parametric mechanism\np(Y |X, ψ) for the treatment has a non-delta prior for ψ.\nF\nEXPERIMENTAL RESULTS: CAUSE AND MECHANISM VARIABILITY FOR\nCAUSAL DISCOVERY\nSetup.\nTo demonstrate that both cause and mechanism variability enable causal structure iden-\ntification, we ran synthetic experiments based on the publicly available repository of the Causal\nde Finetti paper5. We focus on the continuous case, as problems can arise for discrete RVs (e.g.,\nin Lem. 5)—i.e., we follow the protocol described in the “Bivariate Causal Discovery\" paragraph\nin (Guo et al., 2024a, Sec. 6). The continuous experiments used in the original CdF paper consider the\nbivariate case, which we follow to be comparable. The only change in the evaluation protocol is not\nevaluating the CD-NOD method (Huang et al., 2017), as we do not have access to a MatLab license.\nThat is, we compare against FCI (Spirtes et al., 2013), GES (Chickering, 2002), NOTEARS (Zheng\net al., 2018), DirectLinGAM (Shimizu et al., 2011), plus a random baseline.\nFollowing (Guo et al., 2024a, Sec. 6), we describe the DGP in detail. The CdF parameters N = [ψ, θ]\nwere randomly generated with distinct and independent elements in each environment. Samples\nwithin each environment have the noise variables S generated via Laplace distributions conditioned\non the corresponding CdF parameters—i.e., the CdF parameter is the location (mean) of the Laplace\ndistribution. We observe a bivariate vector X = [X1, X2] ∈R2 and aim to uncover the causal\ndirection between X1 and X2. Let the superscript (·)e denote variables contained in environment e.\n5https://github.com/syguo96/Causal-de-Finetti. Our code is available at https://\ngithub.com/rpatrik96/IEM\n26\n"
    },
    {
      "page_number": 27,
      "text": "Published as a conference paper at ICLR 2025\n(a) Causal de Finetti\n(b) Cause variability\n(c) Mechanism variability\nFigure 6: Bivariate causal discovery is possible with cause and mechanism variability: Com-\nparison of the CdF protocol with FCI, GES, NOTEARS, DirectLinGAM, and a random baseline for\ncausal structure discovery in the bivariate case with continuous random variables. The proportion of\ncorrectly identified causal structures is shown against a different number of environments, chosen\nfrom {100, 200, 300, 400, 500}. Shading shows the standard deviation across 100 seeds. (a): the\noriginal CdF setting, reproducing (Guo et al., 2024a, Fig. 3(a)) with non-delta priors for both CdF\nparameters; (b): cause variability with a delta parameter prior for the effect-given-the-cause parameter\nψ; (c): mechanism variability with a delta parameter prior for the cause parameter θ; For details, cf.\nAppx. F\nThen, the data is generated as follows:\nNe ∼Uniform[−1, 1]\n(25)\nSe ∼Laplace(N, 1)\n(26)\nXe = AeSe + Be (Ne)o2 1nonlinear (e),\n(27)\nwhere ◦2 denotes elementwise squaring. Ae ∈R2×2 is a randomly sampled triangular matrix and\nBe = Ae −I, where I is the identity matrix. The causal direction is randomly sampled from\nX1 →X2, X2 →X1, X1 ⊥X2—this ensures that A is either a lower triangular, upper triangular or\ndiagonal matrix. 1nonlinear (e) is an environment-dependent, randomly sampled nonlinear-dependence\nindicator, which models the realistic scenario of invariant causal structure but changing functional\nrelationships.\nWe implement cause and mechanism variability in the above synthetic DGP, which by changing\nthe scm_bivariate_continuous function in the original GitHub repository. There, we set\nthe noise variables for the delta-distributed CdF parameter (θ for mechanism and ψ for cause\nvariability) to be equal to the corresponding parameter value (as that is used as the location of the\nLaplace distribution). This means collapsing the Laplace distribution to a delta distribution for the\ncorresponding CdF parameter in (26).\nFor comparison, we evaluate three settings: the original scenario (with non-delta priors for both\nparameters), cause variability, and mechanism variability. We use, as in the original code, two\nsamples per environment and ablate over {100, 200, 300, 400, 500} environments. Each experiment\nis repeated 100 times. We measure causal structure identification by three conditional independence\ntests with a significance level of α = 0.05. We choose the estimated causal structure to be the one\ncorresponding to the test with the highest p-value.\nResults.\nFig. 6 shows the proportion of correctly identified causal structures for different numbers\nof environments. The Causal-de-Finetti algorithm outperforms all the other methods with an accuracy\nclose to 100%. This holds not just in the original scenario proposed by Guo et al. (2024a) (Fig. 6a),\nbut also in the case of cause and mechanism variability (Figs. 6b and 6c), corroborating our Thm. 2.\n27\n"
    },
    {
      "page_number": 28,
      "text": "Published as a conference paper at ICLR 2025\nG\nACRONYMS\nANM Additive Noise Model\nCD Causal Discovery\nCdF Causal de Finetti\nCRL Causal Representation Learning\nDAG Directed Acyclic Graph\nDGP data generating process\nGCL Generalized Contrastive Learning\ni.i.d. independent and identically distributed\nICA Independent Component Analysis\nICM Independent Causal Mechanisms\nIEM Identifiable Exchangeable Mechanisms\nLVM latent variable model\nMSS Mechanism Shift Score\nOOD out-of-distribution\nRV random variable\nSEM Structural Equation Model\nSMS Sparse Mechanism Shift\nTCL Time-Contrastive Learning\n28\n"
    }
  ]
}