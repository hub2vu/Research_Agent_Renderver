{
  "filename": "tyEyYT267x_Block Diffusion_ Interpolating Between Autoregressive and Diffusion Language Models",
  "original_filename": "tyEyYT267x_Block Diffusion_ Interpolating Between Autoregressive and Diffusion Language Models",
  "references_detected": 27,
  "titles_extracted": 20,
  "titles": [
    "Diffusion forcing: Next-token prediction meets full-sequence diffusion",
    "Better & faster large language models via multi-token prediction",
    "Memdlm: De novo membrane protein design with masked discrete diffusion protein language models",
    "David helps goliath: Inference-time collaboration between small specialized and large general diffusion lms",
    "Denoising diffusion probabilistic models",
    "Visual instruction tuning",
    "Accelerating transformer inference for translation via parallel decoding",
    "Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models",
    "In particular, we focus on masking diffusion processes introduced by Austin et al",
    "B.1 FORWARD PROCESS Under the D3PM framework (Austin et al., 2021), the forward noise process applied independently for each token ℓ∈{1",
    "B.3 SIMPLIFIED NELBO FOR MASKED DIFFUSION PROCESSES Following Sahoo et al",
    "The true posterior for the case where xℓ t ̸= m is q(xℓ s = xℓ t|xℓ t ̸= m) = 1 (if a token is unmasked in the reverse process, it is never remasked",
    "2024a) (Suppl",
    "2024a) (Suppl",
    "2024a) (Suppl",
    "carry-over unmasking",
    "B.5 TIGHTNESS OF THE NELBO For block sizes 1 ≤K ≤L, we show that -log p(x) ≤LK ≤LK+1",
    "Let xb,ℓcorrespond to the token in position ℓ∈[1, L′] of block b",
    "However, modeling all B conditonal terms requires processing both the noised sequence xb t and the conditional context x<b for all b",
    "wrong for their morality"
  ],
  "diagnostics": {
    "has_references_header": true,
    "parsed_items_count": 27,
    "is_numbered": true,
    "id_normalized": false
  }
}