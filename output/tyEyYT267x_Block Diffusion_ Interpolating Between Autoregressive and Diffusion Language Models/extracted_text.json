{
  "filename": "tyEyYT267x_Block Diffusion_ Interpolating Between Autoregressive and Diffusion Language Models.pdf",
  "total_pages": 28,
  "full_text": "Published as a conference paper at ICLR 2025\nBLOCK DIFFUSION: INTERPOLATING BETWEEN AU-\nTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS\nMarianne Arriola† ∗\nAaron Kerem Gokaslan†\nJustin T. Chiu‡\nZhihan Yang†\nZhixuan Qi†\nJiaqi Han¶\nSubham Sekhar Sahoo†\nVolodymyr Kuleshov†\nABSTRACT\nDiffusion language models offer unique benefits over autoregressive models due\nto their potential for parallelized generation and controllability, yet they lag in\nlikelihood modeling and are limited to fixed-length generation. In this work, we\nintroduce a class of block diffusion language models that interpolate between\ndiscrete denoising diffusion and autoregressive models. Block diffusion overcomes\nkey limitations of both approaches by supporting flexible-length generation and\nimproving inference efficiency with KV caching and parallel token sampling. We\npropose a recipe for building effective block diffusion models that includes an\nefficient training algorithm, estimators of gradient variance, and data-driven noise\nschedules to minimize the variance. Block diffusion sets a new state-of-the-art\nperformance among diffusion models on language modeling benchmarks and\nenables generation of arbitrary-length sequences. We provide the code1, along\nwith the model weights and blog post on the project page:\nhttps://m-arriola.com/bd3lms\n1\nINTRODUCTION\nDiffusion models are widely used to generate images (Ho et al., 2020; Dhariwal & Nichol, 2021;\nSahoo et al., 2024b) and videos (Ho et al., 2022; Gupta et al., 2023), and are becoming increasingly\neffective at generating discrete data such as text (Lou et al., 2024; Sahoo et al., 2024a) or biological\nsequences (Avdeyev et al., 2023; Goel et al., 2024). Compared to autoregressive models, diffusion\nmodels have the potential to accelerate generation and improve the controllability of model outputs\n(Schiff et al., 2024; Nisonoff et al., 2024; Li et al., 2024; Sahoo et al., 2024c).\nDiscrete diffusion models currently face at least three limitations. First, in applications such as chat\nsystems, models must generate output sequences of arbitrary length (e.g., a response to a user’s\nquestion). However, most recent diffusion architectures only generate fixed-length vectors (Austin\net al., 2021; Lou et al., 2024). Second, discrete diffusion uses bidirectional context during generation\nand therefore cannot reuse previous computations with KV caching, which makes inference less\nefficient (Israel et al., 2025). Third, the quality of discrete diffusion models, as measured by standard\nmetrics such as perplexity, lags behind autoregressive approaches and further limits their applicability\n(Gulrajani & Hashimoto, 2024; Sahoo et al., 2024a).\nThis paper makes progress towards addressing these limitations by introducing Block Discrete\nDenoising Diffusion Language Models (BD3-LMs), which interpolate between discrete diffusion\nand autoregressive models. Specifically, block diffusion models (also known as semi-autoregressive\nmodels) define an autoregressive probability distribution over blocks of discrete random variables\n(Si et al., 2022; 2023); the conditional probability of a block given previous blocks is specified by a\ndiscrete denoising diffusion model (Austin et al., 2021; Sahoo et al., 2024a).\nDeveloping effective BD3-LMs involves two challenges. First, efficiently computing the training\nobjective for a block diffusion model is not possible using one standard forward pass of a neural\n∗Correspondence to Marianne Arriola: marriola@cs.cornell.edu\n†Cornell Tech, NY, USA.\n¶Stanford University, CA, USA.\n‡ Cohere, NY, USA.\n1Code: https://github.com/kuleshov-group/bd3lms\n1\nPublished as a conference paper at ICLR 2025\nOn September 17, 2016, we will be giving the      release of              \nAutoregression: \nThere are three categories of the average\nThere are three categories of the average rate\nThere are three categories of the average rate of...\nGeneration steps\nDiffusion: \n       the reusability                        will continue to        the\nLower quality\nRepeal the reusability cuts and       the law will continue to reduce the\nRepeal the reusability cuts and prove the law will continue to reduce the deficit.\nBlock Diffusion (Ours):\nOn September 17,       we      be                                               \nParallelizable\nOn September 17, 2016, we will be giving the beta-release of the      to our server     testing ...\nKV caching\nKV caching\nArbitrary-length\nParallelizable\nArbitrary-length\nHigh quality\nHigh quality\nFixed-length\nNo KV caching\nNot Parallelizable\nFigure 1: Block diffusion sequentially generates blocks of tokens by performing diffusion within each\nblock and conditioning on previous blocks. By combining strength from autoregressive and diffusion\nmodels, block diffusion overcomes the limitations of both approaches by supporting variable-length,\nhigher-quality generation and improving inference efficiency with KV caching and parallel sampling.\nnetwork and requires developing specialized algorithms. Second, training is hampered by the high\nvariance of the gradients of the diffusion objective, causing BD3-LMs to under-perform autoregression\neven with a block size of one (when both models should be equivalent). We derive estimators of\ngradient variance, and demonstrate that it is a key contributor to the gap in perplexity between\nautoregression and diffusion. We then propose custom noise processes that minimize gradient\nvariance and make progress towards closing the perplexity gap.\nWe evaluate BD3-LMs on language modeling benchmarks, and demonstrate that they are able to\ngenerate sequences of arbitrary length, including lengths that exceed their training context. In addition,\nBD3-LMs achieve new state-of-the-art perplexities among discrete diffusion models. Compared to\nalternative semi-autoregressive formulations that perform Gaussian diffusion over embeddings (Han\net al., 2022; 2023), our discrete approach features tractable likelihood estimates and yields samples\nwith improved generative perplexity using an order of magnitude fewer generation steps. In summary,\nour work makes the following contributions:\n• We introduce block discrete diffusion language models, which are autoregressive over\nblocks of tokens; conditionals over each block are based on discrete diffusion. Unlike prior\ndiffusion models, block diffusion supports variable-length generation and KV caching.\n• We introduce custom training algorithms for block diffusion models that enable efficiently\nleveraging the entire batch of tokens provided to the model.\n• We identify gradient variance as a limiting factor of the performance of diffusion models,\nand we propose custom data-driven noise schedules that reduce gradient variance.\n• Our results establish a new state-of-the-art perplexity for discrete diffusion and make\nprogress toward closing the gap to autoregressive models.\n2\nBACKGROUND: LANGUAGE MODELING PARADIGMS\nNotation\nWe consider scalar discrete random variables with V categories as ‘one-hot’ column\nvectors in the space V = {x ∈{0, 1}V : P\ni xi = 1} ⊂∆V for the simplex ∆V . Let the V -th\ncategory denote a special [MASK] token, where m ∈V is its one-hot vector. We define x1:L as a\nsequence of L tokens, where xℓ∈V for all tokens ℓ∈{1, . . . , L}, and use VL to denote the set of\nall such sequences. Throughout the work, we simplify notation and refer to the token sequence as\nx and an individual token as xℓ. Finally, let Cat(·; p) be a categorical distribution with probability\np ∈∆V .\n2\nPublished as a conference paper at ICLR 2025\n2.1\nAUTOREGRESSIVE MODELS\nConsider a sequence of L tokens x =\n\u0002\nx1, . . . , xL\u0003\ndrawn from the data distribution q(x). Autore-\ngressive (AR) models define a factorized distribution of the form\nlog pθ(x) =\nL\nX\nℓ=1\nlog pθ(xℓ| x<ℓ),\n(1)\nwhere each pθ(xℓ| x<ℓ) is parameterized directly with a neural network. As a result, AR models\nmay be trained efficiently via next token prediction. However, AR models take L steps to generate L\ntokens due to the sequential dependencies.\n2.2\nDISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS\nDiffusion models fit a model pθ(x) to reverse a forward corruption process q (Sohl-Dickstein et al.,\n2015; Ho et al., 2020; Sahoo et al., 2024b). This process starts with clean data x and defines latent\nvariables xt =\n\u0002\nx1\nt, . . . , xL\nt\n\u0003\nfor t ∈[0, 1], which represent progressively noisier versions of x. Given\na discretization into T steps, we define s(j) = (j −1)/T and t(j) = j/T. For brevity, we drop j\nfrom t(j) and s(j) below; in general, s denotes the time step preceding t.\nThe D3PM framework (Austin et al., 2021) defines q as a Markov forward process acting indepen-\ndently on each token xℓ: q(xℓ\nt | xℓ\ns) = Cat(xℓ\nt; Qtxℓ\ns) where Qt ∈RV ×V is the diffusion matrix.\nThe matrix Qt can model various transformations, including masking, random token changes, and\nrelated word substitutions.\nAn ideal diffusion model pθ is the reverse of the process q. The D3PM framework defines pθ as\npθ(xs | xt) =\nL\nY\nℓ=1\npθ(xℓ\ns | xt) =\nX\nx\n\" L\nY\nℓ=1\nq(xℓ\ns | xℓ\nt, xℓ)pθ(xℓ| xt)\n#\n,\n(2)\nwhere the denoising base model pθ(xℓ| xt) predicts clean token xℓgiven the noisy sequence xt, and\nthe reverse posterior q(xℓ\ns | xℓ\nt, x) is defined following Austin et al. (2021) in Suppl. B.3.\nThe diffusion model pθ is trained using variational inference. Let KL[·] denote the Kullback-Leibler\ndivergence. Then, the Negative ELBO (NELBO) is given by (Sohl-Dickstein et al., 2015):\nL(x; θ) = Eq\n\"\n−log pθ(x|xt(1)) +\nT\nX\nj=1\nDKL[q(xs(j)|xt(j), x)∥pθ(xs(j)|xt(j))] + DKL[q(xt(T )|x)∥pθ(xt(T ))]\n#\n(3)\nThis formalism extends to continuous time via Markov chain (CTMC) theory and admits score-based\ngeneralizations (Song & Ermon, 2019; Lou et al., 2024; Sun et al., 2022). Further simplifications\n(Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025) tighten the ELBO and enhance performance.\n3\nBLOCK DIFFUSION LANGUAGE MODELING\nWe explore a class of Block Discrete Denoising Diffusion Language Models (BD3-LMs) that\ninterpolate between autoregressive and diffusion models by defining an autoregressive distribution\nover blocks of tokens and performing diffusion within each block. We provide a block diffusion\nobjective for maximum likelihood estimation and efficient training and sampling algorithms. We\nshow that for a block size of one, the diffusion objective suffers from high variance despite being\nequivalent to the autoregressive likelihood in expectation. We identify high training variance as a\nlimitation of diffusion models and propose data-driven noise schedules that reduce the variance of the\ngradient updates during training.\n3.1\nBLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES\nWe propose to combine the language modeling paradigms in Sec. 2 by autoregressively modeling\nblocks of tokens and performing diffusion within each block. We group tokens in x into B blocks of\n3\nPublished as a conference paper at ICLR 2025\nlength L′ with B = L/L′ (we assume that B is an integer). We denote each block x(b−1)L′:bL′ from\ntoken at positions (b −1)L′ to bL′ for blocks b ∈{1, . . . , B} as xb for simplicity. Our likelihood\nfactorizes over blocks as\nlog pθ(x) =\nB\nX\nb=1\nlog pθ(xb | x<b),\n(4)\nand each pθ(xb | x<b) is modeled using discrete diffusion over a block of L′ tokens. Specifically, we\ndefine a reverse diffusion process as in (2), but restricted to block b:\npθ(xb\ns | xb\nt, x<b) =\nX\nxb\nq(xb\ns | xb\nt, xb)pθ(xb | xb\nt, x<b)\n(5)\nWe obtain a principled learning objective by applying the NELBO in (3) to each term in (4) to obtain\n−log pθ(x) ≤LBD(x; θ) :=\nB\nX\nb=1\nL(xb, x<b; θ),\n(6)\nwhere each L(xb, x<b; θ) is an instance of (3) applied to log pθ(xb | x<b). Since the model is\nconditioned on x<b, we make the dependence on x<b, θ explicit in L. We denote the sum of these\nterms LBD(x; θ) (itself a valid NELBO).\nModel Architecture\nCrucially, we parameterize the B base denoiser models pθ(xb | xb\nt, x<b) using\na single neural network xθ. The neural network xθ outputs not only the probabilities pθ(xb | xb\nt, x<b),\nbut also computational artifacts for efficient training. This will enable us to compute the loss LBD(x; θ)\nin parallel for all B blocks in a memory-efficient manner. Specifically, we parameterize xθ using a\ntransformer (Vaswani et al., 2017) with a block-causal attention mask. The transformer xθ is applied\nto L tokens, and tokens in block b attend to tokens in blocks 1 to b. When xθ is trained, xb\nθ(xb\nt, x<b)\nyields L′ predictions for denoised tokens in block b based on noised xb\nt and clean x<b.\nIn autoregressive generation, it is normal to cache keys and values for previously generated tokens\nto avoid recomputing them at each step. Similarly, we use Kb, Vb to denote the keys and values at\nblock b, and we define xθ to support these as input and output. The full signature of xθ is\nxb\nlogits, Kb, Vb ←xb\nθ(xb\nt, K1:b−1, V1:b−1) := xb\nθ(xb\nt, x<b),\n(7)\nwhere xb\nlogits are the predictions for the clean xb, and Kb, Vb is the key-value cache in the forward\npass of xθ, and K1:b−1, V1:b−1 are keys and values cached on a forward pass of xθ over x<b (hence\nthe inputs x<b and K1:b−1, V1:b−1 are equivalent).\n3.2\nEFFICIENT TRAINING AND SAMPLING ALGORITHMS\nIdeally, we wish to compute the loss LBD(x; θ) in one forward pass of xθ. However, observe that\ndenoising xb\nt requires a forward pass on this noisy input, while denoising the next blocks requires\nrunning xθ on the clean version xb. Thus every block has to go through the model at least twice.\nTraining\nBased on this observation, we propose a training algorithm with these minimal computa-\ntional requirements (Alg. 1). Specifically, we precompute keys and values K1:B, V1:B for the full\nsequence x in a first forward pass (∅, K1:B, V1:B) ←xθ(x). We then compute denoised predictions\nfor all blocks using xb\nθ(xb\nt, K1:b-1, V1:b-1). Each token passes through xθ twice.\nVectorized Training\nNaively, we would compute the logits by applying xb\nθ(xb\nt, K1:b-1, V1:b-1) in a\nloop B times. We propose a vectorized implementation that computes LBD(x; θ) in one forward pass\non the concatenation xnoisy ⊕x of clean data x with noisy data xnoisy = x1\nt1 ⊕· · · ⊕xB\ntB obtained\nby applying a noise level tb to each block xb. We design an attention mask for xnoisy ⊕x such that\nnoisy tokens attend to other noisy tokens in their block and to all clean tokens in preceding blocks\n(see Suppl. B.6). Our method keeps the overhead of training BD3-LMs tractable and combines with\npretraining to further reduce costs.\n4\nPublished as a conference paper at ICLR 2025\nSampling\nWe sample one block at a time, conditioned on previously sampled blocks (Alg 2).\nWe may use any sampling procedure SAMPLE(xb\nθ, K1:b-1, V1:b-1) to sample from the conditional\ndistribution pθ(xb\ns|xb\nt, x<b), where the context conditioning is generated using cross-attention with\npre-computed keys and values K1:b−1, V1:b−1. Similar to AR models, caching the keys and values\nsaves computation instead of recalculating them when sampling a new block.\nNotably, our block diffusion decoding algorithm enables us to sample sequences of arbitrary length,\nwhereas diffusion models are restricted to fixed-length generation. Further, our sampler admits parallel\ngeneration within each block, whereas AR samplers are constrained to generate token-by-token.\nAlgorithm 1 Block Diffusion Training\nInput: datapoint x, # of blocks B, forward\nnoise process qt(·|x), model xθ, loss LBD\nrepeat\nSample t1, . . . , tB ∼U[0, 1]\n∀b ∈{1, ..., B} : xb\ntb ∼qtb(·|xb)\n∅, K1:B, V1:B ←xθ(x)\n▷KV cache\n∀b: xb\nlogit, ∅, ∅←xb\nθ(xb\ntb, K1:b-1, V1:b-1)\nLet xlogit ←x1\nlogit ⊕· · · ⊕xB\nlogit\nTake gradient step on ∇θLBD(xlogit; θ)\nuntil converged\nAlgorithm 2 Block Diffusion Sampling\nInput: # blocks B, model xθ, diffusion sam-\npling algorithm SAMPLE\nx, K, V ←∅\n▷output & KV cache\nfor b = 1 to B do\nxb ←SAMPLE(xb\nθ, K1:b-1, V1:b-1)\n∅, Kb, Vb ←xb\nθ(xb)\nx ←x1:b−1 ⊕xb\n(K, V) ←(K1:b−1 ⊕Kb, V1:b−1 ⊕Vb)\nend for\nreturn x\n4\nUNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION & AR MODELS\n4.1\nMASKED BD3-LMS\nThe most effective diffusion language models leverage a masking noise process (Austin et al., 2021;\nLou et al., 2024; Sahoo et al., 2024a), where tokens are gradually replaced with a special mask token.\nHere, we introduce masked BD3-LMs, a special class of block diffusion models based on the masked\ndiffusion language modeling framework (Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025).\nMore formally, we adopt a per-token noise process q(xℓ\nt|xℓ) = Cat(xℓ\nt; αtxℓ+ (1 −αt)m) for\ntokens ℓ∈{1, . . . , L} where m is a one-hot encoding of the mask token, and αt ∈[0, 1] is a\nstrictly decreasing function in t, with α0 = 1 and α1 = 0. We employ the linear schedule where the\nprobability of masking a token at time t is 1 −αt. We adopt the simplified objective from Sahoo et al.\n(2024a); Shi et al. (2024); Ou et al. (2025) (the full derivation is provided in Suppl. B.3):\n−log pθ(x) ≤LBD(x; θ) :=\nB\nX\nb=1\nEt∼[0,1]Eq\nα′\nt\n1 −αt\nlog pθ(xb|xb\nt, x<b)\n(8)\nwhere α′\nt is the instantaneous rate of change of αt under the continuous-time extension of (3) that\ntakes T →∞. The NELBO is tight for L′ = 1 but becomes a looser approximation of the true\nnegative log-likelihood for L′ →L (see Suppl. B.5).\n4.2\nCASE STUDY: SINGLE TOKEN GENERATION\nTable 1: Test perplexities for single-\ntoken generation (PPL; ↓) across\n16B tokens on LM1B.\nPPL (↓)\nAR\n22.88\n+ random batch size\n24.37\nBD3-LM L′ = 1\n≤25.56\n+ tuned schedule\n22.88\nOur block diffusion parameterization (8) is equivalent in expec-\ntation to the autoregressive NLL (1) in the limiting case where\nL′ = 1 (see Suppl. B.4). Surprisingly, we find a two point\nperplexity gap between our block diffusion model for L′ = 1\nand AR when training both models on the LM1B dataset.\nAlthough the objectives are equivalent in expectation, we show\nthat the remaining perplexity gap is a result of high training\nvariance. Whereas AR is trained using the cross-entropy of L\ntokens, our block diffusion model for L′ = 1 only computes\nthe cross-entropy for masked tokens xℓ\nt = m ∀ℓ∈{1, . . . L}\n5\nPublished as a conference paper at ICLR 2025\n50k\n100k\n150k\n200k\n250k\n3\n3.2\n3.4\n3.6\n3.8\n4\nModel\nBD3-LM (NELBO)\nBD3-LM (Tuned schedule)\nAR\nAR (random batch size)\nTrain Negative Log-Likelihood (NLL) for Single Token Generation on LM1B\nTrain steps\nNLL\nFigure 2: Train NLLs for modeling the per-token likelihood on LM1B. Models are trained on 16B\ntokens. Training under the discrete diffusion NELBO, where half of the tokens in a batch are masked\non average, has similar training variance to an AR model with a random batch size.\nso that Et∼U[0,1]q(xℓ\nt = m|xℓ) = 0.5. Thus, training on the diffusion objective involves estimating\nloss gradients with 2x fewer tokens and is responsible for higher training variance compared to AR.\nTo close the likelihood gap, we train a BD3-LM for L′ = 1 by designing the forward process to\nfully mask tokens, i.e. q(xℓ\nt = m|xℓ) = 1. Under this schedule, the diffusion objective becomes\nequivalent to the AR objective (Suppl. B.4). In Table 1, we show that training under the block\ndiffusion objective yields the same perplexity as AR training. Empirically, we see that this reduces the\nvariance of the training loss in Figure 2. We verify that tuning the noise schedule reduces the variance\nof the objective by measuring Varx,t [LBD(x; θ)] after training on 328M tokens: while training on the\nNELBO results in a variance of 1.52, training under full masking reduces the variance to 0.11.\n4.3\nDIFFUSION GAP FROM HIGH VARIANCE TRAINING\nNext, we formally describe the issue of gradient variance in training diffusion models. Given our\nempirical observations for single-token generation, we propose an estimator for gradient variance\nthat we use to minimize the variance of diffusion model training for L′ ≥1. While the NELBO is\ninvariant to the choice of noise schedule (Suppl. B.3), this invariance does not hold for our Monte\nCarlo estimator of the loss used during training. As a result, the variance of the estimator and its\ngradients are dependent on the schedule. First, we express the estimator of the NELBO with a batch\nsize K. We denote a batch of sequences as X =\n\u0002\nx(1), x(2), . . . , x(K)\u0003\n, with each x(k) iid∼q(x). We\nobtain the batch NELBO estimator below, where t(k, b) is sampled in sequence k and block b:\nLBD(X; θ) := l(X; θ) = 1\nK\nK\nX\nk=1\nB\nX\nb=1\nα′\nt(k,b)\n1 −αt(k,b)\nlog pθ\n\u0010\nx(k),b | x(k),b\nt(k,b), x(k),<b\u0011\n(9)\nThe variance of the gradient estimator over M batches for each batch Xm ∀m ∈{1, . . . , M} is:\nVarX,t [∇θl(X; θ)] ≈\n1\nM −1\nM\nX\nm=1\n\r\r\r\r\r∇θl(Xm; θ) −1\nM\nM\nX\nm=1\n∇θl(Xm; θ)\n\r\r\r\r\r\n2\n2\n(10)\n5\nLOW-VARIANCE NOISE SCHEDULES FOR BD3-LMS\n5.1\nINTUITION: AVOID EXTREME MASK RATES\nWe aim to identify schedules that minimize the variance of the gradient estimator and make training\nmost efficient. In a masked setting, we want to mask random numbers of tokens, so that the model\n6\nPublished as a conference paper at ICLR 2025\nlearns to undo varying levels of noise, which is important during sampling. However, if we mask\nvery few tokens, reconstructing them is easy and does not provide useful learning signal. If we mask\neverything, the optimal reconstruction are the marginals of each token in the data distribution, which\nis easy to learn, and again is not useful. These extreme masking rates lead to poor high-variance\ngradients: we want to learn how to clip them via a simple and effective new class of schedules.\n5.2\nCLIPPED SCHEDULES FOR LOW-VARIANCE GRADIENTS\nWe propose a class of “clipped” noise schedules that sample mask rates 1 −αt ∼U[β, ω] for\n0 ≤β, ω ≤1. We argue that from the perspective of deriving Monte Carlo gradient estimates, these\nschedules are equivalent to a continuous schedule where the mask probability is approximately 0\nbefore the specified range such that 1 −α<β ≈ϵ and approximately 1 after the specified range\n1 −α>ω ≈1 −ϵ. Consequently, α′\nt is linear within the range: α′\nt ≈1/(β −ω).\n5.3\nDATA-DRIVEN CLIPPED SCHEDULES ACROSS BLOCK SIZES\nAs the optimal mask rates may differ depending on the block size L′, we adaptively learn the schedule\nduring training. While Kingma et al. (2021) perform variance minimization by isolating a variance\nterm using their squared diffusion loss, this strategy is not directly applicable to our variance estimator\nin Equation 10 since we seek to reduce variance across random batches in addition to random tb.\nInstead, we optimize parameters β, ω to directly minimize training variance. To limit the computa-\ntional burden of the optimization, we use the variance of the estimator of the diffusion ELBO as a\nproxy for the gradient estimator to optimize β, ω: minβ,ω VarX,t [L(X; θ, β, ω)]. We perform a grid\nsearch at regular intervals during training to find the optimal β, ω (experimental details in Sec. 6).\nIn Table 2, we show that variance of the diffusion NELBO is correlated with test perplexity. Under a\nrange of “clipped” noise rate distributions, we find that there exists a unique distribution for each\nblock size L′ ∈{4, 16, 128} that minimizes both the variance of the NELBO and the test perplexity.\nTable 2: Perplexities (PPLs; ↓) and variances of the NELBO VarX,t [LBD(X; θ)] (Var. NELBO; ↓).\nModels are trained on LM1B using a linear schedule for 65B tokens, then finetuned for 10B tokens.\nU[0, .5]\nU[.3, .8]\nU[.5, 1]\nU[0, 1]\nL′\nPPL\nVar. NELBO\nPPL\nVar. NELBO\nPPL\nVar. NELBO\nPPL\nVar. NELBO\n128\n31.72\n1.03\n31.78\n1.35\n31.92\n1.83\n31.78\n3.80\n16\n31.27\n7.90\n31.19\n3.62\n31.29\n3.63\n31.33\n7.39\n4\n29.23\n32.68\n29.37\n10.39\n29.16\n8.28\n29.23\n23.65\n6\nEXPERIMENTS\nTable 3: Test perplexities (PPL; ↓) of mod-\nels trained for 65B tokens on LM1B. Best\ndiffusion value is bolded.\nPPL (↓)\nAutoregressive\nTransformer-X Base (Dai et al., 2019)\n23.5\nTransformer (Sahoo et al., 2024a)\n22.83\nDiffusion\nD3PM (absorb) (Austin et al., 2021)\n≤82.34\nSEDD (Lou et al., 2024)\n≤32.68\nMDLM (Sahoo et al., 2024a)\n≤31.78\nBlock diffusion (Ours)\nBD3-LMs L′ = 16\n≤30.60\nL′ = 8\n≤29.83\nL′ = 4\n≤28.23\nWe evaluate BD3-LMs across standard language\nmodeling benchmarks and demonstrate their ability\nto generate arbitrary-length sequences uncondition-\nally. We pre-train a base BD3-LM using the maxi-\nmum block size L′ = L for 850K gradient steps and\nfine-tune under varying L′ for 150K gradient steps\non the One Billion Words dataset (LM1B; Chelba\net al. (2014)) and OpenWebText (OWT; Gokaslan\net al. (2019)). Details on training and inference are\nprovided in Suppl C.\nTo reduce the variance of training on the diffusion\nNELBO, we adaptively learn the range of masking\nrates by optimizing parameters β, ω as described in\nSection 5.3. In practice, we do so using a grid search\nduring every validation epoch (after ∼5K gradient\n7\nPublished as a conference paper at ICLR 2025\nupdates) to identify β, ω: minβ,ω VarX,t [L(X; θ, β, ω)]. During evaluation, we report likelihood\nunder uniformly sampled mask rates (8) as in Austin et al. (2021); Sahoo et al. (2024a).\n6.1\nLIKELIHOOD EVALUATION\nTable 4: Test perplexities (PPL; ↓) on\nOWT for models trained for 524B to-\nkens. Best diffusion value is bolded.\nPPL (↓)\nAR (Sahoo et al., 2024a)\n17.54\nSEDD (Lou et al., 2024)\n≤24.10\nMDLM (Sahoo et al., 2024a)\n≤22.98\nBD3-LMs L′ = 16\n≤22.27\nL′ = 8\n≤21.68\nL′ = 4\n≤20.73\nOn LM1B, BD3-LMs outperform all prior diffusion meth-\nods in Table 3. Compared to MDLM (Sahoo et al., 2024a),\nBD3-LMs achieve up to 13% improvement in perplexity.\nWe observe a similar trend on OpenWebText in Table 4.\nWe also evaluate the ability of BD3-LMs to generalize\nto unseen datasets in a zero-shot setting, following the\nbenchmark from Radford et al. (2019). We evaluate the\nlikelihood of models trained with OWT on datasets Penn\nTree Bank (PTB; (Marcus et al., 1993)), Wikitext (Merity\net al., 2016), LM1B, Lambada (Paperno et al., 2016), AG\nNews (Zhang et al., 2015), and Scientific Papers (Pubmed\nand Arxiv subsets; (Cohan et al., 2018)). In Table 5, BD3-\nLM achieves the best zero-shot perplexity on Pubmed,\nsurpassing AR, and the best perplexity among diffusion models on Wikitext, LM1B, and AG News.\nTable 5: Zero-shot validation perplexities (↓) of models trained for 524B tokens on OWT. All\nperplexities for diffusion models are upper bounds.\nPTB\nWikitext\nLM1B\nLambada\nAG News\nPubmed\nArxiv\nAR\n81.07\n25.32\n51.14\n52.13\n52.11\n48.59\n41.22\nSEDD\n96.33\n35.98\n68.14\n48.93\n67.82\n45.39\n40.03\nMDLM\n90.96\n33.22\n64.94\n48.29\n62.78\n43.13\n37.89\nBD3-LM L′ = 4\n96.81\n31.31\n60.88\n50.03\n61.67\n42.52\n39.20\n6.2\nSAMPLE QUALITY AND VARIABLE-LENGTH SEQUENCE GENERATION\nTable 6: Generation length statistics\nfrom sampling 500 documents from\nmodels trained on OWT.\nMedian\nMax\n# tokens # tokens\nOWT train set\n717\n131K\nAR\n4008\n131K\nSEDD\n1021\n1024\nBD3-LM L′ = 16\n798\n9982\nOne key drawback of many existing diffusion language mod-\nels (e.g,. Austin et al. (2021); Lou et al. (2024)) is that they\ncannot generate full-length sequences that are longer than\nthe length of the output context chosen at training time. The\nOWT dataset is useful for examining this limitation, as it\ncontains many documents that are longer than the training\ncontext length of 1024 tokens.\nWe record generation length statistics of 500 variable-length\nsamples in Table 6. We continue sampling tokens until an\nend-of-sequence token [EOS] is generated or sample qual-\nity significantly degrades (as measured by sample entropy).\nBD3-LMs generate sequences up to ≈10× longer than those\nof SEDD (Lou et al., 2024), which is restricted to the training context size.\nWe also examine the sample quality of BD3-LMs through quantitative and qualitative analyses. In\nTable 7, we generate sequences of lengths L = 1024, 2048 and measure their generative perplexity\nunder GPT2-Large. To sample L = 2048 tokens from MDLM, we use their block-wise decoding\ntechnique (which does not feature block diffusion training as in BD3-LMs).\nWe also compare to SSD-LM (Han et al., 2022), an alternative block diffusion formulation. Unlike\nour discrete diffusion framework, SSD-LM uses Gaussian diffusion and does not support likelihood\nestimation. Further, BD3-LM adopts an efficient sampler from masked diffusion, where the number\nof generation steps (NFEs) is upper-bounded by L since tokens are never remasked (Sahoo et al.,\n2024a; Ou et al., 2025). For SSD-LM, we compare sample quality using T = 1K diffusion steps\nper block, matching their experimental setting (yielding ≥40K NFEs), and T = 25 where NFEs are\ncomparable across methods.\n8\nPublished as a conference paper at ICLR 2025\nTable 7: Generative perplexity (Gen. PPL; ↓) and number of function evaluations (NFEs; ↓) of 300\nsamples of lengths L = 1024, 2048. All models are trained on OWT. AR, SEDD, MDLM, BD3-LMs\nuse 110M parameters and are trained on 524B tokens, while SSD-LM uses 400M parameters and is\npre-trained on 122B tokens. Best diffusion value is bolded. We provide further details in Suppl. C.5.\nL = 1024\nL = 2048\nModel\nGen. PPL\nNFEs\nGen. PPL\nNFEs\nAR\n14.1\n1K\n13.2\n2K\nDiffusion\nSEDD\n52.0\n1K\n–\n–\nMDLM\n46.8\n1K\n41.3\n2K\nBlock Diffusion\nSSD-LM L′ = 25\n37.2\n40K\n35.3\n80K\n281.3\n1K\n281.9\n2K\nBD3-LMs L′ = 16\n33.4\n1K\n31.5\n2K\nL′ = 8\n30.4\n1K\n28.2\n2K\nL′ = 4\n25.7\n1K\n23.6\n2K\nBD3-LMs achieve the best generative perplexities compared to previous diffusion methods. Relative\nto SSD-LM, our discrete approach yields samples with improved generative perplexity using an order\nof magnitude fewer generation steps. We also qualitatively examine samples taken from BD3-LM\nand baselines (AR, MDLM) trained on the OWT dataset; we report samples in Suppl. D. We observe\nthat BD3-LM samples have higher coherence than MDLM samples and approach the quality of AR.\n6.3\nABLATIONS\nWe assess the impact of the design choices in our proposed block diffusion recipes, namely 1)\nselection of the noise schedule and 2) the efficiency improvement of the proposed training algorithm\nrelative to a naive implementation.\nSELECTING NOISE SCHEDULES TO REDUCE TRAINING VARIANCE\nCompared to the linear schedule used in Lou et al. (2024); Sahoo et al. (2024a), training under\n“clipped” noise schedules is the most effective for reducing the training variance which correlates with\ntest perplexity. In Table 8, the ideal “clipped” masking rates, which are optimized during training, are\nspecific to the block size and further motivate our optimization.\nTable 8: Effect of the noise schedule on like-\nlihood estimation. We finetune BD3-LMs\non 3B tokens from LM1B and evaluate on a\nlinear schedule. For clipped schedules, we\ncompare optimal clipping for L′ = 4, 16.\nNoise schedule\nPPL\nVar. NELBO\nL’ = 4\nClipped\nU[0.45, 0.95]\n29.21\n6.24\nU[0.3, 0.8]\n29.38\n10.33\nLinear U[0, 1]\n30.18\n23.45\nLogarithmic\n30.36\n23.53\nSquare root\n31.41\n26.43\nL’ = 16\nClipped\nU[0.45, 0.95]\n31.42\n3.60\nU[0.3, 0.8]\n31.12\n3.58\nLinear U[0, 1]\n31.72\n7.62\nSquare\n31.43\n13.03\nCosine\n31.41\n13.00\nRelative to other standard noise schedules (Chang et al.,\n2022), “clipped” masking achieves the best perfor-\nmance. As heavier masking is effective for the smaller\nblock size L′ = 4, we compare with logarithmic and\nsquare root schedules that also encourage heavy mask-\ning. As lighter masking is optimal for L′ = 16, we\ncompare with square and cosine schedules.\nEFFICIENCY OF TRAINING ALGORITHM\nIn the BD3-LM training algorithm (Sec. 3.2), we com-\npute xlogit using two options. We may perform two for-\nward passes through the network (precomputing keys\nand values for the full sequence x, then computing\ndenoised predictions), or combine these passes by con-\ncatenating the two inputs into the same attention kernel.\nWe find that a single forward pass is more efficient as\nwe reduce memory bandwidth bottlenecks by leverag-\ning efficient attention kernels (Dao et al., 2022; Dong\net al., 2024), see Suppl. B.7. Instead of paying the cost\n9\nPublished as a conference paper at ICLR 2025\nof two passes through the network, we only pay the cost of a more expensive attention operation. Our\nvectorized approach has 20-25% speed-up during training relative to performing two forward passes.\n7\nDISCUSSION AND PRIOR WORK\nComparison to D3PM\nBlock diffusion builds off D3PM (Austin et al., 2021) and applies it to each\nautoregressive conditional. We improve over D3PM in three ways: (1) we extend D3PM beyond fixed\nsequence lengths; (2) we study the perplexity gap of D3PM and AR models, identify gradient variance\nas a contributor, and design variance-minimizing schedules; (3) we improve over the perplexity of\nD3PM models. Our work applies to extensions of D3PM (He et al., 2022; Lou et al., 2024) including\nones in continuous time (Campbell et al., 2022; Sun et al., 2022).\nComparison to MDLM\nBD3-LMs further make use of the perplexity-enhancing improvements\nin MDLM (Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025). We also build upon MDLM: (1)\nwhile Sahoo et al. (2024a) point out that their NELBO is invariant to the noise schedule, we show\nthat the noise schedule has a significant effect on gradient variance; (2) we push the state-of-the-art in\nperplexity beyond MDLM. Note that our perplexity improvements stem not only from block diffusion,\nbut also from optimized schedules, and could enhance standard MDLM and D3PM models.\nComparison to Gaussian Diffusion\nAlternatively, one may perform diffusion over continuous\nembeddings of discrete tokens (Li et al., 2022; Dieleman et al., 2022; Chen et al., 2022). This allows\nusing algorithms for continuous data (Song et al., 2020; Ho & Salimans, 2022), but yields worse\nperplexity (Graves et al., 2023; Gulrajani & Hashimoto, 2024).\nComparison to Semi-Autoregressive Diffusion\nHan et al. (2022; 2023) introduced a block formu-\nlation of Gaussian diffusion. BD3-LMs instead extend Austin et al. (2021), and feature: (1) tractable\nlikelihood estimates for principled evaluation; (2) faster generation, as our number of model calls is\nbounded by the number of generated tokens, while SSD-LM performs orders of magnitude more calls;\n(3) improved sample quality. AR-Diffusion (Wu et al., 2023) extends SSD-LM with a left-to-right\nnoise schedule; Chen et al. (2025); Ye et al. (2024) apply to decision traces and videos; Hao et al.\n(2024); Kong et al. (2025) extend to latent reasoning. PARD (Zhao et al., 2024) applies discrete block\ndiffusion to graphs. In contrast, we (1) interpolate between AR/diffusion performance; (2) support\nKV caching; (3) perform attention within noised blocks, whereas PARD injects new empty blocks.\nAutoregressive diffusion models (Hoogeboom et al., 2021b;a) extend any-order AR models (AO-\nARMs; Uria et al. (2014)) to support parallel sampling. Zheng et al. (2024) prove equivalence\nbetween MDLM and AO-ARM training. Further extensions of ARMs that compete with diffusion\ninclude iterative editing (Gu et al., 2019), parallel and speculative decoding (Gu et al., 2017; Santilli\net al., 2023; Cai et al., 2024; Gloeckle et al., 2024), consistency training (Kou et al., 2024), guidance\n(Sanchez et al., 2023), and cross-modal extensions (Liu et al., 2023; Tian et al., 2025).\nLimitations\nTraining BD3-LMs is more expensive than regular diffusion training. We propose a\nvectorized algorithm that keeps training speed within <2x of diffusion training speed; in our experi-\nments, we also pre-train with a standard diffusion loss to further reduce the speed gap. Additionally,\nBD3-LMs generate blocks sequentially, and hence may face the same speed and controllability con-\nstraints as AR especially when blocks are small. Their optimal block size is task specific (e.g., larger\nfor greater control). BD3-LMs are subject to inherent limitations of generative models, including\nhallucinations (Achiam et al., 2023), copyright infringement (Gokaslan et al., 2024), controllability\n(Schiff et al., 2024; Wang et al., 2023) and harmful outputs (Bai et al., 2022).\n8\nCONCLUSION\nThis work explores block diffusion and is motivated by two problems with existing discrete diffusion:\nthe need to generate arbitrary-length sequences and the perplexity gap to autoregressive models. We\nintroduce BD3-LMs, which represent a block-wise extension of the D3PM framework (Austin et al.,\n2021), and leverage a specialized training algorithm and custom noise schedules that further improve\nperformance. We observe that in addition to being able to generate long-form documents, these\nmodels also improve perplexity, setting a new state-of-the-art among discrete diffusion models.\n10\nPublished as a conference paper at ICLR 2025\nACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING\nThis work was partially funded by the National Science Foundation under awards DGE-1922551,\nCAREER awards 2046760 and 2145577, and by the National Institute of Health under award MIRA\nR35GM151243. Marianne Arriola is supported by a NSF Graduate Research Fellowship under award\nDGE-2139899 and a Hopper-Dean/Bowers CIS Deans Excellence Fellowship. We thank Databricks\nMosaicML for providing access to computational resources.\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774, 2023.\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured\ndenoising diffusion models in discrete state-spaces. Advances in Neural Information Processing\nSystems, 34:17981–17993, 2021.\nPavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score\nmodel for biological sequence generation. In International Conference on Machine Learning, pp.\n1276–1301. PMLR, 2023.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. arXiv preprint arXiv:2212.08073, 2022.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao.\nMedusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv\npreprint arXiv:2401.10774, 2024.\nAndrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and\nArnaud Doucet. A continuous time framework for discrete denoising models. Advances in Neural\nInformation Processing Systems, 35:28266–28279, 2022.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative\nimage transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 11315–11325, 2022.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling,\n2014.\nBoyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann.\nDiffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural\nInformation Processing Systems, 37:24081–24125, 2025.\nTing Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using\ndiffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang,\nand Nazli Goharian. A discourse-aware attention model for abstractive summarization of long\ndocuments. Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018.\ndoi: 10.18653/v1/n18-2097. URL http://dx.doi.org/10.18653/v1/n18-2097.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems,\n35:16344–16359, 2022.\n11\nPublished as a conference paper at ICLR 2025\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. URL\nhttps://arxiv.org/abs/2105.05233.\nSander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H\nRichemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion\nfor categorical data. arXiv preprint arXiv:2211.15089, 2022.\nJuechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: A\nprogramming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496,\n2024.\nFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve.\nBetter & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737,\n2024.\nShrey Goel, Vishrut Thoutam, Edgar Mariano Marroquin, Aaron Gokaslan, Arash Firouzbakht,\nSophia Vincoff, Volodymyr Kuleshov, Huong T Kratochvil, and Pranam Chatterjee. Memdlm: De\nnovo membrane protein design with masked discrete diffusion protein language models. arXiv\npreprint arXiv:2410.16735, 2024.\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http:\n//Skylion007.github.io/OpenWebTextCorpus, 2019.\nAaron Gokaslan, A Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel,\nJonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: Open diffusion\nmodels trained on creative-commons images. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 8250–8260, 2024.\nAlex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian flow\nnetworks. arXiv preprint arXiv:2308.07037, 2023.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive\nneural machine translation. arXiv preprint arXiv:1711.02281, 2017.\nJiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in neural informa-\ntion processing systems, 32, 2019.\nIshaan Gulrajani and Tatsunori B Hashimoto. Likelihood-based diffusion language models. Advances\nin Neural Information Processing Systems, 36, 2024.\nAgrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang,\nand Jose Lezama. Photorealistic video generation with diffusion models, 2023. URL https:\n//arxiv.org/abs/2312.06662.\nXiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based\ndiffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432,\n2022.\nXiaochuang Han, Sachin Kumar, Yulia Tsvetkov, and Marjan Ghazvininejad. David helps goliath:\nInference-time collaboration between small specialized and large general diffusion lms. arXiv\npreprint arXiv:2305.14771, 2023.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong\nTian. Training large language models to reason in a continuous latent space. arXiv preprint\narXiv:2412.06769, 2024.\nZhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu.\nDiffusion-\nbert: Improving generative masked language models with diffusion models. arXiv preprint\narXiv:2211.15029, 2022.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\n12\nPublished as a conference paper at ICLR 2025\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840–6851, 2020.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. arXiv:2204.03458, 2022.\nEmiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and\nTim Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021a.\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows\nand multinomial diffusion: Learning categorical distributions. Advances in Neural Information\nProcessing Systems, 34:12454–12465, 2021b.\nDaniel Israel, Aditya Grover, and Guy Van den Broeck. Enabling autoregressive models to fill in\nmasked tokens. arXiv preprint arXiv:2502.06901, 2025.\nDiederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances\nin neural information processing systems, 34:21696–21707, 2021.\nDeqian Kong, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si,\nChuan Li, Jianwen Xie, Sirui Xie, et al. Scalable language models with posterior inference of\nlatent thought vectors. arXiv preprint arXiv:2502.01567, 2025.\nSiqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. CLLMs: Consistency large\nlanguage models. In Forty-first International Conference on Machine Learning, 2024. URL\nhttps://openreview.net/forum?id=8uzBOVmh8H.\nXiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm\nimproves controllable text generation. Advances in Neural Information Processing Systems, 35:\n4328–4343, 2022.\nXiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso\nBiancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, et al. Derivative-free guidance in continuous\nand discrete diffusion models with soft value-based decoding. arXiv preprint arXiv:2408.08252,\n2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\nneural information processing systems, 36:34892–34916, 2023.\nAaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios\nof the data distribution. In Forty-first International Conference on Machine Learning, 2024. URL\nhttps://openreview.net/forum?id=CNicRIVIPA.\nMitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus\nof english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\nHunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance\nfor discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572, 2024.\nJingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan\nLi.\nYour absorbing discrete diffusion secretly models the conditional distributions of clean\ndata. In The Thirteenth International Conference on Learning Representations, 2025. URL\nhttps://openreview.net/forum?id=sMyXP8Tanm.\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset:\nWord prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525–1534,\nBerlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.\naclweb.org/anthology/P16-1144.\n13\nPublished as a conference paper at ICLR 2025\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pp. 4195–4205, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nSubham Sekhar Sahoo, Marianne Arriola, Aaron Gokaslan, Edgar Mariano Marroquin, Alexander M\nRush, Yair Schiff, Justin T Chiu, and Volodymyr Kuleshov. Simple and effective masked diffusion\nlanguage models. In The Thirty-eighth Annual Conference on Neural Information Processing\nSystems, 2024a. URL https://openreview.net/forum?id=L4uaAR4ArM.\nSubham Sekhar Sahoo, Aaron Gokaslan, Christopher De Sa, and Volodymyr Kuleshov. Diffusion\nmodels with learned adaptive noise. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024b. URL https://openreview.net/forum?id=loMa99A4p8.\nSubham Sekhar Sahoo, John Xavier Morris, Aaron Gokaslan, Srijeeta Biswas, Vitaly Shmatikov,\nand Volodymyr Kuleshov. Zero-order diffusion guidance for inverse problems, 2024c. URL\nhttps://openreview.net/forum?id=JBgBrnhLLL.\nGuillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi,\nand Stella Biderman. Stay on topic with classifier-free guidance. arXiv preprint arXiv:2306.17806,\n2023.\nAndrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo\nMarin, and Emanuele Rodola. Accelerating transformer inference for translation via parallel\ndecoding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 12336–12355, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.acl-long.689. URL https://aclanthology.org/2023.acl-long.\n689.\nYair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre,\nBernardo P de Almeida, Alexander Rush, Thomas Pierrot, and Volodymyr Kuleshov. Simple\nguidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024.\nJiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized\nmasked diffusion for discrete data. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. URL https://openreview.net/forum?id=xcqSOfHt4g.\nPhillip Si, Allan Bishop, and Volodymyr Kuleshov. Autoregressive quantile flows for predictive\nuncertainty estimation. In International Conference on Learning Representations, 2022.\nPhillip Si, Zeyi Chen, Subham Sekhar Sahoo, Yair Schiff, and Volodymyr Kuleshov.\nSemi-\nautoregressive energy flows: exploring likelihood-free training of normalizing flows. In In-\nternational Conference on Machine Learning, pp. 31732–31753. PMLR, 2023.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learning,\npp. 2256–2265. PMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems, 32, 2019.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\nHaoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time\ndiscrete diffusion models. arXiv preprint arXiv:2211.16750, 2022.\n14\nPublished as a conference paper at ICLR 2025\nKeyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling:\nScalable image generation via next-scale prediction. Advances in neural information processing\nsystems, 37:84839–84865, 2025.\nBenigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In\nInternational Conference on Machine Learning, pp. 467–475. PMLR, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nYingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and\nVolodymyr Kuleshov. InfoDiffusion: Representation learning using information maximizing\ndiffusion models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan\nSabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine\nLearning, volume 202 of Proceedings of Machine Learning Research, pp. 36336–36354. PMLR,\n23–29 Jul 2023. URL https://proceedings.mlr.press/v202/wang23ah.html.\nTong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, yelong shen, Jian Jiao, Juntao Li,\nzhongyu wei, Jian Guo, Nan Duan, and Weizhu Chen. AR-diffusion: Auto-regressive diffusion\nmodel for text generation. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?id=0EG6qUQ4xE.\nJiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang,\nZhenguo Li, Wei Bi, et al. Diffusion of thoughts: Chain-of-thought reasoning in diffusion language\nmodels. arXiv preprint arXiv:2402.07754, 2024.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text\nclassification. In NIPS, 2015.\nLingxiao Zhao, Xueying Ding, and Leman Akoglu. Pard: Permutation-invariant autoregressive\ndiffusion for graph generation. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. URL https://openreview.net/forum?id=x4Kk4FxLs3.\nKaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked\ndiffusion models are secretly time-agnostic masked models and exploit inaccurate categorical\nsampling. arXiv preprint arXiv:2409.02908, 2024.\n15\nPublished as a conference paper at ICLR 2025\nCONTENTS\n1\nIntroduction\n1\n2\nBackground: Language Modeling Paradigms\n2\n2.1\nAutoregressive Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nDiscrete Denoising Diffusion Probabilistic Models\n. . . . . . . . . . . . . . . . .\n3\n3\nBlock Diffusion Language Modeling\n3\n3.1\nBlock Diffusion Distributions and Model Architectures . . . . . . . . . . . . . . .\n3\n3.2\nEfficient Training and Sampling Algorithms . . . . . . . . . . . . . . . . . . . . .\n4\n4\nUnderstanding Likelihood Gaps Between Diffusion & AR Models\n5\n4.1\nMasked BD3-LMs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n4.2\nCase Study: Single Token Generation\n. . . . . . . . . . . . . . . . . . . . . . . .\n5\n4.3\nDiffusion Gap from High Variance Training . . . . . . . . . . . . . . . . . . . . .\n6\n5\nLow-Variance Noise Schedules for BD3-LMs\n6\n5.1\nIntuition: Avoid Extreme Mask Rates\n. . . . . . . . . . . . . . . . . . . . . . . .\n6\n5.2\nClipped Schedules for Low-Variance Gradients . . . . . . . . . . . . . . . . . . .\n7\n5.3\nData-Driven Clipped Schedules Across Block Sizes . . . . . . . . . . . . . . . . .\n7\n6\nExperiments\n7\n6.1\nLikelihood Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n6.2\nSample Quality and Variable-Length Sequence Generation . . . . . . . . . . . . .\n8\n6.3\nAblations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n7\nDiscussion and Prior Work\n10\n8\nConclusion\n10\nA Block Diffusion NELBO\n17\nB\nMasked BD3-LMs\n17\nB.1\nForward Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.2\nReverse Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.3\nSimplified NELBO for Masked Diffusion Processes . . . . . . . . . . . . . . . . .\n18\nB.4\nRecovering the NLL from the NELBO for Single Token Generation\n. . . . . . . .\n19\nB.5\nTightness of the NELBO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.6\nSpecialized Attention Masks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.7\nOptimized Attention Kernel with FlexAttention . . . . . . . . . . . . . . . . . . .\n21\nC Experimental Details\n24\n16\nPublished as a conference paper at ICLR 2025\nC.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.2\nArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.3\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.4\nLikelihood Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.5\nInference\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nD Samples\n26\nA\nBLOCK DIFFUSION NELBO\nBelow, we provide the Negative ELBO (NELBO) for the block diffusion parameterization. Recall that\nthe sequence x1:L =\n\u0002\nx1, . . . , xL\u0003\nis factorized over B blocks, which we refer to as x for simplicity,\ndrawn from the data distribution q(x). Specifically, we will factorize the likelihood over B blocks of\nlength L′, then perform diffusion in each block over T discretization steps. Let DKL[·] to denote the\nKullback-Leibler divergence, t, s be shorthand for t(i) = i/T and s(i) = (i −1)/T ∀i ∈[1, T]. We\nderive the NELBO as follows:\n−log pθ(x) = −\nB\nX\nb=1\nlog pθ(xb|x<b)\n= −\nB\nX\nb=1\nlog Eq\npθ(xb\nt(1):t(T )|x<b)\nq(xb\nt(1):t(T )|xb)\n= −\nB\nX\nb=1\nlog Eq\npθ(xb\nt(T )|x<b) QT\ni=1 pθ(xb\ns(i)|xb\nt(i), x<b)\nQT\ni=1 q(xb\nt(i)|xb\ns(i))\n≤\nB\nX\nb=1\n\u0014\n−Eq log pθ(xb|xb\nt= 1\nT , x<b)\n|\n{z\n}\nLrecons\n+ Et∈{ 2\nT ,..., T −1\nT\n,1}EqTDKL\n\u0000q(xb\ns|xb\nt, xb) ∥pθ(xb\ns|xb\nt, x<b)\n\u0001\n|\n{z\n}\nLdiffusion\n+ DKL\n\u0000q(xb\nt=1|xb) ∥pθ(xb\nt=1)\n\u0001\n|\n{z\n}\nLprior\n\u0015\n(11)\nB\nMASKED BD3-LMS\nWe explore a specific class of block diffusion models that builds upon the masked diffusion language\nmodeling framework. In particular, we focus on masking diffusion processes introduced by Austin\net al. (2021) and derive a simplified NELBO under this framework as proposed by Sahoo et al.\n(2024a); Shi et al. (2024); Ou et al. (2025).\nFirst, we define the diffusion matrix Qt for states i ∈{1, . . . , V }. Consider the noise schedule\nfunction αt ∈[0, 1], which is a strictly decreasing function in t satisfying α0 = 1 and α1 = 0.\nDenote the mask index as m = V . The diffusion matrix is defined by Austin et al. (2021) as:\n[Qt]ij =\n\n\n\n1\nif i = j = m\nαt\nif i = j ̸= m\n1 −αt\nif j = m, i ̸= m\n(12)\n17\nPublished as a conference paper at ICLR 2025\nThe diffusion matrix for the forward marginal Qt|s is:\n[Qt|s]ij =\n\n\n\n1\nif i = j = m\nαt|s\nif i = j ̸= m\n1 −αt|s\nif j = m, i ̸= m\n(13)\nwhere αt|s = αt/αs.\nB.1\nFORWARD PROCESS\nUnder the D3PM framework (Austin et al., 2021), the forward noise process applied independently\nfor each token ℓ∈{1, . . . L} is defined using diffusion matrices Qt ∈RV ×V as\nq(xℓ\nt|xℓ) = Cat\n\u0000xℓ\nt; Qtxℓ\u0001\n,\nwith\nQt(i) = Qt(1)Qt(2) . . . Qt(i)\n(14)\nB.2\nREVERSE PROCESS\nLet Qt|s denote the diffusion matrix for the forward marginal. We obtain the reverse posterior\nq(xℓ\ns | xℓ\nt, xℓ) using the diffusion matrices:\nq(xℓ\ns|xℓ\nt, xℓ) = q(xℓ\nt|xℓ\ns, xℓ)q(xℓ\ns|xℓ)\nq(xℓ\nt|xℓ)\n= Cat\n \nxℓ\ns; Qt|sxℓ\nt ⊙Q⊤\ns xℓ\n(xℓ\nt)\n⊤Q⊤\nt xℓ\n!\n(15)\nwhere ⊙denotes the Hadmard product between two vectors.\nB.3\nSIMPLIFIED NELBO FOR MASKED DIFFUSION PROCESSES\nFollowing Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025), we simplify the NELBO in the\ncase of masked diffusion processes. Below, we provide the outline of the NELBO derivation; see the\nfull derivation in Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025).\nWe will first focus on simplifying the diffusion loss term Ldiffusion in Eq. 11. We employ the SUBS-\nparameterization proposed in Sahoo et al. (2024b) which simplifies the denoising model pθ for masked\ndiffusion. In particular, we enforce the following constraints on the design of pθ by leveraging the\nfact that there only exists two possible states in the diffusion process xℓ\nt ∈{xℓ, m} ∀ℓ∈{1, . . . , L}.\n1. Zero Masking Probabilities. We set pθ(xℓ= m|xℓ\nt) = 0 (as the clean sequence x doesn’t\ncontain masks).\n2. Carry-Over Unmasking. The true posterior for the case where xℓ\nt ̸= m is q(xℓ\ns = xℓ\nt|xℓ\nt ̸=\nm) = 1 (if a token is unmasked in the reverse process, it is never remasked). Thus, we\nsimplify the denoising model by setting pθ(xℓ\ns = xℓ\nt|xℓ\nt ̸= m) = 1.\nAs a result, we will only approximate the posterior pθ(xℓ\ns = xℓ|xℓ\nt = m). Let xb,ℓdenote a token in\nthe ℓ-th position in block b ∈{1, . . . , B}. The diffusion loss term becomes:\nLdiffusion =\nB\nX\nb=1\nEtEqT\n\u0002\nDKL\n\u0002\nq(xb\ns|xb\nt, xb)∥pθ(xb\ns|xb\nt, x<b)\n\u0003\u0003\n=\nB\nX\nb=1\nEtEqT\n\n\nL′\nX\nℓ=1\nDKL\nh\nq(xb,ℓ\ns |xb,ℓ\nt , xb,ℓ)∥pθ(xb,ℓ\ns |xb\nt, x<b)\ni\n\n\nDKL is simply the discrete-time diffusion loss for the block b; hence, from Sahoo et al. (2024a) (Suppl. B.1), we get:\n=\nB\nX\nb=1\nEtEqT\n\n\nL′\nX\nℓ=1\nαt −αs\n1 −αt\nlog pθ(xb,ℓ| xb,ℓ\nt , x<b)\n\n\n=\nB\nX\nb=1\nEtEqT\n\u0014αt −αs\n1 −αt\nlog pθ(xb | xb\nt, x<b)\n\u0015\n(16)\n18\nPublished as a conference paper at ICLR 2025\nLastly, we obtain a tighter approximation of the likelihood by taking the diffusion steps T →∞\n(Sahoo et al., 2024a), for which T(αt −αs) = α′\nt:\nLdiffusion =\nB\nX\nb=1\nEt∼[0,1]Eq\n\u0014\nα′\nt\n1 −αt\nlog pθ(xb | xb\nt, x<b)\n\u0015\n(17)\nFor the continuous time case, Sahoo et al. (2024a) (Suppl. A.2.4) show the reconstruction loss reduces\nto 0 as xb\nt(1) ∼limT →∞Cat\n\u0010\n.; xb\nt= 1\nT\n\u0011\n= Cat(.; xb). Using this, we obtain:\nLrecons = −Eq log pθ(xb|xb\nt(1), x<b)\n= −log pθ(xb|xb\nt(1) = xb, x<b)\n= 0\n(18)\nThe prior loss Lprior = DKL\n\u0000q(xb\nt=1|xb) ∥pθ(xb\nt=1)\n\u0001\nalso reduces to 0 because αt=1 = 0 which\nensures q(xb\nt=1|xb) = Cat(.; m) and pθ(xb\nt=1) = Cat(.; m); see Sahoo et al. (2024a) (Suppl. A.2.4).\nFinally, we obtain a simple objective that is a weighted average of cross-entropy terms:\nLBD(x; θ) =\nB\nX\nb=1\nEt∼[0,1]Eq\n\u0014\nα′\nt\n1 −αt\nlog pθ(xb | xb\nt, x<b)\n\u0015\n(19)\nThe above NELBO is invariant to the choice of noise schedule αt; see Sahoo et al. (2024a) (Suppl.\nE.1.1).\nB.4\nRECOVERING THE NLL FROM THE NELBO FOR SINGLE TOKEN GENERATION\nConsider the block diffuson NELBO for a block size of 1 where L′ = 1, B = L. The block diffusion\nNELBO is equivalent to the AR NLL when modeling a single token:\n−log p(x) ≤\nL\nX\nb=1\nEt∼[0,1]Eq\n\u0014\nα′\nt\n1 −αt\nlog pθ(xb | xb\nt, x<b)\n\u0015\n∵α′\nt = −1 and αt = 1 −t,\n= −\nL\nX\nb=1\nEt∼[0,1]Eq\n\u00141\nt log pθ(xb | xb\nt, x<b)\n\u0015\n= −\nL\nX\nb=1\nEt∼[0,1]\n1\nt Eq\n\u0002\nlog pθ(xb | xb\nt, x<b)\n\u0003\nExpanding Eq[.],\n= −\nL\nX\nb=1\nEt∼[0,1]\n1\nt\n\u0014\nq(xb\nt = m|xb) log pθ(xb | xb\nt = m, x<b)\n+ q(xb\nt = xb|xb) log pθ(xb | xb\nt = xb, x<b)\n\u0015\n(20)\nRecall that our denoising model employs the SUBS-parameterization proposed in Sahoo et al. (2024b).\nThe “carry-over unmasking” property ensures that log pθ(xb | xb\nt = xb, x<b) = 0, as an unmasked\ntoken is simply copied over from from the input of the denoising model to the output. Hence, (20)\nreduces to following:\n−log pθ(x) ≤−\nL\nX\nb=1\nEt∼[0,1]\n1\nt q(xb\nt = m|xb) log pθ(xb | xb\nt = m, x<b)\n∵q(xb\nt = m|xb) = t, we get:\n= −\nL\nX\nb=1\nEt∼[0,1] log pθ(xb | xb\nt = m, x<b)\n19\nPublished as a conference paper at ICLR 2025\n= −\nL\nX\nb=1\nlog pθ(xb | m, x<b)\n(21)\nFor single-token generation (L′ = 1) we recover the autoregressive NLL.\nB.5\nTIGHTNESS OF THE NELBO\nFor block sizes 1 ≤K ≤L, we show that -log p(x) ≤LK ≤LK+1. Consider K = 1, where we\nrecover the autoregressive NLL (see Suppl B.4):\nL1 =\nL\nX\nb=1\nlog Et∼[0,1]Eq\nα′\nt\n1 −αt\npθ(xb | xb\nt, x<b)\n= −\nL\nX\nb=1\nlog pθ(xb | m, x<b)\n(22)\nConsider the ELBO for block size K = 2:\nL2 =\nL/2\nX\nb=1\nlog Et∼[0,1]Eq\nα′\nt\n1 −αt\npθ(xb | xb\nt, x<b)\n(23)\nWe show that L1 ≤L2, and this holds for all 1 ≤K ≤L by induction. Let xb,ℓcorrespond to the\ntoken in position ℓ∈[1, L′] of block b. We derive the below inequality:\n−\nL\nX\nb=1\nlog pθ(xb | m, x<b) = −\nL/2\nX\nb=1\nlog Et∼[0,1]Eq\n1\n1 −αt\npθ(xb | xb\nt, x<b)\n= −\nL/2\nX\nb=1\nlog Et∼[0,1]Eq\n2\nY\ni=1\n1\n1 −αt\npθ(xb,ℓ| xb\nt, x<b)\n= −\nL/2\nX\nb=1\nlog\n2\nY\ni=1\nEt∼[0,1]Eq\n1\n1 −αt\npθ(xb,ℓ| xb\nt, x<b)\n≤−\nL/2\nX\nb=1\n2\nX\ni=1\nlog Et∼[0,1]Eq\n1\n1 −αt\npθ(xb,ℓ| xb\nt, x<b)\n(24)\nB.6\nSPECIALIZED ATTENTION MASKS\nWe aim to model conditional probabilities pθ(xb | xb\nt, x<b) for all blocks b ∈[1, B] simultaneously\nby designing an efficient training algorithm with our transformer backbone. However, modeling all\nB conditonal terms requires processing both the noised sequence xb\nt and the conditional context x<b\nfor all b.\nRather than calling the denoising network B times, we process both sequences simultaneously by\nconcatenating them xfull ←xt ⊕x as input to a transformer. We update this sequence xfull of length\n2L tokens using a custom attention mask Mfull ∈{0, 1}2L×2L for efficient training.\nThe full attention mask is comprised of four L × L smaller attention masks:\nMfull =\n\u0014\nMBD\nMOBC\n0\nMBC\n\u0015\nwhere MBD and MOBC are used to update the representation of xt and MBC is used to update the\nrepresentation of x. We define these masks as follows:\n• MBD (Block-diagonal mask): Self-attention mask within noised blocks xb\nt\n[MBD]ij =\n\u001a 1\nif i, j are in the same block\n0\notherwise\n20\nPublished as a conference paper at ICLR 2025\n• MOBC (Offset block-causal mask): Cross-attention to conditional context x<b\n[MOBC]ij =\n\u001a 1\nif j belongs in a block before i\n0\notherwise\n• MBC (Block-causal mask): Attention mask for updating xb\n[MBC]ij =\n\u001a 1\nif j belongs in the same block as i, or a block before i\n0\notherwise\nWe visualize an example attention mask for L = 6 and block size L′ = 2 in Figure 3.\nx1\nt\nx2\nt\nx3\nt\nx1\nx2\nx3\nx1\nt\nx2\nt\nx3\nt\nx1\nx2\nx3\nBlock Diagonal (MBD)\nOffset Block Causal (MOBC)\nBlock Causal (MBC)\nFigure 3: Example of Specialized Attention Mask\nB.7\nOPTIMIZED ATTENTION KERNEL WITH FLEXATTENTION\nAs Figure 3 demonstrates, our attention matrix is extremely sparse. We can exploit this sparsity to\nmassively improve the efficiency of BD3-LMs.\nFlexAttention (Dong et al., 2024) is a compiler-driven programming model that enables efficient\nimplementation of attention mechanisms with structured sparsity in PyTorch. It provides a flexible\ninterface for defining custom attention masks while maintaining high performance comparable to\nmanually optimized attention kernels.\nBelow in Fig. 4 we define a block-wise attention mask, block_diff_mask, based on its definition\nas Mfull ∈{0, 1}2L×2L in Suppl. B.6. We fuse the attention operations into a single FlexAttention\nkernel designed to exploit the sparsity in our attention matrix to increase computational efficiency.\nBy doing so, we perform the following optimizations:\n• Precomputed Block Masking: The create_block_mask utility generates a sparse\nattention mask at compile-time, avoiding per-step computation of invalid attention entries.\nThrough sparsity-aware execution, FlexAttention kernels reduce the number of FLOPs in\nthe attention computation.\n• Reduced Memory Footprint: By leveraging block-level sparsity, the attention mechanism\navoids full materialization of large-scale attention matrices, significantly reducing memory\noverhead. FlexAttention minimizes memory accesses by skipping fully masked blocks.\n• Optimized Computation via torch.compile: The integration of torch.compile\nenables kernel fusion and efficient execution on GPUs by generating optimized Triton-based\nkernels. This efficiently parallelizes masked attention computations using optimized GPU\nexecution paths.\n21\nPublished as a conference paper at ICLR 2025\ndef block_diff_mask(b, h, q_idx, kv_idx, block_size, n):\n\"\"\"\nConstructs the specialized block diffusion attention mask composed of\nthree masks:\n- **Block Diagonal Mask (M_BD)**: Self-attention within noised blocks\n- **Offset Block Causal Mask (M_OBC)**: Cross-attention for\nconditional context\n- **Block Causal Mask (M_BC)**: Attention to update x0\nArgs:\nb, h: Batch and head indices (ignored for mask logic).\nq_idx, kv_idx: Query and Key indices.\nblock_size: Defines the block structure.\nn: Sequence length of x_0 and x_t\nReturns:\nA boolean attention mask.\n\"\"\"\n# Indicate whether token belongs to xt (0) or x0 (1)\nx0_flag_q = (q_idx >= n)\nx0_flag_kv = (kv_idx >= n)\n# Compute block indices\nblock_q = torch.where(x0_flag_q == 1,\n(q_idx - n) // block_size,\nq_idx // block_size)\nblock_kv = torch.where(x0_flag_kv == 1,\n(kv_idx - n) // block_size,\nkv_idx // block_size)\n# **1. Block Diagonal Mask (M_BD) **\nblock_diagonal = (block_q == block_kv) & (x0_flag_q == x0_flag_kv)\n# **2. Offset Block-Causal Mask (M_OBC) **\noffset_block_causal = (\n(block_q > block_kv)\n& (x0_flag_q == 0)\n& (x0_flag_kv == 1)\n)\n# **3. Block-Causal Mask (M_BC) **\nblock_causal = (\n(block_q >= block_kv)\n& (x0_flag_q == 1)\n& (x0_flag_kv == 1)\n)\n# **4. Combine Masks **\nreturn block_diagonal | offset_block_causal | block_causal\nFigure 4: We can adapt the masking strategy from Fig. 3 to a FlexAttention compatible sparse\nmasking function as above. This enables the creation of a customized JIT attention operation that uses\nsignificantly less memory with up to ≈5X speedup over the naive native scaled_dot_product_attention\nimplementation in PyTorch (≥2.5) on a A5000 GPU with L = 1024 and batch size B = 16.\n22\nPublished as a conference paper at ICLR 2025\nfrom torch.nn.attention.flex_attention import flex_attention,\ncreate_block_mask\nfrom functools import partial\n# Define block-wise attention mask\nmy_block_diff_mask = partial(block_diff_mask, seq_len=seq_len, block_size\n=block_size)\n# Generate optimized sparse block mask\nblock_mask = create_block_mask(my_block_diff_mask, None, None, seq_len*2,\nseq_len*2, device=device)\n# Compute attention using FlexAttention\n# Use no-cudagraphs to avoid an extra copy on small compile graphs.\n# Use max-autotune if compiling a larger model all at once.\n@torch.compile(fullgraph=True, mode=\"max-autotune-no-cudagraphs\")\ndef single_pass_block_diff_attn(q, k, v, block_mask):\nreturn flex_attention(q, k, v, block_mask=block_mask)\nFigure 5: Attention computation using FlexAttention with our proposed custom mask.\nThis implementation exploits FlexAttention’s ability to dynamically optimize execution based on\nthe provided sparsity pattern. By precomputing block-level sparsity and leveraging efficient kernel\nfusion, it enables scalable attention computation for long sequences.\nOverall, this approach provides a principled method to accelerate attention computations while\npreserving structured dependency constraints. End-to-end, replacing FlashAttention kernels using a\ncustom mask with FlexAttention kernels leads to ≈15% speedup in a model forward pass. We use a\nsingle A5000 for L = 1024 and batch size B = 16.\n23\nPublished as a conference paper at ICLR 2025\nC\nEXPERIMENTAL DETAILS\nWe closely follow the same training and evaluation setup as used by Sahoo et al. (2024a).\nC.1\nDATASETS\nWe conduct experiments on two datasets: The One Billion Word Dataset (LM1B; Chelba et al.\n(2014)) and OpenWebText (OWT; Gokaslan et al. (2019)). Models trained on LM1B use the\nbert-base-uncased tokenizer and a context length of 128. We report perplexities on the test\nsplit of LM1B. Models trained on OWT use the GPT2 tokenizer Radford et al. (2019) and a context\nlength of 1024. Since OWT does not have a validation split, we leave the last 100k documents for\nvalidation.\nIn preparing LM1B examples, Sahoo et al. (2024a) pad each example to fit in the context length of\nL = 128 tokens. Since most examples consist of only a single sentence, block diffusion modeling\nfor larger block sizes L′ > 4 would not be useful for training. Instead, we concatenate and wrap\nsequences to a length of 128. As a result, we retrain our autoregressive baseline, SEDD, and MDLM\non LM1B with wrapping.\nSimilarly for OWT, we do not pad or truncate sequences, but concatenate them and wrap them to a\nlength of 1024 similar to LM1B. For unconditional generation experiments in Section 6.2, we wish\nto generate sequences longer than the context length seen during training. However, Sahoo et al.\n(2024a) inject beginning-of-sequence and end-of-sequence tokens ([BOS], [EOS] respectively) at the\nbeginning and end of the training context. Thus, baselines from Sahoo et al. (2024a) will generate\nsequences that match the training context size. To examine model generations across varying lengths\nin Section 6.2, we retrain our AR, SEDD, and MDLM baselines without injecting [BOS] and [EOS]\ntokens in the examples. We also adopt this preprocessing convention for training all BD3-LMs on\nOWT.\nC.2\nARCHITECTURE\nThe model architecture augments the diffusion transformer (Peebles & Xie, 2023) with rotary\npositional embeddings (Su et al., 2021). We parameterize our autoregressive baselines, SEDD,\nMDLM, and BD3-LMs with a transformer architecture from Sahoo et al. (2024a) that uses 12 layers,\na hidden dimension of 768, and 12 attention heads. This corresponds to 110M parameters. We do not\ninclude timestep conditioning as Sahoo et al. (2024a) show it does not affect performance. We use\nthe AdamW optimizer with a batch size of 512 and constant learning rate warmup from 0 to 3e-4\nfor 2.5K gradient updates.\nC.3\nTRAINING\nWe train a base BD3-LM using the maximum context length L′ = L for 850K gradient steps. Then,\nwe fine-tune under varying L′ using the noise schedule optimization for 150K gradient steps on the\nOne Billion Words dataset (LM1B) and OpenWebText (OWT). This translates to 65B tokens and\n73 epochs on LM1B, 524B tokens and 60 epochs on OWT. We use 3090, A5000, A6000, and A100\nGPUs.\nC.4\nLIKELIHOOD EVALUATION\nWe use a single Monte Carlo estimate for sampling t to evaluate the likelihood of a token block. We\nadopt a low-discrepancy sampler proposed in Kingma et al. (2021) that reduces the variance of this\nestimate by ensuring the time steps are more evenly spaced across the interval [0,1] following Sahoo\net al. (2024a). In particular, we sample the time step for each block b ∈{1, . . . , B} and sequence k ∈\n{1, . . . , K} from a different partition of the uniform interval t(k, b) ∼U[ (k−1)B+b−1\nKB\n, (k−1)B+b\nKB\n].\nThis low-discrepancy sampler is used for evaluation. For training, each masking probability may be\nsampled from a “clipped\" range 1 −αt ∼U[β, ω]. During training, we uniformly sample t ∈[0, 1]\nunder the low-discrepancy sampler. We then apply a linear interpolation to ensure that the masking\nprobability is linear within the desired range: 1 −αt = β + (ω −β)t.\n24\nPublished as a conference paper at ICLR 2025\nWhen reporting zero-shot likelihoods on benchmark datasets from Radford et al. (2019) using models\ntrained on OWT, we wrap all sequences to 1024 tokens and do not add [EOS] between sequences\nfollowing Sahoo et al. (2024a).\nC.5\nINFERENCE\nGenerative Perplexity\nWe report generative perplexity under GPT2-Large from models trained\non OWT using a context length of 1024 tokens. Since GPT2-Large uses a context size of 1024, we\ncompute the generative perplexity for samples longer than 1024 tokens using a sliding window with a\nstride length of 512 tokens.\nNucleus Sampling\nFollowing SSD-LM (Han et al., 2022), we employ nucleus sampling for BD3-\nLMs and our baselines. For SSD-LM, we use their default hyperparameters p = 0.95 for block size\nL′ = 25. For BD3-LMs, AR and MDLM, we use p = 0.9. For SEDD, we find that p = 0.99 works\nbest.\nNumber of Diffusion Steps\nIn Table 7, BD3-LMs and MDLM use T = 5K diffusion steps. BD3-\nLMs and MDLM use efficient sampling by caching the output of the denoising network as proposed\nby Sahoo et al. (2024a); Ou et al. (2025), which ensures that the number of generation steps does not\nexceed the sample length L. Put simply, once a token is unmasked, it is never remasked as a result of\nthe simplified denoising model (Suppl. B.3). We use MDLM’s block-wise decoding algorithm for\ngenerating variable-length sequences, however these models are not trained with block diffusion. We\nadopt their default stride length of 512 tokens.\nSSD-LM (first row in Table 7) and SEDD use T = 1K diffusion steps. Since block diffusion performs\nT diffusion steps for each block b ∈{1, . . . , B}, SSD-LM undergoes BT generation steps. Thus to\nfairly compare with SSD-LM, we also report generative perplexity for T = 25 diffusion steps so that\nthe number of generation steps does not exceed the sequence length (second row in Table 7).\nImproved Categorical Sampling of Diffusion Models\nWe employ two improvements to Gumbel-\nbased categorical sampling of diffusion models as proposed by Zheng et al. (2024).\nFirst, we use the corrected Gumbel-based categorical sampling from Zheng et al. (2024) by sampling\n64-bit Gumbel variables. Reducing the precision to 32-bit has been shown to significantly truncate\nthe Gumbel variables, lowering the temperature and decreasing the sentence entropy.\nSecond, Zheng et al. (2024) show that the MDLM sampling time scales with the diffusion steps T,\neven though the number of generation steps is bounded by the sequence length. For sample length L\nand vocabulary size V , the sampler requires sampling O(TLV ) uniform variables and performing\nlogarithmic operations on them.\nWe adopt the first-hitting sampler proposed by Zheng et al. (2024) that requires sampling O(LV )\nuniform variables, and thus greatly improves sampling speed especially when T ≫L. The first-\nhitting sampler is theoretically equivalent to the MDLM sampler and leverages two observations: (1)\nthe transition probability is independent of the denoising network, (2) the transition probability is the\nsame for all masked tokens for a given t. Thus, the first timestep where a token is unmasked can be\nanalytically sampled as follows (assuming a linear schedule where αt = 1 −t):\ntn−1 = tnu1/n,\n(25)\nwhere n ∈{L, . . . , 1} denotes the number of masked tokens, un ∼U[0, 1] and tn−1 corresponds to\nthe first timestep where n −1 tokens are masked.\nVariable-Length Sequence Generation\nFor arbitrary-length sequence generation using BD3-LMs\nand AR in Table 6, we continue to sample tokens until the following stopping criteria are met:\n1. an [EOS] token is sampled\n2. the average entropy of the the last 256-token chunk is below 4\nwhere criterion 2 are necessary to prevent run-on samples from compounding errors (for example, a\nsequence of repeating tokens). We find that degenerate samples with low entropy result in significantly\n25\nPublished as a conference paper at ICLR 2025\nlow perplexities under GPT2 and lower the reported generative perplexity. Thus, when a sample\nmeets criterion 2, we regenerate the sample when reporting generative perplexity in Table 7.\nD\nSAMPLES\n<|endoftext|>’s architect, lawyer and San Giovanni concerto art critic Paolo Capacotti, gained attention from fellow gallery members and even invited\nhim to present a retrospective, publishing issues and newspaper interviews.[10] On 6 September, Kissi and his assistants agreed to move to Angelo’s\nMarcus Collection,[10] which included Giorgio Avolivo Arth and Moscolliso (later owned by the artist Belzina Massingolo) and Pan Giazzoglio\nRomeam-Guessle. The businessman, Giovanni Paletti, an outstanding collector, owned the museum and the painting. The level of criminal activity\naround the museum has continued to increase, which is part of several attempts to counter centennial rumors including the possibility that museum staff\nand visitors are tortured and even exposed to del Cavello for the only full year of Francesco Belzina’s life (1999).[4] On the evening of 22 October 2005\nit was reported that earlier that evening, guards had come on duty and began flinging an electric field with umbrellas from the balcony. As the fire\ncontinued, some of the guards sparked an apparent spat from the window of the cathedral. They remained idly watched by a pile of trash left after a\npiano key by Pietro Jolla, who died on 21 October 2005.[10] Just before 3:00 to 3pm on Monday, 27 October 2005, strong winds brought the trash on to\nthe residence that opened on 17 October. Some ruined books and statues were hurled in front from every direction of the window. Some claimed that a\ncustomer Jacques Monet had beaten the hand of photographer Franco Campetti and in some cases had stuck a broken candle in the doorway of the\nmuseum. Andr Romeam-Guessle responded by laughing when he spoke. Giancio Giuliano, the artistic director of the Museum, even tried to told\njournalists and press that ’the patient in the trisomy machine [sic] carried some corpses four hours into the museum, but the whole time it was the guy\nwho stroked the young man who who broke him’. In 2008, Giuliano told the same press that the hours of the destruction are truly \"wrong for their\nmorality\" and further stated that ’We are never satisfied with our decision. We made an informed decision to build the museum after destruction.[5]\nDeaths [ edit ] A little after 12:00 am 17 October 2005, Giuliano and his partner Monica Concerta, noticed that the trash was being thrown by passers-by.\nCaptain Iamienowska leaned over to his film camera and said, in a joking manner, that Iannorello, the chair of the Musceei, was a thief that director\nFrank Nolan said \"he would later be arrested.\" When Iamienowska arrived, the people in question were interviewed by Captain Anderson Tulaqyuk,\na co-man who was initially lying on the scene and whom Iamienowska said was able to stop them from passing in the vicinity. Iguano proceeded\nto collect the trash and the police arrived, and closed the door of the museum.[6] During the war, the statue structure was partially removed and its\ncannons damaged. On the eve of the war, the U.S. Army and Canadian Air Company, who once owned a lot on the Coopers of Paris near Leopold\nStreet, sheltered the POWs, who were briefly overworked. This following years led to condemnation of the organization and artists, including John\nDiDione,[7] returned to the compound to sell the museum items (including many of them originally stolen over the years-more than a million returned);\non 12 October, the sculptor Jack Uellein installed a temporary dome on Del Cavella.[10] Among the changes over the years, Diena featured the lion in\nthe middle of the other two halves of the dome and the lion in the back of the Viduccio.[11] In March 2008, Arvnzo Cissino announced that seven studio\nstudios were built between the summer of 1985 and the last one was completed on November 17, 2005.[10] In the fourth quarter of the mid-2000s, a\nnumber of new structures were constructed by various museums, but most notably by Arvnzo Cissino, the brothers-soldier, as portrayed by architect\nDuvois Mayaven in the illusion of the El Valleratas Humanities and Cultural Center, reconstructed by architect Duvois Benjamin. Just as the very\nsignificant additions to the current management masts of the del Museum are such private three-bedroom rooms as Tradello’s cabin, as well as the\narchitectural aspect from the museum’s south wall. Also new are the rockings performed by MC Urquhart, Aiba Ruiz and March Flez, during a music\nfestival held in the US in 2006. Urquhart was later credited with establishing record label Empire Records as an influence on his music. In<|endoftext>|\nFigure 6: Sample from MDLM (Sahoo et al., 2024a) of length L = 1024 and T = 5K diffusion steps.\nThe generative perplexity of this sample under GPT2-Large is 69.26 and its entropy is 5.6.\n26\nPublished as a conference paper at ICLR 2025\n<|endoftext|>When her child was planning to travel to Mexico, she thought it could be easy for her to find money to pay the trip. But that didn’t happen,\neither. The girl arrived on her doorstep the day before after they were scheduled to return to home and - despite her own passport blocked. It wasn’t\neasy. Even her mother reported a bad visit. A gate to the airport- the gate and flight instructions - were locked at nine a.. \"I had no way to the gate,\" she\ntold The Local. \"I feel like I should keep my passport and their fees.\" At first the police arrived, a British man told an e-mail that parents should only\nhave money for themselves to be fine, but could never explain the situation. Later, she told her daughter that her passport had been detained because she\nwanted to visit her family at a hotel. She felt embarrassed and confused. Nearly two months later, she said, she lost 35(40) on rent and imagined that\nthe traffic-police and customs agents in Bangkok would end up delaying flights and forcing her to stay home. She was worried that her father would\nrefuse allowing her daughter to spend a few days in the country. Meanwhile, the police were sent to search. \"It wasn’t easy for them when a child feels\nlike home for the first time,\" said Mahavram Kaas, a spokesman for the Ministry of Foreign Affairs. He’s referring to a tour arranged by the French and\nFrench foreign ministry, known as Courage in the Child. That tour cost the region 2 million worth of tickets, and cost the Calais family about $25\nmillion in lodging expenses, according to a statement by both the ministry’s behalf (he told the Portuguese police agents) and London’s Embassy in\nLondon (he told the French ambassador they provided a payment for 70,000, which would be used to pay the travel costs for their visits). In 2011, a\nfamily from Calais had moved to the UK aged 15. Their sister left to remain in France at 5 years old. Her brother tried to answer that question. He\nexplained it to reporters at his service station at Calais airport. \"You take the morning. It’s named after you and your little girls,\" he said. The last\nmother was having a 19-month stay with her daughter that night, the police said. The first time she was back her husband took their daughter on a boat\nto the UK, said the mayor of the British Transport Agency. That meant she had plenty of cash-to-go and no money to borrow when she opened an\naccount at Kathmandu Airport a few hours after booking her flight. She panicked. She called the was a Daley’s Nessie (small cash register), saying she\nwas getting better. Her doctors visited her when she returned and her boyfriend quit his job for four months after the visit, she said. \"How do you feel\nlike you are safe?\" A text from a friend left her to the police. \"She says I must go get my wallet,\" said Ajaz. Soon after, she booked a plane ticket to\nParis and took a metro train to Calais. One night, a French policeman would knock on the door of a local council building, open the mailman and the\nphone and tell her she knew that she could not leave at least one week without food. The four months her daughter spent in the UK was exhausting and\nhard, and it reached the stage where she realized she could barely stay in Britain. \"Now no choice but to go home. Then we regret having a daughter,\"\nhe said. He thought for a minute. \"You’ve broken your heart.\" \"Today, my daughter and my boyfriend decided to stay in this country for over two\nmonths,\" he wrote in an email with his daughter in his hand. \"All our flights cancelled and no security. Shame.\" Caines’ family were also put on leave.\nThe French police paid for her car after she rented it, and her female officer used it for the opening ceremony of her press conference in Thailand. The\npolice are still arranging for her family to have their official visit. Although her son is back at work now and his old job, her daughter needs to stay in a\nhospital in Algiers to continue her education. But, finally, her parents will be making their girl home. The daughter was 18 when they opened her\ncase. She was born two months ago. She doesn’t talk about it because it feels like she was still a child, living in Thailand with a small child. Her\nmother, Anzsa Gurdon, came to England as a three-year-old after her brother, Ehab Rahman, was working as a British worker in Calais while living\nin London and studying abroad in Dubai. As a mother earns 2,000 pounds a month, they receive a well-paid living in secure accommodation, some\neven with public transport buses. If they make money, their child stays in the UK, they can set up companies with kids to take care of their children.\nOther countries sometimes also give birth to parents forced to provide child care. The parents are often refugees from their home countries, they’re left\nwithout family, and have forced to leave families. As one former refugee fled Syria, his family was in detention, because when and if their child had\narrived, they would be living somewhere. The detention centers in Western Europe often have a higher rate for asylum seekers. Often there are higher\n\"safe houses\" for young people, and then the people in the center get older when their child comes to stay, but the only family that is a year older is not\nallowed to have children, and usually only if they stay six months. Children are also detained and are asked to show identification. Because in most\ncases the refugees ask only about their identity, they don’t have access to their own documents, and have no other documentation. An activist working\nfor Ireland says he’s against offshore processing. He thinks the charity problem here is like diseases which don’t sufficiently seek out international\nfunding. I think too many countries want to employ \"humanitarian\" children. Now the job Before the refugee crisis in Calais, 1,823 children were living\nin the UK, the UNHCR website shows. A good chunk of those children had landed in refugee camps in Africa, where mostly African migrants were\nsending children from Syria and Libya to their camps, but those numbers didn’t fare so well. \"At the moment when I met the French, it was horrific,\nthey wanted me to put my children in a van. But I was only kidding. It’s something called 10-year vans,\" Aakaz said during an interview. \"I want to\nkeep my children for 10 years. That’s something. It’s like Christmas. The dream of 10 years. . . . For me, the idea that this is a good opportunity, here’s\na chance,\" is that she can sleep with her children. At this point, it’s much more than just about \"alternatives\". They now have to decide, at some point,\nwhether they want to take the chance. Is it a big deal or not? Only in Calais She explained what’s agreed to so the children can go home and can have a\nbetter future. \"I’m very determined. My children want to go home but it’s my life’s personal decision,\" she said. \"Five months. I want them to be home,\n5 months. If they’re not getting jobs properly, I also want to stay home. But I feel good about what I’ve got. She is from a poor country. I don’t owe\nanything to anyone. But I have to work for them. I feel like I can just go to the road and provide accommodation for my children and their children.\"\nBut they all don’t work well because their families have her as a head. \"They want me to have a job in England, but I feel like it’s my home, and I’m not\nscared of work,\" she said. \"And I feel that the opposite would be possible. I think in the future that I can have a job or two there.\" A Second World\nFriday event will be held outside Calais on Saturday, donating 100 euros to the coming week in the money brought up to them by UNHCR through the\nKing Wahab Samba Global Fund. Friends and family expressed their \"weakness\" like many survivors in many countries.\"We just don’t want to accept\nwhat has to happen. We want to put the people back there as soon as possible,\" said one man. Her brother, who is the son of a long term migrant,\nsaid: \"The story of the refugee is not a mother’s story. The story of the refugee is children’s story.\" \"In Calais it’s too young for these kids. They play\noutside or work outside, they just eat, right? I don’t think much has changed. This child with all her food and sleep, she’s too young for life without any\nprotection. We don’t need any protection at all. We need anything that would be safer.\"<|endoftext|>\nFigure 7: Sample from BD3-LM for block size L′ = 16 of length L = 2031 under T = 5K diffusion\nsteps (trained with a context length of L = 1024). The generative perplexity of this sample under\nGPT2-Large is 24.3, and its entropy is 5.5.\n27\nPublished as a conference paper at ICLR 2025\n<|endoftext|>, but Wilson took over the program and turned it around. \"He’s done a tremendous job,\" Caldwell said. \"He’s done a fantastic job.\" The\noffense has always had an abundance of weapons, but it became evident that they weren’t going to have a weapon to actually go after players from the\nslot. Now they’re in two different weapons sets. The top group features Dez Bryant and Mohamed Sanu, and the bottom group features an assortment\nof weapons and pass rushers. The job has become far more complex. The other players can make plays on the ball and get those targets at a higher rate.\nSanu is more of a classic, get to the quarterback and leave the corner open. Dontari Poe got the job done this year and became one of the more effective\nplayers at the position, even in the passing game. However, Dallas has got to figure out how to get their franchise wideouts to contribute on the field.\nThat can be tough. Adding Poe can help get the receiving corps going. C.J. Spiller is a two-time Pro Bowler, but if the Cowboys want to upgrade their\nreceiving corps, he’s going to have to step up in a big way. \"We’ve got to be a little more aggressive with the type of weapons that we have,\" Caldwell\nsaid. \"I think that’s part of the reason why our last two games, especially when you’re playing in Washington, D.C., you’ve got to be aggressive, make\nsure you’re hitting at every catch. When you are, you’re giving up a lot of yards.\" Part of that means taking the quarterback out of the equation and\nhaving him beat coverage a lot more. In the NFC West, you want your offensive weapons to do a better job of running through coverage. The biggest\nthreat that Dallas has is a QB in Ben Roethlisberger. Roethlisberger is far and away the best quarterback in the league, but a lot of the credit has to go\nto his receiver group. Martavis Bryant and Antonio Brown are both big-time receivers, and last year they were in the top 10 of yards per catch and\nreceiving yards in the league. That production will never be sustainable, but if you’re going to be an elite offense, it’s going to take a lot of catching up.\nRoethlisberger is an All-Pro receiver, and he’s not the most dynamic option. But it would take something like Bryant or Brown at a better position, and\nat a slightly lower price, to make him the most productive receiver on the offense. The truth is that Roethlisberger isn’t going to be great. He may only\nhave 18 games left in his career, but he’s been doing it since he was a rookie in 1991. But that’s not the worst thing in the world. Roethlisberger’s ability\nto hit guys on the outside with good movement, vision and running ability is what the Cowboys need in order to keep up with the competition. If he\nkeeps getting better, he could become the best receiver in the league. Follow @walterfootball for updates. Like our Facebook page for more Cowboys\nnews, commentary and conversation.The owner of 1H10 Tree in Charlotte Gardens is taking legal action against the city. Derek Jarman says he’s\nbeen forced to evict his neighbour, Bob, after he took to social media to threaten to burn down his neighbor’s house. \"I’m incredibly furious with the\ncity,\" Jarman told 7.30. \"I’ve been trying to keep my eyes on the prize.\" Tree in Charlotte Gardens saying it had seen ’9,000+ people’ enjoying a great\nweekend The company that owns 1H10 named Bob after a bee and said the tree was frequently targeted because of its unusual location. Bob said he had\nhis concerns about the tree when he was contacted in October. He said they had had ’an ongoing conversation about my neighbor. He called, hung up\nand he was very threatening’ in the 30 days before they turned the tree over to him. A neighbour posted the following online message on 8 October. \"I\nam shocked about the serious problems you are having with your neighbour that has caused you all (sic). You and the 2 of you are making money at the\nexpense of the good people of Charlotte Gardens.\" Bob says he was furious and said he’d just got off the phone with the city manager. \"I told her, ’no,\nI’m going to bring a lawsuit’, and I called the solicitor and tried to get my phone, just hoping the solicitor would help me out. I called again, and I asked\nif I could go to court and to try and get an injunction. \"They told me ’you cannot’, and they said, ’we can’t, we can’t’ because you’re sending people to\nthe police’.\" Tree in Charlotte Gardens (Facebook) He also said he’d threatened the city attorney if he didn’t stop the building from burning down. The\ninternet user tweeted: \"I’s on the tree, but after I said ’threw this away, here’s a spot to burn’, the building started to burn.\" Tree in Charlotte Gardens\n(Facebook) Bob said the neighbour had threatened to burn down the tree, the windows, the living room and his entire backyard. \"It was more than a\nthreat,\" he said. \"He was a very strong person. He’s already damaged so many people in this building. It’s not going to go away.\" Tree in Charlotte\nGardens (Facebook) Jarman says he tried to talk the building owner out of the move, but the building owner’s behaviour had \"deleted him.\" \"I’m going\nto stop him by letter telling him not to come to my house any more,\" he said. \"I have three kids, and if Bob is going to be in my house, I need to make\nsure I have someone who can go in there and protect me. \"My son does a really good job of protecting me, and I’m not going to let that get in the way\nof that.\" Tree in Charlotte Gardens (Facebook) Jarman said Bob had pulled him up on social media, calling him a \"white nut\" and saying: \"For God’s\nsake, stop calling me a white nut. \"I should have shut him up on Facebook.\" He said he sent Bob the letter and thanked him for the support. \"He should\nhave done it because he’s a real artist and he’s a real artist,\" he said. He declined to name the architect of the new tree, but says the firm is the same\none that designs buildings. ’The building is burning down’, neighbour says Bob’s neighbour, Michael Banks, says the fire is an insult to his daughter.\n\"There are two black women that live next door to me and they told me ’you can’t do that’, and then the fire went up and then the building burnt down,\"\nhe said. \"You can’t burn down a house if you don’t burn down the house.\" Coun-Pete Lawrence, the Northumberland MP for Wood Green, says he has\nconcerns about Bob’s neighbours. \"It’s a very, very sad commentary on the state of society and democracy in general,\" he said. \"It’s interesting in a\ncommunity that’s 50,000-plus people, you’ve got your regular residents and well-meaning neighbours who are apparently oblivious to the destruction of\ntheir own home. \"To me, that’s appalling and it is probably a shocking amount of devastation that it’s left behind. \"I would expect there to be outrage as\nwell.\" Bob Jarman fears for his life after the tree was torched Bob says he has told the Northumberland Council that he had already received $1,000 in\nlegal action from the building owner, when he told them about the incident. The building owner has declined to comment on the situation. The builder\nis currently assessing its legal options. \"We’ve got to sort this out and have an understanding with the builder, Mr Banks,\" he said. \"We’ve got to make\nsure we can’t get into into a legal battle with that person and make that person change his mind. \"We don’t want to do anything to cause a scene or\nanybody in the street to be upset.\" Bob Jarman hopes to have an understanding with the builder on its legal options, who have refused to comment.\nTopics: state-parliament, smoking-and-doubt, black-wales-6168, united-kingdom, england First postedWhen the other guys are away playing, do a short\ncommercial to get you fired up for the next work day. Once you make it home, get a few junkies for them. They’ll be very happy to have you, for at\nleast a day. They might not be so happy after a couple of days. Have a bunch of friends and get ready to keep it going. What are you waiting for? Make\nthis long, one-off<|endoftext>\nFigure 8: Sample from an AR model (Sahoo et al., 2024a) with length L = 2003 (trained with a\ncontext length of L = 1024). The generative perplexity of this sample under GPT2-Large is 10.6 and\nits entropy is 5.5.\n28\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Published as a conference paper at ICLR 2025\nBLOCK DIFFUSION: INTERPOLATING BETWEEN AU-\nTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS\nMarianne Arriola† ∗\nAaron Kerem Gokaslan†\nJustin T. Chiu‡\nZhihan Yang†\nZhixuan Qi†\nJiaqi Han¶\nSubham Sekhar Sahoo†\nVolodymyr Kuleshov†\nABSTRACT\nDiffusion language models offer unique benefits over autoregressive models due\nto their potential for parallelized generation and controllability, yet they lag in\nlikelihood modeling and are limited to fixed-length generation. In this work, we\nintroduce a class of block diffusion language models that interpolate between\ndiscrete denoising diffusion and autoregressive models. Block diffusion overcomes\nkey limitations of both approaches by supporting flexible-length generation and\nimproving inference efficiency with KV caching and parallel token sampling. We\npropose a recipe for building effective block diffusion models that includes an\nefficient training algorithm, estimators of gradient variance, and data-driven noise\nschedules to minimize the variance. Block diffusion sets a new state-of-the-art\nperformance among diffusion models on language modeling benchmarks and\nenables generation of arbitrary-length sequences. We provide the code1, along\nwith the model weights and blog post on the project page:\nhttps://m-arriola.com/bd3lms\n1\nINTRODUCTION\nDiffusion models are widely used to generate images (Ho et al., 2020; Dhariwal & Nichol, 2021;\nSahoo et al., 2024b) and videos (Ho et al., 2022; Gupta et al., 2023), and are becoming increasingly\neffective at generating discrete data such as text (Lou et al., 2024; Sahoo et al., 2024a) or biological\nsequences (Avdeyev et al., 2023; Goel et al., 2024). Compared to autoregressive models, diffusion\nmodels have the potential to accelerate generation and improve the controllability of model outputs\n(Schiff et al., 2024; Nisonoff et al., 2024; Li et al., 2024; Sahoo et al., 2024c).\nDiscrete diffusion models currently face at least three limitations. First, in applications such as chat\nsystems, models must generate output sequences of arbitrary length (e.g., a response to a user’s\nquestion). However, most recent diffusion architectures only generate fixed-length vectors (Austin\net al., 2021; Lou et al., 2024). Second, discrete diffusion uses bidirectional context during generation\nand therefore cannot reuse previous computations with KV caching, which makes inference less\nefficient (Israel et al., 2025). Third, the quality of discrete diffusion models, as measured by standard\nmetrics such as perplexity, lags behind autoregressive approaches and further limits their applicability\n(Gulrajani & Hashimoto, 2024; Sahoo et al., 2024a).\nThis paper makes progress towards addressing these limitations by introducing Block Discrete\nDenoising Diffusion Language Models (BD3-LMs), which interpolate between discrete diffusion\nand autoregressive models. Specifically, block diffusion models (also known as semi-autoregressive\nmodels) define an autoregressive probability distribution over blocks of discrete random variables\n(Si et al., 2022; 2023); the conditional probability of a block given previous blocks is specified by a\ndiscrete denoising diffusion model (Austin et al., 2021; Sahoo et al., 2024a).\nDeveloping effective BD3-LMs involves two challenges. First, efficiently computing the training\nobjective for a block diffusion model is not possible using one standard forward pass of a neural\n∗Correspondence to Marianne Arriola: marriola@cs.cornell.edu\n†Cornell Tech, NY, USA.\n¶Stanford University, CA, USA.\n‡ Cohere, NY, USA.\n1Code: https://github.com/kuleshov-group/bd3lms\n1\n"
    },
    {
      "page_number": 2,
      "text": "Published as a conference paper at ICLR 2025\nOn September 17, 2016, we will be giving the      release of              \nAutoregression: \nThere are three categories of the average\nThere are three categories of the average rate\nThere are three categories of the average rate of...\nGeneration steps\nDiffusion: \n       the reusability                        will continue to        the\nLower quality\nRepeal the reusability cuts and       the law will continue to reduce the\nRepeal the reusability cuts and prove the law will continue to reduce the deficit.\nBlock Diffusion (Ours):\nOn September 17,       we      be                                               \nParallelizable\nOn September 17, 2016, we will be giving the beta-release of the      to our server     testing ...\nKV caching\nKV caching\nArbitrary-length\nParallelizable\nArbitrary-length\nHigh quality\nHigh quality\nFixed-length\nNo KV caching\nNot Parallelizable\nFigure 1: Block diffusion sequentially generates blocks of tokens by performing diffusion within each\nblock and conditioning on previous blocks. By combining strength from autoregressive and diffusion\nmodels, block diffusion overcomes the limitations of both approaches by supporting variable-length,\nhigher-quality generation and improving inference efficiency with KV caching and parallel sampling.\nnetwork and requires developing specialized algorithms. Second, training is hampered by the high\nvariance of the gradients of the diffusion objective, causing BD3-LMs to under-perform autoregression\neven with a block size of one (when both models should be equivalent). We derive estimators of\ngradient variance, and demonstrate that it is a key contributor to the gap in perplexity between\nautoregression and diffusion. We then propose custom noise processes that minimize gradient\nvariance and make progress towards closing the perplexity gap.\nWe evaluate BD3-LMs on language modeling benchmarks, and demonstrate that they are able to\ngenerate sequences of arbitrary length, including lengths that exceed their training context. In addition,\nBD3-LMs achieve new state-of-the-art perplexities among discrete diffusion models. Compared to\nalternative semi-autoregressive formulations that perform Gaussian diffusion over embeddings (Han\net al., 2022; 2023), our discrete approach features tractable likelihood estimates and yields samples\nwith improved generative perplexity using an order of magnitude fewer generation steps. In summary,\nour work makes the following contributions:\n• We introduce block discrete diffusion language models, which are autoregressive over\nblocks of tokens; conditionals over each block are based on discrete diffusion. Unlike prior\ndiffusion models, block diffusion supports variable-length generation and KV caching.\n• We introduce custom training algorithms for block diffusion models that enable efficiently\nleveraging the entire batch of tokens provided to the model.\n• We identify gradient variance as a limiting factor of the performance of diffusion models,\nand we propose custom data-driven noise schedules that reduce gradient variance.\n• Our results establish a new state-of-the-art perplexity for discrete diffusion and make\nprogress toward closing the gap to autoregressive models.\n2\nBACKGROUND: LANGUAGE MODELING PARADIGMS\nNotation\nWe consider scalar discrete random variables with V categories as ‘one-hot’ column\nvectors in the space V = {x ∈{0, 1}V : P\ni xi = 1} ⊂∆V for the simplex ∆V . Let the V -th\ncategory denote a special [MASK] token, where m ∈V is its one-hot vector. We define x1:L as a\nsequence of L tokens, where xℓ∈V for all tokens ℓ∈{1, . . . , L}, and use VL to denote the set of\nall such sequences. Throughout the work, we simplify notation and refer to the token sequence as\nx and an individual token as xℓ. Finally, let Cat(·; p) be a categorical distribution with probability\np ∈∆V .\n2\n"
    },
    {
      "page_number": 3,
      "text": "Published as a conference paper at ICLR 2025\n2.1\nAUTOREGRESSIVE MODELS\nConsider a sequence of L tokens x =\n\u0002\nx1, . . . , xL\u0003\ndrawn from the data distribution q(x). Autore-\ngressive (AR) models define a factorized distribution of the form\nlog pθ(x) =\nL\nX\nℓ=1\nlog pθ(xℓ| x<ℓ),\n(1)\nwhere each pθ(xℓ| x<ℓ) is parameterized directly with a neural network. As a result, AR models\nmay be trained efficiently via next token prediction. However, AR models take L steps to generate L\ntokens due to the sequential dependencies.\n2.2\nDISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS\nDiffusion models fit a model pθ(x) to reverse a forward corruption process q (Sohl-Dickstein et al.,\n2015; Ho et al., 2020; Sahoo et al., 2024b). This process starts with clean data x and defines latent\nvariables xt =\n\u0002\nx1\nt, . . . , xL\nt\n\u0003\nfor t ∈[0, 1], which represent progressively noisier versions of x. Given\na discretization into T steps, we define s(j) = (j −1)/T and t(j) = j/T. For brevity, we drop j\nfrom t(j) and s(j) below; in general, s denotes the time step preceding t.\nThe D3PM framework (Austin et al., 2021) defines q as a Markov forward process acting indepen-\ndently on each token xℓ: q(xℓ\nt | xℓ\ns) = Cat(xℓ\nt; Qtxℓ\ns) where Qt ∈RV ×V is the diffusion matrix.\nThe matrix Qt can model various transformations, including masking, random token changes, and\nrelated word substitutions.\nAn ideal diffusion model pθ is the reverse of the process q. The D3PM framework defines pθ as\npθ(xs | xt) =\nL\nY\nℓ=1\npθ(xℓ\ns | xt) =\nX\nx\n\" L\nY\nℓ=1\nq(xℓ\ns | xℓ\nt, xℓ)pθ(xℓ| xt)\n#\n,\n(2)\nwhere the denoising base model pθ(xℓ| xt) predicts clean token xℓgiven the noisy sequence xt, and\nthe reverse posterior q(xℓ\ns | xℓ\nt, x) is defined following Austin et al. (2021) in Suppl. B.3.\nThe diffusion model pθ is trained using variational inference. Let KL[·] denote the Kullback-Leibler\ndivergence. Then, the Negative ELBO (NELBO) is given by (Sohl-Dickstein et al., 2015):\nL(x; θ) = Eq\n\"\n−log pθ(x|xt(1)) +\nT\nX\nj=1\nDKL[q(xs(j)|xt(j), x)∥pθ(xs(j)|xt(j))] + DKL[q(xt(T )|x)∥pθ(xt(T ))]\n#\n(3)\nThis formalism extends to continuous time via Markov chain (CTMC) theory and admits score-based\ngeneralizations (Song & Ermon, 2019; Lou et al., 2024; Sun et al., 2022). Further simplifications\n(Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025) tighten the ELBO and enhance performance.\n3\nBLOCK DIFFUSION LANGUAGE MODELING\nWe explore a class of Block Discrete Denoising Diffusion Language Models (BD3-LMs) that\ninterpolate between autoregressive and diffusion models by defining an autoregressive distribution\nover blocks of tokens and performing diffusion within each block. We provide a block diffusion\nobjective for maximum likelihood estimation and efficient training and sampling algorithms. We\nshow that for a block size of one, the diffusion objective suffers from high variance despite being\nequivalent to the autoregressive likelihood in expectation. We identify high training variance as a\nlimitation of diffusion models and propose data-driven noise schedules that reduce the variance of the\ngradient updates during training.\n3.1\nBLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES\nWe propose to combine the language modeling paradigms in Sec. 2 by autoregressively modeling\nblocks of tokens and performing diffusion within each block. We group tokens in x into B blocks of\n3\n"
    },
    {
      "page_number": 4,
      "text": "Published as a conference paper at ICLR 2025\nlength L′ with B = L/L′ (we assume that B is an integer). We denote each block x(b−1)L′:bL′ from\ntoken at positions (b −1)L′ to bL′ for blocks b ∈{1, . . . , B} as xb for simplicity. Our likelihood\nfactorizes over blocks as\nlog pθ(x) =\nB\nX\nb=1\nlog pθ(xb | x<b),\n(4)\nand each pθ(xb | x<b) is modeled using discrete diffusion over a block of L′ tokens. Specifically, we\ndefine a reverse diffusion process as in (2), but restricted to block b:\npθ(xb\ns | xb\nt, x<b) =\nX\nxb\nq(xb\ns | xb\nt, xb)pθ(xb | xb\nt, x<b)\n(5)\nWe obtain a principled learning objective by applying the NELBO in (3) to each term in (4) to obtain\n−log pθ(x) ≤LBD(x; θ) :=\nB\nX\nb=1\nL(xb, x<b; θ),\n(6)\nwhere each L(xb, x<b; θ) is an instance of (3) applied to log pθ(xb | x<b). Since the model is\nconditioned on x<b, we make the dependence on x<b, θ explicit in L. We denote the sum of these\nterms LBD(x; θ) (itself a valid NELBO).\nModel Architecture\nCrucially, we parameterize the B base denoiser models pθ(xb | xb\nt, x<b) using\na single neural network xθ. The neural network xθ outputs not only the probabilities pθ(xb | xb\nt, x<b),\nbut also computational artifacts for efficient training. This will enable us to compute the loss LBD(x; θ)\nin parallel for all B blocks in a memory-efficient manner. Specifically, we parameterize xθ using a\ntransformer (Vaswani et al., 2017) with a block-causal attention mask. The transformer xθ is applied\nto L tokens, and tokens in block b attend to tokens in blocks 1 to b. When xθ is trained, xb\nθ(xb\nt, x<b)\nyields L′ predictions for denoised tokens in block b based on noised xb\nt and clean x<b.\nIn autoregressive generation, it is normal to cache keys and values for previously generated tokens\nto avoid recomputing them at each step. Similarly, we use Kb, Vb to denote the keys and values at\nblock b, and we define xθ to support these as input and output. The full signature of xθ is\nxb\nlogits, Kb, Vb ←xb\nθ(xb\nt, K1:b−1, V1:b−1) := xb\nθ(xb\nt, x<b),\n(7)\nwhere xb\nlogits are the predictions for the clean xb, and Kb, Vb is the key-value cache in the forward\npass of xθ, and K1:b−1, V1:b−1 are keys and values cached on a forward pass of xθ over x<b (hence\nthe inputs x<b and K1:b−1, V1:b−1 are equivalent).\n3.2\nEFFICIENT TRAINING AND SAMPLING ALGORITHMS\nIdeally, we wish to compute the loss LBD(x; θ) in one forward pass of xθ. However, observe that\ndenoising xb\nt requires a forward pass on this noisy input, while denoising the next blocks requires\nrunning xθ on the clean version xb. Thus every block has to go through the model at least twice.\nTraining\nBased on this observation, we propose a training algorithm with these minimal computa-\ntional requirements (Alg. 1). Specifically, we precompute keys and values K1:B, V1:B for the full\nsequence x in a first forward pass (∅, K1:B, V1:B) ←xθ(x). We then compute denoised predictions\nfor all blocks using xb\nθ(xb\nt, K1:b-1, V1:b-1). Each token passes through xθ twice.\nVectorized Training\nNaively, we would compute the logits by applying xb\nθ(xb\nt, K1:b-1, V1:b-1) in a\nloop B times. We propose a vectorized implementation that computes LBD(x; θ) in one forward pass\non the concatenation xnoisy ⊕x of clean data x with noisy data xnoisy = x1\nt1 ⊕· · · ⊕xB\ntB obtained\nby applying a noise level tb to each block xb. We design an attention mask for xnoisy ⊕x such that\nnoisy tokens attend to other noisy tokens in their block and to all clean tokens in preceding blocks\n(see Suppl. B.6). Our method keeps the overhead of training BD3-LMs tractable and combines with\npretraining to further reduce costs.\n4\n"
    },
    {
      "page_number": 5,
      "text": "Published as a conference paper at ICLR 2025\nSampling\nWe sample one block at a time, conditioned on previously sampled blocks (Alg 2).\nWe may use any sampling procedure SAMPLE(xb\nθ, K1:b-1, V1:b-1) to sample from the conditional\ndistribution pθ(xb\ns|xb\nt, x<b), where the context conditioning is generated using cross-attention with\npre-computed keys and values K1:b−1, V1:b−1. Similar to AR models, caching the keys and values\nsaves computation instead of recalculating them when sampling a new block.\nNotably, our block diffusion decoding algorithm enables us to sample sequences of arbitrary length,\nwhereas diffusion models are restricted to fixed-length generation. Further, our sampler admits parallel\ngeneration within each block, whereas AR samplers are constrained to generate token-by-token.\nAlgorithm 1 Block Diffusion Training\nInput: datapoint x, # of blocks B, forward\nnoise process qt(·|x), model xθ, loss LBD\nrepeat\nSample t1, . . . , tB ∼U[0, 1]\n∀b ∈{1, ..., B} : xb\ntb ∼qtb(·|xb)\n∅, K1:B, V1:B ←xθ(x)\n▷KV cache\n∀b: xb\nlogit, ∅, ∅←xb\nθ(xb\ntb, K1:b-1, V1:b-1)\nLet xlogit ←x1\nlogit ⊕· · · ⊕xB\nlogit\nTake gradient step on ∇θLBD(xlogit; θ)\nuntil converged\nAlgorithm 2 Block Diffusion Sampling\nInput: # blocks B, model xθ, diffusion sam-\npling algorithm SAMPLE\nx, K, V ←∅\n▷output & KV cache\nfor b = 1 to B do\nxb ←SAMPLE(xb\nθ, K1:b-1, V1:b-1)\n∅, Kb, Vb ←xb\nθ(xb)\nx ←x1:b−1 ⊕xb\n(K, V) ←(K1:b−1 ⊕Kb, V1:b−1 ⊕Vb)\nend for\nreturn x\n4\nUNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION & AR MODELS\n4.1\nMASKED BD3-LMS\nThe most effective diffusion language models leverage a masking noise process (Austin et al., 2021;\nLou et al., 2024; Sahoo et al., 2024a), where tokens are gradually replaced with a special mask token.\nHere, we introduce masked BD3-LMs, a special class of block diffusion models based on the masked\ndiffusion language modeling framework (Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025).\nMore formally, we adopt a per-token noise process q(xℓ\nt|xℓ) = Cat(xℓ\nt; αtxℓ+ (1 −αt)m) for\ntokens ℓ∈{1, . . . , L} where m is a one-hot encoding of the mask token, and αt ∈[0, 1] is a\nstrictly decreasing function in t, with α0 = 1 and α1 = 0. We employ the linear schedule where the\nprobability of masking a token at time t is 1 −αt. We adopt the simplified objective from Sahoo et al.\n(2024a); Shi et al. (2024); Ou et al. (2025) (the full derivation is provided in Suppl. B.3):\n−log pθ(x) ≤LBD(x; θ) :=\nB\nX\nb=1\nEt∼[0,1]Eq\nα′\nt\n1 −αt\nlog pθ(xb|xb\nt, x<b)\n(8)\nwhere α′\nt is the instantaneous rate of change of αt under the continuous-time extension of (3) that\ntakes T →∞. The NELBO is tight for L′ = 1 but becomes a looser approximation of the true\nnegative log-likelihood for L′ →L (see Suppl. B.5).\n4.2\nCASE STUDY: SINGLE TOKEN GENERATION\nTable 1: Test perplexities for single-\ntoken generation (PPL; ↓) across\n16B tokens on LM1B.\nPPL (↓)\nAR\n22.88\n+ random batch size\n24.37\nBD3-LM L′ = 1\n≤25.56\n+ tuned schedule\n22.88\nOur block diffusion parameterization (8) is equivalent in expec-\ntation to the autoregressive NLL (1) in the limiting case where\nL′ = 1 (see Suppl. B.4). Surprisingly, we find a two point\nperplexity gap between our block diffusion model for L′ = 1\nand AR when training both models on the LM1B dataset.\nAlthough the objectives are equivalent in expectation, we show\nthat the remaining perplexity gap is a result of high training\nvariance. Whereas AR is trained using the cross-entropy of L\ntokens, our block diffusion model for L′ = 1 only computes\nthe cross-entropy for masked tokens xℓ\nt = m ∀ℓ∈{1, . . . L}\n5\n"
    },
    {
      "page_number": 6,
      "text": "Published as a conference paper at ICLR 2025\n50k\n100k\n150k\n200k\n250k\n3\n3.2\n3.4\n3.6\n3.8\n4\nModel\nBD3-LM (NELBO)\nBD3-LM (Tuned schedule)\nAR\nAR (random batch size)\nTrain Negative Log-Likelihood (NLL) for Single Token Generation on LM1B\nTrain steps\nNLL\nFigure 2: Train NLLs for modeling the per-token likelihood on LM1B. Models are trained on 16B\ntokens. Training under the discrete diffusion NELBO, where half of the tokens in a batch are masked\non average, has similar training variance to an AR model with a random batch size.\nso that Et∼U[0,1]q(xℓ\nt = m|xℓ) = 0.5. Thus, training on the diffusion objective involves estimating\nloss gradients with 2x fewer tokens and is responsible for higher training variance compared to AR.\nTo close the likelihood gap, we train a BD3-LM for L′ = 1 by designing the forward process to\nfully mask tokens, i.e. q(xℓ\nt = m|xℓ) = 1. Under this schedule, the diffusion objective becomes\nequivalent to the AR objective (Suppl. B.4). In Table 1, we show that training under the block\ndiffusion objective yields the same perplexity as AR training. Empirically, we see that this reduces the\nvariance of the training loss in Figure 2. We verify that tuning the noise schedule reduces the variance\nof the objective by measuring Varx,t [LBD(x; θ)] after training on 328M tokens: while training on the\nNELBO results in a variance of 1.52, training under full masking reduces the variance to 0.11.\n4.3\nDIFFUSION GAP FROM HIGH VARIANCE TRAINING\nNext, we formally describe the issue of gradient variance in training diffusion models. Given our\nempirical observations for single-token generation, we propose an estimator for gradient variance\nthat we use to minimize the variance of diffusion model training for L′ ≥1. While the NELBO is\ninvariant to the choice of noise schedule (Suppl. B.3), this invariance does not hold for our Monte\nCarlo estimator of the loss used during training. As a result, the variance of the estimator and its\ngradients are dependent on the schedule. First, we express the estimator of the NELBO with a batch\nsize K. We denote a batch of sequences as X =\n\u0002\nx(1), x(2), . . . , x(K)\u0003\n, with each x(k) iid∼q(x). We\nobtain the batch NELBO estimator below, where t(k, b) is sampled in sequence k and block b:\nLBD(X; θ) := l(X; θ) = 1\nK\nK\nX\nk=1\nB\nX\nb=1\nα′\nt(k,b)\n1 −αt(k,b)\nlog pθ\n\u0010\nx(k),b | x(k),b\nt(k,b), x(k),<b\u0011\n(9)\nThe variance of the gradient estimator over M batches for each batch Xm ∀m ∈{1, . . . , M} is:\nVarX,t [∇θl(X; θ)] ≈\n1\nM −1\nM\nX\nm=1\n\r\r\r\r\r∇θl(Xm; θ) −1\nM\nM\nX\nm=1\n∇θl(Xm; θ)\n\r\r\r\r\r\n2\n2\n(10)\n5\nLOW-VARIANCE NOISE SCHEDULES FOR BD3-LMS\n5.1\nINTUITION: AVOID EXTREME MASK RATES\nWe aim to identify schedules that minimize the variance of the gradient estimator and make training\nmost efficient. In a masked setting, we want to mask random numbers of tokens, so that the model\n6\n"
    },
    {
      "page_number": 7,
      "text": "Published as a conference paper at ICLR 2025\nlearns to undo varying levels of noise, which is important during sampling. However, if we mask\nvery few tokens, reconstructing them is easy and does not provide useful learning signal. If we mask\neverything, the optimal reconstruction are the marginals of each token in the data distribution, which\nis easy to learn, and again is not useful. These extreme masking rates lead to poor high-variance\ngradients: we want to learn how to clip them via a simple and effective new class of schedules.\n5.2\nCLIPPED SCHEDULES FOR LOW-VARIANCE GRADIENTS\nWe propose a class of “clipped” noise schedules that sample mask rates 1 −αt ∼U[β, ω] for\n0 ≤β, ω ≤1. We argue that from the perspective of deriving Monte Carlo gradient estimates, these\nschedules are equivalent to a continuous schedule where the mask probability is approximately 0\nbefore the specified range such that 1 −α<β ≈ϵ and approximately 1 after the specified range\n1 −α>ω ≈1 −ϵ. Consequently, α′\nt is linear within the range: α′\nt ≈1/(β −ω).\n5.3\nDATA-DRIVEN CLIPPED SCHEDULES ACROSS BLOCK SIZES\nAs the optimal mask rates may differ depending on the block size L′, we adaptively learn the schedule\nduring training. While Kingma et al. (2021) perform variance minimization by isolating a variance\nterm using their squared diffusion loss, this strategy is not directly applicable to our variance estimator\nin Equation 10 since we seek to reduce variance across random batches in addition to random tb.\nInstead, we optimize parameters β, ω to directly minimize training variance. To limit the computa-\ntional burden of the optimization, we use the variance of the estimator of the diffusion ELBO as a\nproxy for the gradient estimator to optimize β, ω: minβ,ω VarX,t [L(X; θ, β, ω)]. We perform a grid\nsearch at regular intervals during training to find the optimal β, ω (experimental details in Sec. 6).\nIn Table 2, we show that variance of the diffusion NELBO is correlated with test perplexity. Under a\nrange of “clipped” noise rate distributions, we find that there exists a unique distribution for each\nblock size L′ ∈{4, 16, 128} that minimizes both the variance of the NELBO and the test perplexity.\nTable 2: Perplexities (PPLs; ↓) and variances of the NELBO VarX,t [LBD(X; θ)] (Var. NELBO; ↓).\nModels are trained on LM1B using a linear schedule for 65B tokens, then finetuned for 10B tokens.\nU[0, .5]\nU[.3, .8]\nU[.5, 1]\nU[0, 1]\nL′\nPPL\nVar. NELBO\nPPL\nVar. NELBO\nPPL\nVar. NELBO\nPPL\nVar. NELBO\n128\n31.72\n1.03\n31.78\n1.35\n31.92\n1.83\n31.78\n3.80\n16\n31.27\n7.90\n31.19\n3.62\n31.29\n3.63\n31.33\n7.39\n4\n29.23\n32.68\n29.37\n10.39\n29.16\n8.28\n29.23\n23.65\n6\nEXPERIMENTS\nTable 3: Test perplexities (PPL; ↓) of mod-\nels trained for 65B tokens on LM1B. Best\ndiffusion value is bolded.\nPPL (↓)\nAutoregressive\nTransformer-X Base (Dai et al., 2019)\n23.5\nTransformer (Sahoo et al., 2024a)\n22.83\nDiffusion\nD3PM (absorb) (Austin et al., 2021)\n≤82.34\nSEDD (Lou et al., 2024)\n≤32.68\nMDLM (Sahoo et al., 2024a)\n≤31.78\nBlock diffusion (Ours)\nBD3-LMs L′ = 16\n≤30.60\nL′ = 8\n≤29.83\nL′ = 4\n≤28.23\nWe evaluate BD3-LMs across standard language\nmodeling benchmarks and demonstrate their ability\nto generate arbitrary-length sequences uncondition-\nally. We pre-train a base BD3-LM using the maxi-\nmum block size L′ = L for 850K gradient steps and\nfine-tune under varying L′ for 150K gradient steps\non the One Billion Words dataset (LM1B; Chelba\net al. (2014)) and OpenWebText (OWT; Gokaslan\net al. (2019)). Details on training and inference are\nprovided in Suppl C.\nTo reduce the variance of training on the diffusion\nNELBO, we adaptively learn the range of masking\nrates by optimizing parameters β, ω as described in\nSection 5.3. In practice, we do so using a grid search\nduring every validation epoch (after ∼5K gradient\n7\n"
    },
    {
      "page_number": 8,
      "text": "Published as a conference paper at ICLR 2025\nupdates) to identify β, ω: minβ,ω VarX,t [L(X; θ, β, ω)]. During evaluation, we report likelihood\nunder uniformly sampled mask rates (8) as in Austin et al. (2021); Sahoo et al. (2024a).\n6.1\nLIKELIHOOD EVALUATION\nTable 4: Test perplexities (PPL; ↓) on\nOWT for models trained for 524B to-\nkens. Best diffusion value is bolded.\nPPL (↓)\nAR (Sahoo et al., 2024a)\n17.54\nSEDD (Lou et al., 2024)\n≤24.10\nMDLM (Sahoo et al., 2024a)\n≤22.98\nBD3-LMs L′ = 16\n≤22.27\nL′ = 8\n≤21.68\nL′ = 4\n≤20.73\nOn LM1B, BD3-LMs outperform all prior diffusion meth-\nods in Table 3. Compared to MDLM (Sahoo et al., 2024a),\nBD3-LMs achieve up to 13% improvement in perplexity.\nWe observe a similar trend on OpenWebText in Table 4.\nWe also evaluate the ability of BD3-LMs to generalize\nto unseen datasets in a zero-shot setting, following the\nbenchmark from Radford et al. (2019). We evaluate the\nlikelihood of models trained with OWT on datasets Penn\nTree Bank (PTB; (Marcus et al., 1993)), Wikitext (Merity\net al., 2016), LM1B, Lambada (Paperno et al., 2016), AG\nNews (Zhang et al., 2015), and Scientific Papers (Pubmed\nand Arxiv subsets; (Cohan et al., 2018)). In Table 5, BD3-\nLM achieves the best zero-shot perplexity on Pubmed,\nsurpassing AR, and the best perplexity among diffusion models on Wikitext, LM1B, and AG News.\nTable 5: Zero-shot validation perplexities (↓) of models trained for 524B tokens on OWT. All\nperplexities for diffusion models are upper bounds.\nPTB\nWikitext\nLM1B\nLambada\nAG News\nPubmed\nArxiv\nAR\n81.07\n25.32\n51.14\n52.13\n52.11\n48.59\n41.22\nSEDD\n96.33\n35.98\n68.14\n48.93\n67.82\n45.39\n40.03\nMDLM\n90.96\n33.22\n64.94\n48.29\n62.78\n43.13\n37.89\nBD3-LM L′ = 4\n96.81\n31.31\n60.88\n50.03\n61.67\n42.52\n39.20\n6.2\nSAMPLE QUALITY AND VARIABLE-LENGTH SEQUENCE GENERATION\nTable 6: Generation length statistics\nfrom sampling 500 documents from\nmodels trained on OWT.\nMedian\nMax\n# tokens # tokens\nOWT train set\n717\n131K\nAR\n4008\n131K\nSEDD\n1021\n1024\nBD3-LM L′ = 16\n798\n9982\nOne key drawback of many existing diffusion language mod-\nels (e.g,. Austin et al. (2021); Lou et al. (2024)) is that they\ncannot generate full-length sequences that are longer than\nthe length of the output context chosen at training time. The\nOWT dataset is useful for examining this limitation, as it\ncontains many documents that are longer than the training\ncontext length of 1024 tokens.\nWe record generation length statistics of 500 variable-length\nsamples in Table 6. We continue sampling tokens until an\nend-of-sequence token [EOS] is generated or sample qual-\nity significantly degrades (as measured by sample entropy).\nBD3-LMs generate sequences up to ≈10× longer than those\nof SEDD (Lou et al., 2024), which is restricted to the training context size.\nWe also examine the sample quality of BD3-LMs through quantitative and qualitative analyses. In\nTable 7, we generate sequences of lengths L = 1024, 2048 and measure their generative perplexity\nunder GPT2-Large. To sample L = 2048 tokens from MDLM, we use their block-wise decoding\ntechnique (which does not feature block diffusion training as in BD3-LMs).\nWe also compare to SSD-LM (Han et al., 2022), an alternative block diffusion formulation. Unlike\nour discrete diffusion framework, SSD-LM uses Gaussian diffusion and does not support likelihood\nestimation. Further, BD3-LM adopts an efficient sampler from masked diffusion, where the number\nof generation steps (NFEs) is upper-bounded by L since tokens are never remasked (Sahoo et al.,\n2024a; Ou et al., 2025). For SSD-LM, we compare sample quality using T = 1K diffusion steps\nper block, matching their experimental setting (yielding ≥40K NFEs), and T = 25 where NFEs are\ncomparable across methods.\n8\n"
    },
    {
      "page_number": 9,
      "text": "Published as a conference paper at ICLR 2025\nTable 7: Generative perplexity (Gen. PPL; ↓) and number of function evaluations (NFEs; ↓) of 300\nsamples of lengths L = 1024, 2048. All models are trained on OWT. AR, SEDD, MDLM, BD3-LMs\nuse 110M parameters and are trained on 524B tokens, while SSD-LM uses 400M parameters and is\npre-trained on 122B tokens. Best diffusion value is bolded. We provide further details in Suppl. C.5.\nL = 1024\nL = 2048\nModel\nGen. PPL\nNFEs\nGen. PPL\nNFEs\nAR\n14.1\n1K\n13.2\n2K\nDiffusion\nSEDD\n52.0\n1K\n–\n–\nMDLM\n46.8\n1K\n41.3\n2K\nBlock Diffusion\nSSD-LM L′ = 25\n37.2\n40K\n35.3\n80K\n281.3\n1K\n281.9\n2K\nBD3-LMs L′ = 16\n33.4\n1K\n31.5\n2K\nL′ = 8\n30.4\n1K\n28.2\n2K\nL′ = 4\n25.7\n1K\n23.6\n2K\nBD3-LMs achieve the best generative perplexities compared to previous diffusion methods. Relative\nto SSD-LM, our discrete approach yields samples with improved generative perplexity using an order\nof magnitude fewer generation steps. We also qualitatively examine samples taken from BD3-LM\nand baselines (AR, MDLM) trained on the OWT dataset; we report samples in Suppl. D. We observe\nthat BD3-LM samples have higher coherence than MDLM samples and approach the quality of AR.\n6.3\nABLATIONS\nWe assess the impact of the design choices in our proposed block diffusion recipes, namely 1)\nselection of the noise schedule and 2) the efficiency improvement of the proposed training algorithm\nrelative to a naive implementation.\nSELECTING NOISE SCHEDULES TO REDUCE TRAINING VARIANCE\nCompared to the linear schedule used in Lou et al. (2024); Sahoo et al. (2024a), training under\n“clipped” noise schedules is the most effective for reducing the training variance which correlates with\ntest perplexity. In Table 8, the ideal “clipped” masking rates, which are optimized during training, are\nspecific to the block size and further motivate our optimization.\nTable 8: Effect of the noise schedule on like-\nlihood estimation. We finetune BD3-LMs\non 3B tokens from LM1B and evaluate on a\nlinear schedule. For clipped schedules, we\ncompare optimal clipping for L′ = 4, 16.\nNoise schedule\nPPL\nVar. NELBO\nL’ = 4\nClipped\nU[0.45, 0.95]\n29.21\n6.24\nU[0.3, 0.8]\n29.38\n10.33\nLinear U[0, 1]\n30.18\n23.45\nLogarithmic\n30.36\n23.53\nSquare root\n31.41\n26.43\nL’ = 16\nClipped\nU[0.45, 0.95]\n31.42\n3.60\nU[0.3, 0.8]\n31.12\n3.58\nLinear U[0, 1]\n31.72\n7.62\nSquare\n31.43\n13.03\nCosine\n31.41\n13.00\nRelative to other standard noise schedules (Chang et al.,\n2022), “clipped” masking achieves the best perfor-\nmance. As heavier masking is effective for the smaller\nblock size L′ = 4, we compare with logarithmic and\nsquare root schedules that also encourage heavy mask-\ning. As lighter masking is optimal for L′ = 16, we\ncompare with square and cosine schedules.\nEFFICIENCY OF TRAINING ALGORITHM\nIn the BD3-LM training algorithm (Sec. 3.2), we com-\npute xlogit using two options. We may perform two for-\nward passes through the network (precomputing keys\nand values for the full sequence x, then computing\ndenoised predictions), or combine these passes by con-\ncatenating the two inputs into the same attention kernel.\nWe find that a single forward pass is more efficient as\nwe reduce memory bandwidth bottlenecks by leverag-\ning efficient attention kernels (Dao et al., 2022; Dong\net al., 2024), see Suppl. B.7. Instead of paying the cost\n9\n"
    },
    {
      "page_number": 10,
      "text": "Published as a conference paper at ICLR 2025\nof two passes through the network, we only pay the cost of a more expensive attention operation. Our\nvectorized approach has 20-25% speed-up during training relative to performing two forward passes.\n7\nDISCUSSION AND PRIOR WORK\nComparison to D3PM\nBlock diffusion builds off D3PM (Austin et al., 2021) and applies it to each\nautoregressive conditional. We improve over D3PM in three ways: (1) we extend D3PM beyond fixed\nsequence lengths; (2) we study the perplexity gap of D3PM and AR models, identify gradient variance\nas a contributor, and design variance-minimizing schedules; (3) we improve over the perplexity of\nD3PM models. Our work applies to extensions of D3PM (He et al., 2022; Lou et al., 2024) including\nones in continuous time (Campbell et al., 2022; Sun et al., 2022).\nComparison to MDLM\nBD3-LMs further make use of the perplexity-enhancing improvements\nin MDLM (Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025). We also build upon MDLM: (1)\nwhile Sahoo et al. (2024a) point out that their NELBO is invariant to the noise schedule, we show\nthat the noise schedule has a significant effect on gradient variance; (2) we push the state-of-the-art in\nperplexity beyond MDLM. Note that our perplexity improvements stem not only from block diffusion,\nbut also from optimized schedules, and could enhance standard MDLM and D3PM models.\nComparison to Gaussian Diffusion\nAlternatively, one may perform diffusion over continuous\nembeddings of discrete tokens (Li et al., 2022; Dieleman et al., 2022; Chen et al., 2022). This allows\nusing algorithms for continuous data (Song et al., 2020; Ho & Salimans, 2022), but yields worse\nperplexity (Graves et al., 2023; Gulrajani & Hashimoto, 2024).\nComparison to Semi-Autoregressive Diffusion\nHan et al. (2022; 2023) introduced a block formu-\nlation of Gaussian diffusion. BD3-LMs instead extend Austin et al. (2021), and feature: (1) tractable\nlikelihood estimates for principled evaluation; (2) faster generation, as our number of model calls is\nbounded by the number of generated tokens, while SSD-LM performs orders of magnitude more calls;\n(3) improved sample quality. AR-Diffusion (Wu et al., 2023) extends SSD-LM with a left-to-right\nnoise schedule; Chen et al. (2025); Ye et al. (2024) apply to decision traces and videos; Hao et al.\n(2024); Kong et al. (2025) extend to latent reasoning. PARD (Zhao et al., 2024) applies discrete block\ndiffusion to graphs. In contrast, we (1) interpolate between AR/diffusion performance; (2) support\nKV caching; (3) perform attention within noised blocks, whereas PARD injects new empty blocks.\nAutoregressive diffusion models (Hoogeboom et al., 2021b;a) extend any-order AR models (AO-\nARMs; Uria et al. (2014)) to support parallel sampling. Zheng et al. (2024) prove equivalence\nbetween MDLM and AO-ARM training. Further extensions of ARMs that compete with diffusion\ninclude iterative editing (Gu et al., 2019), parallel and speculative decoding (Gu et al., 2017; Santilli\net al., 2023; Cai et al., 2024; Gloeckle et al., 2024), consistency training (Kou et al., 2024), guidance\n(Sanchez et al., 2023), and cross-modal extensions (Liu et al., 2023; Tian et al., 2025).\nLimitations\nTraining BD3-LMs is more expensive than regular diffusion training. We propose a\nvectorized algorithm that keeps training speed within <2x of diffusion training speed; in our experi-\nments, we also pre-train with a standard diffusion loss to further reduce the speed gap. Additionally,\nBD3-LMs generate blocks sequentially, and hence may face the same speed and controllability con-\nstraints as AR especially when blocks are small. Their optimal block size is task specific (e.g., larger\nfor greater control). BD3-LMs are subject to inherent limitations of generative models, including\nhallucinations (Achiam et al., 2023), copyright infringement (Gokaslan et al., 2024), controllability\n(Schiff et al., 2024; Wang et al., 2023) and harmful outputs (Bai et al., 2022).\n8\nCONCLUSION\nThis work explores block diffusion and is motivated by two problems with existing discrete diffusion:\nthe need to generate arbitrary-length sequences and the perplexity gap to autoregressive models. We\nintroduce BD3-LMs, which represent a block-wise extension of the D3PM framework (Austin et al.,\n2021), and leverage a specialized training algorithm and custom noise schedules that further improve\nperformance. We observe that in addition to being able to generate long-form documents, these\nmodels also improve perplexity, setting a new state-of-the-art among discrete diffusion models.\n10\n"
    },
    {
      "page_number": 11,
      "text": "Published as a conference paper at ICLR 2025\nACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING\nThis work was partially funded by the National Science Foundation under awards DGE-1922551,\nCAREER awards 2046760 and 2145577, and by the National Institute of Health under award MIRA\nR35GM151243. Marianne Arriola is supported by a NSF Graduate Research Fellowship under award\nDGE-2139899 and a Hopper-Dean/Bowers CIS Deans Excellence Fellowship. We thank Databricks\nMosaicML for providing access to computational resources.\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774, 2023.\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured\ndenoising diffusion models in discrete state-spaces. Advances in Neural Information Processing\nSystems, 34:17981–17993, 2021.\nPavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score\nmodel for biological sequence generation. In International Conference on Machine Learning, pp.\n1276–1301. PMLR, 2023.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. arXiv preprint arXiv:2212.08073, 2022.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao.\nMedusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv\npreprint arXiv:2401.10774, 2024.\nAndrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and\nArnaud Doucet. A continuous time framework for discrete denoising models. Advances in Neural\nInformation Processing Systems, 35:28266–28279, 2022.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative\nimage transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 11315–11325, 2022.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling,\n2014.\nBoyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann.\nDiffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural\nInformation Processing Systems, 37:24081–24125, 2025.\nTing Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using\ndiffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang,\nand Nazli Goharian. A discourse-aware attention model for abstractive summarization of long\ndocuments. Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018.\ndoi: 10.18653/v1/n18-2097. URL http://dx.doi.org/10.18653/v1/n18-2097.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems,\n35:16344–16359, 2022.\n11\n"
    },
    {
      "page_number": 12,
      "text": "Published as a conference paper at ICLR 2025\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. URL\nhttps://arxiv.org/abs/2105.05233.\nSander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H\nRichemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion\nfor categorical data. arXiv preprint arXiv:2211.15089, 2022.\nJuechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: A\nprogramming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496,\n2024.\nFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve.\nBetter & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737,\n2024.\nShrey Goel, Vishrut Thoutam, Edgar Mariano Marroquin, Aaron Gokaslan, Arash Firouzbakht,\nSophia Vincoff, Volodymyr Kuleshov, Huong T Kratochvil, and Pranam Chatterjee. Memdlm: De\nnovo membrane protein design with masked discrete diffusion protein language models. arXiv\npreprint arXiv:2410.16735, 2024.\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http:\n//Skylion007.github.io/OpenWebTextCorpus, 2019.\nAaron Gokaslan, A Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel,\nJonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: Open diffusion\nmodels trained on creative-commons images. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 8250–8260, 2024.\nAlex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian flow\nnetworks. arXiv preprint arXiv:2308.07037, 2023.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive\nneural machine translation. arXiv preprint arXiv:1711.02281, 2017.\nJiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in neural informa-\ntion processing systems, 32, 2019.\nIshaan Gulrajani and Tatsunori B Hashimoto. Likelihood-based diffusion language models. Advances\nin Neural Information Processing Systems, 36, 2024.\nAgrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang,\nand Jose Lezama. Photorealistic video generation with diffusion models, 2023. URL https:\n//arxiv.org/abs/2312.06662.\nXiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based\ndiffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432,\n2022.\nXiaochuang Han, Sachin Kumar, Yulia Tsvetkov, and Marjan Ghazvininejad. David helps goliath:\nInference-time collaboration between small specialized and large general diffusion lms. arXiv\npreprint arXiv:2305.14771, 2023.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong\nTian. Training large language models to reason in a continuous latent space. arXiv preprint\narXiv:2412.06769, 2024.\nZhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu.\nDiffusion-\nbert: Improving generative masked language models with diffusion models. arXiv preprint\narXiv:2211.15029, 2022.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\n12\n"
    },
    {
      "page_number": 13,
      "text": "Published as a conference paper at ICLR 2025\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840–6851, 2020.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. arXiv:2204.03458, 2022.\nEmiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and\nTim Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021a.\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows\nand multinomial diffusion: Learning categorical distributions. Advances in Neural Information\nProcessing Systems, 34:12454–12465, 2021b.\nDaniel Israel, Aditya Grover, and Guy Van den Broeck. Enabling autoregressive models to fill in\nmasked tokens. arXiv preprint arXiv:2502.06901, 2025.\nDiederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances\nin neural information processing systems, 34:21696–21707, 2021.\nDeqian Kong, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si,\nChuan Li, Jianwen Xie, Sirui Xie, et al. Scalable language models with posterior inference of\nlatent thought vectors. arXiv preprint arXiv:2502.01567, 2025.\nSiqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. CLLMs: Consistency large\nlanguage models. In Forty-first International Conference on Machine Learning, 2024. URL\nhttps://openreview.net/forum?id=8uzBOVmh8H.\nXiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm\nimproves controllable text generation. Advances in Neural Information Processing Systems, 35:\n4328–4343, 2022.\nXiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso\nBiancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, et al. Derivative-free guidance in continuous\nand discrete diffusion models with soft value-based decoding. arXiv preprint arXiv:2408.08252,\n2024.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\nneural information processing systems, 36:34892–34916, 2023.\nAaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios\nof the data distribution. In Forty-first International Conference on Machine Learning, 2024. URL\nhttps://openreview.net/forum?id=CNicRIVIPA.\nMitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus\nof english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\nHunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance\nfor discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572, 2024.\nJingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan\nLi.\nYour absorbing discrete diffusion secretly models the conditional distributions of clean\ndata. In The Thirteenth International Conference on Learning Representations, 2025. URL\nhttps://openreview.net/forum?id=sMyXP8Tanm.\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset:\nWord prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525–1534,\nBerlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.\naclweb.org/anthology/P16-1144.\n13\n"
    },
    {
      "page_number": 14,
      "text": "Published as a conference paper at ICLR 2025\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pp. 4195–4205, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nSubham Sekhar Sahoo, Marianne Arriola, Aaron Gokaslan, Edgar Mariano Marroquin, Alexander M\nRush, Yair Schiff, Justin T Chiu, and Volodymyr Kuleshov. Simple and effective masked diffusion\nlanguage models. In The Thirty-eighth Annual Conference on Neural Information Processing\nSystems, 2024a. URL https://openreview.net/forum?id=L4uaAR4ArM.\nSubham Sekhar Sahoo, Aaron Gokaslan, Christopher De Sa, and Volodymyr Kuleshov. Diffusion\nmodels with learned adaptive noise. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024b. URL https://openreview.net/forum?id=loMa99A4p8.\nSubham Sekhar Sahoo, John Xavier Morris, Aaron Gokaslan, Srijeeta Biswas, Vitaly Shmatikov,\nand Volodymyr Kuleshov. Zero-order diffusion guidance for inverse problems, 2024c. URL\nhttps://openreview.net/forum?id=JBgBrnhLLL.\nGuillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi,\nand Stella Biderman. Stay on topic with classifier-free guidance. arXiv preprint arXiv:2306.17806,\n2023.\nAndrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo\nMarin, and Emanuele Rodola. Accelerating transformer inference for translation via parallel\ndecoding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 12336–12355, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.acl-long.689. URL https://aclanthology.org/2023.acl-long.\n689.\nYair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre,\nBernardo P de Almeida, Alexander Rush, Thomas Pierrot, and Volodymyr Kuleshov. Simple\nguidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024.\nJiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized\nmasked diffusion for discrete data. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. URL https://openreview.net/forum?id=xcqSOfHt4g.\nPhillip Si, Allan Bishop, and Volodymyr Kuleshov. Autoregressive quantile flows for predictive\nuncertainty estimation. In International Conference on Learning Representations, 2022.\nPhillip Si, Zeyi Chen, Subham Sekhar Sahoo, Yair Schiff, and Volodymyr Kuleshov.\nSemi-\nautoregressive energy flows: exploring likelihood-free training of normalizing flows. In In-\nternational Conference on Machine Learning, pp. 31732–31753. PMLR, 2023.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learning,\npp. 2256–2265. PMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems, 32, 2019.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\nHaoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time\ndiscrete diffusion models. arXiv preprint arXiv:2211.16750, 2022.\n14\n"
    },
    {
      "page_number": 15,
      "text": "Published as a conference paper at ICLR 2025\nKeyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling:\nScalable image generation via next-scale prediction. Advances in neural information processing\nsystems, 37:84839–84865, 2025.\nBenigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In\nInternational Conference on Machine Learning, pp. 467–475. PMLR, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nYingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and\nVolodymyr Kuleshov. InfoDiffusion: Representation learning using information maximizing\ndiffusion models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan\nSabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine\nLearning, volume 202 of Proceedings of Machine Learning Research, pp. 36336–36354. PMLR,\n23–29 Jul 2023. URL https://proceedings.mlr.press/v202/wang23ah.html.\nTong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, yelong shen, Jian Jiao, Juntao Li,\nzhongyu wei, Jian Guo, Nan Duan, and Weizhu Chen. AR-diffusion: Auto-regressive diffusion\nmodel for text generation. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?id=0EG6qUQ4xE.\nJiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang,\nZhenguo Li, Wei Bi, et al. Diffusion of thoughts: Chain-of-thought reasoning in diffusion language\nmodels. arXiv preprint arXiv:2402.07754, 2024.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text\nclassification. In NIPS, 2015.\nLingxiao Zhao, Xueying Ding, and Leman Akoglu. Pard: Permutation-invariant autoregressive\ndiffusion for graph generation. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. URL https://openreview.net/forum?id=x4Kk4FxLs3.\nKaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked\ndiffusion models are secretly time-agnostic masked models and exploit inaccurate categorical\nsampling. arXiv preprint arXiv:2409.02908, 2024.\n15\n"
    },
    {
      "page_number": 16,
      "text": "Published as a conference paper at ICLR 2025\nCONTENTS\n1\nIntroduction\n1\n2\nBackground: Language Modeling Paradigms\n2\n2.1\nAutoregressive Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nDiscrete Denoising Diffusion Probabilistic Models\n. . . . . . . . . . . . . . . . .\n3\n3\nBlock Diffusion Language Modeling\n3\n3.1\nBlock Diffusion Distributions and Model Architectures . . . . . . . . . . . . . . .\n3\n3.2\nEfficient Training and Sampling Algorithms . . . . . . . . . . . . . . . . . . . . .\n4\n4\nUnderstanding Likelihood Gaps Between Diffusion & AR Models\n5\n4.1\nMasked BD3-LMs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n4.2\nCase Study: Single Token Generation\n. . . . . . . . . . . . . . . . . . . . . . . .\n5\n4.3\nDiffusion Gap from High Variance Training . . . . . . . . . . . . . . . . . . . . .\n6\n5\nLow-Variance Noise Schedules for BD3-LMs\n6\n5.1\nIntuition: Avoid Extreme Mask Rates\n. . . . . . . . . . . . . . . . . . . . . . . .\n6\n5.2\nClipped Schedules for Low-Variance Gradients . . . . . . . . . . . . . . . . . . .\n7\n5.3\nData-Driven Clipped Schedules Across Block Sizes . . . . . . . . . . . . . . . . .\n7\n6\nExperiments\n7\n6.1\nLikelihood Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n6.2\nSample Quality and Variable-Length Sequence Generation . . . . . . . . . . . . .\n8\n6.3\nAblations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n7\nDiscussion and Prior Work\n10\n8\nConclusion\n10\nA Block Diffusion NELBO\n17\nB\nMasked BD3-LMs\n17\nB.1\nForward Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.2\nReverse Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.3\nSimplified NELBO for Masked Diffusion Processes . . . . . . . . . . . . . . . . .\n18\nB.4\nRecovering the NLL from the NELBO for Single Token Generation\n. . . . . . . .\n19\nB.5\nTightness of the NELBO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.6\nSpecialized Attention Masks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.7\nOptimized Attention Kernel with FlexAttention . . . . . . . . . . . . . . . . . . .\n21\nC Experimental Details\n24\n16\n"
    },
    {
      "page_number": 17,
      "text": "Published as a conference paper at ICLR 2025\nC.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.2\nArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.3\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.4\nLikelihood Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.5\nInference\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nD Samples\n26\nA\nBLOCK DIFFUSION NELBO\nBelow, we provide the Negative ELBO (NELBO) for the block diffusion parameterization. Recall that\nthe sequence x1:L =\n\u0002\nx1, . . . , xL\u0003\nis factorized over B blocks, which we refer to as x for simplicity,\ndrawn from the data distribution q(x). Specifically, we will factorize the likelihood over B blocks of\nlength L′, then perform diffusion in each block over T discretization steps. Let DKL[·] to denote the\nKullback-Leibler divergence, t, s be shorthand for t(i) = i/T and s(i) = (i −1)/T ∀i ∈[1, T]. We\nderive the NELBO as follows:\n−log pθ(x) = −\nB\nX\nb=1\nlog pθ(xb|x<b)\n= −\nB\nX\nb=1\nlog Eq\npθ(xb\nt(1):t(T )|x<b)\nq(xb\nt(1):t(T )|xb)\n= −\nB\nX\nb=1\nlog Eq\npθ(xb\nt(T )|x<b) QT\ni=1 pθ(xb\ns(i)|xb\nt(i), x<b)\nQT\ni=1 q(xb\nt(i)|xb\ns(i))\n≤\nB\nX\nb=1\n\u0014\n−Eq log pθ(xb|xb\nt= 1\nT , x<b)\n|\n{z\n}\nLrecons\n+ Et∈{ 2\nT ,..., T −1\nT\n,1}EqTDKL\n\u0000q(xb\ns|xb\nt, xb) ∥pθ(xb\ns|xb\nt, x<b)\n\u0001\n|\n{z\n}\nLdiffusion\n+ DKL\n\u0000q(xb\nt=1|xb) ∥pθ(xb\nt=1)\n\u0001\n|\n{z\n}\nLprior\n\u0015\n(11)\nB\nMASKED BD3-LMS\nWe explore a specific class of block diffusion models that builds upon the masked diffusion language\nmodeling framework. In particular, we focus on masking diffusion processes introduced by Austin\net al. (2021) and derive a simplified NELBO under this framework as proposed by Sahoo et al.\n(2024a); Shi et al. (2024); Ou et al. (2025).\nFirst, we define the diffusion matrix Qt for states i ∈{1, . . . , V }. Consider the noise schedule\nfunction αt ∈[0, 1], which is a strictly decreasing function in t satisfying α0 = 1 and α1 = 0.\nDenote the mask index as m = V . The diffusion matrix is defined by Austin et al. (2021) as:\n[Qt]ij =\n\n\n\n1\nif i = j = m\nαt\nif i = j ̸= m\n1 −αt\nif j = m, i ̸= m\n(12)\n17\n"
    },
    {
      "page_number": 18,
      "text": "Published as a conference paper at ICLR 2025\nThe diffusion matrix for the forward marginal Qt|s is:\n[Qt|s]ij =\n\n\n\n1\nif i = j = m\nαt|s\nif i = j ̸= m\n1 −αt|s\nif j = m, i ̸= m\n(13)\nwhere αt|s = αt/αs.\nB.1\nFORWARD PROCESS\nUnder the D3PM framework (Austin et al., 2021), the forward noise process applied independently\nfor each token ℓ∈{1, . . . L} is defined using diffusion matrices Qt ∈RV ×V as\nq(xℓ\nt|xℓ) = Cat\n\u0000xℓ\nt; Qtxℓ\u0001\n,\nwith\nQt(i) = Qt(1)Qt(2) . . . Qt(i)\n(14)\nB.2\nREVERSE PROCESS\nLet Qt|s denote the diffusion matrix for the forward marginal. We obtain the reverse posterior\nq(xℓ\ns | xℓ\nt, xℓ) using the diffusion matrices:\nq(xℓ\ns|xℓ\nt, xℓ) = q(xℓ\nt|xℓ\ns, xℓ)q(xℓ\ns|xℓ)\nq(xℓ\nt|xℓ)\n= Cat\n \nxℓ\ns; Qt|sxℓ\nt ⊙Q⊤\ns xℓ\n(xℓ\nt)\n⊤Q⊤\nt xℓ\n!\n(15)\nwhere ⊙denotes the Hadmard product between two vectors.\nB.3\nSIMPLIFIED NELBO FOR MASKED DIFFUSION PROCESSES\nFollowing Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025), we simplify the NELBO in the\ncase of masked diffusion processes. Below, we provide the outline of the NELBO derivation; see the\nfull derivation in Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025).\nWe will first focus on simplifying the diffusion loss term Ldiffusion in Eq. 11. We employ the SUBS-\nparameterization proposed in Sahoo et al. (2024b) which simplifies the denoising model pθ for masked\ndiffusion. In particular, we enforce the following constraints on the design of pθ by leveraging the\nfact that there only exists two possible states in the diffusion process xℓ\nt ∈{xℓ, m} ∀ℓ∈{1, . . . , L}.\n1. Zero Masking Probabilities. We set pθ(xℓ= m|xℓ\nt) = 0 (as the clean sequence x doesn’t\ncontain masks).\n2. Carry-Over Unmasking. The true posterior for the case where xℓ\nt ̸= m is q(xℓ\ns = xℓ\nt|xℓ\nt ̸=\nm) = 1 (if a token is unmasked in the reverse process, it is never remasked). Thus, we\nsimplify the denoising model by setting pθ(xℓ\ns = xℓ\nt|xℓ\nt ̸= m) = 1.\nAs a result, we will only approximate the posterior pθ(xℓ\ns = xℓ|xℓ\nt = m). Let xb,ℓdenote a token in\nthe ℓ-th position in block b ∈{1, . . . , B}. The diffusion loss term becomes:\nLdiffusion =\nB\nX\nb=1\nEtEqT\n\u0002\nDKL\n\u0002\nq(xb\ns|xb\nt, xb)∥pθ(xb\ns|xb\nt, x<b)\n\u0003\u0003\n=\nB\nX\nb=1\nEtEqT\n\n\nL′\nX\nℓ=1\nDKL\nh\nq(xb,ℓ\ns |xb,ℓ\nt , xb,ℓ)∥pθ(xb,ℓ\ns |xb\nt, x<b)\ni\n\n\nDKL is simply the discrete-time diffusion loss for the block b; hence, from Sahoo et al. (2024a) (Suppl. B.1), we get:\n=\nB\nX\nb=1\nEtEqT\n\n\nL′\nX\nℓ=1\nαt −αs\n1 −αt\nlog pθ(xb,ℓ| xb,ℓ\nt , x<b)\n\n\n=\nB\nX\nb=1\nEtEqT\n\u0014αt −αs\n1 −αt\nlog pθ(xb | xb\nt, x<b)\n\u0015\n(16)\n18\n"
    },
    {
      "page_number": 19,
      "text": "Published as a conference paper at ICLR 2025\nLastly, we obtain a tighter approximation of the likelihood by taking the diffusion steps T →∞\n(Sahoo et al., 2024a), for which T(αt −αs) = α′\nt:\nLdiffusion =\nB\nX\nb=1\nEt∼[0,1]Eq\n\u0014\nα′\nt\n1 −αt\nlog pθ(xb | xb\nt, x<b)\n\u0015\n(17)\nFor the continuous time case, Sahoo et al. (2024a) (Suppl. A.2.4) show the reconstruction loss reduces\nto 0 as xb\nt(1) ∼limT →∞Cat\n\u0010\n.; xb\nt= 1\nT\n\u0011\n= Cat(.; xb). Using this, we obtain:\nLrecons = −Eq log pθ(xb|xb\nt(1), x<b)\n= −log pθ(xb|xb\nt(1) = xb, x<b)\n= 0\n(18)\nThe prior loss Lprior = DKL\n\u0000q(xb\nt=1|xb) ∥pθ(xb\nt=1)\n\u0001\nalso reduces to 0 because αt=1 = 0 which\nensures q(xb\nt=1|xb) = Cat(.; m) and pθ(xb\nt=1) = Cat(.; m); see Sahoo et al. (2024a) (Suppl. A.2.4).\nFinally, we obtain a simple objective that is a weighted average of cross-entropy terms:\nLBD(x; θ) =\nB\nX\nb=1\nEt∼[0,1]Eq\n\u0014\nα′\nt\n1 −αt\nlog pθ(xb | xb\nt, x<b)\n\u0015\n(19)\nThe above NELBO is invariant to the choice of noise schedule αt; see Sahoo et al. (2024a) (Suppl.\nE.1.1).\nB.4\nRECOVERING THE NLL FROM THE NELBO FOR SINGLE TOKEN GENERATION\nConsider the block diffuson NELBO for a block size of 1 where L′ = 1, B = L. The block diffusion\nNELBO is equivalent to the AR NLL when modeling a single token:\n−log p(x) ≤\nL\nX\nb=1\nEt∼[0,1]Eq\n\u0014\nα′\nt\n1 −αt\nlog pθ(xb | xb\nt, x<b)\n\u0015\n∵α′\nt = −1 and αt = 1 −t,\n= −\nL\nX\nb=1\nEt∼[0,1]Eq\n\u00141\nt log pθ(xb | xb\nt, x<b)\n\u0015\n= −\nL\nX\nb=1\nEt∼[0,1]\n1\nt Eq\n\u0002\nlog pθ(xb | xb\nt, x<b)\n\u0003\nExpanding Eq[.],\n= −\nL\nX\nb=1\nEt∼[0,1]\n1\nt\n\u0014\nq(xb\nt = m|xb) log pθ(xb | xb\nt = m, x<b)\n+ q(xb\nt = xb|xb) log pθ(xb | xb\nt = xb, x<b)\n\u0015\n(20)\nRecall that our denoising model employs the SUBS-parameterization proposed in Sahoo et al. (2024b).\nThe “carry-over unmasking” property ensures that log pθ(xb | xb\nt = xb, x<b) = 0, as an unmasked\ntoken is simply copied over from from the input of the denoising model to the output. Hence, (20)\nreduces to following:\n−log pθ(x) ≤−\nL\nX\nb=1\nEt∼[0,1]\n1\nt q(xb\nt = m|xb) log pθ(xb | xb\nt = m, x<b)\n∵q(xb\nt = m|xb) = t, we get:\n= −\nL\nX\nb=1\nEt∼[0,1] log pθ(xb | xb\nt = m, x<b)\n19\n"
    },
    {
      "page_number": 20,
      "text": "Published as a conference paper at ICLR 2025\n= −\nL\nX\nb=1\nlog pθ(xb | m, x<b)\n(21)\nFor single-token generation (L′ = 1) we recover the autoregressive NLL.\nB.5\nTIGHTNESS OF THE NELBO\nFor block sizes 1 ≤K ≤L, we show that -log p(x) ≤LK ≤LK+1. Consider K = 1, where we\nrecover the autoregressive NLL (see Suppl B.4):\nL1 =\nL\nX\nb=1\nlog Et∼[0,1]Eq\nα′\nt\n1 −αt\npθ(xb | xb\nt, x<b)\n= −\nL\nX\nb=1\nlog pθ(xb | m, x<b)\n(22)\nConsider the ELBO for block size K = 2:\nL2 =\nL/2\nX\nb=1\nlog Et∼[0,1]Eq\nα′\nt\n1 −αt\npθ(xb | xb\nt, x<b)\n(23)\nWe show that L1 ≤L2, and this holds for all 1 ≤K ≤L by induction. Let xb,ℓcorrespond to the\ntoken in position ℓ∈[1, L′] of block b. We derive the below inequality:\n−\nL\nX\nb=1\nlog pθ(xb | m, x<b) = −\nL/2\nX\nb=1\nlog Et∼[0,1]Eq\n1\n1 −αt\npθ(xb | xb\nt, x<b)\n= −\nL/2\nX\nb=1\nlog Et∼[0,1]Eq\n2\nY\ni=1\n1\n1 −αt\npθ(xb,ℓ| xb\nt, x<b)\n= −\nL/2\nX\nb=1\nlog\n2\nY\ni=1\nEt∼[0,1]Eq\n1\n1 −αt\npθ(xb,ℓ| xb\nt, x<b)\n≤−\nL/2\nX\nb=1\n2\nX\ni=1\nlog Et∼[0,1]Eq\n1\n1 −αt\npθ(xb,ℓ| xb\nt, x<b)\n(24)\nB.6\nSPECIALIZED ATTENTION MASKS\nWe aim to model conditional probabilities pθ(xb | xb\nt, x<b) for all blocks b ∈[1, B] simultaneously\nby designing an efficient training algorithm with our transformer backbone. However, modeling all\nB conditonal terms requires processing both the noised sequence xb\nt and the conditional context x<b\nfor all b.\nRather than calling the denoising network B times, we process both sequences simultaneously by\nconcatenating them xfull ←xt ⊕x as input to a transformer. We update this sequence xfull of length\n2L tokens using a custom attention mask Mfull ∈{0, 1}2L×2L for efficient training.\nThe full attention mask is comprised of four L × L smaller attention masks:\nMfull =\n\u0014\nMBD\nMOBC\n0\nMBC\n\u0015\nwhere MBD and MOBC are used to update the representation of xt and MBC is used to update the\nrepresentation of x. We define these masks as follows:\n• MBD (Block-diagonal mask): Self-attention mask within noised blocks xb\nt\n[MBD]ij =\n\u001a 1\nif i, j are in the same block\n0\notherwise\n20\n"
    },
    {
      "page_number": 21,
      "text": "Published as a conference paper at ICLR 2025\n• MOBC (Offset block-causal mask): Cross-attention to conditional context x<b\n[MOBC]ij =\n\u001a 1\nif j belongs in a block before i\n0\notherwise\n• MBC (Block-causal mask): Attention mask for updating xb\n[MBC]ij =\n\u001a 1\nif j belongs in the same block as i, or a block before i\n0\notherwise\nWe visualize an example attention mask for L = 6 and block size L′ = 2 in Figure 3.\nx1\nt\nx2\nt\nx3\nt\nx1\nx2\nx3\nx1\nt\nx2\nt\nx3\nt\nx1\nx2\nx3\nBlock Diagonal (MBD)\nOffset Block Causal (MOBC)\nBlock Causal (MBC)\nFigure 3: Example of Specialized Attention Mask\nB.7\nOPTIMIZED ATTENTION KERNEL WITH FLEXATTENTION\nAs Figure 3 demonstrates, our attention matrix is extremely sparse. We can exploit this sparsity to\nmassively improve the efficiency of BD3-LMs.\nFlexAttention (Dong et al., 2024) is a compiler-driven programming model that enables efficient\nimplementation of attention mechanisms with structured sparsity in PyTorch. It provides a flexible\ninterface for defining custom attention masks while maintaining high performance comparable to\nmanually optimized attention kernels.\nBelow in Fig. 4 we define a block-wise attention mask, block_diff_mask, based on its definition\nas Mfull ∈{0, 1}2L×2L in Suppl. B.6. We fuse the attention operations into a single FlexAttention\nkernel designed to exploit the sparsity in our attention matrix to increase computational efficiency.\nBy doing so, we perform the following optimizations:\n• Precomputed Block Masking: The create_block_mask utility generates a sparse\nattention mask at compile-time, avoiding per-step computation of invalid attention entries.\nThrough sparsity-aware execution, FlexAttention kernels reduce the number of FLOPs in\nthe attention computation.\n• Reduced Memory Footprint: By leveraging block-level sparsity, the attention mechanism\navoids full materialization of large-scale attention matrices, significantly reducing memory\noverhead. FlexAttention minimizes memory accesses by skipping fully masked blocks.\n• Optimized Computation via torch.compile: The integration of torch.compile\nenables kernel fusion and efficient execution on GPUs by generating optimized Triton-based\nkernels. This efficiently parallelizes masked attention computations using optimized GPU\nexecution paths.\n21\n"
    },
    {
      "page_number": 22,
      "text": "Published as a conference paper at ICLR 2025\ndef block_diff_mask(b, h, q_idx, kv_idx, block_size, n):\n\"\"\"\nConstructs the specialized block diffusion attention mask composed of\nthree masks:\n- **Block Diagonal Mask (M_BD)**: Self-attention within noised blocks\n- **Offset Block Causal Mask (M_OBC)**: Cross-attention for\nconditional context\n- **Block Causal Mask (M_BC)**: Attention to update x0\nArgs:\nb, h: Batch and head indices (ignored for mask logic).\nq_idx, kv_idx: Query and Key indices.\nblock_size: Defines the block structure.\nn: Sequence length of x_0 and x_t\nReturns:\nA boolean attention mask.\n\"\"\"\n# Indicate whether token belongs to xt (0) or x0 (1)\nx0_flag_q = (q_idx >= n)\nx0_flag_kv = (kv_idx >= n)\n# Compute block indices\nblock_q = torch.where(x0_flag_q == 1,\n(q_idx - n) // block_size,\nq_idx // block_size)\nblock_kv = torch.where(x0_flag_kv == 1,\n(kv_idx - n) // block_size,\nkv_idx // block_size)\n# **1. Block Diagonal Mask (M_BD) **\nblock_diagonal = (block_q == block_kv) & (x0_flag_q == x0_flag_kv)\n# **2. Offset Block-Causal Mask (M_OBC) **\noffset_block_causal = (\n(block_q > block_kv)\n& (x0_flag_q == 0)\n& (x0_flag_kv == 1)\n)\n# **3. Block-Causal Mask (M_BC) **\nblock_causal = (\n(block_q >= block_kv)\n& (x0_flag_q == 1)\n& (x0_flag_kv == 1)\n)\n# **4. Combine Masks **\nreturn block_diagonal | offset_block_causal | block_causal\nFigure 4: We can adapt the masking strategy from Fig. 3 to a FlexAttention compatible sparse\nmasking function as above. This enables the creation of a customized JIT attention operation that uses\nsignificantly less memory with up to ≈5X speedup over the naive native scaled_dot_product_attention\nimplementation in PyTorch (≥2.5) on a A5000 GPU with L = 1024 and batch size B = 16.\n22\n"
    },
    {
      "page_number": 23,
      "text": "Published as a conference paper at ICLR 2025\nfrom torch.nn.attention.flex_attention import flex_attention,\ncreate_block_mask\nfrom functools import partial\n# Define block-wise attention mask\nmy_block_diff_mask = partial(block_diff_mask, seq_len=seq_len, block_size\n=block_size)\n# Generate optimized sparse block mask\nblock_mask = create_block_mask(my_block_diff_mask, None, None, seq_len*2,\nseq_len*2, device=device)\n# Compute attention using FlexAttention\n# Use no-cudagraphs to avoid an extra copy on small compile graphs.\n# Use max-autotune if compiling a larger model all at once.\n@torch.compile(fullgraph=True, mode=\"max-autotune-no-cudagraphs\")\ndef single_pass_block_diff_attn(q, k, v, block_mask):\nreturn flex_attention(q, k, v, block_mask=block_mask)\nFigure 5: Attention computation using FlexAttention with our proposed custom mask.\nThis implementation exploits FlexAttention’s ability to dynamically optimize execution based on\nthe provided sparsity pattern. By precomputing block-level sparsity and leveraging efficient kernel\nfusion, it enables scalable attention computation for long sequences.\nOverall, this approach provides a principled method to accelerate attention computations while\npreserving structured dependency constraints. End-to-end, replacing FlashAttention kernels using a\ncustom mask with FlexAttention kernels leads to ≈15% speedup in a model forward pass. We use a\nsingle A5000 for L = 1024 and batch size B = 16.\n23\n"
    },
    {
      "page_number": 24,
      "text": "Published as a conference paper at ICLR 2025\nC\nEXPERIMENTAL DETAILS\nWe closely follow the same training and evaluation setup as used by Sahoo et al. (2024a).\nC.1\nDATASETS\nWe conduct experiments on two datasets: The One Billion Word Dataset (LM1B; Chelba et al.\n(2014)) and OpenWebText (OWT; Gokaslan et al. (2019)). Models trained on LM1B use the\nbert-base-uncased tokenizer and a context length of 128. We report perplexities on the test\nsplit of LM1B. Models trained on OWT use the GPT2 tokenizer Radford et al. (2019) and a context\nlength of 1024. Since OWT does not have a validation split, we leave the last 100k documents for\nvalidation.\nIn preparing LM1B examples, Sahoo et al. (2024a) pad each example to fit in the context length of\nL = 128 tokens. Since most examples consist of only a single sentence, block diffusion modeling\nfor larger block sizes L′ > 4 would not be useful for training. Instead, we concatenate and wrap\nsequences to a length of 128. As a result, we retrain our autoregressive baseline, SEDD, and MDLM\non LM1B with wrapping.\nSimilarly for OWT, we do not pad or truncate sequences, but concatenate them and wrap them to a\nlength of 1024 similar to LM1B. For unconditional generation experiments in Section 6.2, we wish\nto generate sequences longer than the context length seen during training. However, Sahoo et al.\n(2024a) inject beginning-of-sequence and end-of-sequence tokens ([BOS], [EOS] respectively) at the\nbeginning and end of the training context. Thus, baselines from Sahoo et al. (2024a) will generate\nsequences that match the training context size. To examine model generations across varying lengths\nin Section 6.2, we retrain our AR, SEDD, and MDLM baselines without injecting [BOS] and [EOS]\ntokens in the examples. We also adopt this preprocessing convention for training all BD3-LMs on\nOWT.\nC.2\nARCHITECTURE\nThe model architecture augments the diffusion transformer (Peebles & Xie, 2023) with rotary\npositional embeddings (Su et al., 2021). We parameterize our autoregressive baselines, SEDD,\nMDLM, and BD3-LMs with a transformer architecture from Sahoo et al. (2024a) that uses 12 layers,\na hidden dimension of 768, and 12 attention heads. This corresponds to 110M parameters. We do not\ninclude timestep conditioning as Sahoo et al. (2024a) show it does not affect performance. We use\nthe AdamW optimizer with a batch size of 512 and constant learning rate warmup from 0 to 3e-4\nfor 2.5K gradient updates.\nC.3\nTRAINING\nWe train a base BD3-LM using the maximum context length L′ = L for 850K gradient steps. Then,\nwe fine-tune under varying L′ using the noise schedule optimization for 150K gradient steps on the\nOne Billion Words dataset (LM1B) and OpenWebText (OWT). This translates to 65B tokens and\n73 epochs on LM1B, 524B tokens and 60 epochs on OWT. We use 3090, A5000, A6000, and A100\nGPUs.\nC.4\nLIKELIHOOD EVALUATION\nWe use a single Monte Carlo estimate for sampling t to evaluate the likelihood of a token block. We\nadopt a low-discrepancy sampler proposed in Kingma et al. (2021) that reduces the variance of this\nestimate by ensuring the time steps are more evenly spaced across the interval [0,1] following Sahoo\net al. (2024a). In particular, we sample the time step for each block b ∈{1, . . . , B} and sequence k ∈\n{1, . . . , K} from a different partition of the uniform interval t(k, b) ∼U[ (k−1)B+b−1\nKB\n, (k−1)B+b\nKB\n].\nThis low-discrepancy sampler is used for evaluation. For training, each masking probability may be\nsampled from a “clipped\" range 1 −αt ∼U[β, ω]. During training, we uniformly sample t ∈[0, 1]\nunder the low-discrepancy sampler. We then apply a linear interpolation to ensure that the masking\nprobability is linear within the desired range: 1 −αt = β + (ω −β)t.\n24\n"
    },
    {
      "page_number": 25,
      "text": "Published as a conference paper at ICLR 2025\nWhen reporting zero-shot likelihoods on benchmark datasets from Radford et al. (2019) using models\ntrained on OWT, we wrap all sequences to 1024 tokens and do not add [EOS] between sequences\nfollowing Sahoo et al. (2024a).\nC.5\nINFERENCE\nGenerative Perplexity\nWe report generative perplexity under GPT2-Large from models trained\non OWT using a context length of 1024 tokens. Since GPT2-Large uses a context size of 1024, we\ncompute the generative perplexity for samples longer than 1024 tokens using a sliding window with a\nstride length of 512 tokens.\nNucleus Sampling\nFollowing SSD-LM (Han et al., 2022), we employ nucleus sampling for BD3-\nLMs and our baselines. For SSD-LM, we use their default hyperparameters p = 0.95 for block size\nL′ = 25. For BD3-LMs, AR and MDLM, we use p = 0.9. For SEDD, we find that p = 0.99 works\nbest.\nNumber of Diffusion Steps\nIn Table 7, BD3-LMs and MDLM use T = 5K diffusion steps. BD3-\nLMs and MDLM use efficient sampling by caching the output of the denoising network as proposed\nby Sahoo et al. (2024a); Ou et al. (2025), which ensures that the number of generation steps does not\nexceed the sample length L. Put simply, once a token is unmasked, it is never remasked as a result of\nthe simplified denoising model (Suppl. B.3). We use MDLM’s block-wise decoding algorithm for\ngenerating variable-length sequences, however these models are not trained with block diffusion. We\nadopt their default stride length of 512 tokens.\nSSD-LM (first row in Table 7) and SEDD use T = 1K diffusion steps. Since block diffusion performs\nT diffusion steps for each block b ∈{1, . . . , B}, SSD-LM undergoes BT generation steps. Thus to\nfairly compare with SSD-LM, we also report generative perplexity for T = 25 diffusion steps so that\nthe number of generation steps does not exceed the sequence length (second row in Table 7).\nImproved Categorical Sampling of Diffusion Models\nWe employ two improvements to Gumbel-\nbased categorical sampling of diffusion models as proposed by Zheng et al. (2024).\nFirst, we use the corrected Gumbel-based categorical sampling from Zheng et al. (2024) by sampling\n64-bit Gumbel variables. Reducing the precision to 32-bit has been shown to significantly truncate\nthe Gumbel variables, lowering the temperature and decreasing the sentence entropy.\nSecond, Zheng et al. (2024) show that the MDLM sampling time scales with the diffusion steps T,\neven though the number of generation steps is bounded by the sequence length. For sample length L\nand vocabulary size V , the sampler requires sampling O(TLV ) uniform variables and performing\nlogarithmic operations on them.\nWe adopt the first-hitting sampler proposed by Zheng et al. (2024) that requires sampling O(LV )\nuniform variables, and thus greatly improves sampling speed especially when T ≫L. The first-\nhitting sampler is theoretically equivalent to the MDLM sampler and leverages two observations: (1)\nthe transition probability is independent of the denoising network, (2) the transition probability is the\nsame for all masked tokens for a given t. Thus, the first timestep where a token is unmasked can be\nanalytically sampled as follows (assuming a linear schedule where αt = 1 −t):\ntn−1 = tnu1/n,\n(25)\nwhere n ∈{L, . . . , 1} denotes the number of masked tokens, un ∼U[0, 1] and tn−1 corresponds to\nthe first timestep where n −1 tokens are masked.\nVariable-Length Sequence Generation\nFor arbitrary-length sequence generation using BD3-LMs\nand AR in Table 6, we continue to sample tokens until the following stopping criteria are met:\n1. an [EOS] token is sampled\n2. the average entropy of the the last 256-token chunk is below 4\nwhere criterion 2 are necessary to prevent run-on samples from compounding errors (for example, a\nsequence of repeating tokens). We find that degenerate samples with low entropy result in significantly\n25\n"
    },
    {
      "page_number": 26,
      "text": "Published as a conference paper at ICLR 2025\nlow perplexities under GPT2 and lower the reported generative perplexity. Thus, when a sample\nmeets criterion 2, we regenerate the sample when reporting generative perplexity in Table 7.\nD\nSAMPLES\n<|endoftext|>’s architect, lawyer and San Giovanni concerto art critic Paolo Capacotti, gained attention from fellow gallery members and even invited\nhim to present a retrospective, publishing issues and newspaper interviews.[10] On 6 September, Kissi and his assistants agreed to move to Angelo’s\nMarcus Collection,[10] which included Giorgio Avolivo Arth and Moscolliso (later owned by the artist Belzina Massingolo) and Pan Giazzoglio\nRomeam-Guessle. The businessman, Giovanni Paletti, an outstanding collector, owned the museum and the painting. The level of criminal activity\naround the museum has continued to increase, which is part of several attempts to counter centennial rumors including the possibility that museum staff\nand visitors are tortured and even exposed to del Cavello for the only full year of Francesco Belzina’s life (1999).[4] On the evening of 22 October 2005\nit was reported that earlier that evening, guards had come on duty and began flinging an electric field with umbrellas from the balcony. As the fire\ncontinued, some of the guards sparked an apparent spat from the window of the cathedral. They remained idly watched by a pile of trash left after a\npiano key by Pietro Jolla, who died on 21 October 2005.[10] Just before 3:00 to 3pm on Monday, 27 October 2005, strong winds brought the trash on to\nthe residence that opened on 17 October. Some ruined books and statues were hurled in front from every direction of the window. Some claimed that a\ncustomer Jacques Monet had beaten the hand of photographer Franco Campetti and in some cases had stuck a broken candle in the doorway of the\nmuseum. Andr Romeam-Guessle responded by laughing when he spoke. Giancio Giuliano, the artistic director of the Museum, even tried to told\njournalists and press that ’the patient in the trisomy machine [sic] carried some corpses four hours into the museum, but the whole time it was the guy\nwho stroked the young man who who broke him’. In 2008, Giuliano told the same press that the hours of the destruction are truly \"wrong for their\nmorality\" and further stated that ’We are never satisfied with our decision. We made an informed decision to build the museum after destruction.[5]\nDeaths [ edit ] A little after 12:00 am 17 October 2005, Giuliano and his partner Monica Concerta, noticed that the trash was being thrown by passers-by.\nCaptain Iamienowska leaned over to his film camera and said, in a joking manner, that Iannorello, the chair of the Musceei, was a thief that director\nFrank Nolan said \"he would later be arrested.\" When Iamienowska arrived, the people in question were interviewed by Captain Anderson Tulaqyuk,\na co-man who was initially lying on the scene and whom Iamienowska said was able to stop them from passing in the vicinity. Iguano proceeded\nto collect the trash and the police arrived, and closed the door of the museum.[6] During the war, the statue structure was partially removed and its\ncannons damaged. On the eve of the war, the U.S. Army and Canadian Air Company, who once owned a lot on the Coopers of Paris near Leopold\nStreet, sheltered the POWs, who were briefly overworked. This following years led to condemnation of the organization and artists, including John\nDiDione,[7] returned to the compound to sell the museum items (including many of them originally stolen over the years-more than a million returned);\non 12 October, the sculptor Jack Uellein installed a temporary dome on Del Cavella.[10] Among the changes over the years, Diena featured the lion in\nthe middle of the other two halves of the dome and the lion in the back of the Viduccio.[11] In March 2008, Arvnzo Cissino announced that seven studio\nstudios were built between the summer of 1985 and the last one was completed on November 17, 2005.[10] In the fourth quarter of the mid-2000s, a\nnumber of new structures were constructed by various museums, but most notably by Arvnzo Cissino, the brothers-soldier, as portrayed by architect\nDuvois Mayaven in the illusion of the El Valleratas Humanities and Cultural Center, reconstructed by architect Duvois Benjamin. Just as the very\nsignificant additions to the current management masts of the del Museum are such private three-bedroom rooms as Tradello’s cabin, as well as the\narchitectural aspect from the museum’s south wall. Also new are the rockings performed by MC Urquhart, Aiba Ruiz and March Flez, during a music\nfestival held in the US in 2006. Urquhart was later credited with establishing record label Empire Records as an influence on his music. In<|endoftext>|\nFigure 6: Sample from MDLM (Sahoo et al., 2024a) of length L = 1024 and T = 5K diffusion steps.\nThe generative perplexity of this sample under GPT2-Large is 69.26 and its entropy is 5.6.\n26\n"
    },
    {
      "page_number": 27,
      "text": "Published as a conference paper at ICLR 2025\n<|endoftext|>When her child was planning to travel to Mexico, she thought it could be easy for her to find money to pay the trip. But that didn’t happen,\neither. The girl arrived on her doorstep the day before after they were scheduled to return to home and - despite her own passport blocked. It wasn’t\neasy. Even her mother reported a bad visit. A gate to the airport- the gate and flight instructions - were locked at nine a.. \"I had no way to the gate,\" she\ntold The Local. \"I feel like I should keep my passport and their fees.\" At first the police arrived, a British man told an e-mail that parents should only\nhave money for themselves to be fine, but could never explain the situation. Later, she told her daughter that her passport had been detained because she\nwanted to visit her family at a hotel. She felt embarrassed and confused. Nearly two months later, she said, she lost 35(40) on rent and imagined that\nthe traffic-police and customs agents in Bangkok would end up delaying flights and forcing her to stay home. She was worried that her father would\nrefuse allowing her daughter to spend a few days in the country. Meanwhile, the police were sent to search. \"It wasn’t easy for them when a child feels\nlike home for the first time,\" said Mahavram Kaas, a spokesman for the Ministry of Foreign Affairs. He’s referring to a tour arranged by the French and\nFrench foreign ministry, known as Courage in the Child. That tour cost the region 2 million worth of tickets, and cost the Calais family about $25\nmillion in lodging expenses, according to a statement by both the ministry’s behalf (he told the Portuguese police agents) and London’s Embassy in\nLondon (he told the French ambassador they provided a payment for 70,000, which would be used to pay the travel costs for their visits). In 2011, a\nfamily from Calais had moved to the UK aged 15. Their sister left to remain in France at 5 years old. Her brother tried to answer that question. He\nexplained it to reporters at his service station at Calais airport. \"You take the morning. It’s named after you and your little girls,\" he said. The last\nmother was having a 19-month stay with her daughter that night, the police said. The first time she was back her husband took their daughter on a boat\nto the UK, said the mayor of the British Transport Agency. That meant she had plenty of cash-to-go and no money to borrow when she opened an\naccount at Kathmandu Airport a few hours after booking her flight. She panicked. She called the was a Daley’s Nessie (small cash register), saying she\nwas getting better. Her doctors visited her when she returned and her boyfriend quit his job for four months after the visit, she said. \"How do you feel\nlike you are safe?\" A text from a friend left her to the police. \"She says I must go get my wallet,\" said Ajaz. Soon after, she booked a plane ticket to\nParis and took a metro train to Calais. One night, a French policeman would knock on the door of a local council building, open the mailman and the\nphone and tell her she knew that she could not leave at least one week without food. The four months her daughter spent in the UK was exhausting and\nhard, and it reached the stage where she realized she could barely stay in Britain. \"Now no choice but to go home. Then we regret having a daughter,\"\nhe said. He thought for a minute. \"You’ve broken your heart.\" \"Today, my daughter and my boyfriend decided to stay in this country for over two\nmonths,\" he wrote in an email with his daughter in his hand. \"All our flights cancelled and no security. Shame.\" Caines’ family were also put on leave.\nThe French police paid for her car after she rented it, and her female officer used it for the opening ceremony of her press conference in Thailand. The\npolice are still arranging for her family to have their official visit. Although her son is back at work now and his old job, her daughter needs to stay in a\nhospital in Algiers to continue her education. But, finally, her parents will be making their girl home. The daughter was 18 when they opened her\ncase. She was born two months ago. She doesn’t talk about it because it feels like she was still a child, living in Thailand with a small child. Her\nmother, Anzsa Gurdon, came to England as a three-year-old after her brother, Ehab Rahman, was working as a British worker in Calais while living\nin London and studying abroad in Dubai. As a mother earns 2,000 pounds a month, they receive a well-paid living in secure accommodation, some\neven with public transport buses. If they make money, their child stays in the UK, they can set up companies with kids to take care of their children.\nOther countries sometimes also give birth to parents forced to provide child care. The parents are often refugees from their home countries, they’re left\nwithout family, and have forced to leave families. As one former refugee fled Syria, his family was in detention, because when and if their child had\narrived, they would be living somewhere. The detention centers in Western Europe often have a higher rate for asylum seekers. Often there are higher\n\"safe houses\" for young people, and then the people in the center get older when their child comes to stay, but the only family that is a year older is not\nallowed to have children, and usually only if they stay six months. Children are also detained and are asked to show identification. Because in most\ncases the refugees ask only about their identity, they don’t have access to their own documents, and have no other documentation. An activist working\nfor Ireland says he’s against offshore processing. He thinks the charity problem here is like diseases which don’t sufficiently seek out international\nfunding. I think too many countries want to employ \"humanitarian\" children. Now the job Before the refugee crisis in Calais, 1,823 children were living\nin the UK, the UNHCR website shows. A good chunk of those children had landed in refugee camps in Africa, where mostly African migrants were\nsending children from Syria and Libya to their camps, but those numbers didn’t fare so well. \"At the moment when I met the French, it was horrific,\nthey wanted me to put my children in a van. But I was only kidding. It’s something called 10-year vans,\" Aakaz said during an interview. \"I want to\nkeep my children for 10 years. That’s something. It’s like Christmas. The dream of 10 years. . . . For me, the idea that this is a good opportunity, here’s\na chance,\" is that she can sleep with her children. At this point, it’s much more than just about \"alternatives\". They now have to decide, at some point,\nwhether they want to take the chance. Is it a big deal or not? Only in Calais She explained what’s agreed to so the children can go home and can have a\nbetter future. \"I’m very determined. My children want to go home but it’s my life’s personal decision,\" she said. \"Five months. I want them to be home,\n5 months. If they’re not getting jobs properly, I also want to stay home. But I feel good about what I’ve got. She is from a poor country. I don’t owe\nanything to anyone. But I have to work for them. I feel like I can just go to the road and provide accommodation for my children and their children.\"\nBut they all don’t work well because their families have her as a head. \"They want me to have a job in England, but I feel like it’s my home, and I’m not\nscared of work,\" she said. \"And I feel that the opposite would be possible. I think in the future that I can have a job or two there.\" A Second World\nFriday event will be held outside Calais on Saturday, donating 100 euros to the coming week in the money brought up to them by UNHCR through the\nKing Wahab Samba Global Fund. Friends and family expressed their \"weakness\" like many survivors in many countries.\"We just don’t want to accept\nwhat has to happen. We want to put the people back there as soon as possible,\" said one man. Her brother, who is the son of a long term migrant,\nsaid: \"The story of the refugee is not a mother’s story. The story of the refugee is children’s story.\" \"In Calais it’s too young for these kids. They play\noutside or work outside, they just eat, right? I don’t think much has changed. This child with all her food and sleep, she’s too young for life without any\nprotection. We don’t need any protection at all. We need anything that would be safer.\"<|endoftext|>\nFigure 7: Sample from BD3-LM for block size L′ = 16 of length L = 2031 under T = 5K diffusion\nsteps (trained with a context length of L = 1024). The generative perplexity of this sample under\nGPT2-Large is 24.3, and its entropy is 5.5.\n27\n"
    },
    {
      "page_number": 28,
      "text": "Published as a conference paper at ICLR 2025\n<|endoftext|>, but Wilson took over the program and turned it around. \"He’s done a tremendous job,\" Caldwell said. \"He’s done a fantastic job.\" The\noffense has always had an abundance of weapons, but it became evident that they weren’t going to have a weapon to actually go after players from the\nslot. Now they’re in two different weapons sets. The top group features Dez Bryant and Mohamed Sanu, and the bottom group features an assortment\nof weapons and pass rushers. The job has become far more complex. The other players can make plays on the ball and get those targets at a higher rate.\nSanu is more of a classic, get to the quarterback and leave the corner open. Dontari Poe got the job done this year and became one of the more effective\nplayers at the position, even in the passing game. However, Dallas has got to figure out how to get their franchise wideouts to contribute on the field.\nThat can be tough. Adding Poe can help get the receiving corps going. C.J. Spiller is a two-time Pro Bowler, but if the Cowboys want to upgrade their\nreceiving corps, he’s going to have to step up in a big way. \"We’ve got to be a little more aggressive with the type of weapons that we have,\" Caldwell\nsaid. \"I think that’s part of the reason why our last two games, especially when you’re playing in Washington, D.C., you’ve got to be aggressive, make\nsure you’re hitting at every catch. When you are, you’re giving up a lot of yards.\" Part of that means taking the quarterback out of the equation and\nhaving him beat coverage a lot more. In the NFC West, you want your offensive weapons to do a better job of running through coverage. The biggest\nthreat that Dallas has is a QB in Ben Roethlisberger. Roethlisberger is far and away the best quarterback in the league, but a lot of the credit has to go\nto his receiver group. Martavis Bryant and Antonio Brown are both big-time receivers, and last year they were in the top 10 of yards per catch and\nreceiving yards in the league. That production will never be sustainable, but if you’re going to be an elite offense, it’s going to take a lot of catching up.\nRoethlisberger is an All-Pro receiver, and he’s not the most dynamic option. But it would take something like Bryant or Brown at a better position, and\nat a slightly lower price, to make him the most productive receiver on the offense. The truth is that Roethlisberger isn’t going to be great. He may only\nhave 18 games left in his career, but he’s been doing it since he was a rookie in 1991. But that’s not the worst thing in the world. Roethlisberger’s ability\nto hit guys on the outside with good movement, vision and running ability is what the Cowboys need in order to keep up with the competition. If he\nkeeps getting better, he could become the best receiver in the league. Follow @walterfootball for updates. Like our Facebook page for more Cowboys\nnews, commentary and conversation.The owner of 1H10 Tree in Charlotte Gardens is taking legal action against the city. Derek Jarman says he’s\nbeen forced to evict his neighbour, Bob, after he took to social media to threaten to burn down his neighbor’s house. \"I’m incredibly furious with the\ncity,\" Jarman told 7.30. \"I’ve been trying to keep my eyes on the prize.\" Tree in Charlotte Gardens saying it had seen ’9,000+ people’ enjoying a great\nweekend The company that owns 1H10 named Bob after a bee and said the tree was frequently targeted because of its unusual location. Bob said he had\nhis concerns about the tree when he was contacted in October. He said they had had ’an ongoing conversation about my neighbor. He called, hung up\nand he was very threatening’ in the 30 days before they turned the tree over to him. A neighbour posted the following online message on 8 October. \"I\nam shocked about the serious problems you are having with your neighbour that has caused you all (sic). You and the 2 of you are making money at the\nexpense of the good people of Charlotte Gardens.\" Bob says he was furious and said he’d just got off the phone with the city manager. \"I told her, ’no,\nI’m going to bring a lawsuit’, and I called the solicitor and tried to get my phone, just hoping the solicitor would help me out. I called again, and I asked\nif I could go to court and to try and get an injunction. \"They told me ’you cannot’, and they said, ’we can’t, we can’t’ because you’re sending people to\nthe police’.\" Tree in Charlotte Gardens (Facebook) He also said he’d threatened the city attorney if he didn’t stop the building from burning down. The\ninternet user tweeted: \"I’s on the tree, but after I said ’threw this away, here’s a spot to burn’, the building started to burn.\" Tree in Charlotte Gardens\n(Facebook) Bob said the neighbour had threatened to burn down the tree, the windows, the living room and his entire backyard. \"It was more than a\nthreat,\" he said. \"He was a very strong person. He’s already damaged so many people in this building. It’s not going to go away.\" Tree in Charlotte\nGardens (Facebook) Jarman says he tried to talk the building owner out of the move, but the building owner’s behaviour had \"deleted him.\" \"I’m going\nto stop him by letter telling him not to come to my house any more,\" he said. \"I have three kids, and if Bob is going to be in my house, I need to make\nsure I have someone who can go in there and protect me. \"My son does a really good job of protecting me, and I’m not going to let that get in the way\nof that.\" Tree in Charlotte Gardens (Facebook) Jarman said Bob had pulled him up on social media, calling him a \"white nut\" and saying: \"For God’s\nsake, stop calling me a white nut. \"I should have shut him up on Facebook.\" He said he sent Bob the letter and thanked him for the support. \"He should\nhave done it because he’s a real artist and he’s a real artist,\" he said. He declined to name the architect of the new tree, but says the firm is the same\none that designs buildings. ’The building is burning down’, neighbour says Bob’s neighbour, Michael Banks, says the fire is an insult to his daughter.\n\"There are two black women that live next door to me and they told me ’you can’t do that’, and then the fire went up and then the building burnt down,\"\nhe said. \"You can’t burn down a house if you don’t burn down the house.\" Coun-Pete Lawrence, the Northumberland MP for Wood Green, says he has\nconcerns about Bob’s neighbours. \"It’s a very, very sad commentary on the state of society and democracy in general,\" he said. \"It’s interesting in a\ncommunity that’s 50,000-plus people, you’ve got your regular residents and well-meaning neighbours who are apparently oblivious to the destruction of\ntheir own home. \"To me, that’s appalling and it is probably a shocking amount of devastation that it’s left behind. \"I would expect there to be outrage as\nwell.\" Bob Jarman fears for his life after the tree was torched Bob says he has told the Northumberland Council that he had already received $1,000 in\nlegal action from the building owner, when he told them about the incident. The building owner has declined to comment on the situation. The builder\nis currently assessing its legal options. \"We’ve got to sort this out and have an understanding with the builder, Mr Banks,\" he said. \"We’ve got to make\nsure we can’t get into into a legal battle with that person and make that person change his mind. \"We don’t want to do anything to cause a scene or\nanybody in the street to be upset.\" Bob Jarman hopes to have an understanding with the builder on its legal options, who have refused to comment.\nTopics: state-parliament, smoking-and-doubt, black-wales-6168, united-kingdom, england First postedWhen the other guys are away playing, do a short\ncommercial to get you fired up for the next work day. Once you make it home, get a few junkies for them. They’ll be very happy to have you, for at\nleast a day. They might not be so happy after a couple of days. Have a bunch of friends and get ready to keep it going. What are you waiting for? Make\nthis long, one-off<|endoftext>\nFigure 8: Sample from an AR model (Sahoo et al., 2024a) with length L = 2003 (trained with a\ncontext length of L = 1024). The generative perplexity of this sample under GPT2-Large is 10.6 and\nits entropy is 5.5.\n28\n"
    }
  ]
}