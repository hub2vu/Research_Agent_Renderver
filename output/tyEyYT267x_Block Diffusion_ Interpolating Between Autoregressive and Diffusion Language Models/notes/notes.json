{
  "notes": [
    {
      "id": "note_1769772212407_jwcnw0",
      "title": "ABSTRACT",
      "pages": {
        "manual": "",
        "translation": "초록  \n확산 언어 모델은 병렬 생성 및 제어 가능성 덕분에 자기 회귀 모델에 비해 독특한 이점을 제공하지만, 가능성 모델링에서 뒤처지고 고정 길이 생성에 제한됩니다. 본 연구에서는 이산 디노이징 확산과 자기 회귀 모델 간의 보간을 수행하는 블록 확산 언어 모델의 한 클래스를 소개합니다. 블록 확산은 유연한 길이 생성을 지원하고 KV 캐싱 및 병렬 토큰 샘플링을 통해 추론 효율성을 개선함으로써 두 접근 방식의 주요 한계를 극복합니다. 우리는 효율적인 훈련 알고리즘, 그래디언트 분산 추정기 및 분산을 최소화하기 위한 데이터 기반 노이즈 스케줄을 포함하는 효과적인 블록 확산 모델 구축을 위한 레시피를 제안합니다. 블록 확산은 언어 모델링 벤치마크에서 확산 모델 간의 새로운 최첨단 성능을 설정하고 임의 길이 시퀀스 생성을 가능하게 합니다. 우리는 코드1, 모델 가중치 및 프로젝트 페이지의 블로그 게시물을 제공합니다: https://m-arriola.com/bd3lms",
        "analysis": "- **3줄 핵심 요약**: \n  - 확산 언어 모델은 병렬 생성과 제어 가능성에서 이점을 제공하지만, 가능도 모델링과 고정 길이 생성에서 한계가 있습니다.\n  - 블록 확산 언어 모델은 이러한 한계를 극복하여 유연한 길이의 생성과 효율적인 추론을 지원합니다.\n  - 이 모델은 새로운 최첨단 성능을 달성하며, 임의 길이의 시퀀스를 생성할 수 있습니다.\n\n- **상세 해설**: \n  - **첫 번째 문장**에서는 확산 언어 모델의 장점과 단점을 설명합니다. 이 모델들은 병렬로 데이터를 생성할 수 있고, 생성된 결과를 더 잘 제어할 수 있는 가능성을 제공하지만, 기존의 가능도 모델링에서는 성능이 떨어지고, 생성할 수 있는 데이터의 길이가 고정되어 있다는 한계가 있습니다.\n  \n  - **두 번째 문장**에서는 블록 확산 언어 모델을 소개하며, 이 모델이 기존 확산 모델과 자기회귀 모델의 중간 형태로서 두 모델의 주요 단점을 극복한다고 설명합니다. 특히, 블록 확산 모델은 유연한 길이의 데이터를 생성할 수 있으며, KV 캐싱과 병렬 토큰 샘플링을 통해 추론 효율성을 개선합니다.\n  \n  - **세 번째 문장**에서는 블록 확산 모델을 효과적으로 구축하기 위한 방법론을 제안합니다. 여기에는 효율적인 학습 알고리즘, 그래디언트 분산 추정기, 그리고 분산을 최소화하기 위한 데이터 기반의 노이즈 스케줄이 포함됩니다. 이러한 접근 방식은 확산 모델의 성능을 크게 향상시켜, 언어 모델링 벤치마크에서 새로운 최첨단 성능을 달성하게 합니다. 또한, 이 모델은 임의의 길이 시퀀스를 생성할 수 있는 능력을 제공합니다.\n\n- **주요 개념/용어**:\n  - **확산 언어 모델 (Diffusion Language Models)**: 데이터 생성 과정에서 노이즈를 추가하고 이를 제거하는 과정을 통해 데이터를 생성하는 모델로, 병렬 생성과 제어 가능성에서 장점을 가집니다.\n  - **자기회귀 모델 (Autoregressive Models)**: 이전에 생성된 데이터를 기반으로 다음 데이터를 순차적으로 예측하는 모델로, 일반적으로 높은 가능도 모델링 성능을 보입니다.\n  - **KV 캐싱 (KV Caching)**: 모델 추론 시 이전 계산 결과를 저장하여 재사용함으로써 효율성을 높이는 기법입니다.\n  - **병렬 토큰 샘플링 (Parallel Token Sampling)**: 여러 토큰을 동시에 생성할 수 있는 방법으로, 생성 속도를 높이는 데 기여합니다.\n  - **그래디언트 분산 (Gradient Variance)**: 학습 과정에서의 그래디언트의 변동성으로, 이를 줄이는 것이 모델의 성능 향상에 중요합니다."
      },
      "activePage": "translation",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 266,
        "endIndex": 1319,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_l65wpn",
      "title": "1 INTRODUCTION",
      "pages": {
        "manual": "",
        "translation": "1  \nINTRODUCTION  \n확산 모델은 이미지를 생성하는 데 널리 사용됩니다 (Ho et al., 2020; Dhariwal & Nichol, 2021; Sahoo et al., 2024b) 및 비디오 (Ho et al., 2022; Gupta et al., 2023) 생성에 사용되며, 텍스트 (Lou et al., 2024; Sahoo et al., 2024a) 또는 생물학적 서열 (Avdeyev et al., 2023; Goel et al., 2024)과 같은 이산 데이터를 생성하는 데 점점 더 효과적이 되고 있습니다. 자기 회귀 모델과 비교할 때, 확산 모델은 생성 속도를 가속화하고 모델 출력의 제어 가능성을 개선할 잠재력을 가지고 있습니다 (Schiff et al., 2024; Nisonoff et al., 2024; Li et al., 2024; Sahoo et al., 2024c). 현재 이산 확산 모델은 최소한 세 가지 제한 사항에 직면해 있습니다. 첫째, 채팅 시스템과 같은 응용 프로그램에서 모델은 임의의 길이의 출력 시퀀스를 생성해야 합니다 (예: 사용자의 질문에 대한 응답). 그러나 최근의 대부분의 확산 아키텍처는 고정 길이 벡터만 생성합니다 (Austin et al., 2021; Lou et al., 2024). 둘째, 이산 확산은 생성 중에 양방향 컨텍스트를 사용하므로 KV 캐싱으로 이전 계산을 재사용할 수 없어 추론 효율성이 떨어집니다 (Israel et al., 2025). 셋째, 이산 확산 모델의 품질은 당황도와 같은 표준 메트릭으로 측정할 때 자기 회귀 접근 방식에 비해 뒤처지며, 이는 그들의 적용 가능성을 더욱 제한합니다 (Gulrajani & Hashimoto, 2024; Sahoo et al., 2024a). 본 논문은 이 제한 사항을 해결하기 위한 진전을 이루며, 이산 확산과 자기 회귀 모델 간의 보간을 통해 블록 이산 디노이징 확산 언어 모델(Block Discrete Denoising Diffusion Language Models, BD3-LMs)을 소개합니다. 구체적으로, 블록 확산 모델(반자기 회귀 모델이라고도 함)은 이산 확률 변수 블록에 대한 자기 회귀 확률 분포를 정의합니다 (Si et al., 2022; 2023); 이전 블록을 고려한 블록의 조건부 확률은 이산 디노이징 확산 모델에 의해 지정됩니다 (Austin et al., 2021; Sahoo et al., 2024a). 효과적인 BD3-LMs를 개발하는 데는 두 가지 도전 과제가 있습니다. 첫째, 블록 확산 모델에 대한 훈련 목표를 효율적으로 계산하는 것은 신경망의 표준 전방 패스를 사용하여 불가능하며, 전문 알고리즘을 개발해야 합니다. 둘째, 훈련은 확산 목표의 그래디언트의 높은 분산으로 인해 방해받아 BD3-LMs가 블록 크기가 1일 때에도 자기 회귀보다 성능이 떨어집니다 (두 모델이 동등해야 할 때). 우리는 그래디언트 분산의 추정기를 도출하고, 이것이 자기 회귀와 확산 간의 당황도 차이에 기여하는 주요 요소임을 보여줍니다. 그런 다음 그래디언트 분산을 최소화하는 맞춤형 노이즈 프로세스를 제안하고 당황도 차이를 줄이기 위한 진전을 이룹니다. 우리는 언어 모델링 벤치마크에서 BD3-LMs를 평가하고, 이들이 훈련 컨텍스트를 초과하는 길이를 포함하여 임의의 길이의 시퀀스를 생성할 수 있음을 보여줍니다. 또한, BD3-LMs는 이산 확산 모델 중에서 새로운 최첨단 당황도를 달성합니다. 임베딩에 대해 가우시안 확산을 수행하는 대체 반자기 회귀 공식 (Han et al., 2022; 2023)과 비교할 때, 우리의 이산 접근 방식은 처리 가능한 우도 추정치를 특징으로 하며, 훨씬 적은 생성 단계로 개선된 생성 당황도를 가진 샘플을 생성합니다. 요약하자면, 우리의 연구는 다음과 같은 기여를 합니다:  \n• 우리는 토큰 블록에 대해 자기 회귀적인 블록 이산 확산 언어 모델을 소개하며, 각 블록에 대한 조건부는 이산 확산에 기반합니다.\n\n이전의 확산 모델과 달리, 블록 확산은 가변 길이 생성을 지원하며 KV 캐싱을 지원합니다. • 우리는 모델에 제공된 전체 토큰 배치를 효율적으로 활용할 수 있도록 하는 블록 확산 모델을 위한 맞춤형 훈련 알고리즘을 소개합니다. • 우리는 그래디언트 분산이 확산 모델 성능의 제한 요소임을 확인하고, 그래디언트 분산을 줄이는 맞춤형 데이터 기반 노이즈 스케줄을 제안합니다. • 우리의 결과는 이산 확산에 대한 새로운 최첨단 당황도를 확립하고 자기 회귀 모델과의 격차를 줄이는 방향으로 진전을 이룹니다.",
        "analysis": "- **3줄 핵심 요약**: \n  - 확산 모델은 이미지와 비디오 생성에서 시작하여 점차 텍스트와 생물학적 서열 생성에도 사용되고 있습니다.\n  - 기존의 이산 확산 모델은 고정된 길이의 벡터만 생성할 수 있고, KV 캐싱을 활용하지 못해 비효율적이며, 퍼플렉시티 측면에서 자회귀 모델보다 성능이 떨어집니다.\n  - 이 논문에서는 이러한 한계를 극복하기 위해 블록 이산 확산 언어 모델(BD3-LMs)을 제안하며, 이는 자회귀 모델과 이산 확산 모델을 결합하여 다양한 길이의 시퀀스를 효율적으로 생성할 수 있습니다.\n\n- **상세 해설**: \n  - **확산 모델의 활용과 장점**: 확산 모델은 이미지와 비디오 생성에서 그 효과가 입증되었으며, 최근에는 텍스트와 생물학적 서열 생성에도 점차 활용되고 있습니다. 이러한 모델은 자회귀 모델에 비해 생성 속도를 높이고, 출력의 제어 가능성을 개선할 잠재력을 가지고 있습니다.\n  \n  - **이산 확산 모델의 한계**: 현재의 이산 확산 모델은 세 가지 주요 한계를 가지고 있습니다. 첫째, 고정된 길이의 벡터만 생성할 수 있어, 채팅 시스템과 같은 응용 프로그램에서 임의의 길이의 출력을 생성하는 데 어려움이 있습니다. 둘째, 양방향 컨텍스트를 사용하여 이전 계산을 재사용하지 못해 KV 캐싱을 활용할 수 없어 비효율적입니다. 셋째, 퍼플렉시티와 같은 표준 측정 기준에서 자회귀 모델보다 성능이 떨어집니다.\n  \n  - **블록 이산 확산 언어 모델(BD3-LMs)의 제안**: 이 논문에서는 이러한 한계를 극복하기 위해 블록 이산 확산 언어 모델을 제안합니다. 이 모델은 블록 단위의 이산 확산 모델을 사용하여 자회귀 확률 분포를 정의하고, 이전 블록을 기반으로 한 조건부 확률을 통해 다양한 길이의 시퀀스를 생성할 수 있습니다. 이를 통해 기존 모델의 한계를 극복하고 효율적인 학습 목표 계산과 퍼플렉시티 개선을 목표로 합니다.\n\n- **주요 개념/용어**:\n  - **확산 모델(Diffusion Models)**: 데이터의 점진적인 노이즈 추가 및 제거 과정을 통해 새로운 데이터를 생성하는 모델로, 주로 이미지 생성에 사용되었으나 텍스트 생성에도 활용되고 있습니다.\n  - **자회귀 모델(Autoregressive Models)**: 시퀀스의 각 요소를 이전 요소에 기반하여 순차적으로 예측하는 모델로, 일반적으로 높은 퍼플렉시티 성능을 보입니다.\n  - **퍼플렉시티(Perplexity)**: 언어 모델의 성능을 측정하는 지표로, 낮을수록 모델이 데이터를 더 잘 예측함을 의미합니다.\n  - **KV 캐싱(KV Caching)**: 모델의 이전 계산 결과를 저장하여 반복적인 계산을 피하고 효율성을 높이는 기술입니다.\n  - **블록 이산 확산 언어 모델(Block Discrete Denoising Diffusion Language Models, BD3-LMs)**: 자회귀 모델과 이산 확산 모델의 장점을 결합하여 다양한 길이의 시퀀스를 효율적으로 생성할 수 있는 새로운 모델입니다."
      },
      "activePage": "manual",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 1319,
        "endIndex": 6866,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_smjlml",
      "title": "2 BACKGROUND: LANGUAGE MODELING PARADIGMS",
      "pages": {
        "manual": "",
        "analysis": "- **3줄 핵심 요약**: \n  - 이 섹션에서는 언어 모델링에서 사용되는 기본 개념과 표기법을 소개합니다.\n  - 'one-hot' 벡터를 사용하여 범주형 변수와 토큰 시퀀스를 표현하며, 이를 통해 모델이 데이터를 어떻게 처리하는지 설명합니다.\n  - 범주형 분포와 [MASK] 토큰의 역할을 정의하여, 이후 모델링 과정에서의 기초를 마련합니다.\n\n- **상세 해설**: \n  - 이 섹션은 언어 모델링 패러다임의 기초를 설명하는 부분입니다. 먼저, 언어 모델링에서 사용되는 표기법을 정의합니다. 여기서는 범주형 변수, 즉 여러 카테고리 중 하나의 값을 가질 수 있는 변수를 'one-hot' 벡터 형태로 표현합니다. 'one-hot' 벡터는 특정 카테고리에 해당하는 위치에만 1의 값을 갖고 나머지는 0인 벡터입니다. 이는 모델이 각 토큰을 고유하게 식별할 수 있도록 돕습니다.\n  - V개의 카테고리를 가진 범주형 변수는 V차원의 단순체(simplex) 공간에 속하며, 이 공간 내에서 모든 요소의 합은 1이 됩니다. 이때, V번째 카테고리는 특별한 [MASK] 토큰을 나타내며, 이는 모델이 특정 위치의 정보를 숨기거나 대체할 때 사용됩니다.\n  - x1:L은 L개의 토큰으로 이루어진 시퀀스를 나타내며, 각 토큰 xℓ은 범주형 변수로 표현됩니다. VL은 이러한 시퀀스의 집합을 의미합니다. 이 표기법은 모델이 데이터를 처리하고 학습하는 데 있어 기본적인 틀을 제공합니다. 마지막으로, Cat(·; p)는 확률 p를 갖는 범주형 분포를 나타내며, 이는 모델이 각 토큰의 발생 확률을 계산하는 데 사용됩니다.\n\n- **주요 개념/용어**:\n  - **'one-hot' 벡터**: 범주형 데이터를 표현하기 위한 방법으로, 특정 카테고리에 해당하는 위치에만 1의 값을 갖고 나머지는 0인 벡터입니다.\n  - **단순체(simplex) 공간**: 모든 요소의 합이 1이 되는 벡터 공간으로, 범주형 변수의 확률 분포를 표현하는 데 사용됩니다.\n  - **[MASK] 토큰**: 모델이 특정 위치의 정보를 숨기거나 대체할 때 사용하는 특별한 토큰입니다.\n  - **범주형 분포(Categorical Distribution)**: 여러 카테고리 중 하나의 값을 가지는 확률 분포로, 각 카테고리의 발생 확률을 나타냅니다."
      },
      "activePage": "analysis",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 6866,
        "endIndex": 9843,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_8i500v",
      "title": "3 BLOCK DIFFUSION LANGUAGE MODELING",
      "pages": {
        "manual": "",
        "analysis": "- **3줄 핵심 요약**: \n  - 블록 디퓨전 언어 모델은 디스크리트 디노이징 디퓨전과 오토리그레시브 모델을 결합하여 두 접근법의 한계를 극복합니다.\n  - 이 모델은 유연한 길이의 생성과 KV 캐싱을 통한 효율적인 추론을 지원하며, 병렬 토큰 샘플링을 가능하게 합니다.\n  - 블록 디퓨전은 새로운 학습 알고리즘, 그래디언트 분산 추정기, 데이터 기반 노이즈 스케줄을 통해 성능을 향상시킵니다.\n\n- **상세 해설**: \n  - 이 섹션에서는 블록 디퓨전 언어 모델이라는 새로운 접근법을 소개합니다. 이 모델은 디스크리트 디노이징 디퓨전과 오토리그레시브 모델의 장점을 결합하여, 두 모델이 가진 고유의 한계를 극복하고자 합니다. 블록 디퓨전은 블록 단위로 토큰을 생성하며, 각 블록의 조건부 확률은 디스크리트 디노이징 디퓨전 모델에 의해 결정됩니다. \n  - 블록 디퓨전 모델의 주요 장점은 유연한 길이의 시퀀스를 생성할 수 있다는 점입니다. 이는 기존 디퓨전 모델이 고정된 길이의 벡터만 생성할 수 있다는 한계를 극복합니다. 또한, KV 캐싱을 통해 이전 계산을 재사용할 수 있어 추론 효율성을 높이며, 병렬 토큰 샘플링을 지원하여 생성 속도를 개선합니다.\n  - 효과적인 블록 디퓨전 모델을 구축하기 위해, 새로운 학습 알고리즘과 그래디언트 분산 추정기를 도입합니다. 그래디언트 분산은 디퓨전 모델의 성능에 제한 요소로 작용할 수 있기 때문에, 이를 최소화하기 위한 데이터 기반 노이즈 스케줄을 제안합니다. 이러한 접근은 디퓨전 모델의 성능을 향상시키고, 오토리그레시브 모델과의 성능 격차를 줄이는 데 기여합니다.\n\n- **주요 개념/용어**:\n  - **블록 디퓨전 모델**: 디스크리트 디노이징 디퓨전과 오토리그레시브 모델의 장점을 결합한 모델로, 블록 단위로 토큰을 생성합니다.\n  - **디스크리트 디노이징 디퓨전**: 노이즈가 추가된 데이터를 원래의 깨끗한 데이터로 복원하는 과정을 모델링한 방법입니다.\n  - **오토리그레시브 모델**: 이전 토큰에 기반하여 다음 토큰의 확률을 예측하는 모델입니다.\n  - **KV 캐싱**: 이전 계산 결과를 저장하여 추후 재사용함으로써 추론 과정을 효율화하는 기술입니다.\n  - **그래디언트 분산**: 학습 과정에서 그래디언트의 변동성을 의미하며, 이는 모델의 수렴 속도와 성능에 영향을 미칠 수 있습니다."
      },
      "activePage": "analysis",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 9843,
        "endIndex": 10593,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_1p5xan",
      "title": "3.1 BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES",
      "pages": {
        "manual": "",
        "analysis": "- **3줄 핵심 요약**: \n  - 블록 디퓨전 모델은 토큰 블록을 생성하며, 각 블록은 이전 블록에 조건화된 디퓨전 과정을 통해 생성됩니다.\n  - 이 모델은 오토리그레시브 모델과 디퓨전 모델의 장점을 결합하여 가변 길이의 고품질 텍스트 생성을 지원하고, KV 캐싱과 병렬 샘플링을 통해 추론 효율성을 개선합니다.\n  - 블록 디퓨전 모델은 기존의 디퓨전 모델보다 더 높은 퍼플렉시티 성능을 보여주며, 오토리그레시브 모델과의 성능 격차를 줄이는 데 기여합니다.\n\n- **상세 해설**:\n  - 이 섹션에서는 블록 디퓨전 모델의 작동 방식을 설명합니다. 블록 디퓨전 모델은 텍스트 생성 시 토큰을 개별적으로 생성하는 대신, 여러 개의 토큰을 포함하는 블록 단위로 생성합니다. 각 블록은 이전 블록의 정보를 바탕으로 조건화되어 생성되며, 이를 통해 오토리그레시브 모델의 순차적 의존성을 줄이고 병렬화가 가능합니다.\n  - 블록 디퓨전 모델은 오토리그레시브 모델의 장점인 고품질 텍스트 생성을 유지하면서도, 디퓨전 모델의 장점인 병렬화 가능성과 가변 길이 생성을 결합합니다. 이를 통해 모델은 다양한 길이의 텍스트를 효율적으로 생성할 수 있으며, KV 캐싱을 통해 이전 계산을 재사용하여 추론 속도를 높입니다.\n  - 또한, 이 섹션에서는 블록 디퓨전 모델의 성능을 개선하기 위해 제안된 방법들을 소개합니다. 특히, 그래디언트 분산을 줄이기 위한 맞춤형 노이즈 프로세스를 도입하여, 디퓨전 모델의 퍼플렉시티를 향상시키고 오토리그레시브 모델과의 성능 차이를 줄이는 데 초점을 맞추고 있습니다.\n\n- **주요 개념/용어**:\n  - **블록 디퓨전 모델**: 여러 개의 토큰을 포함하는 블록 단위로 텍스트를 생성하는 모델로, 각 블록은 이전 블록에 조건화되어 생성됩니다.\n  - **KV 캐싱**: 이전 계산 결과를 저장하여 추론 시 재사용함으로써 효율성을 높이는 기술입니다.\n  - **퍼플렉시티**: 언어 모델의 성능을 측정하는 지표로, 낮을수록 모델의 예측이 정확함을 의미합니다.\n  - **그래디언트 분산**: 모델 학습 시 그래디언트의 변동성을 의미하며, 이를 줄이면 학습 안정성과 성능이 향상됩니다."
      },
      "activePage": "analysis",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 10593,
        "endIndex": 13101,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_ihr2ln",
      "title": "3.2 EFFICIENT TRAINING AND SAMPLING ALGORITHMS",
      "pages": {
        "manual": "",
        "analysis": "- **3줄 핵심 요약**: \n  - 블록 디퓨전 언어 모델의 효율적인 훈련과 샘플링 알고리즘은 모델의 성능을 향상시키기 위해 필수적입니다.\n  - 고유한 노이즈 프로세스를 통해 그래디언트 분산을 줄이고, 이를 통해 퍼플렉시티 격차를 줄이는 것이 목표입니다.\n  - 이 섹션에서는 블록 디퓨전 모델의 훈련 효율성을 높이기 위한 알고리즘적 접근을 제시합니다.\n\n- **상세 해설**: \n  - 이 섹션은 블록 디퓨전 언어 모델의 훈련과 샘플링 과정에서의 효율성을 높이기 위한 방법을 다룹니다. 블록 디퓨전 모델은 각 블록의 토큰을 디퓨전 과정을 통해 생성하고, 이전 블록에 조건부로 의존하는 방식으로 작동합니다. 이러한 방식은 기존의 오토리그레시브 모델과 디퓨전 모델의 장점을 결합하여, 더 나은 품질의 텍스트 생성과 효율적인 추론을 가능하게 합니다.\n  \n  - 그러나 블록 디퓨전 모델의 훈련 과정에서는 그래디언트의 높은 분산이 문제로 작용하여 모델의 성능을 저하시킬 수 있습니다. 이를 해결하기 위해, 이 섹션에서는 그래디언트 분산을 줄이기 위한 데이터 기반의 노이즈 스케줄을 제안합니다. 이러한 접근은 퍼플렉시티(모델의 예측 정확성을 나타내는 지표) 격차를 줄이는 데 기여합니다.\n  \n  - 또한, 블록 디퓨전 모델은 효율적인 훈련을 위해 전체 토큰 배치를 효과적으로 활용할 수 있는 맞춤형 알고리즘을 도입합니다. 이는 모델이 더 적은 생성 단계로 높은 품질의 샘플을 생성할 수 있도록 돕습니다.\n\n- **주요 개념/용어**:\n  - **블록 디퓨전 모델**: 디퓨전 모델과 오토리그레시브 모델의 장점을 결합하여, 블록 단위로 토큰을 생성하는 언어 모델.\n  - **그래디언트 분산**: 모델 훈련 시 그래디언트의 변동성으로, 높은 분산은 훈련의 불안정성을 초래할 수 있음.\n  - **퍼플렉시티**: 언어 모델의 성능을 평가하는 지표로, 낮을수록 모델의 예측이 정확함을 의미.\n  - **노이즈 스케줄**: 디퓨전 과정에서 노이즈를 추가하는 방법을 조절하는 계획으로, 모델의 훈련 효율성과 성능에 영향을 미침."
      },
      "activePage": "analysis",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 13101,
        "endIndex": 14449,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_xibpki",
      "title": "4 UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION & AR Models",
      "pages": {
        "manual": "",
        "analysis": "- **3줄 핵심 요약**: \n  - 이 섹션에서는 블록 디퓨전 언어 모델(BD3-LMs)이 오토회귀 모델에 비해 성능이 떨어지는 이유로 그래디언트 분산의 높은 변동성을 지적합니다.\n  - 그래디언트 분산을 줄이기 위한 맞춤형 노이즈 프로세스를 제안하여 오토회귀 모델과의 성능 격차를 줄이고자 합니다.\n  - BD3-LMs는 임의의 길이 시퀀스를 생성할 수 있으며, 기존의 디퓨전 모델보다 우수한 퍼플렉시티를 달성합니다.\n\n- **상세 해설**: \n  - 이 섹션은 블록 디퓨전 언어 모델(BD3-LMs)이 오토회귀 모델에 비해 퍼플렉시티(perplexity)에서 열세인 이유를 분석합니다. BD3-LMs는 블록 단위로 토큰을 생성하는데, 이 과정에서 그래디언트의 변동성이 커져 학습 효율이 떨어집니다. 이는 특히 블록 크기가 1일 때 두 모델이 이론적으로 동일한 성능을 보여야 함에도 불구하고 BD3-LMs가 저조한 성능을 보이는 이유로 지적됩니다.\n  - 이러한 문제를 해결하기 위해, 연구진은 그래디언트 분산을 줄이는 맞춤형 노이즈 프로세스를 제안합니다. 이를 통해 BD3-LMs의 퍼플렉시티를 개선하고, 오토회귀 모델과의 성능 격차를 줄이는 데 기여합니다.\n  - BD3-LMs는 임의의 길이 시퀀스를 생성할 수 있으며, 기존의 디퓨전 모델보다 적은 생성 단계로 더 나은 퍼플렉시티를 달성합니다. 이는 블록 단위의 오토회귀와 디퓨전 모델의 장점을 결합한 결과로, 모델의 효율성과 성능을 동시에 향상시킵니다.\n\n- **주요 개념/용어**:\n  - **퍼플렉시티(perplexity)**: 언어 모델의 성능을 측정하는 지표로, 모델이 주어진 데이터에 대해 얼마나 잘 예측하는지를 나타냅니다. 값이 낮을수록 모델의 성능이 좋음을 의미합니다.\n  - **그래디언트 분산(gradient variance)**: 모델 학습 시 그래디언트의 변동성을 나타내며, 높은 변동성은 학습의 불안정성을 초래할 수 있습니다.\n  - **노이즈 프로세스(noise process)**: 모델 학습 시 그래디언트 분산을 줄이기 위해 데이터에 인위적으로 추가하는 잡음 처리 과정입니다."
      },
      "activePage": "analysis",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 14449,
        "endIndex": 20590,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_pzd5xc",
      "title": "5.1 INTUITION: AVOID EXTREME MASK RATES",
      "pages": {
        "manual": ""
      },
      "activePage": "manual",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 20590,
        "endIndex": 20827,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_osxsvv",
      "title": "6 EXPERIMENTS",
      "pages": {
        "manual": "",
        "analysis": "- **3줄 핵심 요약**: \n  - 실험 섹션에서는 블록 디퓨전 언어 모델의 성능을 평가하여, 기존의 오토리그레시브 모델과 비교합니다.\n  - 블록 디퓨전 모델은 다양한 길이의 텍스트를 생성할 수 있으며, 기존 디퓨전 모델보다 더 나은 퍼플렉시티를 달성합니다.\n  - 실험 결과는 블록 디퓨전 모델이 오토리그레시브 모델과의 퍼플렉시티 격차를 줄이는 데 성공했음을 보여줍니다.\n\n- **상세 해설**: \n  - 실험 섹션에서는 블록 디퓨전 언어 모델(BD3-LMs)의 성능을 다양한 언어 모델링 벤치마크에서 평가합니다. 이 모델은 기존의 디퓨전 모델과 달리 임의의 길이의 시퀀스를 생성할 수 있는 능력을 가지고 있습니다. 실험 결과, BD3-LMs는 기존의 디퓨전 모델들보다 더 나은 퍼플렉시티를 기록하며, 이는 모델이 더 정확한 언어 예측을 한다는 것을 의미합니다.\n  - 또한, BD3-LMs는 오토리그레시브 모델과 비교했을 때 퍼플렉시티 격차를 줄이는 데 성공했습니다. 이는 블록 디퓨전 모델이 오토리그레시브 모델의 장점을 일부 흡수하여 성능을 개선했음을 시사합니다. 특히, BD3-LMs는 KV 캐싱과 병렬 토큰 샘플링을 통해 효율적인 추론을 가능하게 합니다.\n  - 이러한 실험 결과는 블록 디퓨전 모델이 기존의 한계를 극복하고, 텍스트 생성의 유연성과 효율성을 동시에 달성할 수 있음을 보여줍니다. 이는 언어 모델링 분야에서의 중요한 진전을 의미하며, 향후 다양한 응용 분야에서 활용될 가능성을 제시합니다.\n\n- **주요 개념/용어**:\n  - **블록 디퓨전 언어 모델(BD3-LMs)**: 블록 단위로 디퓨전 과정을 수행하여 텍스트를 생성하는 모델로, 오토리그레시브 모델과 디퓨전 모델의 장점을 결합한 형태입니다.\n  - **퍼플렉시티(Perplexity)**: 언어 모델의 성능을 측정하는 지표로, 낮을수록 모델이 더 정확하게 다음 단어를 예측할 수 있음을 의미합니다.\n  - **KV 캐싱**: 이전 계산 결과를 저장하여 추론 시 반복 계산을 줄이는 기법으로, 모델의 효율성을 높이는 데 기여합니다."
      },
      "activePage": "analysis",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 20827,
        "endIndex": 24612,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772643276_rybypz",
      "title": "7 Discussion And Prior Work",
      "pages": {
        "manual": "",
        "analysis": "- **3줄 핵심 요약**:  \n  1. 오토레그레시브 모델은 각 토큰이 이전 토큰들에 의존하는 방식으로 확률 분포를 정의합니다.  \n  2. 이러한 모델은 다음 토큰 예측을 통해 효율적으로 학습할 수 있지만, 토큰 수만큼의 단계가 필요해 생성 속도가 느립니다.  \n  3. 이 섹션은 오토레그레시브 모델의 기본적인 작동 원리와 그 한계를 설명합니다.\n\n- **상세 해설**:  \n  오토레그레시브 모델은 주어진 데이터 분포에서 L개의 토큰으로 구성된 시퀀스를 고려합니다. 이 모델은 각 토큰이 그 이전의 모든 토큰들에 의존하는 형태로 확률 분포를 정의합니다. 수식으로는 각 토큰의 조건부 확률을 곱하여 전체 시퀀스의 로그 확률을 계산합니다. 이러한 방식은 각 토큰을 예측할 때 이전 토큰들에 대한 정보를 활용하므로, 다음 토큰 예측을 통해 효율적으로 학습할 수 있습니다. 하지만, 이러한 종속성 때문에 시퀀스의 길이 L만큼의 단계가 필요하여 생성 속도가 느려지는 단점이 있습니다. 이 섹션은 오토레그레시브 모델의 작동 원리와 함께, 그로 인해 발생하는 효율성 문제를 설명하고 있습니다.\n\n- **주요 개념/용어**:  \n  - **오토레그레시브 모델 (Autoregressive Model)**: 시퀀스의 각 요소가 이전 요소들에 의존하는 방식으로 확률을 모델링하는 방법입니다.  \n  - **조건부 확률 (Conditional Probability)**: 어떤 사건이 발생했을 때, 다른 사건이 발생할 확률을 의미합니다.  \n  - **로그 확률 (Log Probability)**: 확률의 곱셈을 덧셈으로 변환하여 계산을 용이하게 하는 방법입니다.  \n  - **다음 토큰 예측 (Next Token Prediction)**: 주어진 시퀀스에서 다음에 올 토큰을 예측하는 작업으로, 오토레그레시브 모델의 학습 방식입니다."
      },
      "activePage": "analysis",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 24612,
        "endIndex": 35600,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_s42i80",
      "title": "8 CONCLUSION",
      "pages": {
        "manual": "",
        "translation": "8  \n결론  \n이 연구는 블록 확산을 탐구하며 기존의 이산 확산과 관련된 두 가지 문제, 즉 임의 길이 시퀀스를 생성할 필요성과 자기 회귀 모델에 대한 당혹감 차이를 동기로 삼고 있습니다. 우리는 D3PM 프레임워크(Austin et al., 2021)의 블록 단위 확장을 나타내는 BD3-LMs를 도입하고, 성능을 더욱 향상시키는 전문화된 훈련 알고리즘과 맞춤형 노이즈 스케줄을 활용합니다. 우리는 이 모델들이 긴 형식의 문서를 생성할 수 있을 뿐만 아니라 당혹감도 개선하여 이산 확산 모델 중 새로운 최첨단을 설정한다는 것을 관찰합니다.  \n10  \n2025년 ICLR에서 컨퍼런스 논문으로 발표됨  \n감사의 말 및 자금 출처 공개  \n이 연구는 DGE-1922551, CAREER 상 2046760 및 2145577에 따라 국립 과학 재단의 부분적인 지원을 받았으며, MIRA R35GM151243 상에 따라 국립 보건원으로부터 지원을 받았습니다. Marianne Arriola는 DGE-2139899에 따라 NSF 대학원 연구 장학금과 Hopper-Dean/Bowers CIS 학장 우수 장학금의 지원을 받고 있습니다. 우리는 컴퓨팅 자원에 대한 접근을 제공해 준 Databricks MosaicML에 감사드립니다.",
        "analysis": "- **3줄 핵심 요약**: \n  - 오토회귀 모델은 각 토큰을 이전 토큰에 기반하여 순차적으로 예측하는 방식으로 작동합니다.\n  - 이러한 모델은 다음 토큰 예측을 통해 효율적으로 훈련될 수 있지만, 토큰 수에 비례하여 생성 시간이 늘어나는 단점이 있습니다.\n  - 오토회귀 모델의 구조는 각 토큰의 조건부 확률을 신경망으로 직접 매개변수화하여 정의합니다.\n\n- **상세 해설**: \n  - 오토회귀 모델은 L개의 토큰으로 구성된 시퀀스를 다루며, 이 시퀀스는 데이터 분포 q(x)로부터 추출됩니다. 이 모델은 확률 분포를 각 토큰의 조건부 확률로 분해하여 정의합니다. 즉, 각 토큰 xℓ는 이전의 모든 토큰 x<ℓ에 기반하여 예측됩니다.\n  - 이러한 방식은 신경망을 통해 각 조건부 확률 pθ(xℓ| x<ℓ)를 직접 매개변수화하여 구현됩니다. 이로 인해 오토회귀 모델은 다음 토큰을 예측하는 방식으로 효율적으로 훈련이 가능합니다.\n  - 그러나 오토회귀 모델은 시퀀스의 각 토큰을 순차적으로 생성해야 하므로, L개의 토큰을 생성하는 데 L단계가 필요합니다. 이는 생성 과정에서의 시간적 비효율성을 초래할 수 있습니다.\n\n- **주요 개념/용어**:\n  - **오토회귀 모델 (Autoregressive Model)**: 시퀀스의 각 요소를 이전 요소들에 기반하여 예측하는 모델. 주로 시계열 데이터나 자연어 처리에서 사용됩니다.\n  - **조건부 확률 (Conditional Probability)**: 어떤 사건이 주어졌을 때 다른 사건이 발생할 확률을 의미합니다. 여기서는 각 토큰이 이전 토큰들에 기반하여 발생할 확률을 나타냅니다.\n  - **신경망 (Neural Network)**: 데이터의 패턴을 학습하여 예측을 수행하는 기계 학습 모델. 오토회귀 모델에서는 각 토큰의 조건부 확률을 매개변수화하는 데 사용됩니다."
      },
      "activePage": "translation",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 35600,
        "endIndex": 36717,
        "sourceFile": "json"
      }
    },
    {
      "id": "note_1769772212407_6k6xiu",
      "title": "REFERENCES",
      "pages": {
        "manual": ""
      },
      "activePage": "manual",
      "isOpen": true,
      "sectionBoundary": {
        "startIndex": 36717,
        "endIndex": 54383,
        "sourceFile": "json"
      }
    }
  ],
  "updated_at": "2026-01-30T11:53:55.379421"
}