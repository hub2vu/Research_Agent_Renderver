[
  {
    "ref_no": 2014,
    "title": "Diffusion forcing: Next-token prediction meets full-sequence diffusion",
    "ids": {
      "doi": "10.18653/v1/n18-2097",
      "arxiv": "2208.04202",
      "url": "http://dx.doi.org/10.18653/v1/n18-2097.Zihang",
      "year": "2025"
    },
    "graph_id": "10.48550_arxiv.2208.04202",
    "raw_text": "Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Pro..."
  },
  {
    "ref_no": 2024,
    "title": "Better & faster large language models via multi-token prediction",
    "ids": {
      "arxiv": "2404.19737"
    },
    "graph_id": "10.48550_arxiv.2404.19737",
    "raw_text": "Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737,"
  },
  {
    "ref_no": 2024,
    "title": "Memdlm: De novo membrane protein design with masked discrete diffusion protein language models",
    "ids": {
      "arxiv": "2410.16735",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2410.16735",
    "raw_text": "Shrey Goel, Vishrut Thoutam, Edgar Mariano Marroquin, Aaron Gokaslan, Arash Firouzbakht, Sophia Vincoff, Volodymyr Kuleshov, Huong T Kratochvil, and Pranam Chatterjee. Memdlm: De novo membrane protein..."
  },
  {
    "ref_no": 2022,
    "title": "David helps goliath: Inference-time collaboration between small specialized and large general diffusion lms",
    "ids": {
      "arxiv": "2305.14771",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2305.14771",
    "raw_text": "Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, and Marjan Ghazvininejad. David helps goliath: Inference-time collaboration between small specialized and large general diffusion lms. arXiv preprint arXi..."
  },
  {
    "ref_no": 2022,
    "title": "Denoising diffusion probabilistic models",
    "ids": {
      "arxiv": "2204.03458",
      "url": "https://openreview.net/forum?id=8uzBOVmh8H.Xiang",
      "year": "2020"
    },
    "graph_id": "10.48550_arxiv.2204.03458",
    "raw_text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko,..."
  },
  {
    "ref_no": 2024,
    "title": "Visual instruction tuning",
    "ids": {
      "arxiv": "2406.01572",
      "url": "https://openreview.net/forum?id=CNicRIVIPA.Mitch",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2406.01572",
    "raw_text": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:34892–34916, 2023. Aaron Lou, Chenlin Meng, and Stefano Ermon...."
  },
  {
    "ref_no": 2023,
    "title": "Accelerating transformer inference for translation via parallel decoding",
    "ids": {
      "doi": "10.18653/v1/2023.acl-long.689",
      "arxiv": "2412.10193",
      "url": "https://aclanthology.org/2023.acl-long.689.Yair",
      "year": "2023"
    },
    "graph_id": "10.48550_arxiv.2412.10193",
    "raw_text": "Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. Accelerating transformer inference for translation via parallel decoding...."
  },
  {
    "ref_no": 2023,
    "title": "Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models",
    "ids": {
      "arxiv": "2402.07754",
      "url": "https://openreview.net/forum?id=0EG6qUQ4xE.Jiacheng",
      "year": "2024"
    },
    "graph_id": "10.48550_arxiv.2402.07754",
    "raw_text": "URL https://openreview.net/forum?id=0EG6qUQ4xE.Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, et al. Diffusion of thoughts: Chain-of-t..."
  },
  {
    "ref_no": 11,
    "title": "In particular, we focus on masking diffusion processes introduced by Austin et al",
    "ids": {
      "year": "2021"
    },
    "graph_id": "",
    "raw_text": "B MASKED BD3-LMS We explore a specific class of block diffusion models that builds upon the masked diffusion language modeling framework. In particular, we focus on masking diffusion processes introdu..."
  },
  {
    "ref_no": 12,
    "title": "",
    "ids": {},
    "graph_id": "",
    "raw_text": "The diffusion matrix for the forward marginal Qt|s is: [Qt|s]ij =    if i = j = m αt|s if i = j ̸= m 1 −αt|s if j = m, i ̸= m"
  },
  {
    "ref_no": 13,
    "title": "B.1 FORWARD PROCESS Under the D3PM framework (Austin et al., 2021), the forward noise process applied independently for each token ℓ∈{1",
    "ids": {
      "year": "2021"
    },
    "graph_id": "",
    "raw_text": "where αt|s = αt/αs. B.1 FORWARD PROCESS Under the D3PM framework (Austin et al., 2021), the forward noise process applied independently for each token ℓ∈{1, . . . L} is defined using diffusion matrice..."
  },
  {
    "ref_no": 14,
    "title": "",
    "ids": {},
    "graph_id": "",
    "raw_text": "B.2 REVERSE PROCESS Let Qt|s denote the diffusion matrix for the forward marginal. We obtain the reverse posterior q(xℓ s | xℓ t, xℓ) using the diffusion matrices: q(xℓ s|xℓ t, xℓ) = q(xℓ t|xℓ s, xℓ)q..."
  },
  {
    "ref_no": 15,
    "title": "B.3 SIMPLIFIED NELBO FOR MASKED DIFFUSION PROCESSES Following Sahoo et al",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "where ⊙denotes the Hadmard product between two vectors. B.3 SIMPLIFIED NELBO FOR MASKED DIFFUSION PROCESSES Following Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025), we simplify the NELBO i..."
  },
  {
    "ref_no": 1,
    "title": "",
    "ids": {},
    "graph_id": "",
    "raw_text": "Zero Masking Probabilities. We set pθ(xℓ= m|xℓ t) = 0 (as the clean sequence x doesn’t contain masks)."
  },
  {
    "ref_no": 2,
    "title": "The true posterior for the case where xℓ t ̸= m is q(xℓ s = xℓ t|xℓ t ̸= m) = 1 (if a token is unmasked in the reverse process, it is never remasked",
    "ids": {},
    "graph_id": "",
    "raw_text": "Carry-Over Unmasking. The true posterior for the case where xℓ t ̸= m is q(xℓ s = xℓ t|xℓ t ̸= m) = 1 (if a token is unmasked in the reverse process, it is never remasked). Thus, we simplify the denoi..."
  },
  {
    "ref_no": 16,
    "title": "",
    "ids": {},
    "graph_id": "",
    "raw_text": "Lastly, we obtain a tighter approximation of the likelihood by taking the diffusion steps T →∞ (Sahoo et al., 2024a), for which T(αt −αs) = α′ t: Ldiffusion = B X b=1 Et∼[0,1]Eq \u0014 α′ t 1 −αt log pθ(xb..."
  },
  {
    "ref_no": 17,
    "title": "2024a) (Suppl",
    "ids": {},
    "graph_id": "",
    "raw_text": "For the continuous time case, Sahoo et al. (2024a) (Suppl. A.2.4) show the reconstruction loss reduces to 0 as xb t(1) ∼limT →∞Cat \u0010 .; xb t= 1 T \u0011 = Cat(.; xb). Using this, we obtain: Lrecons = −Eq l..."
  },
  {
    "ref_no": 18,
    "title": "2024a) (Suppl",
    "ids": {},
    "graph_id": "",
    "raw_text": "The prior loss Lprior = DKL \u0000q(xb t=1|xb) ∥pθ(xb t=1) \u0001 also reduces to 0 because αt=1 = 0 which ensures q(xb t=1|xb) = Cat(.; m) and pθ(xb t=1) = Cat(.; m); see Sahoo et al. (2024a) (Suppl. A.2.4). F..."
  },
  {
    "ref_no": 19,
    "title": "2024a) (Suppl",
    "ids": {},
    "graph_id": "",
    "raw_text": "The above NELBO is invariant to the choice of noise schedule αt; see Sahoo et al. (2024a) (Suppl. E.1.1). B.4 RECOVERING THE NLL FROM THE NELBO FOR SINGLE TOKEN GENERATION Consider the block diffuson ..."
  },
  {
    "ref_no": 20,
    "title": "carry-over unmasking",
    "ids": {},
    "graph_id": "",
    "raw_text": "Recall that our denoising model employs the SUBS-parameterization proposed in Sahoo et al. (2024b). The “carry-over unmasking” property ensures that log pθ(xb | xb t = xb, x<b) = 0, as an unmasked tok..."
  },
  {
    "ref_no": 21,
    "title": "B.5 TIGHTNESS OF THE NELBO For block sizes 1 ≤K ≤L, we show that -log p(x) ≤LK ≤LK+1",
    "ids": {},
    "graph_id": "",
    "raw_text": "For single-token generation (L′ = 1) we recover the autoregressive NLL. B.5 TIGHTNESS OF THE NELBO For block sizes 1 ≤K ≤L, we show that -log p(x) ≤LK ≤LK+1. Consider K = 1, where we recover the autor..."
  },
  {
    "ref_no": 22,
    "title": "",
    "ids": {},
    "graph_id": "",
    "raw_text": "Consider the ELBO for block size K = 2: L2 = L/2 X b=1 log Et∼[0,1]Eq α′ t 1 −αt pθ(xb | xb t, x<b)"
  },
  {
    "ref_no": 23,
    "title": "Let xb,ℓcorrespond to the token in position ℓ∈[1, L′] of block b",
    "ids": {},
    "graph_id": "",
    "raw_text": "We show that L1 ≤L2, and this holds for all 1 ≤K ≤L by induction. Let xb,ℓcorrespond to the token in position ℓ∈[1, L′] of block b. We derive the below inequality: − L X b=1 log pθ(xb | m, x<b) = − L/..."
  },
  {
    "ref_no": 24,
    "title": "However, modeling all B conditonal terms requires processing both the noised sequence xb t and the conditional context x<b for all b",
    "ids": {
      "year": "2024"
    },
    "graph_id": "",
    "raw_text": "B.6 SPECIALIZED ATTENTION MASKS We aim to model conditional probabilities pθ(xb | xb t, x<b) for all blocks b ∈[1, B] simultaneously by designing an efficient training algorithm with our transformer b..."
  },
  {
    "ref_no": 25,
    "title": "",
    "ids": {},
    "graph_id": "",
    "raw_text": "where n ∈{L, . . . , 1} denotes the number of masked tokens, un ∼U[0, 1] and tn−1 corresponds to the first timestep where n −1 tokens are masked. Variable-Length Sequence Generation For arbitrary-leng..."
  },
  {
    "ref_no": 1,
    "title": "",
    "ids": {},
    "graph_id": "",
    "raw_text": "an [EOS] token is sampled"
  },
  {
    "ref_no": 2,
    "title": "wrong for their morality",
    "ids": {
      "year": "1999"
    },
    "graph_id": "",
    "raw_text": "the average entropy of the the last 256-token chunk is below 4 where criterion 2 are necessary to prevent run-on samples from compounding errors (for example, a sequence of repeating tokens). We find ..."
  }
]