{
  "filename": "10.48550_arxiv.2506.07976.pdf",
  "total_pages": 41,
  "full_text": "arXiv:2506.07976v2  [cs.LG]  10 Jun 2025\nThinking vs. Doing: Agents that Reason by\nScaling Test-Time Interaction\nJunhong Shen*1,2, Hao Bai*3, Lunjun Zhang4, Yifei Zhou5, Amrith Setlur1, Shengbang Tong7, Diego Caples6, Nan\nJiang3, Tong Zhang3, Ameet Talwalkar1 and Aviral Kumar1\n1Carnegie Mellon University, 2Scribe, 3University of Illinois Urbana-Champaign, 4University of Toronto, 5University of California,\nBerkeley, 6The AGI Company, 7New York University\nFigure 1: We propose a new-axis of test-time scaling for agents: scaling the number of interaction steps. Unlike traditional\nmethods that emphasize longer reasoning per step, we show that acting more helps gain new information from the environment\nand improve task performance (detailed results of the left plot in Section 4.2).\nAbstract: The current paradigm of test-time scaling relies on generating long reasoning traces (‚Äúthinking‚Äù more)\nbefore producing a response. In agent problems that require interaction, this can be done by generating thinking\ntraces before acting in the world. However, this process does not allow agents to acquire new information from the\nenvironment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped\ndimension of test-time scaling that increases the agent‚Äôs interaction horizon to enable running rich behaviors such\nas exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of\nthis scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction\nscaling without any training can improve task success on web benchmarks non-trivially. Building on this, we\nintroduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains\nagents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art\nopen-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables\nagents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful,\ncomplementary axis to scaling per-step compute, offering new avenues for training adaptive agents.\nProject page: https://test-time-interaction.github.io\nCode: https://github.com/test-time-interaction/TTI\n1. Introduction\nRecent advances in foundation models have enabled a shift from static language models to interactive\nagents that perform multi-step tasks in dynamic environments like browsers [1‚Äì6], terminals [7], and\nthe physical world [8‚Äì13]. These agents operate in closed-loop settings where each action changes the\ncurrent state of the world and affects future interaction with the environment. As a result, interactive\nagents must plan under uncertainty and adapt to failures in real time to be successful. How can we build\nagents that succeed in such interactive settings?\nCurrent post-training approaches produce reactive agents that respond to immediate observations but\nstruggle with evolving or uncertain task dynamics. Methods like supervised fine-tuning (SFT) on expert\n*Equal contribution. Corresponding author(s): junhongs@andrew.cmu.edu, haob2@illinois.edu\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\ndemonstrations [14‚Äì18] or reinforcement learning (RL) with task rewards [19‚Äì23] typically train agents\nto predict a single best action at each step. Even with test-time scaling, where agents are prompted to\n‚Äúthink‚Äù longer before prescribing an action [24‚Äì26], they are still optimized to select the most effective\naction based on the agent‚Äôs internal state. While sufficient for fully observable and stationary tasks,\nreactive policies based on the agent‚Äôs internal estimate of the task state are often suboptimal in partially\nobservable (e.g., incomplete details visible on a page) or non-stationary (e.g., fluctuating prices during\nflight booking) settings, where adaptive, information-seeking behavior is critical.\nIn this paper, we argue that instead of reactive ‚Äúoptimal‚Äù policies, agents should learn adaptive policies that\ncan collect new information from the environment and adjust their behaviors on-the-fly. A pre-requisite\nfor such adaptability is the ability to take more actions during deployment than those prescribed by an\nexpert trajectory. We therefore propose a new dimension of test-time scaling: increasing the number of\ninteraction steps of the agent. This allows agents to have sufficient context and time to attempt different\nbehaviors. For example, in a hotel booking task, an agent must first browse many listings to compare\nuser reviews and check availability before selecting the best option. Interaction scaling is orthogonal to\nexisting methods based on chain-of-thought (CoT), which emphasize deeper reasoning per step but do\nnot support information-gathering from the environment. This notion of information gain is unique to\nagentic tasks with partial observability and requires interaction, not merely larger per-step compute. For\ninstance, an agent that reasons deeply about one selected hotel without interacting further may miss\nbetter options that show up only after exploration.\nAlthough the idea of interaction scaling is conceptually straightforward, extending it to post-training\nand teaching agents to scale interaction autonomously presents key challenges. Without appropriate\ntraining signals, agents may overfit to exploratory behaviors like blindly clicking links but not making\nprogress toward the actual task objective, wasting the additional steps. To tackle this issue, we propose\nto combine online RL with a curriculum that prescribes how to scale the interaction horizon, training\nagents that first learn effective exploitation before extending their horizon to explore.\nWe instantiate our approach in the domain of web agents, a widely applicable setting with well-established\nbenchmarks. We first show that scaling test-time interaction via prompting the agent to ‚Äúthink and act\nagain‚Äù after it decides to terminate can already improve the task success rate from 23% to ‚â•28% on\nWebArena [2] (see Figure 3 for details). While this increases trajectory length and the number of tokens\ngenerated, spending an equivalent amount of compute on conventional test-time scaling methods like\nforcing the agent to think for longer [27] or running best-of-ùëõ[28‚Äì30] yields less than a 3% gain. These\nfindings validate interaction scaling as a promising and complementary axis of test-time scaling.\nWe then move beyond prompting and develop TTI (Test-Time Interaction), a curriculum-based RL\napproach that trains agents to adaptively scale interaction by gradually increasing the rollout horizon. We\nscale TTI to large and diverse training sets (>100K tasks across ‚àº20 domains) by integrating it with an\nautomated pipeline that generates synthetic tasks for online data collection. TTI achieves state-of-the-art\nperformance among open-source agents trained on open data on both WebVoyager [1] and WebArena [2],\nusing only a 12B Gemma 3 model, improving over the non-fine-tuned agent by 9% and 8%, respectively.\nOur analysis further shows that curriculum training enables adaptive exploration: agents learn to initiate\nnew searches or backtrack in complex tasks, while following efficient paths in simpler ones.\nIn summary, we introduce interaction scaling as a new dimension for test-time scaling of agents. We\npropose TTI to train agents by adjusting interaction horizon dynamically. Our results show that TTI\nyields strong empirical gains and offers promising directions for domains beyond web navigation.\n2\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n2. Related Work\nScaffolded foundation models as web agents. Prior works use external control structures to scaffold\nfoundation models via modular prompting [6, 31‚Äì37], programs [38‚Äì42], or feedback mechanisms [43‚Äì\n46]. These methods often rely on proprietary models like GPT-4 [47] or Claude [48]. Thus, progress\nis driven by designing better prompts and workflows for planning [49‚Äì52], self-correction [53], self-\nevaluation [54, 55], or by integrating external modules such as memory [56] or retrieval systems [57].\nMore recently, developing specialized agents has become a promising direction. ScribeAgent [14] first uses\nreal-world data to demonstrate that simple fine-tuning can outperform most prompt-based agents. Prior\nwork also builds automated data curation workflows [58‚Äì60] and distillation methods [18]. Despite these\nefforts, scaffolding methods remain fundamentally limited: they do not enable agents to self-improve\nthrough interaction, and rely on fixed wrappers that lack adaptability across diverse environments.\nRL training for foundation model agents. RL-based approaches enable agents to autonomously\nimprove through interaction. Prior work has explored DPO [61], actor-critic [20, 21, 62], or distributed\nsampling [63]. Pipelines like PAE [19] and Learn-By-Interact [64] support automatic task generation,\nexploration, and labeling. However, most of these approaches lack mechanisms for test-time exploration,\nlimiting the agent‚Äôs ability to adapt its behavior over long horizons, especially under partially observable\nconditions. As Bai et al. [20] note, continued training after deployment is often required just to maintain\nperformance with these methods. Our work addresses this limitation by scaling test-time interaction\nas an independent dimension, allowing agents to refine behavior while acting. Curriculum-based RL\nhas been applied in AutoWebGLM [65] and WebRL [23], where curricula are based on estimated task\ndifficulty derived from the complexity of LLM-generated instructions. While our approach also induces\na progression from simpler to more complex behaviors, it does so by gradually increasing the allowed\ninteraction horizon rather than relying on explicit measures of task difficulty.\nScaling test-time compute. Increasing test-time compute via best-of-ùëõsampling [29], beam search [66,\n67], or verifiers [68‚Äì70] has shown to improve performance in reasoning-heavy tasks. In non-interactive\nsettings like math and competitive coding, recent methods train models to generate long CoT and scale\nreasoning internally [e.g., 27, 71‚Äì73]. As for multi-turn interactive settings, most existing works simply\nintegrate CoT prompting into the agent system to enhance per-step reasoning [e.g., 52, 74]. EXACT [75]\nscales up the search process for each action, GenRM-CoT [76] the number of verifiers, and Jin et al. [77]\nthe number of agents. However, none of these efforts studies the benefits of scaling over the time horizon,\nwhere the agent can explore alternatives, backtrack, or gather more information before committing to\ncertain actions. Our work extends this line of research by introducing test-time scaling of interaction.\nAs we will show in our empirical results (Section 4.2), the benefits of scaling test-time interaction go\nbeyond test-time scaling (or ‚Äúreasoning‚Äù) before taking an action, within a given time step, because each\nextra step of interaction with the environment provides new information to the agentic policy, whereas\nthinking for longer simply reorganizes information that the agent already has.\n3. Problem Setup\nWe consider solving a web task as a finite-horizon sequential decision-making process guided by a reward\nobjective1. Formally, the environment implements a transition function that evolves over time and provides\nan observation ùëúùë°at step ùë°reflecting the current task state (details regarding the parameterization of the\nobservation space are shown below). The agent policy ùúãis parameterized by a multi-modal foundation\n1While this work centers on web agents, we believe the insights should generalize to other agent problem domains, and we\nhope future work will extend these ideas beyond web agents and web navigation.\n3\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nmodel that maps observation history ùëú1:ùë°‚àí1 and action history ùëé1:ùë°‚àí1 to the next action ùëéùë°. These histories\nallow the policy to represent rich, context-dependent behaviors enabled via interaction (details about\nthe design of the action space are shown below). We denote the environment horizon, or the maximum\nnumber of interaction steps allowed in the environment, as ‚Ñé. For each task, the actual interaction process\nends when the agent issues a stop signal or reaches the step limit ‚Ñé. Let ‚Ñéstop ‚àà(0, ‚Ñé] denote the actual\nnumber of steps taken. The agent receives a reward of 1 for task success, and 0 otherwise.\nObservation space design. Following [2, 14, 19], we consider an observation space consisting of the\ntask goal, the URL, and a structured representation of the current web page that includes both the\naccessibility tree of the web page and a screenshot augmented with a set-of-marks overlay [1], which\nassigns a unique identifier to each element that the agent can interact with. While the agent has access\nto all past observations in principle, doing so quickly exhausts the context window in practice, so we\ntruncate observation history to the most recent three steps, similar to prior works [19]. However, the\nagent still has access to all of its past actions. We show in Appendix B an example observation.\nAction space parameterization. We adopt a discrete action space with six actions: click, type, scroll, go\nback, search (e.g., Google or Bing), and stop the task with an answer. Following Yang et al. [33], we do\nnot consider compound actions like goto[url], as complex action spaces can hinder performance. For\na detailed definition of each action, see the agent prompt in Appendix D.1.\n4. Scaling Test-Time Interaction: A New Dimension of Agent Scaling\nPrior methods for LLM test-time compute scaling usually scale the number of thinking tokens at each\nstep [73, 78‚Äì80], but this does not enable an agent to engage in longer interactions with the environment\nto collect new information. In principle, scaling the maximum number of interaction steps should allow\nthe agent to employ richer behavioral strategies such as re-attempts, backtracking, and recovery. We will\nnow verify this hypothesis via controlled experiments on WebArena [2]. We will then build upon these\ninsights to develop TTI, an online RL method to explicitly train agents to optimize test-time interactions.\nControlled experiment setup. We choose WebArena [2] as our testbed, primarily because it enables\nreproducible interaction with diverse domains (OneStopShop, Reddit, GitLab, CMS, and OpenStreetMap)\nand is equipped with ground truth evaluators. We randomly sample 62 tasks for testing and reserve the\nremaining 750 for online training (see Section 5.1). To ensure sufficient interaction, we set a generous\ntest-time limit of ‚Ñé= 30, which is well above the average length of around 6 steps required by most\ntasks [56, 81]. Note that experiments in this section are for analysis rather than benchmarking purposes.\nWe will show more experimental results on multiple benchmarks in Section 6.\nTable 1: Base results av-\neraged over three runs.\nPrompt\nTask SR (%)\nAction Only\n14.76\nCoT\n23.81\nTo study the effect of increasing ‚Ñé, we use a simple prompting-based agent with\nGemma 3 12B [82] base model, which observes the web page and outputs an\naction via a single model call. It does not leverage any retrieval, verifiers, or other\nexternal modules, ensuring any performance gains come solely from increased\n‚Ñébut not auxiliary scaffolding. We also study whether it is beneficial to prompt\nthe agent to generate a reasoning trace before acting (see Appendix D.1 for the\ntemplates). As Table 1 shows, CoT prompting yields significantly higher task success rate (SR) than\ndirect action generation, setting a baseline of 23.81% on the test tasks. While we also tried more complex\nprompts that explicitly ask the agent to summarize the state, assess progress, and reflect, they did not\nyield significant gains (see Appendix D.3). We thus adopt a simple chain-of-thought (CoT) approach as\nthe default prompting strategy for experiments in this section.\n4\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n4.1. Scaling Test-Time Interaction by Acting Longer\nFigure 2: Scaling test-time interaction by prompting the agent to\n‚Äúre-check‚Äù its answer. More re-checks lead to longer trajectories (dots)\nand higher task success rates (bars), indicating at least some correlation\nbetween acting for longer and higher success rates.\nTo study the impact of test-time interaction\nscaling, we introduce a purely inference-\ntime ‚Äúcheck-again‚Äù mechanism: after the\nagent issues the task completion action, we\nexplicitly prompt it to reconsider its decision\nby ‚ÄúYou just signaled task completion. Let‚Äôs\npause and think again...‚Äù We can extend\nre-checking from double-check (two passes)\nto triple-check (three passes) and beyond,\nusing slightly varied prompt phrasings for each pass. Detailed prompts are in Appendix D.4.\nAs shown in Figure 2, prompting the agent to re-check not only increases the actual interaction length\n‚Ñéùë†ùë°ùëúùëù(line plots), but also improves the success rates on most WebArena domains (bar plots). When\nbeing asked to ‚Äúcheck-again‚Äù, the agent either reaffirms its decision (e.g., ‚ÄúI previously stated the driving time\nwas approximately 30 minutes....30 minutes seems plausible with typical traffic conditions. I‚Äôll stick with my previous\nanswer.‚Äù) or revises it upon reflection (e.g., ‚ÄúMy apologies. I jumped to a conclusion prematurely. Although\nthe address book *displays* the desired address, the task requires me to *change* it....I should click button [24] to\nmodify the address.‚Äù). In particular, it changes its action ‚àº25% of the time after double-checking. This\nhighlights the potential of interaction scaling: when given sufficient time, the agent is likely to explore\nalternatives before reaching an answer. The chances of the answer being correct could thus be higher.\nHowever, we do observe that repeatedly prompting the agent to re-check can sometimes lead to confusion\nor hallucination, causing it to revise correct answers into incorrect ones. This may explain the performance\ndrop observed in domains like Map. This is perhaps an inevitable limitation of scaling test-time interaction\nvia prompting alone, akin to how prompting is not an effective way to even scale per-step test-time compute\n(see self-correction results for prompted models in Qu et al. [83]). We discuss this limitation further in\nSection 4.2 and address by training the agents explicitly.\n4.2. Scaling Test-Time Interaction vs. Per-Step Test-Time Compute\nNext, we examine the effect of scaling interaction compared to scaling per-step reasoning: Given a total\ntoken budget, should agents prioritize more interaction steps or generating longer reasoning traces at\neach step? To explore this, we study two conventional test-time compute scaling methods.\nPer-step budget forcing. Following Muennighoff et al. [27], we prompt the agent to ‚Äúwait and think\nagain‚Äù after generating an initial CoT, encouraging more intermediate reasoning before it commits to an\naction. We vary the number of forced waits from 1 to 4. Despite trying various prompts to induce longer\nthinking from the agent (see Appendix D.4), our agent changes its actions only 15% of the time, and most\nof these changes involve reordering subtasks rather than exploring alternative paths or error correction.\nPer-step best-of-ùëõ. At each step, we sample ùëõ‚àà{3, 5, 7} candidate actions and select one via majority\nvoting, similar to [14]. We did not scale ùëõup further because best-of-ùëõis compute-intensive for multi-step\nsettings (ùëõ¬∑ ‚Ñémore expensive than the baseline per rollout), and we observe diminishing returns from\nsampling more actions, despite tuning sampling hyperparameters such as the temperature.\nFinding 1: Gaining new information via interaction beats thinking more a single step. Figure 3 (top)\nplots the task success against total compute, measured by the number of tokens per trajectory in log\nscale. Among the three strategies, interaction scaling (green) shows the steepest upward trend, achieving\n5\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nFigure 3: Top: Task success rate vs. total compute cost\n(measured in tokens per trajectory, log scale). Interaction\nscaling consistently achieves the highest success rate for a\ngiven compute budget. Bottom: Decomposition of com-\npute into tokens per step and number of interaction steps\n(excluding the initial point for each approach). Interaction\nscaling achieves superior performance by distributing com-\npute across more steps with lower per-step cost, in contrast\nto methods that put more tokens in fewer steps.\nthe highest success rate as the allowed token bud-\ngets increase. Budget forcing (blue) yields moderate\ngains but plateaus around 0.26. Despite incurring\nthe highest cost, best-of-ùëõ(orange) brings the least\nimprovements, suggesting that repeatedly sampling\nactions per step is a less effective use of compute in\ninteractive tasks.\nA natural question that arises here is: how should we\ndistribute a bounded compute budget between running\nmore interaction steps vs. reasoning longer? These two\ndimensions present different costs per unit, which\nmay not be known apriori. Hence, we illustrate how\nscaling the number of interaction steps and the num-\nber of tokens per step affect performance and the total\nnumber of tokens individually. Figure 3 (bottom) de-\ncomposes total compute into tokens per step (y-axis)\nand steps per rollout (x-axis), so the shaded areas\nindicate the average total compute required per tra-\njectory. Interaction scaling extends along the x-axis,\nwhile per-step reasoning scales along y-axis. We find\nthat scaling across steps is more effective than scal-\ning within steps in WebArena tasks (Figure 3, top),\nlikely because the former enables the agent to gather\nnew information and enrich its context. This ability\nto query and observe external feedback is unique to\nagentic settings but not single-turn question-answering tasks. While standard per-step reasoning is con-\nstrained by the information already available at each step, our approach takes advantage of this dynamic\ninteraction. However, in principle, one can combine more reasoning per-step with more interaction, at\nthe cost of spending many tokens.\nFinding 2: Prompting alone is insufficient for interaction scaling. While our results highlight the po-\ntential of scaling interaction, the ‚Äúcheck-again‚Äù strategy only allows the agent to revisit its behavior\nupon task completion, it does not enable it to implement nuanced behaviors such as switching between\nexploration and exploitation in the middle of a rollout. We also experimented with combining interaction\nscaling with budget forcing and best-of-ùëõ(Appendix Table 6) and observe that simply increasing test-time\ncompute via prompting does not yield additive gains. In fact, the final downward trend in Figure 3\n(top) suggests that asking the agent to re-check too many times or think for too long can confuse it and\ndegrade performance. This shows the need for methods that train agents to optimize for best behavior\nwhen scaling test-time interaction, rather than na√Øve prompting.\nTakeaways: Scaling test-time interaction vs. test-time compute\nUnder a fixed compute budget (measured by total tokens), gaining information through longer interaction\nwith the environment can be more effective than solely deepening per-step reasoning. While longer chain-\nof-thought can improve local decision quality, interaction scaling offers a complementary and often more\ncompute-efficient way to enable agents to adapt and explore over longer horizons.\n6\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n5. TTI: Curriculum-Based Online RL for Scaling Interaction\nHow can we extend beyond prompting-based scaling to training agents that can effectively utilize\ninteraction scaling? A natural starting point is to draw inspiration from current approaches for optimizing\ntest-time compute [30, 71, 72] and extend these ideas to interactive settings. Specifically, we can run\nreinforcement learning (RL) with binary task rewards and longer task horizons. However, is this approach\nsufficient? We first describe the key challenges in learning to scale test-time interaction, and then develop\nour approach to address them via curriculum learning.\n5.1. Challenges of Training Agents with Fixed, Long Horizons\nA natural way to encourage the agent to learn to take more steps is to train at long horizons. To study\nthis, we can run the simplest form of REINFORCE [84] with binary rewards ùëÖ(¬∑), also known as online\nfiltered behavior cloning (BC) or online STaR [20, 85]. Only successful trajectories are retained, and the\nagent is updated by maximizing the log-likelihood of actions conditioned on those high-reward rollouts:\narg max\nùúÉ\nEùíØ‚àºtasks\n‚é°\n‚é¢‚é¢‚é¢‚é¢‚é£\nEùëú0:‚Ñé,ùëé0:‚Ñé‚àí1‚àºùúã(¬∑|ùíØ)\n‚é°\n‚é¢‚é¢‚é¢‚é¢‚é£\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éù\n‚Ñé‚àí1\n‚àëÔ∏Å\nùë°=0\nlog ùúãùúÉ(ùëéùë°| ùëú‚â§ùë°, ùíØ)\nlikelihood of trajectory\n‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚é†\n¬∑ 1\n‚é°\n‚é£ùëÖ(ùëú0:‚Ñé, ùíØ)\nreward\n= 1\n‚é§\n‚é¶\n‚é§\n‚é•‚é•‚é•‚é•‚é¶\n‚é§\n‚é•‚é•‚é•‚é•‚é¶\n(5.1)\nWe use filtered BC as it is stable throughout training (no negative gradient [86]), has been utilized pre-\nviously for training agents [20], and is a good ‚Äúfirst-choice‚Äù algorithm for studying the role of interaction\nscaling. We scale the agent‚Äôs horizon on WebArena, varying ‚Ñé‚àà{5, 10, 20}. Smaller ‚Ñéexposes the agent\nonly to exploitative rollouts that succeed within allowed time steps, while larger ‚Ñéalso includes more\nexploratory rollouts. We use the non-test tasks for rollout. Details are in Appendix D.5.\nAs shown in Figure 4 (left), the agent trained with ‚Ñé= 5 learns quickly, likely because on-policy RL\nis more sample-efficient at smaller horizons, but it also quickly overfits, and performance decreases\nwith more training (x-axis)2. This agent often terminates prematurely during evaluation despite being\nallowed to interact for much longer time. Conversely, agents trained at longer horizons generally learn\npolicies that are quite stochastic and learn significantly more slowly due to higher variance of the loss and\ncredit assignment challenges due to longer horizons [e.g., 87‚Äì89]. We manually inspect the trajectories\nand find that the ‚Ñé= 20 agent tends to associate exploratory actions such as ‚Äúgoing back‚Äù or ‚Äútrying\nrandom links‚Äù with high rewards initially. Noisy credit assignment with ‚Ñé= 20 slows learning, and only\nafter several iterations do the agents begin to recover and produce more robust policies. The impact\nof horizon is domain-dependent: in complex domains requiring exploration (e.g., CMS), long-horizon\nagents outperform, while in simpler settings (e.g., Reddit), performance differences are minimal. We\nalso note that the total number of tokens generated per action slightly decreases throughout training,\nindicating that the training does not incentivize increasing the length of reasoning directly.\nImportantly, although the interaction length increases as expected for ‚Ñé= 20 (Figure 4, right), noisy\ncredit assignment and slower learning suggests that simply setting ‚Ñéto be large is insufficient to learn\nto scale test-time interaction reliably. These observations motivate our method‚Äôs core idea: rather than\nfixing the horizon throughout training, our approach aims to scale their interaction length dynamically.\n2We use the same training hyperparameters (Appendix D.5) across all settings for fair comparison. While reducing the\nlearning rate can help reduce performance drop, it does not address the core issue with small horizons: fewer successful rollouts\nand shorter trajectories lead to significantly less training data.\n7\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nFigure 4: Online RL with different values of maximum interaction horizon. Left: success rates for different domains. ‚ÄúHarder‚Äù\nmeans generally lower success rate. Right: average rollout length (‚Ñéstop) on the evaluation set.\n5.2. Our Approach: Curriculum Over Interaction Horizon\nTo address these challenges, we propose TTI (Test-Time Interaction), a curriculum-based online RL\napproach that trains the agent with short trajectories initially and gradually exposes it to longer ones.\nExisting curriculum learning methods in RL [e.g., 90‚Äì94] or web agents [23, 65] typically prioritize easier\ntasks before harder ones, often relying on predefined heuristics or external measures of task complexity.\nIn contrast, we define curriculum progression in terms of the maximum number of steps an agent is\nallowed per trajectory. While interaction length can serve as a rough proxy for task difficulty, our approach\ndoes not require explicit labeling or estimation of task complexity.\nHow do we design a curriculum over the interaction horizon ‚Ñé? Ideally, the curriculum should allow\nthe agent to first learn basic, ‚Äúatomic‚Äù skills to solve easier tasks, then progressively tackle complex\nones via skill chaining and exploration. To enable this kind of behavior, we explored two strategies:\n(1) a conservative, additive increase in ‚Ñéper iteration, giving the agent sufficient time to solidify core\ntask-solving skills; and (2) a more aggressive, multiplicative increase, which assumes the agent can quickly\nacquire the basic skills and benefit from earlier exposure to exploratory behavior. Formally, for iteration ùëñ:\n‚Ñéùëñ:= clip(‚Ñémin + ùëñ, ‚Ñémax)\n(Additive curriculum)\n(5.2)\n‚Ñéùëñ:= clip(‚Ñémin ¬∑ ùëñ, ‚Ñémax)\n(Multiplicative curriculum)\n(5.3)\nWe store the rollouts in a replay buffer and assign higher weights to more recent trajectories. The full\npseudocode for TTI and implementation details are provided in Appendix C.\nTable 2: Comparing vari-\nous curricula. A multiplica-\ntive curriculum produces the\nbest success rate.\nSchedule\nTask SR (%)\nAdditive\n29.50\nMultiplicative\n32.25\nEmpirical insights. We instantiate these two strategies in WebArena, using\nthe non-test tasks for online training. We set ‚Ñémin to 10 and ‚Ñémax to 30, and\napply the schedules on top of filtered BC. Evaluation results after 10 iterations\nare shown in Table 2. Multiplicative curriculum outperforms the additive\none, possibly because it exposes the agent to longer horizons early on and\nhelps prevent it from overfitting prematurely to shortcut behaviors like always\ntaking the shortest path. Based on these findings, we adopt the multiplicative\ncurriculum as the default for TTI. Table 2 further shows that even with limited data (‚àº700 training\ntasks), TTI outperforms fixed ‚Ñé= 20 in Figure 4 by nearly 3%, using 40% fewer training steps over 10\niterations. Next, we demonstrate that this advantage carries over to large-scale online RL training.\nTakeaways: Curriculum training enables effective interaction scaling\nOur approach, TTI, gradually scales the horizon using a curriculum, leading to better performance than\nfixed-horizon baselines. The multiplicative schedule improves both learning efficiency and task success rate.\n8\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n6. Experiments: Scaling Up to Realistic Benchmarks\nWe now provide a comprehensive evaluation of TTI in large-scale, realistic environments. More generally,\nwe demonstrate the effectiveness of training models to make good use of test-time interaction.\nFigure 5: Summary of our main results on WebVoyager and WebArena benchmarks.\nTTI consistently outperforms fixed-horizon training with both short and long horizons.\nSynthetic task generation\nand evaluation. To enable\nlarge-scale\ntraining\nwith-\nout training on the bench-\nmark itself, we adopt syn-\nthetic task generation in-\nspired by PAE [19]. Apart\nfrom prompting LLMs with\nseed examples or demos, we\nalso deploy an exploratory\nagent to freely interact with\nwebsites and propose more\ngrounded, diverse tasks. For\nevaluation, we leverage a\nprompting-based verifier that\nuses action histories and\nscreenshots to label rollouts, achieving 88.9% accuracy compared to WebArena‚Äôs ground-truth evaluator.\nDetails and prompt templates are in Appendix E.1. We evaluate agents on (1) WebVoyager [1] with 427\ntasks across 13 domains (we replace Google search with Bing due to ReCaptcha issues); and (2) full\nWebArena [2] with 812 tasks. We choose these benchmarks as they are widely used [e.g., 4].\nTraining. We obtain 128K synthetic tasks across diverse real-world domains for WebVoyager and 11K\ntasks for WebArena‚Äôs self-hosted domains (data released with the code). We train separate agents for each\nbenchmark to avoid contaminating real-domain agents with synthetic environment data, as WebArena\ndomains still have many differences from their real-world counterparts. We use Gemma 3 12B [82] as\nthe base model, sampling 512 tasks per iteration for rollouts and updating with 512 successful on-policy\ntrajectories. We apply a multiplicative curriculum with ‚Ñémin = 10 and ‚Ñémax = 30. We use vLLM [95] to\nsample rollouts and use DeepSpeed Zero 3 [96] with NVIDIA H100 GPUs for training. The evaluator\nis a Gemma 3 27B model, prompted to detect successful trajectories, which can then be used for the\nonline filtered BC procedure. Other hyperparameters such as the number of iterations, learning rate,\nand the exact schedule of TTI can be found in Appendix E.3.\nComparisons. We evaluate zero-shot Gemma 3 12B and approaches that utilize a fixed horizon with\n‚Ñé‚àà{10, 30}. We also compare to closed-source agents (e.g., those based on GPT-4 [47] and Claude [48]),\nopen-weight models trained on proprietary data (e.g., UI-TARS [97]), and fully open-weight, open-data\nmodels (e.g., PAE [19]). A detailed list of the prior approaches can be found in the result tables.\n6.1. WebVoyager Results and Analysis\nState-of-the-art open-weight, open-data performance. We report the overall task success rates (SR)\non WebVoyager in Table 3 and Figure 5 (left). The TTI-trained Gemma 3 12B achieves an average\nSR of 64.8%, setting a new state-of-the-art among open agents trained purely on public data. While\nprevious methods such as UI-TARS achieves a strong SR of 84.8%, they rely on private human-annotated\n9\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nTable 3: WebVoyager results. Training a Gemma3 12B model using TTI attains the best performance among open-weight agents\ntrained on open-source synthetic data in aggregate on the WebVoyager benchmark. Baseline results are taken from Zhou et al.\n[19], Qin et al. [97], and illustrate that TTI improves over the best prior open-model, open-source data approach by 30%.\nModel\nAverage Allrecipes Amazon Apple ArXiv GitHub ESPN Coursera Cambridge BBC Map Search HuggingFace WolframAlpha\nProprietary Model\nClaude 3.7\n84.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nClaude 3.5\n50.5\n50.0\n68.3\n60.4\n46.5\n58.5\n27.3\n78.6\n86.0\n36.6 58.5\n30.2\n44.2\n66.7\nOpenAI CUA\n87.0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nAgent E\n73.1\n71.1\n70.7\n74.4\n62.8\n82.9\n77.3\n85.7\n81.4\n73.8 87.8\n90.7\n81.0\n95.7\nOpen Model, Proprietary Human-Annotated Data\nUI-TARS-1.5\n84.8\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nOpen Model, Open Synthetic Data\nLLaVa-34B SFT\n22.2\n6.8\n26.8\n23.3\n16.3\n4.9\n8.6\n26.8\n67.4\n16.7 12.2\n23.3\n20.9\n38.1\nPAE-LLaVa-7B\n22.3\n14.3\n37.5\n17.5\n19.0\n14.6\n0.0\n33.3\n52.4\n18.6 22.5\n23.3\n19.0\n24.4\nPAE-LLaVa-34B\n33.0\n22.7\n53.7\n38.5\n25.6\n14.6\n13.6\n42.9\n74.4\n39.0 22.0\n18.6\n25.6\n42.9\nGemma 3 12B\n55.8\n25.7\n32.3\n45.5\n60.6\n54.8\n60.6\n56.3\n69.6\n65.6 54.8\n72.7\n66.7\n61.1\nFixed ‚Ñé= 10\n59.1\n25.7\n74.1\n51.5\n75.7\n70.9\n44.1\n59.3\n66.7\n50.0 41.9\n60.6\n75.5\n72.2\nFixed ‚Ñé= 30\n45.2\n20.0\n41.9\n60.6\n42.4\n41.9\n50.0\n34.4\n60.6\n25.0 29.0\n63.6\n45.5\n69.4\nTTI (Ours)\n64.8\n57.1\n48.3\n69.6\n66.6\n45.2\n56.3\n46.9\n85.2\n81.2 66.7\n72.7\n75.7\n79.4\ndata that remains inaccessible to the open-source community. In contrast, TTI is trained entirely on\nsynthetic data and interestingly, this data is generated by the base model (Gemma 3 12B) itself, meaning\nthat our training protocol implements a form of self-improvement. This shows the practicality of our\napproach, but also highlights the need for future work on developing high-quality open data comparable\nto human-generated ones. TTI also obtains the highest SR in 8 out of 13 domains, illustrating its efficacy.\nTTI outperforms fixed-horizon via adaptive exploration. Table 3 also shows that our curriculum ap-\nproach outperforms fixed ‚Ñé= 10 baseline by 5.7% and fixed ‚Ñé= 30 baseline by 19.6% in average accuracy.\nTo better understand the use of interaction within a training rollout, we plot the average number of interac-\ntion steps on a held-out validation set with 78 tasks in Figure 6 (a). Note that the agent trained with ‚Ñé= 10\nlearns to continuously reduce the maximum number of steps it spends in a rollout, while ‚Ñé= 30 quickly\ndrifts into aimless exploration and executes a larger number of steps pre-maturely in training, hindering\nperformance. This aligns with our findings in Section 5.1 that simply running training at a longer horizon\nmay not be sufficient for obtaining effective interaction scaling. In fact, we find that when training with\nTTI, the interaction length of the agent‚Äôs rollouts first decreases but then starts to increase as the maximum\nallowed horizon increases, indicating that an adaptive curriculum enables effective interaction scaling.\nFigure 6 (d) shows that the task success rate also grows over time and correlates with the expanding\nhorizon. While the average task success rates for TTI are better, we observe notable per-domain differences.\nFigure 6 (e) shows representative per-domain success rates. On domains like Allrecipes and Cambridge,\nTTI significantly outperforms fixed-horizon and zero-shot approaches, improving success rates by 31.4%\nand 15.6%, respectively, likely because these domains are highly information-dense and benefit from\nextended exploration enabled by adaptive interaction scaling. However, in domains like Amazon and\nGitHub, TTI underperforms the baselines. We notice that the base model already has strong knowledge\nabout domain-specific terminologies (e.g., commit history, forks, stars) in these domains, likely because\nthey are more prevalent than the others, resulting in high base performance. Inspecting the rollouts,\nwe find that instead of using built-in filters and sorting, TTI can engage in exploration behaviors such\nas initiating Bing searches or consulting external sites. This exposes the agent to noisy or distracting\ninformation, reducing task success. We discuss these cases in Section 6.2.\nLearning dynamics of TTI. To study how TTI enhances the ‚Äúwithin-rollout‚Äù exploration capabilities of\n10\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n15\n10\n5\n0\n5\n10\n15\nDifference (%)\na\nMax Horizon\n0 1\n5\n10\nIter\nTrajectory Length (Centered)\nTTI\nFixed h = 10\nFixed h = 30\n15\n10\n5\n0\nb\nMax Horizon\n0 1\n5\n10\nIter\nGoBack & Bing (Centered)\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\nc\nMax Horizon\n0 1\n5\n10\nIter\nTokens Per Step (Centered)\n50.0\n52.5\n55.0\n57.5\n60.0\nSuccess Rate (%)\nd\nMax Horizon\n0\n5\n10\nIter\nTTI Average Success Rate\n0\n25\nRelative SR (%)\n0\n5\n10\nIter\nAllrecipes\n0\n20\nRelative SR (%)\n0\n5\n10\nIter\nCambridge Dictionary\n0\n20\nRelative SR (%)\n0\n5\n10\nIter\nHuggingface\n10\n0\nRelative SR (%)\n0\n5\n10\nIter\nAmazon\ne\nFigure 6: Dynamics of TTI during training. For TTI, the green area represents the phase where the maximum allowed\ninteraction horizon is the largest (‚Ñé= 30), per our multiplicative schedule. All results are evaluated on a held-out subset of\nWebVoyager, not on the training tasks. a: Average trajectory length, i.e. the average number of steps taken in a trajectory\nnormalized by the average length at the first iteration (iteration 0). b: Ratio of the sum of GoBack and Bing actions out of all\nactions normalized by the first iteration. c: The average number of tokens each action uses, i.e., the average CoT length during\nagent reasoning. d: Average task success rates for TTI on the held-out validation set of tasks. e: Per-domain success rates for\nTTI. All results are evaluated on a held-out subset of WebVoyager tasks. Observe that TTI learns to utilize more interaction\nsteps and explores by calling GoBack and Bing actions once the maximum allowed horizon increases to peak value (i.e., in the\ngreen shaded area). The number of tokens appearing every step reduces for TTI compared to the initialization, resulting in\nsignificantly shorter CoT compared to the run with ‚Ñé= 10.\nthe agent, we measure the number of GoBack and Bing actions over the course of training. GoBack actions\nmeasure the number of retries the agent makes within an episode to get unstuck during exploration.\nBing actions correspond to the number of times the agent attempts to seek information by moving to\nbing.com. As shown in Figure 6 (a, b, and d), the performance of TTI improves substantially as the\nnumber of GoBack and Bing actions and the trajectory length grow.\nAlso note that the trajectory length and the numbers of GoBack and Bing actions begin to increase with\nTTI, once the maximum allowed horizon length is increased as a part of the curriculum schedule (this\nregime is shown by the green shaded area in Figure 6). In contrast, these quantities continuously decrease\nover the course of training for the run with a lower number of maximum interaction steps (‚Ñé= 10). We\nalso find that the trajectory length shoots up substantially for the run with ‚Ñé= 30 and this correlates\nwith worse performance. Finally, as shown in Figure 6 (c) we also note that as the agent‚Äôs trajectory\ngrows longer with TTI, the number of tokens appearing in per-step reasoning actually becomes smaller.\nThis implies that our agent is automatically learning to tradeoff interaction for per-step compute in order\nto attain higher performance, and perhaps prevents any issues with overthinking.\n11\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n6.2. Case Studies: Strengths and Failure Modes\nWe conduct detailed case studies to analyze how TTI behaves across tasks and domains. These cases\nhighlight both the strengths and remaining limitations of our approach.\nStrength: Effective exploration in complex tasks (example visualized in Appendix E.4). For complex,\nexploratory tasks that require information retrieval, TTI trains the agent to extend its interaction horizon\nthrough search and backtracking, thus gathering and comparing information before making decisions.\nFor instance, when tasked to find the baking temperature of an apple pie recipe with 4+ stars and 50+\nreviews, our agent first selects a recipe but encounters a pop-up it cannot dismiss due to backend issues.\nIt then tries another recipe but finds no baking temperature. Returning to the listing again, it correctly\nidentifies one that meets all criteria. We also observe that such behaviors emerge progressively. In early\ntraining with shorter horizon, TTI-agent tends to stick to the first recipe it finds, keeps retrying it and\nsaying ‚ÄúI remember seeing one with 613 ratings earlier‚Äù instead of seeking alternatives. Only after the\ntraining horizon becomes longer does it learn to explore and backtrack. This illustrates that when TTI\nruns a curriculum over interaction length, it teaches agents to adjust their horizon within a task and shift\nfrom exploitation to exploration. In contrast, training with a fixed short horizon can make it difficult to\ndevelop such exploratory behaviors.\nStrength: Strategic exploitation in simple tasks (Appendix E.5). For simpler tasks with clear, deter-\nministic paths (e.g., form filling or direct lookups), TTI-agent completes tasks efficiently without over-\nexploration. For example, when instructed to find the ‚Äútop trending open-source project on machine\nlearning‚Äù in GitHub, the agent goes directly to the Open-Source menu, selects the Trending tab, and\nperforms search. This shows that TTI balances exploration and exploitation based on task context.\nDespite these strengths, we also observe characteristic failure modes that point to areas for improvement\nand may partly explain the agent‚Äôs lower performance on domains like GitHub.\nFailure mode: over-reliance on resets (Appendix E.6). When an action fails, our agent can reset the\ntask by returning to the Bing search page rather than attempting recovery within the target domain. This\nsuggests the agent treats search as a universal fallback, even when more domain-specific actions (e.g.,\nrevisiting menus, refining filters) would be more effective. We also observe repeated resets within the\nsame trajectory, indicating a lack of adaptive error recovery. While agents can extend horizons through\nboth resetting and backtracking, the latter is often more natural. This highlights an area where TTI could\nimprove by guiding exploration more systematically and enforcing structure. We believe that using dense\nrewards [69, 72] or running full multi-turn RL with value functions [98] may address this issue.\nFailure mode: limited self-verification (Appendix E.7). We also observe that the agent can fail to verify\nits actions against the task goal, especially in the last step. In one case, the agent identifies a 2021 GitHub\nrepository for a task requiring one from 2022. While it explicitly acknowledges the mismatch, ‚ÄúIt was\ncreated in 2021, not 2022, so it doesn‚Äôt meet the criteria‚Äù, it still submits it as the answer. This implies\nlimited self-verification ability and could be mitigated by longer, more deliberate per-step reasoning. An\nimportant next step is to enrich TTI with the cognitive behaviors that enable per-step reasoning [99].\n6.3. WebArena Results and Analysis\nWe further assess TTI on the full WebArena [2] (we only use the prompting-based verifier for training,\nbut use the original benchmark evaluators for evaluation). As shown in Table 4 and Figure 5 (right), TTI\nobtains the highest performance among open-source agents trained entirely using a self-improvement\n12\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nTable 4: Full WebArena results. For proprietary agents, we include the top 8 from the official leaderboard. Observe that TTI\nattains the best performance on an average, with especially better performance on the CMS domain.\nMethod\nBackbone\nAverage\nShopping\nCMS\nReddit\nGitLab\nMaps\nProprietary-Based\nIBM CUGA [100]\n-\n61.7\n-\n-\n-\n-\n-\nOpenAI CUA [4]\n-\n58.1\n-\n-\n-\n-\n-\nJace AI [101]\n-\n57.1\n-\n-\n-\n-\n-\nScribeAgent [14]\nGPT-4o + Qwen2.5 32B\n53.0\n45.8\n37.9\n73.7\n59.7\n56.3\nAgentSymbiotic [18]\nClaude 3.5 + Llama 3.1 8B\n48.5\n48.7\n41.2\n63.2\n47.2\n57.8\nLearn-by-Interact [64]\nClaude 3.5 Sonnet\n48\n-\n-\n-\n-\n-\nAgentOccam-Judge [33]\nGPT-4\n45.7\n43.3\n46.2\n67.0\n38.9\n52.3\nWebPilot [34]\nGPT-4o\n37.2\n36.9\n24.7\n65.1\n39.4\n33.9\nFully Open-Source\nLearn-by-Interact [64]\nCodestral 22B\n24.2\n-\n-\n-\n-\n-\n(Self-Improvement)\nAgentTrek [37]\nQwen2.5 32B\n22.4\n-\n-\n-\n-\n-\nAutoWebGLM [65]\nChatGLM3 6B\n18.2\n-\n-\n-\n-\n-\nNNetnav [102]\nLlama 3.1 8B\n7.2\n7.4\n4.2\n0\n0\n28.5\nZero-Shot Baseline\nGemma 3 12B\n18.3\n26.7\n8.7\n30.9\n5.5\n27.7\nFixed ‚Ñé= 10\nGemma 3 12B\n23.8\n28.4\n15.6\n26.0\n13.2\n34.7\nFixed ‚Ñé= 30\nGemma 3 12B\n19.0\n25.7\n9.7\n29.8\n8.7\n28.57\nTTI (Ours)\nGemma 3 12B\n26.1\n33.9\n15.5\n35.3\n15.7\n40.5\nprocedure on self-generated data, without relying on proprietary models for task completion or distillation.\nWhile TTI improves over the zero-shot baseline by 7.8%, the gains are smaller than on WebVoyager,\npossibly because: (1) WebArena tasks are more complex, as reflected in lower accuracies even for\nproprietary models, leading to fewer successful rollouts per iteration and slower learning; (2) Agents\noccasionally attempt actions that are valid in real-world domains but invalid in WebArena counterparts\n(for example, search works reliably on Reddit but fails in WebArena‚Äôs Postmill due to environment bugs).\nMore experiment details are in Appendix D.6.\nFigure 7: We apply test-time re-checks to\nTTI checkpoints. Note that inference-time\nre-checking improves performance on all TTI\ncheckpoints, and more importantly, training\nfurther with TTI improves performance more\nthan inference-time re-checks as expected.\nFurther scaling. While TTI equips agents with the ability to ad-\njust their interaction horizon during deployment, an open ques-\ntion remains: Can we further amplify performance by combining\nTTI with inference-time interaction scaling techniques such as\nre-checking as discussed in Section 4? To explore this, we ap-\nply the ‚Äúcheck-again‚Äù strategy (Section 4) to intermediate TTI\ncheckpoints. Due to the high evaluation cost associated with\nevaluating on full WebVoyager or WebArena, we leverage the\nWebArena subset checkpoints obtained in Section 5.2.\nAs shown in Figure 7, applying re-checking on top of TTI improves\ntask success across various training stages. The benefits are more\nobvious in the early stages of training, when the agent has a\nstronger bias to terminate prematurely. As training progresses,\nTTI encourages longer interaction traces that naturally incorpo-\nrate behaviors like re-checking, reducing the added benefit of explicit re-checks. Nonetheless, even in\nlater stages, re-checking does continue to provide modest gains in performance, serving as a safety-check\nfor well-trained agents.\n13\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n7. Conclusion and Future Work\nIn this work, we introduced interaction scaling as a new dimension of test-time scaling for interactive\nagents. Through empirical studies on web agents, we validate that interaction scaling enables agents to\nexplore and adapt dynamically, significantly improving task performance. Despite the promising results,\nthere are a number of avenues for future work which we list below.\n‚Ä¢ Extension to other agentic problem domains. While we only study the effects of interaction\nscaling on web environments, we believe that this procedure is likely going to be even more effective\nin domains that admit more stochasticity and uncertainty, such as robotic control or open-world\ncompute use problems. In these settings, solving tasks would require gather information first before\nattempting to solve the task. By utilizing interaction as an effective tool for information gathering,\nwe can attain much better performance.\n‚Ä¢ Balancing the tradeoff between thinking more and acting more. While we illustrate that\nspending a given amount of total token budget on acting for longer can be more effective than\nreasoning for longer, an interesting open question is how we should incentivize RL training to\nbest balance thinking and acting during the process of learning. In our experiments, we observed\nan increased preference towards acting longer, with the number of tokens in per-step reasoning\ndecreasing compared to the initialization. While this results in better use of total overall test-time\ncompute given our results in Section 4, it is unclear as to what a general thumb rule and training\nprocedure should be to attain the best tradeoff between acting longer and thinking more.\n‚Ä¢ Improved RL algorithms for powering interaction scaling. The RL approach we utilize in this\nwork is extremely simple as it corresponds to online filtered behavior cloning (BC). An immediate\nnext promising direction is to extend TTI to utilize negative gradients [86] via GRPO [78] or\nPPO [103]. However, stochasticity and, even non-stationarity, in interactive agent environments\nsuggests that RL algorithms that train value functions [21, 98] are likely to be more successful\nat effective credit assignment as the task horizon is scaled further. In addition, we will need to\naddress challenges pertaining to memory and context length, that is very likely to overflow as we\nscale horizon even further. Tackling any of these challenges would be exciting for future work.\nAcknowledgments\nWe thank Jiayi Pan, Shanda Li, and Yuxiao Qu for feedback and informative discussions on an earlier\nversion of this paper. This work was supported in part by the National Science Foundation grants\nIIS1705121, IIS1838017, IIS2046613, IIS2112471, the Office of Naval Research under N00014-24-1-\n2206, and funding from Meta, Morgan Stanley, Amazon, Google, and Scribe. We specially thank Scribe,\nThe AGI company, and CMU FLAME Center (Orchard cluster) for providing a part of the GPU resources\nthat supported this work. Any opinions, findings and recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of any of these funding agencies or\nemployers.\n14\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nReferences\n[1] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan,\nand Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models,\n2024. URL https://arxiv.org/abs/2401.13919.\n[2] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nTianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web\nenvironment for building autonomous agents. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=oKn9c6ytLx.\n[3] Claude. Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku, 2024. URL\nhttps://www.anthropic.com/news/3-5-models-and-computer-use.\n[4] OpenAI.\nIntroducing\noperator,\n2025.\nURL\nhttps://openai.com/index/\nintroducing-operator/.\n[5] Magnus M√ºller and Gregor ≈Ωuniƒç. Browser use: Enable ai to control your browser, 2024. URL\nhttps://github.com/browser-use/browser-use.\n[6] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan\nWang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents,\n2023.\n[7] Claude.\nYour code‚Äôs new collaborator, 2025.\nURL https://www.anthropic.com/\nclaude-code.\n[8] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess,\nAdnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh,\nLachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin\nLeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z.\nRen, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner,\nQuan Vuong, Homer Rich Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky.\npi0.5: a vision-language-action model with open-world generalization. 2025. URL https:\n//api.semanticscholar.org/CorpusID:277993634.\n[9] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining\nXie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-\nmaking agents via reinforcement learning. ArXiv, abs/2405.10292, 2024. URL https://api.\nsemanticscholar.org/CorpusID:269790773.\n[10] Luyao Yuan, Zipeng Fu, Jingyue Shen, Lu Xu, Junhong Shen, and Song-Chun Zhu. Emergence of\npragmatics from referential game between theory of mind agents, 2021. URL https://arxiv.\norg/abs/2001.07752.\n[11] Luyao Yuan, Dongruo Zhou, Junhong Shen, Jingdong Gao, Jeffrey L Chen, Quanquan\nGu, Ying Nian Wu, and Song-Chun Zhu.\nIterative teacher-aware learning.\nIn M. Ran-\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in\nNeural Information Processing Systems, volume 34, pages 29231‚Äì29245. Curran Associates,\n15\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nInc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/\nf48c04ffab49ff0e5d1176244fdfb65c-Paper.pdf.\n[12] Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, and Lili Yu. Mixture-\nof-mamba: Enhancing multi-modal state-space models with modality-aware sparsity, 2025. URL\nhttps://arxiv.org/abs/2501.16295.\n[13] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James\nTanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy\nGroom, Sergey Levine, and Chelsea Finn. Hi robot: Open-ended instruction following with\nhierarchical vision-language-action models. ArXiv, abs/2502.19417, 2025. URL https://api.\nsemanticscholar.org/CorpusID:276618098.\n[14] Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and\nAmeet Talwalkar. Scribeagent: Towards specialized web agents using production-scale workflow\ndata. ArXiv, abs/2411.15004, 2024. URL https://api.semanticscholar.org/CorpusID:\n274192657.\n[15] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web. In Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://\nopenreview.net/forum?id=kiYqbO3wqw.\n[16] Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, and Ameet\nTalwalkar. Cross-modal fine-tuning: align then refine. In Proceedings of the 40th International\nConference on Machine Learning, 2023.\n[17] Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolo Fusi. Tag-llm:\nRepurposing general-purpose llms for specialized domains, 2024.\n[18] Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Z.\nAgudelo, Peter Qian, and Tianlong Chen. Symbiotic cooperation for web agents: Harnessing\ncomplementary strengths of large and small llms. ArXiv, abs/2502.07942, 2025.\n[19] Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, and\nErran L. Li. Proposer-agent-evaluator(pae): Autonomous skill discovery for foundation model\ninternet agents. ArXiv, abs/2412.13194, 2024.\n[20] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Di-\ngirl: Training in-the-wild device-control agents with autonomous reinforcement learning. ArXiv,\nabs/2406.11896, 2024.\n[21] Hao Bai, Yifei Zhou, Erran L. Li, Sergey Levine, and Aviral Kumar. Digi-q: Learning q-value\nfunctions for training device-control agents. ArXiv, abs/2502.15760, 2025.\n[22] Junhong Shen and Lin F. Yang. Theoretically principled deep rl acceleration via nearest neighbor\nfunction approximation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(11):9558‚Äì\n9566, May 2021. doi: 10.1609/aaai.v35i11.17151. URL https://ojs.aaai.org/index.\nphp/AAAI/article/view/17151.\n16\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[23] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue\nYang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. Webrl:\nTraining llm web agents via self-evolving online curriculum reinforcement learning, 2025. URL\nhttps://arxiv.org/abs/2411.02337.\n[24] Claude. Claude takes research to new places, 2025. URL https://www.anthropic.com/\nnews/research.\n[25] OpenAI.\nIntroducing\ndeep\nresearch,\n2025.\nURL https://openai.com/index/\nintroducing-deep-research/.\n[26] Google Gemini. Gemini deep research, 2025. URL https://gemini.google/overview/\ndeep-research/?hl=en.\n[27] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Fei-Fei Li, Hanna Hajishirzi, Luke S.\nZettlemoyer, Percy Liang, Emmanuel J. Candes, and Tatsunori Hashimoto. s1: Simple test-\ntime scaling. ArXiv, abs/2501.19393, 2025. URL https://api.semanticscholar.org/\nCorpusID:276079693.\n[28] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws:\nAn empirical analysis of compute-optimal inference for LLM problem-solving. In The Thirteenth\nInternational Conference on Learning Representations, 2025. URL https://openreview.net/\nforum?id=VNckp7JEHn.\n[29] Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan,\nCraig Boutilier, Rishabh Agarwal, Aviral Kumar, and Aleksandra Faust. Inference-aware fine-\ntuning for best-of-n sampling in large language models. ArXiv, abs/2412.15287, 2024. URL\nhttps://api.semanticscholar.org/CorpusID:274965054.\n[30] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute\noptimally can be more effective than scaling parameters for reasoning. In The Thirteenth Interna-\ntional Conference on Learning Representations, 2025. URL https://openreview.net/forum?\nid=4FWAwZtd2n.\n[31] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and\nJie Tang. Webglm: Towards an efficient web-enhanced question answering system with human\npreferences, 2023.\n[32] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane\nGu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models,\n2024. URL https://arxiv.org/abs/2305.11854.\n[33] Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and\nHuzefa Rangwala. Agentoccam: A simple yet strong baseline for llm-based web agents, 2024.\nURL https://arxiv.org/abs/2410.13825.\n[34] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and\nautonomous multi-agent system for web task execution with strategic exploration, 2024. URL\nhttps://arxiv.org/abs/2408.15978.\n17\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[35] Junhong Shen, Tanya Marwah, and Ameet Talwalkar. UPS: Efficiently building foundation models\nfor PDE solving via cross-modal adaptation. Transactions on Machine Learning Research, 2024.\nISSN 2835-8856. URL https://openreview.net/forum?id=0r9mhjRv1E.\n[36] Junhong Shen, Kushal Tirumala, Michihiro Yasunaga, Ishan Misra, Luke Zettlemoyer, Lili Yu, and\nChunting Zhou. Cat: Content-adaptive image tokenization, 2025. URL https://arxiv.org/\nabs/2501.03120.\n[37] Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong,\nand Tao Yu. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. ArXiv,\nabs/2412.09605, 2024.\n[38] Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based web\nagents, 2024. URL https://arxiv.org/abs/2410.16464.\n[39] Zongzhe Xu, Ritvik Gupta, Wenduo Cheng, Alexander Shen, Junhong Shen, Ameet Talwalkar, and\nMikhail Khodak. Specialized foundation models struggle to beat supervised baselines, 2024. URL\nhttps://arxiv.org/abs/2411.02796.\n[40] Junhong Shen, Abdul Hannan Faruqi, Yifan Jiang, and Nima Maftoon. Mathematical reconstruction\nof patient-specific vascular networks based on clinical images and global optimization. IEEE Access,\n9:20648‚Äì20661, 2021. doi: 10.1109/ACCESS.2021.3052501.\n[41] Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, and Ameet\nTalwalkar. Codepde: An inference framework for llm-driven pde solver generation, 2025. URL\nhttps://arxiv.org/abs/2505.08783.\n[42] Boyuan Zheng, Michael Y. Fatemi, Xiaolong Jin, Zora Zhiruo Wang, Apurva Gandhi, Yueqi Song,\nYu Gu, Jayanth Srinivasa, Gaowen Liu, Graham Neubig, and Yu Su. Skillweaver: Web agents can\nself-improve by discovering and honing skills. 2025. URL https://api.semanticscholar.\norg/CorpusID:277634081.\n[43] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous\nevaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.\n[44] Junhong Shen, Mikhail Khodak, and Ameet Talwalkar. Efficient architecture search for diverse\ntasks. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[45] Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, and Ameet Talwalkar.\nNAS-bench-360: Benchmarking neural architecture search on diverse tasks. In Advances in Neural\nInformation Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2022.\n[46] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae,\nand Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for\nlarge language model agents. arXiv preprint arXiv:2403.08978, 2024.\n[47] OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774.\n[48] Anthropic. Introducing the next generation of claude, 2024. URL https://www.anthropic.\ncom/news/claude-3-family.\n18\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[49] Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for\nweb actions. In Conference on Language Modeling (COLM), 2024. URL https://arxiv.org/\nabs/2310.03720.\n[50] Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, and Ravi Kokku.\nAgent-e: From autonomous web navigation to foundational design principles in agentic systems.\nArXiv, abs/2407.13032, 2024.\n[51] Wenduo Cheng, Junhong Shen, Mikhail Khodak, Jian Ma, and Ameet Talwalkar. L2g: Repurposing\nlanguage models for genomics tasks. bioRxiv, 2024. doi: 10.1101/2024.12.09.627422. URL\nhttps://www.biorxiv.org/content/early/2024/12/11/2024.12.09.627422.\n[52] Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anu-\nmanchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents\nfor long-horizon tasks, 2025.\n[53] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava,\nCosmin Paduraru, George Tucker, Doina Precup, Feryal M. P. Behbahani, and Aleksandra Faust.\nTraining language models to self-correct via reinforcement learning. ArXiv, abs/2409.12917,\n2024.\n[54] Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language\nmodel agents. arXiv preprint arXiv:2407.01476, 2024.\n[55] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar\nprompting with memory for computer control. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=Pc8AU1aF5e.\n[56] Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv\npreprint arXiv:2409.07429, 2024.\n[57] Revanth Gangi Reddy, Sagnik Mukherjee, Jeonghwan Kim, Zhenhailong Wang, Dilek Hakkani-Tur,\nand Heng Ji. Infogent: An agent-based framework for web information aggregation. ArXiv,\nabs/2410.19054, 2024.\n[58] Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. Bagel: Boot-\nstrapping agents by guiding exploration with language. arXiv preprint arXiv:2403.08140, 2024.\n[59] Shikhar Murty, Dzmitry Bahdanau, and Christopher D. Manning. Nnetscape navigator: Complex\ndemonstrations for web agents without a demonstrator, 2024. URL https://arxiv.org/abs/\n2410.02907.\n[60] Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, and Ruslan Salakhutdinov. To-\nwards internet-scale training for agents. ArXiv, abs/2502.06776, 2025. URL https://api.\nsemanticscholar.org/CorpusID:276249229.\n[61] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Ramesh Motwani, Chelsea Finn, Divyansh Garg,\nand Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. ArXiv,\nabs/2408.07199, 2024. URL https://api.semanticscholar.org/CorpusID:271865516.\n19\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[62] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar.\nArcher: Training\nlanguage model agents via hierarchical multi-turn rl. ArXiv, abs/2402.19446, 2024. URL https:\n//api.semanticscholar.org/CorpusID:268091206.\n[63] Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. Distrl: An asyn-\nchronous distributed reinforcement learning framework for on-device control agents. ArXiv,\nabs/2410.14803, 2024. URL https://api.semanticscholar.org/CorpusID:273501605.\n[64] Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan √ñ. Arik. Learn-by-\ninteract: A data-centric framework for self-adaptive agents in realistic environments. ArXiv,\nabs/2501.10893, 2025.\n[65] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen\nZhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: A large language model-based\nweb navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pages 5295‚Äî-5306, 2024.\n[66] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally\ncan be more effective than scaling model parameters.\nArXiv, abs/2408.03314, 2024.\nURL\nhttps://api.semanticscholar.org/CorpusID:271719990.\n[67] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws:\nAn empirical analysis of compute-optimal inference for problem-solving with language models.\n2024. URL https://api.semanticscholar.org/CorpusID:271601023.\n[68] Karl Cobbe, Vineet Kosaraju, Mo Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL https:\n//api.semanticscholar.org/CorpusID:239998651.\n[69] Amrith Rajagopal Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh\nAgarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling\nautomated process verifiers for llm reasoning.\nArXiv, abs/2410.08146, 2024.\nURL https:\n//api.semanticscholar.org/CorpusID:273233462.\n[70] Amrith Rajagopal Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar.\nScaling test-\ntime compute without verification or rl is suboptimal. ArXiv, abs/2502.12118, 2025. URL\nhttps://api.semanticscholar.org/CorpusID:276422443.\n[71] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learn-\ning. ArXiv, abs/2501.12948, 2025. URL https://api.semanticscholar.org/CorpusID:\n275789950.\n[72] Yuxiao Qu, Matthew Y. R. Yang, Amrith Rajagopal Setlur, Lewis Tunstall, Edward Beeching, Ruslan\nSalakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-\ntuning. ArXiv, abs/2503.07572, 2025. URL https://api.semanticscholar.org/CorpusID:\n276928248.\n[73] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin,\nKefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu,\n20\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nLi Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in\nllm agents via multi-turn reinforcement learning, 2025. URL https://arxiv.org/abs/2504.\n20073.\n[74] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models, 2023. URL https://arxiv.org/\nabs/2210.03629.\n[75] Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu. Exact:\nTeaching ai agents to explore with reflective-mcts and exploratory learning. ArXiv, abs/2410.02052,\n2024. URL https://api.semanticscholar.org/CorpusID:273098809.\n[76] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.\nGenerative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240,\n2024.\n[77] Can Jin, Hongwu Peng, Qixin Zhang, Yujin Tang, Dimitris N. Metaxas, and Tong Che. Two\nheads are better than one: Test-time scaling of multi-agent collaborative reasoning. 2025. URL\nhttps://api.semanticscholar.org/CorpusID:277781795.\n[78] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li,\nYu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models. ArXiv, abs/2402.03300, 2024.\n[79] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan,\nWen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to\nreason with search for llms via reinforcement learning, 2025. URL https://arxiv.org/abs/\n2503.19470.\n[80] Xu Wan, Wenyue Xu, Chao Yang, and Mingyang Sun. Think twice, act once: A co-evolution\nframework of llm and rl for large-scale decision making, 2025. URL https://arxiv.org/abs/\n2506.02522.\n[81] Zora Zhiruo Wang, Apurva Gandhi, Graham Neubig, and Daniel Fried. Inducing programmatic skills\nfor agentic tasks. 2025. URL https://api.semanticscholar.org/CorpusID:277634286.\n[82] Gemma Team. Gemma 3 technical report. ArXiv, abs/2503.19786, 2025. URL https://api.\nsemanticscholar.org/CorpusID:277313563.\n[83] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching\nlanguage model agents how to self-improve. Advances in Neural Information Processing Systems,\n37:55249‚Äì55285, 2024.\n[84] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods\nfor reinforcement learning with function approximation. Advances in neural information processing\nsystems, 12, 1999.\n[85] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems, 35:15476‚Äì15488, 2022.\n21\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[86] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano\nErmon, Chelsea Finn, and Aviral Kumar.\nPreference Fine-Tuning of LLMs Should Leverage\nSuboptimal, On-Policy Data, ICML 2024.\n[87] Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M. Bayen, Sham M. Kakade,\nIgor Mordatch, and P. Abbeel. Variance reduction for policy gradient with action-dependent\nfactorized baselines. ArXiv, abs/1803.07246, 2018. URL https://api.semanticscholar.\norg/CorpusID:4043645.\n[88] Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement of\npolicy gradient estimation. Neural networks : the official journal of the International Neural Network\nSociety, 26:118‚Äì29, 2011. URL https://api.semanticscholar.org/CorpusID:2274728.\n[89] Dotan Di Castro, Aviv Tamar, and Shie Mannor. Policy gradients with variance related risk criteria.\narXiv: Learning, 2012. URL https://api.semanticscholar.org/CorpusID:3109162.\n[90] Yoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In\nInternational Conference on Machine Learning, 2009. URL https://api.semanticscholar.\norg/CorpusID:873046.\n[91] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone.\nCurriculum learning for reinforcement learning domains: A framework and survey.\nArXiv,\nabs/2003.04960, 2020. URL https://api.semanticscholar.org/CorpusID:212657666.\n[92] Xin Wang, Yudong Chen, and Wenwu Zhu. A survey on curriculum learning. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 44:4555‚Äì4576, 2021. URL https://api.\nsemanticscholar.org/CorpusID:232362223.\n[93] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher‚Äìstudent curriculum\nlearning. IEEE Transactions on Neural Networks and Learning Systems, 31:3732‚Äì3740, 2017. URL\nhttps://api.semanticscholar.org/CorpusID:8432394.\n[94] Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Joshua Tobin, P. Abbeel, and Wojciech Zaremba. Hindsight experience replay. In\nNeurIPS, 2017. URL https://api.semanticscholar.org/CorpusID:3532908.\n[95] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\n[96] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. Proceedings of\nthe 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:221191193.\n[97] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao\nLi, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native\nagents. arXiv preprint arXiv:2501.12326, 2025.\n22\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[98] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language\nmodel agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.\n[99] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, nathan lile, and Noah D. Goodman. Cognitive\nbehaviors that enable self-improving reasoners, or, four habits of highly effective stars. ArXiv,\nabs/2503.01307, 2025. URL https://api.semanticscholar.org/CorpusID:276741915.\n[100] Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, Ido Levy, Aviad Sela, Asaf Adi, and Nir\nMashkif. Towards enterprise-ready computer using generalist agent. ArXiv, abs/2503.01861,\n2025. URL https://api.semanticscholar.org/CorpusID:276775684.\n[101] JaceAI.\nAwa\n1.5\nachieves\nbreakthrough\nperformance\non\nwe-\nbarena\nbenchmark,\n2024.\nURL\nhttps://www.jace.ai/post/\nawa-1-5-achieves-breakthrough-performance-on-webarena-benchmark.\n[102] Shikhar Murty, Dzmitry Bahdanau, and Christopher D. Manning. Nnetnav: Unsupervised learning\nof browser agents through environment interaction in the wild. 2024. URL https://api.\nsemanticscholar.org/CorpusID:273162280.\n[103] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347, 2017.\n23\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nAppendices\nA. Broader Impact\nThis work contributes to the development of more adaptive and capable AI agents by introducing a\nnew test-time scaling dimension focused on interaction rather than per-step reasoning alone. While this\napproach improves robustness and generalization in open-ended environments, it also raises important\nconsiderations. Increased agent autonomy can amplify both the benefits and risks of deployment in\nreal-world systems. Moreover, agents capable of richer behaviors could be applied to sensitive domains\n(e.g., customer service, education, or automation workflows) where unintended actions could have\nlarge impacts. We encourage future work to consider ethical safeguards, interpretability tools, and\nhuman-in-the-loop designs when deploying interaction-scaled agents. Our experiments are conducted\nentirely in simulated environments, and we hope this work inspires further research on controllable and\ntrustworthy agent behavior under realistic constraints.\nB. Observation Space Design\nWe use the screenshot accompanied with the web page‚Äôs accessibility tree as our main observation. We\nstudy two versions of accessibility tree. Rich accessibility tree is modified from the WebArena code and\nlooks like:\n[21]: RootWebArea ‚ÄôDashboard / Magento Admin‚Äô focused: True; [0]: link ‚ÄôMagento Admin Panel‚Äô;\n[1]: link ‚ÄôDASHBOARD‚Äô; [2]: link ‚ÄôSALES‚Äô; [3]: link ‚ÄôCATALOG‚Äô; [4]: link ‚ÄôCUSTOMERS‚Äô; [5]: link\n‚ÄôMARKETING‚Äô; [6]: link ‚ÄôCONTENT‚Äô; [7]: link ‚ÄôREPORTS‚Äô; [8]: link ‚ÄôSTORES‚Äô; [22]: link ‚ÄôSYSTEM‚Äô; [23]:\nlink ‚ÄôFIND PARTNERS & EXTENSIONS‚Äô; [24]: heading ‚ÄôDashboard‚Äô; [9]: link ‚Äôadmin‚Äô; [10]: link ‚Äù; [25]:\nStaticText ‚ÄôScope:‚Äô; [12]: button ‚ÄôAll Store Views‚Äô hasPopup: menu; [13]: link ‚ÄôWhat is this?‚Äô; [14]: button\n‚ÄôReload Data‚Äô...\nSimple accessibility tree is modified from the PAE code and looks like:\n[1]: \"Dashboard\"; [2]: \"Sales\"; [3]: \"Catalog\"; [4]: \"Customers\"; [5]: \"Marketing\"; [6]: \"Content\"; [7]:\n\"Reports\"; [8]: \"Stores\"; [9]: \"admin\"; [12]: <button> \"All Store Views\"; [13]: \"What is this?\"; [14]:\n<button> \"Reload Data\"; [15]: \"Go to Advanced Reporting\"; [16]: \"here\";...\nRich tree contains more details such as the HTML tag and attributes like required, hasPopup compared\nto simple tree. However, it is much longer than simple tree and hence harder to optimize due to the\nincreased context length.\nC. TTI Implementation\nWe provide the pseudocode in Algorithm 1. For the replay buffer, to encourage the agent to learn from\nmore recent examples, we assign weights based on recency when sampling rollouts to update the agent:\nfor the ùëò-th trajectory added to the buffer, its weight is\nùëò\n|ùíü|.\n24\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nAlgorithm 1 TTI: Filtered Behavior Cloning with Interaction Scheduling\n1: Input: Agent policy ùúãùúÉ, Evaluator ‚Ñõ, Environment ‚Ñ∞, Learning rate ùõº, Replay buffer ùíü, Interaction\nscheduler hyperparameters ‚Ñémin, ‚Ñémax\n2: Initialize policy ùúãùúÉfrom pretrained model\n3: Initialize replay buffer ùíü‚Üê{}\n4: for each episode ùëñdo\n5:\nSet interaction horizon ‚Ñéùëñ‚Üêget_schedule(ùëñ, ‚Ñémin, ‚Ñémax)\n6:\nfor each rollout to collect do\n7:\nInitialize environment: ùë†0 ‚àº‚Ñ∞\n8:\nfor each ùë°in [1, ‚Ñéùëñ] do\n9:\nObserve current state ùë†ùë°\n10:\nPredict action ÀÜùëéùë°‚ÜêùúãùúÉ(ùë†ùë°)\n11:\nExecute action ÀÜùëéùë°in environment\n12:\nObserve next state ùë†ùë°+1\n13:\nif episode done then\n14:\nCompute reward ùëüùë°‚Üê‚Ñõ(ùë†ùë°, ÀÜùëéùë°)\n15:\nelse\n16:\nùëüùë°‚Üê0\n17:\nend if\n18:\nStore transition: ùíü‚Üêùíü‚à™{(ùë†ùë°, ÀÜùëéùë°, ùëüùë°, ùë†ùë°+1)}\n19:\nend for\n20:\nend for\n21:\nfor sample successful trajectory in ùíüdo\n22:\nfor ùë°= 1 to ‚Ñéstop do\n23:\nAccumulate loss: ùêø(ùúÉ) ‚Üêùêø(ùúÉ) + CrossEntropy(ùúãùúÉ(ùë†ùë°), ÀÜùëéùë°)\n24:\nend for\n25:\nend for\n26:\nUpdate policy: ùúÉ‚ÜêùúÉ‚àíùõº‚àáùúÉùêø(ùúÉ)\n27: end for\nD. WebArena Experiments\nD.1. WebArena Agent Prompt\nCoT Prompt\nImagine you are an agent browsing the web, just like humans. Now you need to complete a task.\nIn each iteration, you will receive an observation that includes the accessibility tree of the webpage\nand a screenshot of the current viewpoint. The accessbility tree contains information about the\nweb elements and their properties. The screenshot will feature numerical labels placed in the TOP\nLEFT corner of web elements in th current viewpoint. Carefully analyze the webpage information\nto identify the numerical label corresponding to the web element that requires interaction, then\nfollow the guidelines and choose one of the following actions:\n1. Click a web element.\n25\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n2. Delete existing content in a textbox and then type content.\n3. Scroll up or down the whole window.\n4. Go back, returning to the previous webpage.\n5. Answer. This action should only be chosen when all questions in the task have been solved.\nCorrespondingly, action should STRICTLY follow the format specified by one of the following lines:\nClick [numerical_label]\nType [numerical_label] [content]\nScroll [up/down]\nGoBack\nANSWER [content]\nSome examples are:\nClick [8]\nType [22] [Boston]\nScroll [down]\nANSWER [06516]\nKey guidelines you MUST follow:\n* Action guidelines *\n- Use either screenshot or accessibility tree to obtain the numerical_label. Sometimes the accessi-\nbility tree captures more elements than the screenshot. It‚Äôs safe to select these elements without\nscrolling\n- For text input, use Type action directly (no need to click first). All existing texts in the textbox\nwill be deleted automatically before typing\n- Preserve text inside quotation marks exactly as provided by user\n- You must not repeat the same actions over and over again. If the same action doesn‚Äôt work, try\nalternative approaches\n- Use ANSWER only after completing ALL task requirements\n- Wrap content for Type and ANSWER with square brackets ‚Äò[]‚Äò\n- Do not add quotation marks for search queries\n* Web navigation hints *\n{hint}\nYour reply should strictly follow the format:\nThought: Your reasoning trace. A good practice is to summarize information on the current web\npage that are relevant to the task goal, then generate a high-level plan that contains the sequence\nof actions you probably need to take\nAction: Based on this reasoning, identify the single most optimal action. You should output it in\nthe format specified above (under \"STRICTLY follow the format\")\nAfter each action, you‚Äôll receive a new observation. Proceed until task completion. Now solve the\nfollowing task.\nTask: {task_goal}\nCurrent URL: {url}\nScreenshot of current viewpoint: attached\nAccessibility tree of current viewpoint: {accessibility_tree}\nBeyond the above CoT prompt, we also tried using a more complex prompt for the thought process.\nHowever, this does not lead to significant gain in downstream accuracy (see Table 5), but it could increase\n26\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\ntraining and inference cost, so we did not use it in the end.\nComplex Prompt\nThought: You must analyze the current webpage thoroughly to guide your decision-making. Show\nyour reasoning through these steps:\n- Summarization: Begin by understanding the page context - identify what type of page you‚Äôre\non (search results, form, article, etc.) and how it relates to your objective. Summarize important\ninformation on the webpage that might be relevant to task completion. Especially when the\ntask requires to return some answers to a specific question, you should note down intermediate\ninformation that helps generate the answer.\n- Planning: Generate a checklist of subtasks required for completion and cross-out the subtasks\nyou‚Äôve completed. Identify the next logical subtask.\n- Verification: Verify all information you‚Äôve entered so far. Check that your inputs match require-\nments in terms of spelling and format (you should not change the user-specified information,\neven if there‚Äôre grammar errors). Verify if any selections for dropdown items align with the task\nobjective. Identify if there‚Äôre necessary fields that have not been filled in. Note that if the last few\nsteps are repeating the same action, there must be missing or incorrect information.\n- Backtracking: If the task requires exploring multiple webpages (e.g., orders, posts, item pages,\netc) to find out an answer, consider if you need to issue GoBack and return to the previous web\npage.\n- Candidate Generation: After all the above reasoning, list the most relevant possible actions,\nevaluate pros and cons of each action, and finally select the most effective action to progress task.\nAction: Choose ONE of the following action formats:\n- Click [numerical_label] - Click a specific element\n- Type [numerical_label] [content] - Input text into a field\n- Scroll [up/down] - Navigate the page vertically\n- GoBack - Return to previous webpage\n- ANSWER [content] - Provide final answer when task is complete\nD.2. WebArena Domain-Specific Prompts\nBelow are the content replacing ‚Äú{hint}‚Äù in the general prompt.\nGeneral Hint\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Clear filters before setting new ones\nReddit\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Pay attention to words like \"latest\", \"newest\", \"hottest\" in the task objective, which require clicking\nthe dropdown menu and select \"New\" or \"Top\" with the correct time range\n27\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n- When selecting a subforum, you can either browse the dropdown menu in the \"Submit\" page or\nnavigate to \"Forums\" and check all subforums by clicking on \"Next\" to go over all pages. You must\ntry to find a subforum that exactly matches your query. If there‚Äôs no exact match, pick the most\nrelevant one, ideally the subforum is about objects or locations contained in the given objective\n- \"Trending\" means \"hot\"\n- To find out all posts or replies from a user, click the user name and then click \"Submissions\" or\n\"Comments\"\nCMS\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Clear filters before setting new ones\n- Use date format: month/day/year (e.g., 1/1/16, 12/31/24)\n- When searching phone numbers, remove the country code\n- When searching product name, use single but not plural form\n- When the web page contains a table, aggregate the rows with the same item\nShopping\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Sort items by price by clicking the dropdown menu and set descending/ascending direction\n- When searching product name, use single but not plural form\n- If the objective requires only finding an item, stop at the item page without adding to cart\n- To find out the quality of a product, search the item, click on review, and inspect its review\n- Click \"Page Next\" to iterate over all orders\n- Since there‚Äôs no way to filter order history, click \"View Order\" for every order within a date range\nand inspect individually. If the condition is not met, go back\nGitLab\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Clear filters before setting new ones\n- When searching a repo in gitlab, type only the project name after \"/\" in the search box\nMap\n- Always remember to interact with dropdown options after expanding\n- When searching for a place, remove prepositions like in/on/by/at. For example, use \"starbucks,\ncraig street\" instead of \"starbucks on craig street\". Put the city name at the end\n- When there is no results shown up after search, rephrase the address and try again\n- To find direction between two points, after entering the from and to addresses, select the correct\ntransportation (foot/bicycle/car) before clicking \"Go\"\n28\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n- When the given location is not a geological address, use your knowledge to infer the address\nD.3. CoT Experiments for Base Agent\nTo enable efficient rollout collection, we spin up multiple Docker containers on a single GPU according to\nthe official WebArena repository. We use the vLLM [95] engine for inference and apply the following\ninference hyperparameters for most of our experiments.\n‚Ä¢ max_new_tokens: 1024\n‚Ä¢ max_attached_imgs: 4\n‚Ä¢ temperature: 1\n‚Ä¢ top_p: 0.95\nWe randomly subsample 62 test tasks for analysis purposes. Below are the results of zero-shot agent vs\nCoT prompting. ‚ÄúCoT‚Äù uses the ‚ÄúGeneral Prompt‚Äù in Section D.1. ‚ÄúComplex CoT‚Äù uses the ‚ÄúComplex\nPrompt‚Äù in Section D.1.\nTable 5: Base agent results averaged over 3 runs on WebArena subset.\nPrompt\nTask SR (%)\nAction Only\n14.76\nCoT\n23.81\nComplex CoT\n23.33\nD.4. Scaling Trade-off Experiments\n‚ÄúCheck-again‚Äù for interaction scaling.\nAfter the agent outputs the task-stop signal, we append the\nfollowing prompts to the observation to induce it to check again.\nCheck-Again Prompt\nImportant: You returned an answer in the last step. Let‚Äôs pause, check the web page, and think\nagain. If you still think the task is finished, double-check your answer, revise it if need, and return a\nfinal answer. If not, continue the task. Your output should still be in the same ‚ÄúThought:...Action:...‚Äù\nformat.\nWhen applying multiple re-checks, we slightly vary the prompts such as ‚Äò‚ÄòBefore you finalize the answer,\nre-evaluate it in terms of the current web page‚Äîwhat exactly supports or contradicts it?‚Äù or ‚ÄúWhy do I\nbelieve this answer is correct? What on the page justifies it? Could an alternative answer be better?‚Äù Please\nrefer to the code base for the exact prompt used.\nPer-step budget forcing.\nFollowing [72], we use the phrases below to induce longer per-step thinking.\nThe phrases are different to ensure that the model does not run into the scenario of endless repeating a\nphrase.\n29\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n‚Ä¢ First time: Wait, let me think deeper.\n‚Ä¢ Second time: But let me double-check.\n‚Ä¢ Third time: But hold on.\nPer-step best-of-ùëõ.\nWe tried both selecting by log likelihood and majority voting, with the latter\nshowing slightly better results.\nAdditional results for combined scaling.\nBeyond evaluating each scaling method separately, we also\ntried combining methods along different axes.\nTable 6: Comparing different inference-time prompting strategies. Results averaged over 3 runs on WebArena subset.\nAll methods are applied once.\nInference-Time Strategy\nTask SR (%)\nBaseline\n23.81\nCheck-again\n26.14\nBudget-forcing\n24.81\nBest-of-ùëõ\n25.03\nCheck-again + Budget-forcing\n26.33\nCheck-again + Best-of-ùëõ\n27.36\nD.5. TTI and Online Filtered BC Hyperparameters for Preliminary Experiments\nWe use the following hyperparameters to obtain the training curves in Figure 4. During training, the\nvision_tower of Gemma 3 is kept frozen because it is frozen during pretraining. Other hyperparameters\ncan be found in our code.\n‚Ä¢ num_iteration: 10\n‚Ä¢ actor_epochs: 1 # number of epochs to update the actor\n‚Ä¢ rollout_size: 512\n‚Ä¢ num_update_sample_per_iteration: 512\n‚Ä¢ lr: 1e-6\n‚Ä¢ optimizer: AdamW\n‚Ä¢ scheduler: WarmupCosineLR\n‚Ä¢ batch_size: 4\n‚Ä¢ grad_accum_steps: 2\n‚Ä¢ training_gpu_size: 4\n‚Ä¢ eval_horizon: 30\nFor multiplicative curriculum, we use the schedule: 10, 20, 30, 30, ... For additive curriculum, we use the\nschedule: 10, 11, 12, 13, ...\nD.6. TTI Hyperparameters for Full WebArena Experiments\nWe use the following hyperparameters to obtain the full WebArena results for Table 4.\n30\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n‚Ä¢ num_iteration: 10\n‚Ä¢ horizon_schedule: 10, 20, 20, 30, 30, 30, ...\n‚Ä¢ actor_epochs: 1 # number of epochs to update the actor\n‚Ä¢ rollout_size: 512\n‚Ä¢ num_update_sample_per_iteration: 512\n‚Ä¢ lr: 1e-6\n‚Ä¢ optimizer: AdamW\n‚Ä¢ scheduler: WarmupCosineLR\n‚Ä¢ batch_size: 4\n‚Ä¢ grad_accum_steps: 2\n‚Ä¢ training_gpu_size: 4\n‚Ä¢ eval_horizon: 30\nE. WebVoyager Experiments\nE.1. Task Generator & Evaluator Prompt\nTask Generator Prompt\nYou are a website exploration assistant tasked with discovering potential tasks on websites. These\ntasks should be similar to a user-specified task and aim to complete some high-level goals such as\nbooking restaurants in a website. Your goal is to freely explore websites and propose tasks similar\nto a given set of examples. For each iteration, you‚Äôll receive:\n- An observation with the webpage‚Äôs accessibility tree\n- A screenshot showing numerical labels in the TOP LEFT corner of web elements\nYou will then generate possible tasks while exploring the website. You should imagine tasks that\nare likely proposed by a most likely user of this website. You‚Äôll be given a set of examples for\nreference, but you must not output tasks that are the same as the given examples. The generated\ntasks must be realistic and at least require 3 steps to complete. It cannot be too simple.\n## Response Format and Available Actions\nYour reply for each iteration must strictly follow this format:\nThought: Analyze the current webpage thoroughly to guide your exploration. Examine the\nwebpage‚Äôs structure, content, and interactive elements to identify potential tasks that users might\nperform on this site. Decide whether you want to keep exploring or output some tasks\nTasks:\nIf you think you are ready to generate some tasks, output them in the follow-\ning format (note that different tasks are separated with double semicolons): GENERATE\n[task1;answer1;;task2;answer2]\nAction: Then, to continue with your exploration, choose ONE of the following action formats:\n- Click [numerical_label] - Click a specific element\n- Type [numerical_label] [content] - Input text into a field\n- Scroll [up/down] - Navigate the page vertically\n- GoBack - Return to previous webpage\nExamples:\nClick [8]\nType [22] [Boston]\n31\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nScroll [down]\nGENERATE [Find the company‚Äôs phone number;(555) 123-4567;;Locate the price of the basic\nsubscription plan;$19.99/month]\nYour final output should look like:\nThought: ...\nTasks: GENERATE [...] (this is optional, only generate when you are confident)\nAction: ...\n## Critical Guidelines\n### Action Rules\n- Use either screenshot or accessibility tree to obtain the numerical_label\n- For text input, use Type action directly (no need to click first)\n- Ensure proposed tasks are diverse and demonstrate different aspects of the website. The tasks\nmust have diverse difficulty and require different number of steps (3-20) to complete.\n- Tasks should be clear, specific, achievable, and self-contained. It cannot be too general, e.g.,\nrelated to √§ny post¬®, √§ny product¬®, √§ny place¬®. It must not depend on any context or actions that you\nhave performed, i.e., you must assume zero prior knowledge when someone wants to complete\nthe task\n- Your task should be objective and unambiguous. The carry-out of the task should NOT BE\nDEPENDENT on the user‚Äôs personal information such as the CURRENT TIME OR LOCATION\n- Your tasks should be able to be evaluated OBJECTIVELY. That is, by looking at the last three\nscreenshots and the answer provided by an agent, it should be possible to tell without ambiguity\nwhether the task was completed successfully or not\n- Answers should be precise (e.g., exact prices, specific information, exact text)\n- Your should output both operational tasks (the goal is to complete some steps) and information\nretrieval tasks (the goal is to find some answer to return)\n- You must refer to the examples given and mimic the complexity and task structure. See how\nthese tasks are self-contained and realistic\n- Your proposed task cannot be a single action like click, type! Tasks like ‚ÄôDetermine the number of\nuses for that term‚Äô is unacceptable because it is ambiguous as a stand-alone task; ‚ÄôUncheck Use\nsystem value‚Äô is unacceptable because it is not a complete task; ‚ÄôLocate the total revenue for the\nlast month‚Äô is unacceptable because ‚Äôlast month‚Äô is ambiguous;\nAfter each action, you‚Äôll receive a new observation. Continue exploring and generating tasks.\nHere‚Äôre some examples: {example}\nCurrent URL: {url}\nScreenshot of current viewpoint: attached\nAccessibility tree of current viewpoint: {accessibility_tree}\nEvaluator Prompt\nYou are an expert in evaluating the performance of a web navigation agent. The agent is designed\nto help a human user navigate a website to complete a task. Your goal is to decide whether the\nagent‚Äôs execution is successful or not.\nAs an evaluator, you will be presented with three primary components to assist you in your role:\n1. Web Task Instruction: This is a clear and specific directive provided in natural language, detailing\n32\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nthe online activity to be carried out.\n2. Result Response: This is a textual response obtained after the execution of the web task. It\nserves as textual result in response to the instruction.\n3. Result Screenshots: This is a visual representation of the screen showing the result or interme-\ndiate state of performing a web task. It serves as visual proof of the actions taken in response to\nthe instruction.\n‚Äì You SHOULD NOT make assumptions based on information not presented in the screenshot when\ncomparing it to the instructions.\n‚Äì Your primary responsibility is to conduct a thorough assessment of the web task instruction\nagainst the outcome depicted in the screenshot and in the response, evaluating whether the actions\ntaken align with the given instructions.\n‚Äì NOTE that the instruction may involve more than one task, for example, locating the garage and\nsummarizing the review. Failing to complete either task, such as not providing a summary, should\nbe considered unsuccessful.\n‚Äì NOTE that the screenshot is authentic, but the response provided by LLM is generated at the end\nof web browsing, and there may be discrepancies between the text and the screenshots.\n‚Äì Note that if the content in the Result response is not mentioned on or different from the screenshot,\nmark it as not success.\n‚Äì NOTE that the task may be impossible to complete, in which case the agent should indicate\nthis in the response. CAREFULLY VERIFY THE SCREENSHOT TO DETERMINE IF THE TASK IS\nIMPOSSIBLE TO COMPLETE. Be aware that the agent may fail because of its incorrect actions,\nplease do not mark it as impossible if the agent fails because of its incorrect actions.\nYou should explicit consider the following criterion:\n- Whether the claims in the response can be verified by the screenshot. E.g. if the response claims\nthe distance between two places, the screenshot should show the direction. YOU SHOULD EXPECT\nTHAT THERE IS A HIGH CHANCE THAT THE AGENT WILL MAKE UP AN ANSWER NOT VERIFIED\nBY THE SCREENSHOT.\n- Whether the agent completes EXACTLY what the task asks for. E.g. if the task asks to find a\nspecific place, the agent should not find a similar place.\nIn your responses:\nYou should first provide thoughts EXPLICITLY VERIFY ALL THREE CRITERION and then provide a\ndefinitive verdict on whether the task has been successfully accomplished, either as ‚ÄôSUCCESS‚Äô or\n‚ÄôNOT SUCCESS‚Äô.\nA task is ‚ÄôSUCCESS‚Äô only when all of the criteria are met. If any of the criteria are not met, the\ntask should be considered ‚ÄôNOT SUCCESS‚Äô.\nE.2. Agent Prompt\nWebVoayager\nImagine you are a robot browsing the web, just like humans. Now you need to complete a task. In\neach iteration, you will receive an observation that includes the accessibility tree of the webpage\nand a screenshot of the current viewpoint. The accessbility tree contains information about the\nweb elements and their properties. The screenshot will feature numerical labels placed in the TOP\n33\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nLEFT corner of web elements in the current viewpoint. Carefully analyze the webpage information\nto identify the numerical label corresponding to the web element that requires interaction, then\nfollow the guidelines and choose one of the following actions:\n1. Click a web element.\n2. Delete existing content in a textbox and then type content.\n3. Scroll up or down the whole window.\n4. Go back, returning to the previous webpage.\n5. Navigate to Bing‚Äôs homepage.\n6. Answer. This action should only be chosen when all questions in the task have been solved.\nCorrespondingly, action should STRICTLY follow the format specified by one of the following lines:\nClick [numerical_label]\nType [numerical_label] [content]\nScroll [up/down]\nGoBack\nBing\nANSWER [content]\nSome examples are:\nClick [8]\nType [22] [Boston]\nScroll [down]\nBing\nANSWER [06516]\nKey guidelines you MUST follow:\n* Action guidelines *\n1. The predicted action should be based on elements as long as it‚Äôs accessibility tree OR screenshot.\nSometimes, accessibility tree or screenshot captures more elements than the other, but it‚Äôs fine to\nuse either one.\n2. To input text for search bars, no need to click textbox first, directly type content. After typing,\nthe system automatically hits ‚ÄôENTER‚Äô key.\n3. When a complex task involves multiple questions or steps, select ‚ÄôANSWER‚Äô only at the very\nend, after addressing all of these questions or steps. Double check the formatting requirements in\nthe task when ANSWER. Always think twice before using ‚ÄôANSWER‚Äô action!!!\n4. When specifying the content for ‚ÄôType‚Äô and ‚ÄôANSWER‚Äô actions, be sure to wrap the content with\n‚Äô[]‚Äô.\n5. Use ‚ÄòGoBack‚Äò to return to the previous state, use it when you find the previous action incorrect.\n6. When you see a pop-up page, you should immediately ‚ÄòGoBack‚Äò to the previous page.\n7. Use ‚ÄòBing‚Äò when you need to navigate to a different website or search for new information.\nYour reply should strictly follow the format:\nThought: Your reasoning trace. A good practice is to follow this format:\n- Observation summary: where are you at now? list all elements that are related to the task goal.\ne.g. if you‚Äôre trying to filter something out, list all filters visible.\n- Planning: what sequence of actions do you need take to achieve the task goal? give a high-level\noverview of the steps you need to take.\n- Possible actions: to achieve that plan, what are potential actions you need to do immediately and\n34\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nwhat‚Äôs their effect? List at least 3 actions and analyze each of them.\nAction: Based on this reasoning, identify the single most optimal action. You should output it in\nthe format specified above (\"...STRICTLY follow the format...\").\nAfter you issue an action, the user will execute it and provide a new observation. Now solve the\nfollowing task.\nTask: {task_goal}\nCurrent URL: {url}\nScreenshot of current viewpoint: attached\nAccessibility tree of current viewpoint: {accessibility_tree}\nE.3. Experiment Details\nWe use the following hyperparameters to obtain the WebVoyager results.\n‚Ä¢ num_iteration: 12\n‚Ä¢ horizon_schedule: 10, 20, 20, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30\n‚Ä¢ actor_epochs: 1 # number of epochs to update the actor\n‚Ä¢ rollout_size: 512\n‚Ä¢ num_update_sample_per_iteration: 512\n‚Ä¢ lr: 4e-6\n‚Ä¢ optimizer: AdamW\n‚Ä¢ scheduler: WarmupCosineLR\n‚Ä¢ batch_size_per_gpu: 4\n‚Ä¢ grad_accum_steps: 2\n‚Ä¢ training_gpu_size: 6\n‚Ä¢ eval_horizon: 30 # note that train horizon is different for different methods, but evaluation horizon\nis kept the same\nIn Figure 6, the green area starts from iteration 6, because this is the first iteration where it‚Äôs possible to\nhave all trajectories sampled for training have a horizon of ùêª= 30.\n35\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nE.4. Case Studies: Strengths 1\nTask: Locate a recipe for an American apple pie on Allrecipes with a rating of at least 4 stars and more\nthan 50 reviews. Note the maximum temperature mentioned in the Directions.\nFully trained agent explores:\nStep 1-8\nStep 9-15\n36\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nEarly-stage agent prefers exploitation:\n37\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nE.5. Case Studies: Strengths 2\nTask: Identify the latest top-trending open-source project in the category of ‚ÄòMachine Learning‚Äô on GitHub,\nand check the number of stars it has received.\n38\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nE.6. Case Studies: Fail Modes 1\nTask: On Apple‚Äôs website, how many different types of keyboards are available when customizing your\n14-inch MacBook Pro?\nStep 1-8\nStep 9-16\n39\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nTask: Which university maintains and manages ArXiv. Accessing the university‚Äôs website from ArXiv, how\nmany undergraduate students are currently at the university.\n40\nThinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nE.7. Case Studies: Fail Modes 2\nTask: Identify a new open-source project on GitHub related to ‚ÄòAI agriculture‚Äô that created in 2022, and\nnote its main programming language and description.\n41\n",
  "pages": [
    {
      "page_number": 1,
      "text": "arXiv:2506.07976v2  [cs.LG]  10 Jun 2025\nThinking vs. Doing: Agents that Reason by\nScaling Test-Time Interaction\nJunhong Shen*1,2, Hao Bai*3, Lunjun Zhang4, Yifei Zhou5, Amrith Setlur1, Shengbang Tong7, Diego Caples6, Nan\nJiang3, Tong Zhang3, Ameet Talwalkar1 and Aviral Kumar1\n1Carnegie Mellon University, 2Scribe, 3University of Illinois Urbana-Champaign, 4University of Toronto, 5University of California,\nBerkeley, 6The AGI Company, 7New York University\nFigure 1: We propose a new-axis of test-time scaling for agents: scaling the number of interaction steps. Unlike traditional\nmethods that emphasize longer reasoning per step, we show that acting more helps gain new information from the environment\nand improve task performance (detailed results of the left plot in Section 4.2).\nAbstract: The current paradigm of test-time scaling relies on generating long reasoning traces (‚Äúthinking‚Äù more)\nbefore producing a response. In agent problems that require interaction, this can be done by generating thinking\ntraces before acting in the world. However, this process does not allow agents to acquire new information from the\nenvironment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped\ndimension of test-time scaling that increases the agent‚Äôs interaction horizon to enable running rich behaviors such\nas exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of\nthis scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction\nscaling without any training can improve task success on web benchmarks non-trivially. Building on this, we\nintroduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains\nagents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art\nopen-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables\nagents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful,\ncomplementary axis to scaling per-step compute, offering new avenues for training adaptive agents.\nProject page: https://test-time-interaction.github.io\nCode: https://github.com/test-time-interaction/TTI\n1. Introduction\nRecent advances in foundation models have enabled a shift from static language models to interactive\nagents that perform multi-step tasks in dynamic environments like browsers [1‚Äì6], terminals [7], and\nthe physical world [8‚Äì13]. These agents operate in closed-loop settings where each action changes the\ncurrent state of the world and affects future interaction with the environment. As a result, interactive\nagents must plan under uncertainty and adapt to failures in real time to be successful. How can we build\nagents that succeed in such interactive settings?\nCurrent post-training approaches produce reactive agents that respond to immediate observations but\nstruggle with evolving or uncertain task dynamics. Methods like supervised fine-tuning (SFT) on expert\n*Equal contribution. Corresponding author(s): junhongs@andrew.cmu.edu, haob2@illinois.edu\n"
    },
    {
      "page_number": 2,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\ndemonstrations [14‚Äì18] or reinforcement learning (RL) with task rewards [19‚Äì23] typically train agents\nto predict a single best action at each step. Even with test-time scaling, where agents are prompted to\n‚Äúthink‚Äù longer before prescribing an action [24‚Äì26], they are still optimized to select the most effective\naction based on the agent‚Äôs internal state. While sufficient for fully observable and stationary tasks,\nreactive policies based on the agent‚Äôs internal estimate of the task state are often suboptimal in partially\nobservable (e.g., incomplete details visible on a page) or non-stationary (e.g., fluctuating prices during\nflight booking) settings, where adaptive, information-seeking behavior is critical.\nIn this paper, we argue that instead of reactive ‚Äúoptimal‚Äù policies, agents should learn adaptive policies that\ncan collect new information from the environment and adjust their behaviors on-the-fly. A pre-requisite\nfor such adaptability is the ability to take more actions during deployment than those prescribed by an\nexpert trajectory. We therefore propose a new dimension of test-time scaling: increasing the number of\ninteraction steps of the agent. This allows agents to have sufficient context and time to attempt different\nbehaviors. For example, in a hotel booking task, an agent must first browse many listings to compare\nuser reviews and check availability before selecting the best option. Interaction scaling is orthogonal to\nexisting methods based on chain-of-thought (CoT), which emphasize deeper reasoning per step but do\nnot support information-gathering from the environment. This notion of information gain is unique to\nagentic tasks with partial observability and requires interaction, not merely larger per-step compute. For\ninstance, an agent that reasons deeply about one selected hotel without interacting further may miss\nbetter options that show up only after exploration.\nAlthough the idea of interaction scaling is conceptually straightforward, extending it to post-training\nand teaching agents to scale interaction autonomously presents key challenges. Without appropriate\ntraining signals, agents may overfit to exploratory behaviors like blindly clicking links but not making\nprogress toward the actual task objective, wasting the additional steps. To tackle this issue, we propose\nto combine online RL with a curriculum that prescribes how to scale the interaction horizon, training\nagents that first learn effective exploitation before extending their horizon to explore.\nWe instantiate our approach in the domain of web agents, a widely applicable setting with well-established\nbenchmarks. We first show that scaling test-time interaction via prompting the agent to ‚Äúthink and act\nagain‚Äù after it decides to terminate can already improve the task success rate from 23% to ‚â•28% on\nWebArena [2] (see Figure 3 for details). While this increases trajectory length and the number of tokens\ngenerated, spending an equivalent amount of compute on conventional test-time scaling methods like\nforcing the agent to think for longer [27] or running best-of-ùëõ[28‚Äì30] yields less than a 3% gain. These\nfindings validate interaction scaling as a promising and complementary axis of test-time scaling.\nWe then move beyond prompting and develop TTI (Test-Time Interaction), a curriculum-based RL\napproach that trains agents to adaptively scale interaction by gradually increasing the rollout horizon. We\nscale TTI to large and diverse training sets (>100K tasks across ‚àº20 domains) by integrating it with an\nautomated pipeline that generates synthetic tasks for online data collection. TTI achieves state-of-the-art\nperformance among open-source agents trained on open data on both WebVoyager [1] and WebArena [2],\nusing only a 12B Gemma 3 model, improving over the non-fine-tuned agent by 9% and 8%, respectively.\nOur analysis further shows that curriculum training enables adaptive exploration: agents learn to initiate\nnew searches or backtrack in complex tasks, while following efficient paths in simpler ones.\nIn summary, we introduce interaction scaling as a new dimension for test-time scaling of agents. We\npropose TTI to train agents by adjusting interaction horizon dynamically. Our results show that TTI\nyields strong empirical gains and offers promising directions for domains beyond web navigation.\n2\n"
    },
    {
      "page_number": 3,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n2. Related Work\nScaffolded foundation models as web agents. Prior works use external control structures to scaffold\nfoundation models via modular prompting [6, 31‚Äì37], programs [38‚Äì42], or feedback mechanisms [43‚Äì\n46]. These methods often rely on proprietary models like GPT-4 [47] or Claude [48]. Thus, progress\nis driven by designing better prompts and workflows for planning [49‚Äì52], self-correction [53], self-\nevaluation [54, 55], or by integrating external modules such as memory [56] or retrieval systems [57].\nMore recently, developing specialized agents has become a promising direction. ScribeAgent [14] first uses\nreal-world data to demonstrate that simple fine-tuning can outperform most prompt-based agents. Prior\nwork also builds automated data curation workflows [58‚Äì60] and distillation methods [18]. Despite these\nefforts, scaffolding methods remain fundamentally limited: they do not enable agents to self-improve\nthrough interaction, and rely on fixed wrappers that lack adaptability across diverse environments.\nRL training for foundation model agents. RL-based approaches enable agents to autonomously\nimprove through interaction. Prior work has explored DPO [61], actor-critic [20, 21, 62], or distributed\nsampling [63]. Pipelines like PAE [19] and Learn-By-Interact [64] support automatic task generation,\nexploration, and labeling. However, most of these approaches lack mechanisms for test-time exploration,\nlimiting the agent‚Äôs ability to adapt its behavior over long horizons, especially under partially observable\nconditions. As Bai et al. [20] note, continued training after deployment is often required just to maintain\nperformance with these methods. Our work addresses this limitation by scaling test-time interaction\nas an independent dimension, allowing agents to refine behavior while acting. Curriculum-based RL\nhas been applied in AutoWebGLM [65] and WebRL [23], where curricula are based on estimated task\ndifficulty derived from the complexity of LLM-generated instructions. While our approach also induces\na progression from simpler to more complex behaviors, it does so by gradually increasing the allowed\ninteraction horizon rather than relying on explicit measures of task difficulty.\nScaling test-time compute. Increasing test-time compute via best-of-ùëõsampling [29], beam search [66,\n67], or verifiers [68‚Äì70] has shown to improve performance in reasoning-heavy tasks. In non-interactive\nsettings like math and competitive coding, recent methods train models to generate long CoT and scale\nreasoning internally [e.g., 27, 71‚Äì73]. As for multi-turn interactive settings, most existing works simply\nintegrate CoT prompting into the agent system to enhance per-step reasoning [e.g., 52, 74]. EXACT [75]\nscales up the search process for each action, GenRM-CoT [76] the number of verifiers, and Jin et al. [77]\nthe number of agents. However, none of these efforts studies the benefits of scaling over the time horizon,\nwhere the agent can explore alternatives, backtrack, or gather more information before committing to\ncertain actions. Our work extends this line of research by introducing test-time scaling of interaction.\nAs we will show in our empirical results (Section 4.2), the benefits of scaling test-time interaction go\nbeyond test-time scaling (or ‚Äúreasoning‚Äù) before taking an action, within a given time step, because each\nextra step of interaction with the environment provides new information to the agentic policy, whereas\nthinking for longer simply reorganizes information that the agent already has.\n3. Problem Setup\nWe consider solving a web task as a finite-horizon sequential decision-making process guided by a reward\nobjective1. Formally, the environment implements a transition function that evolves over time and provides\nan observation ùëúùë°at step ùë°reflecting the current task state (details regarding the parameterization of the\nobservation space are shown below). The agent policy ùúãis parameterized by a multi-modal foundation\n1While this work centers on web agents, we believe the insights should generalize to other agent problem domains, and we\nhope future work will extend these ideas beyond web agents and web navigation.\n3\n"
    },
    {
      "page_number": 4,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nmodel that maps observation history ùëú1:ùë°‚àí1 and action history ùëé1:ùë°‚àí1 to the next action ùëéùë°. These histories\nallow the policy to represent rich, context-dependent behaviors enabled via interaction (details about\nthe design of the action space are shown below). We denote the environment horizon, or the maximum\nnumber of interaction steps allowed in the environment, as ‚Ñé. For each task, the actual interaction process\nends when the agent issues a stop signal or reaches the step limit ‚Ñé. Let ‚Ñéstop ‚àà(0, ‚Ñé] denote the actual\nnumber of steps taken. The agent receives a reward of 1 for task success, and 0 otherwise.\nObservation space design. Following [2, 14, 19], we consider an observation space consisting of the\ntask goal, the URL, and a structured representation of the current web page that includes both the\naccessibility tree of the web page and a screenshot augmented with a set-of-marks overlay [1], which\nassigns a unique identifier to each element that the agent can interact with. While the agent has access\nto all past observations in principle, doing so quickly exhausts the context window in practice, so we\ntruncate observation history to the most recent three steps, similar to prior works [19]. However, the\nagent still has access to all of its past actions. We show in Appendix B an example observation.\nAction space parameterization. We adopt a discrete action space with six actions: click, type, scroll, go\nback, search (e.g., Google or Bing), and stop the task with an answer. Following Yang et al. [33], we do\nnot consider compound actions like goto[url], as complex action spaces can hinder performance. For\na detailed definition of each action, see the agent prompt in Appendix D.1.\n4. Scaling Test-Time Interaction: A New Dimension of Agent Scaling\nPrior methods for LLM test-time compute scaling usually scale the number of thinking tokens at each\nstep [73, 78‚Äì80], but this does not enable an agent to engage in longer interactions with the environment\nto collect new information. In principle, scaling the maximum number of interaction steps should allow\nthe agent to employ richer behavioral strategies such as re-attempts, backtracking, and recovery. We will\nnow verify this hypothesis via controlled experiments on WebArena [2]. We will then build upon these\ninsights to develop TTI, an online RL method to explicitly train agents to optimize test-time interactions.\nControlled experiment setup. We choose WebArena [2] as our testbed, primarily because it enables\nreproducible interaction with diverse domains (OneStopShop, Reddit, GitLab, CMS, and OpenStreetMap)\nand is equipped with ground truth evaluators. We randomly sample 62 tasks for testing and reserve the\nremaining 750 for online training (see Section 5.1). To ensure sufficient interaction, we set a generous\ntest-time limit of ‚Ñé= 30, which is well above the average length of around 6 steps required by most\ntasks [56, 81]. Note that experiments in this section are for analysis rather than benchmarking purposes.\nWe will show more experimental results on multiple benchmarks in Section 6.\nTable 1: Base results av-\neraged over three runs.\nPrompt\nTask SR (%)\nAction Only\n14.76\nCoT\n23.81\nTo study the effect of increasing ‚Ñé, we use a simple prompting-based agent with\nGemma 3 12B [82] base model, which observes the web page and outputs an\naction via a single model call. It does not leverage any retrieval, verifiers, or other\nexternal modules, ensuring any performance gains come solely from increased\n‚Ñébut not auxiliary scaffolding. We also study whether it is beneficial to prompt\nthe agent to generate a reasoning trace before acting (see Appendix D.1 for the\ntemplates). As Table 1 shows, CoT prompting yields significantly higher task success rate (SR) than\ndirect action generation, setting a baseline of 23.81% on the test tasks. While we also tried more complex\nprompts that explicitly ask the agent to summarize the state, assess progress, and reflect, they did not\nyield significant gains (see Appendix D.3). We thus adopt a simple chain-of-thought (CoT) approach as\nthe default prompting strategy for experiments in this section.\n4\n"
    },
    {
      "page_number": 5,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n4.1. Scaling Test-Time Interaction by Acting Longer\nFigure 2: Scaling test-time interaction by prompting the agent to\n‚Äúre-check‚Äù its answer. More re-checks lead to longer trajectories (dots)\nand higher task success rates (bars), indicating at least some correlation\nbetween acting for longer and higher success rates.\nTo study the impact of test-time interaction\nscaling, we introduce a purely inference-\ntime ‚Äúcheck-again‚Äù mechanism: after the\nagent issues the task completion action, we\nexplicitly prompt it to reconsider its decision\nby ‚ÄúYou just signaled task completion. Let‚Äôs\npause and think again...‚Äù We can extend\nre-checking from double-check (two passes)\nto triple-check (three passes) and beyond,\nusing slightly varied prompt phrasings for each pass. Detailed prompts are in Appendix D.4.\nAs shown in Figure 2, prompting the agent to re-check not only increases the actual interaction length\n‚Ñéùë†ùë°ùëúùëù(line plots), but also improves the success rates on most WebArena domains (bar plots). When\nbeing asked to ‚Äúcheck-again‚Äù, the agent either reaffirms its decision (e.g., ‚ÄúI previously stated the driving time\nwas approximately 30 minutes....30 minutes seems plausible with typical traffic conditions. I‚Äôll stick with my previous\nanswer.‚Äù) or revises it upon reflection (e.g., ‚ÄúMy apologies. I jumped to a conclusion prematurely. Although\nthe address book *displays* the desired address, the task requires me to *change* it....I should click button [24] to\nmodify the address.‚Äù). In particular, it changes its action ‚àº25% of the time after double-checking. This\nhighlights the potential of interaction scaling: when given sufficient time, the agent is likely to explore\nalternatives before reaching an answer. The chances of the answer being correct could thus be higher.\nHowever, we do observe that repeatedly prompting the agent to re-check can sometimes lead to confusion\nor hallucination, causing it to revise correct answers into incorrect ones. This may explain the performance\ndrop observed in domains like Map. This is perhaps an inevitable limitation of scaling test-time interaction\nvia prompting alone, akin to how prompting is not an effective way to even scale per-step test-time compute\n(see self-correction results for prompted models in Qu et al. [83]). We discuss this limitation further in\nSection 4.2 and address by training the agents explicitly.\n4.2. Scaling Test-Time Interaction vs. Per-Step Test-Time Compute\nNext, we examine the effect of scaling interaction compared to scaling per-step reasoning: Given a total\ntoken budget, should agents prioritize more interaction steps or generating longer reasoning traces at\neach step? To explore this, we study two conventional test-time compute scaling methods.\nPer-step budget forcing. Following Muennighoff et al. [27], we prompt the agent to ‚Äúwait and think\nagain‚Äù after generating an initial CoT, encouraging more intermediate reasoning before it commits to an\naction. We vary the number of forced waits from 1 to 4. Despite trying various prompts to induce longer\nthinking from the agent (see Appendix D.4), our agent changes its actions only 15% of the time, and most\nof these changes involve reordering subtasks rather than exploring alternative paths or error correction.\nPer-step best-of-ùëõ. At each step, we sample ùëõ‚àà{3, 5, 7} candidate actions and select one via majority\nvoting, similar to [14]. We did not scale ùëõup further because best-of-ùëõis compute-intensive for multi-step\nsettings (ùëõ¬∑ ‚Ñémore expensive than the baseline per rollout), and we observe diminishing returns from\nsampling more actions, despite tuning sampling hyperparameters such as the temperature.\nFinding 1: Gaining new information via interaction beats thinking more a single step. Figure 3 (top)\nplots the task success against total compute, measured by the number of tokens per trajectory in log\nscale. Among the three strategies, interaction scaling (green) shows the steepest upward trend, achieving\n5\n"
    },
    {
      "page_number": 6,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nFigure 3: Top: Task success rate vs. total compute cost\n(measured in tokens per trajectory, log scale). Interaction\nscaling consistently achieves the highest success rate for a\ngiven compute budget. Bottom: Decomposition of com-\npute into tokens per step and number of interaction steps\n(excluding the initial point for each approach). Interaction\nscaling achieves superior performance by distributing com-\npute across more steps with lower per-step cost, in contrast\nto methods that put more tokens in fewer steps.\nthe highest success rate as the allowed token bud-\ngets increase. Budget forcing (blue) yields moderate\ngains but plateaus around 0.26. Despite incurring\nthe highest cost, best-of-ùëõ(orange) brings the least\nimprovements, suggesting that repeatedly sampling\nactions per step is a less effective use of compute in\ninteractive tasks.\nA natural question that arises here is: how should we\ndistribute a bounded compute budget between running\nmore interaction steps vs. reasoning longer? These two\ndimensions present different costs per unit, which\nmay not be known apriori. Hence, we illustrate how\nscaling the number of interaction steps and the num-\nber of tokens per step affect performance and the total\nnumber of tokens individually. Figure 3 (bottom) de-\ncomposes total compute into tokens per step (y-axis)\nand steps per rollout (x-axis), so the shaded areas\nindicate the average total compute required per tra-\njectory. Interaction scaling extends along the x-axis,\nwhile per-step reasoning scales along y-axis. We find\nthat scaling across steps is more effective than scal-\ning within steps in WebArena tasks (Figure 3, top),\nlikely because the former enables the agent to gather\nnew information and enrich its context. This ability\nto query and observe external feedback is unique to\nagentic settings but not single-turn question-answering tasks. While standard per-step reasoning is con-\nstrained by the information already available at each step, our approach takes advantage of this dynamic\ninteraction. However, in principle, one can combine more reasoning per-step with more interaction, at\nthe cost of spending many tokens.\nFinding 2: Prompting alone is insufficient for interaction scaling. While our results highlight the po-\ntential of scaling interaction, the ‚Äúcheck-again‚Äù strategy only allows the agent to revisit its behavior\nupon task completion, it does not enable it to implement nuanced behaviors such as switching between\nexploration and exploitation in the middle of a rollout. We also experimented with combining interaction\nscaling with budget forcing and best-of-ùëõ(Appendix Table 6) and observe that simply increasing test-time\ncompute via prompting does not yield additive gains. In fact, the final downward trend in Figure 3\n(top) suggests that asking the agent to re-check too many times or think for too long can confuse it and\ndegrade performance. This shows the need for methods that train agents to optimize for best behavior\nwhen scaling test-time interaction, rather than na√Øve prompting.\nTakeaways: Scaling test-time interaction vs. test-time compute\nUnder a fixed compute budget (measured by total tokens), gaining information through longer interaction\nwith the environment can be more effective than solely deepening per-step reasoning. While longer chain-\nof-thought can improve local decision quality, interaction scaling offers a complementary and often more\ncompute-efficient way to enable agents to adapt and explore over longer horizons.\n6\n"
    },
    {
      "page_number": 7,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n5. TTI: Curriculum-Based Online RL for Scaling Interaction\nHow can we extend beyond prompting-based scaling to training agents that can effectively utilize\ninteraction scaling? A natural starting point is to draw inspiration from current approaches for optimizing\ntest-time compute [30, 71, 72] and extend these ideas to interactive settings. Specifically, we can run\nreinforcement learning (RL) with binary task rewards and longer task horizons. However, is this approach\nsufficient? We first describe the key challenges in learning to scale test-time interaction, and then develop\nour approach to address them via curriculum learning.\n5.1. Challenges of Training Agents with Fixed, Long Horizons\nA natural way to encourage the agent to learn to take more steps is to train at long horizons. To study\nthis, we can run the simplest form of REINFORCE [84] with binary rewards ùëÖ(¬∑), also known as online\nfiltered behavior cloning (BC) or online STaR [20, 85]. Only successful trajectories are retained, and the\nagent is updated by maximizing the log-likelihood of actions conditioned on those high-reward rollouts:\narg max\nùúÉ\nEùíØ‚àºtasks\n‚é°\n‚é¢‚é¢‚é¢‚é¢‚é£\nEùëú0:‚Ñé,ùëé0:‚Ñé‚àí1‚àºùúã(¬∑|ùíØ)\n‚é°\n‚é¢‚é¢‚é¢‚é¢‚é£\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éù\n‚Ñé‚àí1\n‚àëÔ∏Å\nùë°=0\nlog ùúãùúÉ(ùëéùë°| ùëú‚â§ùë°, ùíØ)\nlikelihood of trajectory\n‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚é†\n¬∑ 1\n‚é°\n‚é£ùëÖ(ùëú0:‚Ñé, ùíØ)\nreward\n= 1\n‚é§\n‚é¶\n‚é§\n‚é•‚é•‚é•‚é•‚é¶\n‚é§\n‚é•‚é•‚é•‚é•‚é¶\n(5.1)\nWe use filtered BC as it is stable throughout training (no negative gradient [86]), has been utilized pre-\nviously for training agents [20], and is a good ‚Äúfirst-choice‚Äù algorithm for studying the role of interaction\nscaling. We scale the agent‚Äôs horizon on WebArena, varying ‚Ñé‚àà{5, 10, 20}. Smaller ‚Ñéexposes the agent\nonly to exploitative rollouts that succeed within allowed time steps, while larger ‚Ñéalso includes more\nexploratory rollouts. We use the non-test tasks for rollout. Details are in Appendix D.5.\nAs shown in Figure 4 (left), the agent trained with ‚Ñé= 5 learns quickly, likely because on-policy RL\nis more sample-efficient at smaller horizons, but it also quickly overfits, and performance decreases\nwith more training (x-axis)2. This agent often terminates prematurely during evaluation despite being\nallowed to interact for much longer time. Conversely, agents trained at longer horizons generally learn\npolicies that are quite stochastic and learn significantly more slowly due to higher variance of the loss and\ncredit assignment challenges due to longer horizons [e.g., 87‚Äì89]. We manually inspect the trajectories\nand find that the ‚Ñé= 20 agent tends to associate exploratory actions such as ‚Äúgoing back‚Äù or ‚Äútrying\nrandom links‚Äù with high rewards initially. Noisy credit assignment with ‚Ñé= 20 slows learning, and only\nafter several iterations do the agents begin to recover and produce more robust policies. The impact\nof horizon is domain-dependent: in complex domains requiring exploration (e.g., CMS), long-horizon\nagents outperform, while in simpler settings (e.g., Reddit), performance differences are minimal. We\nalso note that the total number of tokens generated per action slightly decreases throughout training,\nindicating that the training does not incentivize increasing the length of reasoning directly.\nImportantly, although the interaction length increases as expected for ‚Ñé= 20 (Figure 4, right), noisy\ncredit assignment and slower learning suggests that simply setting ‚Ñéto be large is insufficient to learn\nto scale test-time interaction reliably. These observations motivate our method‚Äôs core idea: rather than\nfixing the horizon throughout training, our approach aims to scale their interaction length dynamically.\n2We use the same training hyperparameters (Appendix D.5) across all settings for fair comparison. While reducing the\nlearning rate can help reduce performance drop, it does not address the core issue with small horizons: fewer successful rollouts\nand shorter trajectories lead to significantly less training data.\n7\n"
    },
    {
      "page_number": 8,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nFigure 4: Online RL with different values of maximum interaction horizon. Left: success rates for different domains. ‚ÄúHarder‚Äù\nmeans generally lower success rate. Right: average rollout length (‚Ñéstop) on the evaluation set.\n5.2. Our Approach: Curriculum Over Interaction Horizon\nTo address these challenges, we propose TTI (Test-Time Interaction), a curriculum-based online RL\napproach that trains the agent with short trajectories initially and gradually exposes it to longer ones.\nExisting curriculum learning methods in RL [e.g., 90‚Äì94] or web agents [23, 65] typically prioritize easier\ntasks before harder ones, often relying on predefined heuristics or external measures of task complexity.\nIn contrast, we define curriculum progression in terms of the maximum number of steps an agent is\nallowed per trajectory. While interaction length can serve as a rough proxy for task difficulty, our approach\ndoes not require explicit labeling or estimation of task complexity.\nHow do we design a curriculum over the interaction horizon ‚Ñé? Ideally, the curriculum should allow\nthe agent to first learn basic, ‚Äúatomic‚Äù skills to solve easier tasks, then progressively tackle complex\nones via skill chaining and exploration. To enable this kind of behavior, we explored two strategies:\n(1) a conservative, additive increase in ‚Ñéper iteration, giving the agent sufficient time to solidify core\ntask-solving skills; and (2) a more aggressive, multiplicative increase, which assumes the agent can quickly\nacquire the basic skills and benefit from earlier exposure to exploratory behavior. Formally, for iteration ùëñ:\n‚Ñéùëñ:= clip(‚Ñémin + ùëñ, ‚Ñémax)\n(Additive curriculum)\n(5.2)\n‚Ñéùëñ:= clip(‚Ñémin ¬∑ ùëñ, ‚Ñémax)\n(Multiplicative curriculum)\n(5.3)\nWe store the rollouts in a replay buffer and assign higher weights to more recent trajectories. The full\npseudocode for TTI and implementation details are provided in Appendix C.\nTable 2: Comparing vari-\nous curricula. A multiplica-\ntive curriculum produces the\nbest success rate.\nSchedule\nTask SR (%)\nAdditive\n29.50\nMultiplicative\n32.25\nEmpirical insights. We instantiate these two strategies in WebArena, using\nthe non-test tasks for online training. We set ‚Ñémin to 10 and ‚Ñémax to 30, and\napply the schedules on top of filtered BC. Evaluation results after 10 iterations\nare shown in Table 2. Multiplicative curriculum outperforms the additive\none, possibly because it exposes the agent to longer horizons early on and\nhelps prevent it from overfitting prematurely to shortcut behaviors like always\ntaking the shortest path. Based on these findings, we adopt the multiplicative\ncurriculum as the default for TTI. Table 2 further shows that even with limited data (‚àº700 training\ntasks), TTI outperforms fixed ‚Ñé= 20 in Figure 4 by nearly 3%, using 40% fewer training steps over 10\niterations. Next, we demonstrate that this advantage carries over to large-scale online RL training.\nTakeaways: Curriculum training enables effective interaction scaling\nOur approach, TTI, gradually scales the horizon using a curriculum, leading to better performance than\nfixed-horizon baselines. The multiplicative schedule improves both learning efficiency and task success rate.\n8\n"
    },
    {
      "page_number": 9,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n6. Experiments: Scaling Up to Realistic Benchmarks\nWe now provide a comprehensive evaluation of TTI in large-scale, realistic environments. More generally,\nwe demonstrate the effectiveness of training models to make good use of test-time interaction.\nFigure 5: Summary of our main results on WebVoyager and WebArena benchmarks.\nTTI consistently outperforms fixed-horizon training with both short and long horizons.\nSynthetic task generation\nand evaluation. To enable\nlarge-scale\ntraining\nwith-\nout training on the bench-\nmark itself, we adopt syn-\nthetic task generation in-\nspired by PAE [19]. Apart\nfrom prompting LLMs with\nseed examples or demos, we\nalso deploy an exploratory\nagent to freely interact with\nwebsites and propose more\ngrounded, diverse tasks. For\nevaluation, we leverage a\nprompting-based verifier that\nuses action histories and\nscreenshots to label rollouts, achieving 88.9% accuracy compared to WebArena‚Äôs ground-truth evaluator.\nDetails and prompt templates are in Appendix E.1. We evaluate agents on (1) WebVoyager [1] with 427\ntasks across 13 domains (we replace Google search with Bing due to ReCaptcha issues); and (2) full\nWebArena [2] with 812 tasks. We choose these benchmarks as they are widely used [e.g., 4].\nTraining. We obtain 128K synthetic tasks across diverse real-world domains for WebVoyager and 11K\ntasks for WebArena‚Äôs self-hosted domains (data released with the code). We train separate agents for each\nbenchmark to avoid contaminating real-domain agents with synthetic environment data, as WebArena\ndomains still have many differences from their real-world counterparts. We use Gemma 3 12B [82] as\nthe base model, sampling 512 tasks per iteration for rollouts and updating with 512 successful on-policy\ntrajectories. We apply a multiplicative curriculum with ‚Ñémin = 10 and ‚Ñémax = 30. We use vLLM [95] to\nsample rollouts and use DeepSpeed Zero 3 [96] with NVIDIA H100 GPUs for training. The evaluator\nis a Gemma 3 27B model, prompted to detect successful trajectories, which can then be used for the\nonline filtered BC procedure. Other hyperparameters such as the number of iterations, learning rate,\nand the exact schedule of TTI can be found in Appendix E.3.\nComparisons. We evaluate zero-shot Gemma 3 12B and approaches that utilize a fixed horizon with\n‚Ñé‚àà{10, 30}. We also compare to closed-source agents (e.g., those based on GPT-4 [47] and Claude [48]),\nopen-weight models trained on proprietary data (e.g., UI-TARS [97]), and fully open-weight, open-data\nmodels (e.g., PAE [19]). A detailed list of the prior approaches can be found in the result tables.\n6.1. WebVoyager Results and Analysis\nState-of-the-art open-weight, open-data performance. We report the overall task success rates (SR)\non WebVoyager in Table 3 and Figure 5 (left). The TTI-trained Gemma 3 12B achieves an average\nSR of 64.8%, setting a new state-of-the-art among open agents trained purely on public data. While\nprevious methods such as UI-TARS achieves a strong SR of 84.8%, they rely on private human-annotated\n9\n"
    },
    {
      "page_number": 10,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nTable 3: WebVoyager results. Training a Gemma3 12B model using TTI attains the best performance among open-weight agents\ntrained on open-source synthetic data in aggregate on the WebVoyager benchmark. Baseline results are taken from Zhou et al.\n[19], Qin et al. [97], and illustrate that TTI improves over the best prior open-model, open-source data approach by 30%.\nModel\nAverage Allrecipes Amazon Apple ArXiv GitHub ESPN Coursera Cambridge BBC Map Search HuggingFace WolframAlpha\nProprietary Model\nClaude 3.7\n84.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nClaude 3.5\n50.5\n50.0\n68.3\n60.4\n46.5\n58.5\n27.3\n78.6\n86.0\n36.6 58.5\n30.2\n44.2\n66.7\nOpenAI CUA\n87.0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nAgent E\n73.1\n71.1\n70.7\n74.4\n62.8\n82.9\n77.3\n85.7\n81.4\n73.8 87.8\n90.7\n81.0\n95.7\nOpen Model, Proprietary Human-Annotated Data\nUI-TARS-1.5\n84.8\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nOpen Model, Open Synthetic Data\nLLaVa-34B SFT\n22.2\n6.8\n26.8\n23.3\n16.3\n4.9\n8.6\n26.8\n67.4\n16.7 12.2\n23.3\n20.9\n38.1\nPAE-LLaVa-7B\n22.3\n14.3\n37.5\n17.5\n19.0\n14.6\n0.0\n33.3\n52.4\n18.6 22.5\n23.3\n19.0\n24.4\nPAE-LLaVa-34B\n33.0\n22.7\n53.7\n38.5\n25.6\n14.6\n13.6\n42.9\n74.4\n39.0 22.0\n18.6\n25.6\n42.9\nGemma 3 12B\n55.8\n25.7\n32.3\n45.5\n60.6\n54.8\n60.6\n56.3\n69.6\n65.6 54.8\n72.7\n66.7\n61.1\nFixed ‚Ñé= 10\n59.1\n25.7\n74.1\n51.5\n75.7\n70.9\n44.1\n59.3\n66.7\n50.0 41.9\n60.6\n75.5\n72.2\nFixed ‚Ñé= 30\n45.2\n20.0\n41.9\n60.6\n42.4\n41.9\n50.0\n34.4\n60.6\n25.0 29.0\n63.6\n45.5\n69.4\nTTI (Ours)\n64.8\n57.1\n48.3\n69.6\n66.6\n45.2\n56.3\n46.9\n85.2\n81.2 66.7\n72.7\n75.7\n79.4\ndata that remains inaccessible to the open-source community. In contrast, TTI is trained entirely on\nsynthetic data and interestingly, this data is generated by the base model (Gemma 3 12B) itself, meaning\nthat our training protocol implements a form of self-improvement. This shows the practicality of our\napproach, but also highlights the need for future work on developing high-quality open data comparable\nto human-generated ones. TTI also obtains the highest SR in 8 out of 13 domains, illustrating its efficacy.\nTTI outperforms fixed-horizon via adaptive exploration. Table 3 also shows that our curriculum ap-\nproach outperforms fixed ‚Ñé= 10 baseline by 5.7% and fixed ‚Ñé= 30 baseline by 19.6% in average accuracy.\nTo better understand the use of interaction within a training rollout, we plot the average number of interac-\ntion steps on a held-out validation set with 78 tasks in Figure 6 (a). Note that the agent trained with ‚Ñé= 10\nlearns to continuously reduce the maximum number of steps it spends in a rollout, while ‚Ñé= 30 quickly\ndrifts into aimless exploration and executes a larger number of steps pre-maturely in training, hindering\nperformance. This aligns with our findings in Section 5.1 that simply running training at a longer horizon\nmay not be sufficient for obtaining effective interaction scaling. In fact, we find that when training with\nTTI, the interaction length of the agent‚Äôs rollouts first decreases but then starts to increase as the maximum\nallowed horizon increases, indicating that an adaptive curriculum enables effective interaction scaling.\nFigure 6 (d) shows that the task success rate also grows over time and correlates with the expanding\nhorizon. While the average task success rates for TTI are better, we observe notable per-domain differences.\nFigure 6 (e) shows representative per-domain success rates. On domains like Allrecipes and Cambridge,\nTTI significantly outperforms fixed-horizon and zero-shot approaches, improving success rates by 31.4%\nand 15.6%, respectively, likely because these domains are highly information-dense and benefit from\nextended exploration enabled by adaptive interaction scaling. However, in domains like Amazon and\nGitHub, TTI underperforms the baselines. We notice that the base model already has strong knowledge\nabout domain-specific terminologies (e.g., commit history, forks, stars) in these domains, likely because\nthey are more prevalent than the others, resulting in high base performance. Inspecting the rollouts,\nwe find that instead of using built-in filters and sorting, TTI can engage in exploration behaviors such\nas initiating Bing searches or consulting external sites. This exposes the agent to noisy or distracting\ninformation, reducing task success. We discuss these cases in Section 6.2.\nLearning dynamics of TTI. To study how TTI enhances the ‚Äúwithin-rollout‚Äù exploration capabilities of\n10\n"
    },
    {
      "page_number": 11,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n15\n10\n5\n0\n5\n10\n15\nDifference (%)\na\nMax Horizon\n0 1\n5\n10\nIter\nTrajectory Length (Centered)\nTTI\nFixed h = 10\nFixed h = 30\n15\n10\n5\n0\nb\nMax Horizon\n0 1\n5\n10\nIter\nGoBack & Bing (Centered)\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\nc\nMax Horizon\n0 1\n5\n10\nIter\nTokens Per Step (Centered)\n50.0\n52.5\n55.0\n57.5\n60.0\nSuccess Rate (%)\nd\nMax Horizon\n0\n5\n10\nIter\nTTI Average Success Rate\n0\n25\nRelative SR (%)\n0\n5\n10\nIter\nAllrecipes\n0\n20\nRelative SR (%)\n0\n5\n10\nIter\nCambridge Dictionary\n0\n20\nRelative SR (%)\n0\n5\n10\nIter\nHuggingface\n10\n0\nRelative SR (%)\n0\n5\n10\nIter\nAmazon\ne\nFigure 6: Dynamics of TTI during training. For TTI, the green area represents the phase where the maximum allowed\ninteraction horizon is the largest (‚Ñé= 30), per our multiplicative schedule. All results are evaluated on a held-out subset of\nWebVoyager, not on the training tasks. a: Average trajectory length, i.e. the average number of steps taken in a trajectory\nnormalized by the average length at the first iteration (iteration 0). b: Ratio of the sum of GoBack and Bing actions out of all\nactions normalized by the first iteration. c: The average number of tokens each action uses, i.e., the average CoT length during\nagent reasoning. d: Average task success rates for TTI on the held-out validation set of tasks. e: Per-domain success rates for\nTTI. All results are evaluated on a held-out subset of WebVoyager tasks. Observe that TTI learns to utilize more interaction\nsteps and explores by calling GoBack and Bing actions once the maximum allowed horizon increases to peak value (i.e., in the\ngreen shaded area). The number of tokens appearing every step reduces for TTI compared to the initialization, resulting in\nsignificantly shorter CoT compared to the run with ‚Ñé= 10.\nthe agent, we measure the number of GoBack and Bing actions over the course of training. GoBack actions\nmeasure the number of retries the agent makes within an episode to get unstuck during exploration.\nBing actions correspond to the number of times the agent attempts to seek information by moving to\nbing.com. As shown in Figure 6 (a, b, and d), the performance of TTI improves substantially as the\nnumber of GoBack and Bing actions and the trajectory length grow.\nAlso note that the trajectory length and the numbers of GoBack and Bing actions begin to increase with\nTTI, once the maximum allowed horizon length is increased as a part of the curriculum schedule (this\nregime is shown by the green shaded area in Figure 6). In contrast, these quantities continuously decrease\nover the course of training for the run with a lower number of maximum interaction steps (‚Ñé= 10). We\nalso find that the trajectory length shoots up substantially for the run with ‚Ñé= 30 and this correlates\nwith worse performance. Finally, as shown in Figure 6 (c) we also note that as the agent‚Äôs trajectory\ngrows longer with TTI, the number of tokens appearing in per-step reasoning actually becomes smaller.\nThis implies that our agent is automatically learning to tradeoff interaction for per-step compute in order\nto attain higher performance, and perhaps prevents any issues with overthinking.\n11\n"
    },
    {
      "page_number": 12,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n6.2. Case Studies: Strengths and Failure Modes\nWe conduct detailed case studies to analyze how TTI behaves across tasks and domains. These cases\nhighlight both the strengths and remaining limitations of our approach.\nStrength: Effective exploration in complex tasks (example visualized in Appendix E.4). For complex,\nexploratory tasks that require information retrieval, TTI trains the agent to extend its interaction horizon\nthrough search and backtracking, thus gathering and comparing information before making decisions.\nFor instance, when tasked to find the baking temperature of an apple pie recipe with 4+ stars and 50+\nreviews, our agent first selects a recipe but encounters a pop-up it cannot dismiss due to backend issues.\nIt then tries another recipe but finds no baking temperature. Returning to the listing again, it correctly\nidentifies one that meets all criteria. We also observe that such behaviors emerge progressively. In early\ntraining with shorter horizon, TTI-agent tends to stick to the first recipe it finds, keeps retrying it and\nsaying ‚ÄúI remember seeing one with 613 ratings earlier‚Äù instead of seeking alternatives. Only after the\ntraining horizon becomes longer does it learn to explore and backtrack. This illustrates that when TTI\nruns a curriculum over interaction length, it teaches agents to adjust their horizon within a task and shift\nfrom exploitation to exploration. In contrast, training with a fixed short horizon can make it difficult to\ndevelop such exploratory behaviors.\nStrength: Strategic exploitation in simple tasks (Appendix E.5). For simpler tasks with clear, deter-\nministic paths (e.g., form filling or direct lookups), TTI-agent completes tasks efficiently without over-\nexploration. For example, when instructed to find the ‚Äútop trending open-source project on machine\nlearning‚Äù in GitHub, the agent goes directly to the Open-Source menu, selects the Trending tab, and\nperforms search. This shows that TTI balances exploration and exploitation based on task context.\nDespite these strengths, we also observe characteristic failure modes that point to areas for improvement\nand may partly explain the agent‚Äôs lower performance on domains like GitHub.\nFailure mode: over-reliance on resets (Appendix E.6). When an action fails, our agent can reset the\ntask by returning to the Bing search page rather than attempting recovery within the target domain. This\nsuggests the agent treats search as a universal fallback, even when more domain-specific actions (e.g.,\nrevisiting menus, refining filters) would be more effective. We also observe repeated resets within the\nsame trajectory, indicating a lack of adaptive error recovery. While agents can extend horizons through\nboth resetting and backtracking, the latter is often more natural. This highlights an area where TTI could\nimprove by guiding exploration more systematically and enforcing structure. We believe that using dense\nrewards [69, 72] or running full multi-turn RL with value functions [98] may address this issue.\nFailure mode: limited self-verification (Appendix E.7). We also observe that the agent can fail to verify\nits actions against the task goal, especially in the last step. In one case, the agent identifies a 2021 GitHub\nrepository for a task requiring one from 2022. While it explicitly acknowledges the mismatch, ‚ÄúIt was\ncreated in 2021, not 2022, so it doesn‚Äôt meet the criteria‚Äù, it still submits it as the answer. This implies\nlimited self-verification ability and could be mitigated by longer, more deliberate per-step reasoning. An\nimportant next step is to enrich TTI with the cognitive behaviors that enable per-step reasoning [99].\n6.3. WebArena Results and Analysis\nWe further assess TTI on the full WebArena [2] (we only use the prompting-based verifier for training,\nbut use the original benchmark evaluators for evaluation). As shown in Table 4 and Figure 5 (right), TTI\nobtains the highest performance among open-source agents trained entirely using a self-improvement\n12\n"
    },
    {
      "page_number": 13,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nTable 4: Full WebArena results. For proprietary agents, we include the top 8 from the official leaderboard. Observe that TTI\nattains the best performance on an average, with especially better performance on the CMS domain.\nMethod\nBackbone\nAverage\nShopping\nCMS\nReddit\nGitLab\nMaps\nProprietary-Based\nIBM CUGA [100]\n-\n61.7\n-\n-\n-\n-\n-\nOpenAI CUA [4]\n-\n58.1\n-\n-\n-\n-\n-\nJace AI [101]\n-\n57.1\n-\n-\n-\n-\n-\nScribeAgent [14]\nGPT-4o + Qwen2.5 32B\n53.0\n45.8\n37.9\n73.7\n59.7\n56.3\nAgentSymbiotic [18]\nClaude 3.5 + Llama 3.1 8B\n48.5\n48.7\n41.2\n63.2\n47.2\n57.8\nLearn-by-Interact [64]\nClaude 3.5 Sonnet\n48\n-\n-\n-\n-\n-\nAgentOccam-Judge [33]\nGPT-4\n45.7\n43.3\n46.2\n67.0\n38.9\n52.3\nWebPilot [34]\nGPT-4o\n37.2\n36.9\n24.7\n65.1\n39.4\n33.9\nFully Open-Source\nLearn-by-Interact [64]\nCodestral 22B\n24.2\n-\n-\n-\n-\n-\n(Self-Improvement)\nAgentTrek [37]\nQwen2.5 32B\n22.4\n-\n-\n-\n-\n-\nAutoWebGLM [65]\nChatGLM3 6B\n18.2\n-\n-\n-\n-\n-\nNNetnav [102]\nLlama 3.1 8B\n7.2\n7.4\n4.2\n0\n0\n28.5\nZero-Shot Baseline\nGemma 3 12B\n18.3\n26.7\n8.7\n30.9\n5.5\n27.7\nFixed ‚Ñé= 10\nGemma 3 12B\n23.8\n28.4\n15.6\n26.0\n13.2\n34.7\nFixed ‚Ñé= 30\nGemma 3 12B\n19.0\n25.7\n9.7\n29.8\n8.7\n28.57\nTTI (Ours)\nGemma 3 12B\n26.1\n33.9\n15.5\n35.3\n15.7\n40.5\nprocedure on self-generated data, without relying on proprietary models for task completion or distillation.\nWhile TTI improves over the zero-shot baseline by 7.8%, the gains are smaller than on WebVoyager,\npossibly because: (1) WebArena tasks are more complex, as reflected in lower accuracies even for\nproprietary models, leading to fewer successful rollouts per iteration and slower learning; (2) Agents\noccasionally attempt actions that are valid in real-world domains but invalid in WebArena counterparts\n(for example, search works reliably on Reddit but fails in WebArena‚Äôs Postmill due to environment bugs).\nMore experiment details are in Appendix D.6.\nFigure 7: We apply test-time re-checks to\nTTI checkpoints. Note that inference-time\nre-checking improves performance on all TTI\ncheckpoints, and more importantly, training\nfurther with TTI improves performance more\nthan inference-time re-checks as expected.\nFurther scaling. While TTI equips agents with the ability to ad-\njust their interaction horizon during deployment, an open ques-\ntion remains: Can we further amplify performance by combining\nTTI with inference-time interaction scaling techniques such as\nre-checking as discussed in Section 4? To explore this, we ap-\nply the ‚Äúcheck-again‚Äù strategy (Section 4) to intermediate TTI\ncheckpoints. Due to the high evaluation cost associated with\nevaluating on full WebVoyager or WebArena, we leverage the\nWebArena subset checkpoints obtained in Section 5.2.\nAs shown in Figure 7, applying re-checking on top of TTI improves\ntask success across various training stages. The benefits are more\nobvious in the early stages of training, when the agent has a\nstronger bias to terminate prematurely. As training progresses,\nTTI encourages longer interaction traces that naturally incorpo-\nrate behaviors like re-checking, reducing the added benefit of explicit re-checks. Nonetheless, even in\nlater stages, re-checking does continue to provide modest gains in performance, serving as a safety-check\nfor well-trained agents.\n13\n"
    },
    {
      "page_number": 14,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n7. Conclusion and Future Work\nIn this work, we introduced interaction scaling as a new dimension of test-time scaling for interactive\nagents. Through empirical studies on web agents, we validate that interaction scaling enables agents to\nexplore and adapt dynamically, significantly improving task performance. Despite the promising results,\nthere are a number of avenues for future work which we list below.\n‚Ä¢ Extension to other agentic problem domains. While we only study the effects of interaction\nscaling on web environments, we believe that this procedure is likely going to be even more effective\nin domains that admit more stochasticity and uncertainty, such as robotic control or open-world\ncompute use problems. In these settings, solving tasks would require gather information first before\nattempting to solve the task. By utilizing interaction as an effective tool for information gathering,\nwe can attain much better performance.\n‚Ä¢ Balancing the tradeoff between thinking more and acting more. While we illustrate that\nspending a given amount of total token budget on acting for longer can be more effective than\nreasoning for longer, an interesting open question is how we should incentivize RL training to\nbest balance thinking and acting during the process of learning. In our experiments, we observed\nan increased preference towards acting longer, with the number of tokens in per-step reasoning\ndecreasing compared to the initialization. While this results in better use of total overall test-time\ncompute given our results in Section 4, it is unclear as to what a general thumb rule and training\nprocedure should be to attain the best tradeoff between acting longer and thinking more.\n‚Ä¢ Improved RL algorithms for powering interaction scaling. The RL approach we utilize in this\nwork is extremely simple as it corresponds to online filtered behavior cloning (BC). An immediate\nnext promising direction is to extend TTI to utilize negative gradients [86] via GRPO [78] or\nPPO [103]. However, stochasticity and, even non-stationarity, in interactive agent environments\nsuggests that RL algorithms that train value functions [21, 98] are likely to be more successful\nat effective credit assignment as the task horizon is scaled further. In addition, we will need to\naddress challenges pertaining to memory and context length, that is very likely to overflow as we\nscale horizon even further. Tackling any of these challenges would be exciting for future work.\nAcknowledgments\nWe thank Jiayi Pan, Shanda Li, and Yuxiao Qu for feedback and informative discussions on an earlier\nversion of this paper. This work was supported in part by the National Science Foundation grants\nIIS1705121, IIS1838017, IIS2046613, IIS2112471, the Office of Naval Research under N00014-24-1-\n2206, and funding from Meta, Morgan Stanley, Amazon, Google, and Scribe. We specially thank Scribe,\nThe AGI company, and CMU FLAME Center (Orchard cluster) for providing a part of the GPU resources\nthat supported this work. Any opinions, findings and recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of any of these funding agencies or\nemployers.\n14\n"
    },
    {
      "page_number": 15,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nReferences\n[1] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan,\nand Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models,\n2024. URL https://arxiv.org/abs/2401.13919.\n[2] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nTianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web\nenvironment for building autonomous agents. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=oKn9c6ytLx.\n[3] Claude. Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku, 2024. URL\nhttps://www.anthropic.com/news/3-5-models-and-computer-use.\n[4] OpenAI.\nIntroducing\noperator,\n2025.\nURL\nhttps://openai.com/index/\nintroducing-operator/.\n[5] Magnus M√ºller and Gregor ≈Ωuniƒç. Browser use: Enable ai to control your browser, 2024. URL\nhttps://github.com/browser-use/browser-use.\n[6] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan\nWang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents,\n2023.\n[7] Claude.\nYour code‚Äôs new collaborator, 2025.\nURL https://www.anthropic.com/\nclaude-code.\n[8] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess,\nAdnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh,\nLachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin\nLeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z.\nRen, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner,\nQuan Vuong, Homer Rich Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky.\npi0.5: a vision-language-action model with open-world generalization. 2025. URL https:\n//api.semanticscholar.org/CorpusID:277993634.\n[9] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining\nXie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-\nmaking agents via reinforcement learning. ArXiv, abs/2405.10292, 2024. URL https://api.\nsemanticscholar.org/CorpusID:269790773.\n[10] Luyao Yuan, Zipeng Fu, Jingyue Shen, Lu Xu, Junhong Shen, and Song-Chun Zhu. Emergence of\npragmatics from referential game between theory of mind agents, 2021. URL https://arxiv.\norg/abs/2001.07752.\n[11] Luyao Yuan, Dongruo Zhou, Junhong Shen, Jingdong Gao, Jeffrey L Chen, Quanquan\nGu, Ying Nian Wu, and Song-Chun Zhu.\nIterative teacher-aware learning.\nIn M. Ran-\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in\nNeural Information Processing Systems, volume 34, pages 29231‚Äì29245. Curran Associates,\n15\n"
    },
    {
      "page_number": 16,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nInc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/\nf48c04ffab49ff0e5d1176244fdfb65c-Paper.pdf.\n[12] Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, and Lili Yu. Mixture-\nof-mamba: Enhancing multi-modal state-space models with modality-aware sparsity, 2025. URL\nhttps://arxiv.org/abs/2501.16295.\n[13] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James\nTanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy\nGroom, Sergey Levine, and Chelsea Finn. Hi robot: Open-ended instruction following with\nhierarchical vision-language-action models. ArXiv, abs/2502.19417, 2025. URL https://api.\nsemanticscholar.org/CorpusID:276618098.\n[14] Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and\nAmeet Talwalkar. Scribeagent: Towards specialized web agents using production-scale workflow\ndata. ArXiv, abs/2411.15004, 2024. URL https://api.semanticscholar.org/CorpusID:\n274192657.\n[15] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web. In Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://\nopenreview.net/forum?id=kiYqbO3wqw.\n[16] Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, and Ameet\nTalwalkar. Cross-modal fine-tuning: align then refine. In Proceedings of the 40th International\nConference on Machine Learning, 2023.\n[17] Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolo Fusi. Tag-llm:\nRepurposing general-purpose llms for specialized domains, 2024.\n[18] Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Z.\nAgudelo, Peter Qian, and Tianlong Chen. Symbiotic cooperation for web agents: Harnessing\ncomplementary strengths of large and small llms. ArXiv, abs/2502.07942, 2025.\n[19] Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, and\nErran L. Li. Proposer-agent-evaluator(pae): Autonomous skill discovery for foundation model\ninternet agents. ArXiv, abs/2412.13194, 2024.\n[20] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Di-\ngirl: Training in-the-wild device-control agents with autonomous reinforcement learning. ArXiv,\nabs/2406.11896, 2024.\n[21] Hao Bai, Yifei Zhou, Erran L. Li, Sergey Levine, and Aviral Kumar. Digi-q: Learning q-value\nfunctions for training device-control agents. ArXiv, abs/2502.15760, 2025.\n[22] Junhong Shen and Lin F. Yang. Theoretically principled deep rl acceleration via nearest neighbor\nfunction approximation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(11):9558‚Äì\n9566, May 2021. doi: 10.1609/aaai.v35i11.17151. URL https://ojs.aaai.org/index.\nphp/AAAI/article/view/17151.\n16\n"
    },
    {
      "page_number": 17,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[23] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue\nYang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. Webrl:\nTraining llm web agents via self-evolving online curriculum reinforcement learning, 2025. URL\nhttps://arxiv.org/abs/2411.02337.\n[24] Claude. Claude takes research to new places, 2025. URL https://www.anthropic.com/\nnews/research.\n[25] OpenAI.\nIntroducing\ndeep\nresearch,\n2025.\nURL https://openai.com/index/\nintroducing-deep-research/.\n[26] Google Gemini. Gemini deep research, 2025. URL https://gemini.google/overview/\ndeep-research/?hl=en.\n[27] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Fei-Fei Li, Hanna Hajishirzi, Luke S.\nZettlemoyer, Percy Liang, Emmanuel J. Candes, and Tatsunori Hashimoto. s1: Simple test-\ntime scaling. ArXiv, abs/2501.19393, 2025. URL https://api.semanticscholar.org/\nCorpusID:276079693.\n[28] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws:\nAn empirical analysis of compute-optimal inference for LLM problem-solving. In The Thirteenth\nInternational Conference on Learning Representations, 2025. URL https://openreview.net/\nforum?id=VNckp7JEHn.\n[29] Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan,\nCraig Boutilier, Rishabh Agarwal, Aviral Kumar, and Aleksandra Faust. Inference-aware fine-\ntuning for best-of-n sampling in large language models. ArXiv, abs/2412.15287, 2024. URL\nhttps://api.semanticscholar.org/CorpusID:274965054.\n[30] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute\noptimally can be more effective than scaling parameters for reasoning. In The Thirteenth Interna-\ntional Conference on Learning Representations, 2025. URL https://openreview.net/forum?\nid=4FWAwZtd2n.\n[31] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and\nJie Tang. Webglm: Towards an efficient web-enhanced question answering system with human\npreferences, 2023.\n[32] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane\nGu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models,\n2024. URL https://arxiv.org/abs/2305.11854.\n[33] Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and\nHuzefa Rangwala. Agentoccam: A simple yet strong baseline for llm-based web agents, 2024.\nURL https://arxiv.org/abs/2410.13825.\n[34] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and\nautonomous multi-agent system for web task execution with strategic exploration, 2024. URL\nhttps://arxiv.org/abs/2408.15978.\n17\n"
    },
    {
      "page_number": 18,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[35] Junhong Shen, Tanya Marwah, and Ameet Talwalkar. UPS: Efficiently building foundation models\nfor PDE solving via cross-modal adaptation. Transactions on Machine Learning Research, 2024.\nISSN 2835-8856. URL https://openreview.net/forum?id=0r9mhjRv1E.\n[36] Junhong Shen, Kushal Tirumala, Michihiro Yasunaga, Ishan Misra, Luke Zettlemoyer, Lili Yu, and\nChunting Zhou. Cat: Content-adaptive image tokenization, 2025. URL https://arxiv.org/\nabs/2501.03120.\n[37] Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong,\nand Tao Yu. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. ArXiv,\nabs/2412.09605, 2024.\n[38] Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based web\nagents, 2024. URL https://arxiv.org/abs/2410.16464.\n[39] Zongzhe Xu, Ritvik Gupta, Wenduo Cheng, Alexander Shen, Junhong Shen, Ameet Talwalkar, and\nMikhail Khodak. Specialized foundation models struggle to beat supervised baselines, 2024. URL\nhttps://arxiv.org/abs/2411.02796.\n[40] Junhong Shen, Abdul Hannan Faruqi, Yifan Jiang, and Nima Maftoon. Mathematical reconstruction\nof patient-specific vascular networks based on clinical images and global optimization. IEEE Access,\n9:20648‚Äì20661, 2021. doi: 10.1109/ACCESS.2021.3052501.\n[41] Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, and Ameet\nTalwalkar. Codepde: An inference framework for llm-driven pde solver generation, 2025. URL\nhttps://arxiv.org/abs/2505.08783.\n[42] Boyuan Zheng, Michael Y. Fatemi, Xiaolong Jin, Zora Zhiruo Wang, Apurva Gandhi, Yueqi Song,\nYu Gu, Jayanth Srinivasa, Gaowen Liu, Graham Neubig, and Yu Su. Skillweaver: Web agents can\nself-improve by discovering and honing skills. 2025. URL https://api.semanticscholar.\norg/CorpusID:277634081.\n[43] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous\nevaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.\n[44] Junhong Shen, Mikhail Khodak, and Ameet Talwalkar. Efficient architecture search for diverse\ntasks. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[45] Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, and Ameet Talwalkar.\nNAS-bench-360: Benchmarking neural architecture search on diverse tasks. In Advances in Neural\nInformation Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2022.\n[46] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae,\nand Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for\nlarge language model agents. arXiv preprint arXiv:2403.08978, 2024.\n[47] OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774.\n[48] Anthropic. Introducing the next generation of claude, 2024. URL https://www.anthropic.\ncom/news/claude-3-family.\n18\n"
    },
    {
      "page_number": 19,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[49] Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for\nweb actions. In Conference on Language Modeling (COLM), 2024. URL https://arxiv.org/\nabs/2310.03720.\n[50] Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, and Ravi Kokku.\nAgent-e: From autonomous web navigation to foundational design principles in agentic systems.\nArXiv, abs/2407.13032, 2024.\n[51] Wenduo Cheng, Junhong Shen, Mikhail Khodak, Jian Ma, and Ameet Talwalkar. L2g: Repurposing\nlanguage models for genomics tasks. bioRxiv, 2024. doi: 10.1101/2024.12.09.627422. URL\nhttps://www.biorxiv.org/content/early/2024/12/11/2024.12.09.627422.\n[52] Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anu-\nmanchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents\nfor long-horizon tasks, 2025.\n[53] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava,\nCosmin Paduraru, George Tucker, Doina Precup, Feryal M. P. Behbahani, and Aleksandra Faust.\nTraining language models to self-correct via reinforcement learning. ArXiv, abs/2409.12917,\n2024.\n[54] Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language\nmodel agents. arXiv preprint arXiv:2407.01476, 2024.\n[55] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar\nprompting with memory for computer control. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=Pc8AU1aF5e.\n[56] Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv\npreprint arXiv:2409.07429, 2024.\n[57] Revanth Gangi Reddy, Sagnik Mukherjee, Jeonghwan Kim, Zhenhailong Wang, Dilek Hakkani-Tur,\nand Heng Ji. Infogent: An agent-based framework for web information aggregation. ArXiv,\nabs/2410.19054, 2024.\n[58] Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. Bagel: Boot-\nstrapping agents by guiding exploration with language. arXiv preprint arXiv:2403.08140, 2024.\n[59] Shikhar Murty, Dzmitry Bahdanau, and Christopher D. Manning. Nnetscape navigator: Complex\ndemonstrations for web agents without a demonstrator, 2024. URL https://arxiv.org/abs/\n2410.02907.\n[60] Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, and Ruslan Salakhutdinov. To-\nwards internet-scale training for agents. ArXiv, abs/2502.06776, 2025. URL https://api.\nsemanticscholar.org/CorpusID:276249229.\n[61] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Ramesh Motwani, Chelsea Finn, Divyansh Garg,\nand Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. ArXiv,\nabs/2408.07199, 2024. URL https://api.semanticscholar.org/CorpusID:271865516.\n19\n"
    },
    {
      "page_number": 20,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[62] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar.\nArcher: Training\nlanguage model agents via hierarchical multi-turn rl. ArXiv, abs/2402.19446, 2024. URL https:\n//api.semanticscholar.org/CorpusID:268091206.\n[63] Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. Distrl: An asyn-\nchronous distributed reinforcement learning framework for on-device control agents. ArXiv,\nabs/2410.14803, 2024. URL https://api.semanticscholar.org/CorpusID:273501605.\n[64] Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan √ñ. Arik. Learn-by-\ninteract: A data-centric framework for self-adaptive agents in realistic environments. ArXiv,\nabs/2501.10893, 2025.\n[65] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen\nZhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: A large language model-based\nweb navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pages 5295‚Äî-5306, 2024.\n[66] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally\ncan be more effective than scaling model parameters.\nArXiv, abs/2408.03314, 2024.\nURL\nhttps://api.semanticscholar.org/CorpusID:271719990.\n[67] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws:\nAn empirical analysis of compute-optimal inference for problem-solving with language models.\n2024. URL https://api.semanticscholar.org/CorpusID:271601023.\n[68] Karl Cobbe, Vineet Kosaraju, Mo Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL https:\n//api.semanticscholar.org/CorpusID:239998651.\n[69] Amrith Rajagopal Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh\nAgarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling\nautomated process verifiers for llm reasoning.\nArXiv, abs/2410.08146, 2024.\nURL https:\n//api.semanticscholar.org/CorpusID:273233462.\n[70] Amrith Rajagopal Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar.\nScaling test-\ntime compute without verification or rl is suboptimal. ArXiv, abs/2502.12118, 2025. URL\nhttps://api.semanticscholar.org/CorpusID:276422443.\n[71] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learn-\ning. ArXiv, abs/2501.12948, 2025. URL https://api.semanticscholar.org/CorpusID:\n275789950.\n[72] Yuxiao Qu, Matthew Y. R. Yang, Amrith Rajagopal Setlur, Lewis Tunstall, Edward Beeching, Ruslan\nSalakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-\ntuning. ArXiv, abs/2503.07572, 2025. URL https://api.semanticscholar.org/CorpusID:\n276928248.\n[73] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin,\nKefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu,\n20\n"
    },
    {
      "page_number": 21,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nLi Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in\nllm agents via multi-turn reinforcement learning, 2025. URL https://arxiv.org/abs/2504.\n20073.\n[74] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models, 2023. URL https://arxiv.org/\nabs/2210.03629.\n[75] Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu. Exact:\nTeaching ai agents to explore with reflective-mcts and exploratory learning. ArXiv, abs/2410.02052,\n2024. URL https://api.semanticscholar.org/CorpusID:273098809.\n[76] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.\nGenerative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240,\n2024.\n[77] Can Jin, Hongwu Peng, Qixin Zhang, Yujin Tang, Dimitris N. Metaxas, and Tong Che. Two\nheads are better than one: Test-time scaling of multi-agent collaborative reasoning. 2025. URL\nhttps://api.semanticscholar.org/CorpusID:277781795.\n[78] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li,\nYu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models. ArXiv, abs/2402.03300, 2024.\n[79] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan,\nWen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to\nreason with search for llms via reinforcement learning, 2025. URL https://arxiv.org/abs/\n2503.19470.\n[80] Xu Wan, Wenyue Xu, Chao Yang, and Mingyang Sun. Think twice, act once: A co-evolution\nframework of llm and rl for large-scale decision making, 2025. URL https://arxiv.org/abs/\n2506.02522.\n[81] Zora Zhiruo Wang, Apurva Gandhi, Graham Neubig, and Daniel Fried. Inducing programmatic skills\nfor agentic tasks. 2025. URL https://api.semanticscholar.org/CorpusID:277634286.\n[82] Gemma Team. Gemma 3 technical report. ArXiv, abs/2503.19786, 2025. URL https://api.\nsemanticscholar.org/CorpusID:277313563.\n[83] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching\nlanguage model agents how to self-improve. Advances in Neural Information Processing Systems,\n37:55249‚Äì55285, 2024.\n[84] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods\nfor reinforcement learning with function approximation. Advances in neural information processing\nsystems, 12, 1999.\n[85] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems, 35:15476‚Äì15488, 2022.\n21\n"
    },
    {
      "page_number": 22,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[86] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano\nErmon, Chelsea Finn, and Aviral Kumar.\nPreference Fine-Tuning of LLMs Should Leverage\nSuboptimal, On-Policy Data, ICML 2024.\n[87] Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M. Bayen, Sham M. Kakade,\nIgor Mordatch, and P. Abbeel. Variance reduction for policy gradient with action-dependent\nfactorized baselines. ArXiv, abs/1803.07246, 2018. URL https://api.semanticscholar.\norg/CorpusID:4043645.\n[88] Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement of\npolicy gradient estimation. Neural networks : the official journal of the International Neural Network\nSociety, 26:118‚Äì29, 2011. URL https://api.semanticscholar.org/CorpusID:2274728.\n[89] Dotan Di Castro, Aviv Tamar, and Shie Mannor. Policy gradients with variance related risk criteria.\narXiv: Learning, 2012. URL https://api.semanticscholar.org/CorpusID:3109162.\n[90] Yoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In\nInternational Conference on Machine Learning, 2009. URL https://api.semanticscholar.\norg/CorpusID:873046.\n[91] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone.\nCurriculum learning for reinforcement learning domains: A framework and survey.\nArXiv,\nabs/2003.04960, 2020. URL https://api.semanticscholar.org/CorpusID:212657666.\n[92] Xin Wang, Yudong Chen, and Wenwu Zhu. A survey on curriculum learning. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 44:4555‚Äì4576, 2021. URL https://api.\nsemanticscholar.org/CorpusID:232362223.\n[93] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher‚Äìstudent curriculum\nlearning. IEEE Transactions on Neural Networks and Learning Systems, 31:3732‚Äì3740, 2017. URL\nhttps://api.semanticscholar.org/CorpusID:8432394.\n[94] Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Joshua Tobin, P. Abbeel, and Wojciech Zaremba. Hindsight experience replay. In\nNeurIPS, 2017. URL https://api.semanticscholar.org/CorpusID:3532908.\n[95] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\n[96] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. Proceedings of\nthe 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:221191193.\n[97] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao\nLi, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native\nagents. arXiv preprint arXiv:2501.12326, 2025.\n22\n"
    },
    {
      "page_number": 23,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n[98] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language\nmodel agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.\n[99] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, nathan lile, and Noah D. Goodman. Cognitive\nbehaviors that enable self-improving reasoners, or, four habits of highly effective stars. ArXiv,\nabs/2503.01307, 2025. URL https://api.semanticscholar.org/CorpusID:276741915.\n[100] Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, Ido Levy, Aviad Sela, Asaf Adi, and Nir\nMashkif. Towards enterprise-ready computer using generalist agent. ArXiv, abs/2503.01861,\n2025. URL https://api.semanticscholar.org/CorpusID:276775684.\n[101] JaceAI.\nAwa\n1.5\nachieves\nbreakthrough\nperformance\non\nwe-\nbarena\nbenchmark,\n2024.\nURL\nhttps://www.jace.ai/post/\nawa-1-5-achieves-breakthrough-performance-on-webarena-benchmark.\n[102] Shikhar Murty, Dzmitry Bahdanau, and Christopher D. Manning. Nnetnav: Unsupervised learning\nof browser agents through environment interaction in the wild. 2024. URL https://api.\nsemanticscholar.org/CorpusID:273162280.\n[103] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347, 2017.\n23\n"
    },
    {
      "page_number": 24,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nAppendices\nA. Broader Impact\nThis work contributes to the development of more adaptive and capable AI agents by introducing a\nnew test-time scaling dimension focused on interaction rather than per-step reasoning alone. While this\napproach improves robustness and generalization in open-ended environments, it also raises important\nconsiderations. Increased agent autonomy can amplify both the benefits and risks of deployment in\nreal-world systems. Moreover, agents capable of richer behaviors could be applied to sensitive domains\n(e.g., customer service, education, or automation workflows) where unintended actions could have\nlarge impacts. We encourage future work to consider ethical safeguards, interpretability tools, and\nhuman-in-the-loop designs when deploying interaction-scaled agents. Our experiments are conducted\nentirely in simulated environments, and we hope this work inspires further research on controllable and\ntrustworthy agent behavior under realistic constraints.\nB. Observation Space Design\nWe use the screenshot accompanied with the web page‚Äôs accessibility tree as our main observation. We\nstudy two versions of accessibility tree. Rich accessibility tree is modified from the WebArena code and\nlooks like:\n[21]: RootWebArea ‚ÄôDashboard / Magento Admin‚Äô focused: True; [0]: link ‚ÄôMagento Admin Panel‚Äô;\n[1]: link ‚ÄôDASHBOARD‚Äô; [2]: link ‚ÄôSALES‚Äô; [3]: link ‚ÄôCATALOG‚Äô; [4]: link ‚ÄôCUSTOMERS‚Äô; [5]: link\n‚ÄôMARKETING‚Äô; [6]: link ‚ÄôCONTENT‚Äô; [7]: link ‚ÄôREPORTS‚Äô; [8]: link ‚ÄôSTORES‚Äô; [22]: link ‚ÄôSYSTEM‚Äô; [23]:\nlink ‚ÄôFIND PARTNERS & EXTENSIONS‚Äô; [24]: heading ‚ÄôDashboard‚Äô; [9]: link ‚Äôadmin‚Äô; [10]: link ‚Äù; [25]:\nStaticText ‚ÄôScope:‚Äô; [12]: button ‚ÄôAll Store Views‚Äô hasPopup: menu; [13]: link ‚ÄôWhat is this?‚Äô; [14]: button\n‚ÄôReload Data‚Äô...\nSimple accessibility tree is modified from the PAE code and looks like:\n[1]: \"Dashboard\"; [2]: \"Sales\"; [3]: \"Catalog\"; [4]: \"Customers\"; [5]: \"Marketing\"; [6]: \"Content\"; [7]:\n\"Reports\"; [8]: \"Stores\"; [9]: \"admin\"; [12]: <button> \"All Store Views\"; [13]: \"What is this?\"; [14]:\n<button> \"Reload Data\"; [15]: \"Go to Advanced Reporting\"; [16]: \"here\";...\nRich tree contains more details such as the HTML tag and attributes like required, hasPopup compared\nto simple tree. However, it is much longer than simple tree and hence harder to optimize due to the\nincreased context length.\nC. TTI Implementation\nWe provide the pseudocode in Algorithm 1. For the replay buffer, to encourage the agent to learn from\nmore recent examples, we assign weights based on recency when sampling rollouts to update the agent:\nfor the ùëò-th trajectory added to the buffer, its weight is\nùëò\n|ùíü|.\n24\n"
    },
    {
      "page_number": 25,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nAlgorithm 1 TTI: Filtered Behavior Cloning with Interaction Scheduling\n1: Input: Agent policy ùúãùúÉ, Evaluator ‚Ñõ, Environment ‚Ñ∞, Learning rate ùõº, Replay buffer ùíü, Interaction\nscheduler hyperparameters ‚Ñémin, ‚Ñémax\n2: Initialize policy ùúãùúÉfrom pretrained model\n3: Initialize replay buffer ùíü‚Üê{}\n4: for each episode ùëñdo\n5:\nSet interaction horizon ‚Ñéùëñ‚Üêget_schedule(ùëñ, ‚Ñémin, ‚Ñémax)\n6:\nfor each rollout to collect do\n7:\nInitialize environment: ùë†0 ‚àº‚Ñ∞\n8:\nfor each ùë°in [1, ‚Ñéùëñ] do\n9:\nObserve current state ùë†ùë°\n10:\nPredict action ÀÜùëéùë°‚ÜêùúãùúÉ(ùë†ùë°)\n11:\nExecute action ÀÜùëéùë°in environment\n12:\nObserve next state ùë†ùë°+1\n13:\nif episode done then\n14:\nCompute reward ùëüùë°‚Üê‚Ñõ(ùë†ùë°, ÀÜùëéùë°)\n15:\nelse\n16:\nùëüùë°‚Üê0\n17:\nend if\n18:\nStore transition: ùíü‚Üêùíü‚à™{(ùë†ùë°, ÀÜùëéùë°, ùëüùë°, ùë†ùë°+1)}\n19:\nend for\n20:\nend for\n21:\nfor sample successful trajectory in ùíüdo\n22:\nfor ùë°= 1 to ‚Ñéstop do\n23:\nAccumulate loss: ùêø(ùúÉ) ‚Üêùêø(ùúÉ) + CrossEntropy(ùúãùúÉ(ùë†ùë°), ÀÜùëéùë°)\n24:\nend for\n25:\nend for\n26:\nUpdate policy: ùúÉ‚ÜêùúÉ‚àíùõº‚àáùúÉùêø(ùúÉ)\n27: end for\nD. WebArena Experiments\nD.1. WebArena Agent Prompt\nCoT Prompt\nImagine you are an agent browsing the web, just like humans. Now you need to complete a task.\nIn each iteration, you will receive an observation that includes the accessibility tree of the webpage\nand a screenshot of the current viewpoint. The accessbility tree contains information about the\nweb elements and their properties. The screenshot will feature numerical labels placed in the TOP\nLEFT corner of web elements in th current viewpoint. Carefully analyze the webpage information\nto identify the numerical label corresponding to the web element that requires interaction, then\nfollow the guidelines and choose one of the following actions:\n1. Click a web element.\n25\n"
    },
    {
      "page_number": 26,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n2. Delete existing content in a textbox and then type content.\n3. Scroll up or down the whole window.\n4. Go back, returning to the previous webpage.\n5. Answer. This action should only be chosen when all questions in the task have been solved.\nCorrespondingly, action should STRICTLY follow the format specified by one of the following lines:\nClick [numerical_label]\nType [numerical_label] [content]\nScroll [up/down]\nGoBack\nANSWER [content]\nSome examples are:\nClick [8]\nType [22] [Boston]\nScroll [down]\nANSWER [06516]\nKey guidelines you MUST follow:\n* Action guidelines *\n- Use either screenshot or accessibility tree to obtain the numerical_label. Sometimes the accessi-\nbility tree captures more elements than the screenshot. It‚Äôs safe to select these elements without\nscrolling\n- For text input, use Type action directly (no need to click first). All existing texts in the textbox\nwill be deleted automatically before typing\n- Preserve text inside quotation marks exactly as provided by user\n- You must not repeat the same actions over and over again. If the same action doesn‚Äôt work, try\nalternative approaches\n- Use ANSWER only after completing ALL task requirements\n- Wrap content for Type and ANSWER with square brackets ‚Äò[]‚Äò\n- Do not add quotation marks for search queries\n* Web navigation hints *\n{hint}\nYour reply should strictly follow the format:\nThought: Your reasoning trace. A good practice is to summarize information on the current web\npage that are relevant to the task goal, then generate a high-level plan that contains the sequence\nof actions you probably need to take\nAction: Based on this reasoning, identify the single most optimal action. You should output it in\nthe format specified above (under \"STRICTLY follow the format\")\nAfter each action, you‚Äôll receive a new observation. Proceed until task completion. Now solve the\nfollowing task.\nTask: {task_goal}\nCurrent URL: {url}\nScreenshot of current viewpoint: attached\nAccessibility tree of current viewpoint: {accessibility_tree}\nBeyond the above CoT prompt, we also tried using a more complex prompt for the thought process.\nHowever, this does not lead to significant gain in downstream accuracy (see Table 5), but it could increase\n26\n"
    },
    {
      "page_number": 27,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\ntraining and inference cost, so we did not use it in the end.\nComplex Prompt\nThought: You must analyze the current webpage thoroughly to guide your decision-making. Show\nyour reasoning through these steps:\n- Summarization: Begin by understanding the page context - identify what type of page you‚Äôre\non (search results, form, article, etc.) and how it relates to your objective. Summarize important\ninformation on the webpage that might be relevant to task completion. Especially when the\ntask requires to return some answers to a specific question, you should note down intermediate\ninformation that helps generate the answer.\n- Planning: Generate a checklist of subtasks required for completion and cross-out the subtasks\nyou‚Äôve completed. Identify the next logical subtask.\n- Verification: Verify all information you‚Äôve entered so far. Check that your inputs match require-\nments in terms of spelling and format (you should not change the user-specified information,\neven if there‚Äôre grammar errors). Verify if any selections for dropdown items align with the task\nobjective. Identify if there‚Äôre necessary fields that have not been filled in. Note that if the last few\nsteps are repeating the same action, there must be missing or incorrect information.\n- Backtracking: If the task requires exploring multiple webpages (e.g., orders, posts, item pages,\netc) to find out an answer, consider if you need to issue GoBack and return to the previous web\npage.\n- Candidate Generation: After all the above reasoning, list the most relevant possible actions,\nevaluate pros and cons of each action, and finally select the most effective action to progress task.\nAction: Choose ONE of the following action formats:\n- Click [numerical_label] - Click a specific element\n- Type [numerical_label] [content] - Input text into a field\n- Scroll [up/down] - Navigate the page vertically\n- GoBack - Return to previous webpage\n- ANSWER [content] - Provide final answer when task is complete\nD.2. WebArena Domain-Specific Prompts\nBelow are the content replacing ‚Äú{hint}‚Äù in the general prompt.\nGeneral Hint\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Clear filters before setting new ones\nReddit\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Pay attention to words like \"latest\", \"newest\", \"hottest\" in the task objective, which require clicking\nthe dropdown menu and select \"New\" or \"Top\" with the correct time range\n27\n"
    },
    {
      "page_number": 28,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n- When selecting a subforum, you can either browse the dropdown menu in the \"Submit\" page or\nnavigate to \"Forums\" and check all subforums by clicking on \"Next\" to go over all pages. You must\ntry to find a subforum that exactly matches your query. If there‚Äôs no exact match, pick the most\nrelevant one, ideally the subforum is about objects or locations contained in the given objective\n- \"Trending\" means \"hot\"\n- To find out all posts or replies from a user, click the user name and then click \"Submissions\" or\n\"Comments\"\nCMS\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Clear filters before setting new ones\n- Use date format: month/day/year (e.g., 1/1/16, 12/31/24)\n- When searching phone numbers, remove the country code\n- When searching product name, use single but not plural form\n- When the web page contains a table, aggregate the rows with the same item\nShopping\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Sort items by price by clicking the dropdown menu and set descending/ascending direction\n- When searching product name, use single but not plural form\n- If the objective requires only finding an item, stop at the item page without adding to cart\n- To find out the quality of a product, search the item, click on review, and inspect its review\n- Click \"Page Next\" to iterate over all orders\n- Since there‚Äôs no way to filter order history, click \"View Order\" for every order within a date range\nand inspect individually. If the condition is not met, go back\nGitLab\n- Always save progress through appropriate buttons (Save, Submit, Post, etc.)\n- Always remember to interact with dropdown options after expanding\n- Clear filters before setting new ones\n- When searching a repo in gitlab, type only the project name after \"/\" in the search box\nMap\n- Always remember to interact with dropdown options after expanding\n- When searching for a place, remove prepositions like in/on/by/at. For example, use \"starbucks,\ncraig street\" instead of \"starbucks on craig street\". Put the city name at the end\n- When there is no results shown up after search, rephrase the address and try again\n- To find direction between two points, after entering the from and to addresses, select the correct\ntransportation (foot/bicycle/car) before clicking \"Go\"\n28\n"
    },
    {
      "page_number": 29,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n- When the given location is not a geological address, use your knowledge to infer the address\nD.3. CoT Experiments for Base Agent\nTo enable efficient rollout collection, we spin up multiple Docker containers on a single GPU according to\nthe official WebArena repository. We use the vLLM [95] engine for inference and apply the following\ninference hyperparameters for most of our experiments.\n‚Ä¢ max_new_tokens: 1024\n‚Ä¢ max_attached_imgs: 4\n‚Ä¢ temperature: 1\n‚Ä¢ top_p: 0.95\nWe randomly subsample 62 test tasks for analysis purposes. Below are the results of zero-shot agent vs\nCoT prompting. ‚ÄúCoT‚Äù uses the ‚ÄúGeneral Prompt‚Äù in Section D.1. ‚ÄúComplex CoT‚Äù uses the ‚ÄúComplex\nPrompt‚Äù in Section D.1.\nTable 5: Base agent results averaged over 3 runs on WebArena subset.\nPrompt\nTask SR (%)\nAction Only\n14.76\nCoT\n23.81\nComplex CoT\n23.33\nD.4. Scaling Trade-off Experiments\n‚ÄúCheck-again‚Äù for interaction scaling.\nAfter the agent outputs the task-stop signal, we append the\nfollowing prompts to the observation to induce it to check again.\nCheck-Again Prompt\nImportant: You returned an answer in the last step. Let‚Äôs pause, check the web page, and think\nagain. If you still think the task is finished, double-check your answer, revise it if need, and return a\nfinal answer. If not, continue the task. Your output should still be in the same ‚ÄúThought:...Action:...‚Äù\nformat.\nWhen applying multiple re-checks, we slightly vary the prompts such as ‚Äò‚ÄòBefore you finalize the answer,\nre-evaluate it in terms of the current web page‚Äîwhat exactly supports or contradicts it?‚Äù or ‚ÄúWhy do I\nbelieve this answer is correct? What on the page justifies it? Could an alternative answer be better?‚Äù Please\nrefer to the code base for the exact prompt used.\nPer-step budget forcing.\nFollowing [72], we use the phrases below to induce longer per-step thinking.\nThe phrases are different to ensure that the model does not run into the scenario of endless repeating a\nphrase.\n29\n"
    },
    {
      "page_number": 30,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n‚Ä¢ First time: Wait, let me think deeper.\n‚Ä¢ Second time: But let me double-check.\n‚Ä¢ Third time: But hold on.\nPer-step best-of-ùëõ.\nWe tried both selecting by log likelihood and majority voting, with the latter\nshowing slightly better results.\nAdditional results for combined scaling.\nBeyond evaluating each scaling method separately, we also\ntried combining methods along different axes.\nTable 6: Comparing different inference-time prompting strategies. Results averaged over 3 runs on WebArena subset.\nAll methods are applied once.\nInference-Time Strategy\nTask SR (%)\nBaseline\n23.81\nCheck-again\n26.14\nBudget-forcing\n24.81\nBest-of-ùëõ\n25.03\nCheck-again + Budget-forcing\n26.33\nCheck-again + Best-of-ùëõ\n27.36\nD.5. TTI and Online Filtered BC Hyperparameters for Preliminary Experiments\nWe use the following hyperparameters to obtain the training curves in Figure 4. During training, the\nvision_tower of Gemma 3 is kept frozen because it is frozen during pretraining. Other hyperparameters\ncan be found in our code.\n‚Ä¢ num_iteration: 10\n‚Ä¢ actor_epochs: 1 # number of epochs to update the actor\n‚Ä¢ rollout_size: 512\n‚Ä¢ num_update_sample_per_iteration: 512\n‚Ä¢ lr: 1e-6\n‚Ä¢ optimizer: AdamW\n‚Ä¢ scheduler: WarmupCosineLR\n‚Ä¢ batch_size: 4\n‚Ä¢ grad_accum_steps: 2\n‚Ä¢ training_gpu_size: 4\n‚Ä¢ eval_horizon: 30\nFor multiplicative curriculum, we use the schedule: 10, 20, 30, 30, ... For additive curriculum, we use the\nschedule: 10, 11, 12, 13, ...\nD.6. TTI Hyperparameters for Full WebArena Experiments\nWe use the following hyperparameters to obtain the full WebArena results for Table 4.\n30\n"
    },
    {
      "page_number": 31,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\n‚Ä¢ num_iteration: 10\n‚Ä¢ horizon_schedule: 10, 20, 20, 30, 30, 30, ...\n‚Ä¢ actor_epochs: 1 # number of epochs to update the actor\n‚Ä¢ rollout_size: 512\n‚Ä¢ num_update_sample_per_iteration: 512\n‚Ä¢ lr: 1e-6\n‚Ä¢ optimizer: AdamW\n‚Ä¢ scheduler: WarmupCosineLR\n‚Ä¢ batch_size: 4\n‚Ä¢ grad_accum_steps: 2\n‚Ä¢ training_gpu_size: 4\n‚Ä¢ eval_horizon: 30\nE. WebVoyager Experiments\nE.1. Task Generator & Evaluator Prompt\nTask Generator Prompt\nYou are a website exploration assistant tasked with discovering potential tasks on websites. These\ntasks should be similar to a user-specified task and aim to complete some high-level goals such as\nbooking restaurants in a website. Your goal is to freely explore websites and propose tasks similar\nto a given set of examples. For each iteration, you‚Äôll receive:\n- An observation with the webpage‚Äôs accessibility tree\n- A screenshot showing numerical labels in the TOP LEFT corner of web elements\nYou will then generate possible tasks while exploring the website. You should imagine tasks that\nare likely proposed by a most likely user of this website. You‚Äôll be given a set of examples for\nreference, but you must not output tasks that are the same as the given examples. The generated\ntasks must be realistic and at least require 3 steps to complete. It cannot be too simple.\n## Response Format and Available Actions\nYour reply for each iteration must strictly follow this format:\nThought: Analyze the current webpage thoroughly to guide your exploration. Examine the\nwebpage‚Äôs structure, content, and interactive elements to identify potential tasks that users might\nperform on this site. Decide whether you want to keep exploring or output some tasks\nTasks:\nIf you think you are ready to generate some tasks, output them in the follow-\ning format (note that different tasks are separated with double semicolons): GENERATE\n[task1;answer1;;task2;answer2]\nAction: Then, to continue with your exploration, choose ONE of the following action formats:\n- Click [numerical_label] - Click a specific element\n- Type [numerical_label] [content] - Input text into a field\n- Scroll [up/down] - Navigate the page vertically\n- GoBack - Return to previous webpage\nExamples:\nClick [8]\nType [22] [Boston]\n31\n"
    },
    {
      "page_number": 32,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nScroll [down]\nGENERATE [Find the company‚Äôs phone number;(555) 123-4567;;Locate the price of the basic\nsubscription plan;$19.99/month]\nYour final output should look like:\nThought: ...\nTasks: GENERATE [...] (this is optional, only generate when you are confident)\nAction: ...\n## Critical Guidelines\n### Action Rules\n- Use either screenshot or accessibility tree to obtain the numerical_label\n- For text input, use Type action directly (no need to click first)\n- Ensure proposed tasks are diverse and demonstrate different aspects of the website. The tasks\nmust have diverse difficulty and require different number of steps (3-20) to complete.\n- Tasks should be clear, specific, achievable, and self-contained. It cannot be too general, e.g.,\nrelated to √§ny post¬®, √§ny product¬®, √§ny place¬®. It must not depend on any context or actions that you\nhave performed, i.e., you must assume zero prior knowledge when someone wants to complete\nthe task\n- Your task should be objective and unambiguous. The carry-out of the task should NOT BE\nDEPENDENT on the user‚Äôs personal information such as the CURRENT TIME OR LOCATION\n- Your tasks should be able to be evaluated OBJECTIVELY. That is, by looking at the last three\nscreenshots and the answer provided by an agent, it should be possible to tell without ambiguity\nwhether the task was completed successfully or not\n- Answers should be precise (e.g., exact prices, specific information, exact text)\n- Your should output both operational tasks (the goal is to complete some steps) and information\nretrieval tasks (the goal is to find some answer to return)\n- You must refer to the examples given and mimic the complexity and task structure. See how\nthese tasks are self-contained and realistic\n- Your proposed task cannot be a single action like click, type! Tasks like ‚ÄôDetermine the number of\nuses for that term‚Äô is unacceptable because it is ambiguous as a stand-alone task; ‚ÄôUncheck Use\nsystem value‚Äô is unacceptable because it is not a complete task; ‚ÄôLocate the total revenue for the\nlast month‚Äô is unacceptable because ‚Äôlast month‚Äô is ambiguous;\nAfter each action, you‚Äôll receive a new observation. Continue exploring and generating tasks.\nHere‚Äôre some examples: {example}\nCurrent URL: {url}\nScreenshot of current viewpoint: attached\nAccessibility tree of current viewpoint: {accessibility_tree}\nEvaluator Prompt\nYou are an expert in evaluating the performance of a web navigation agent. The agent is designed\nto help a human user navigate a website to complete a task. Your goal is to decide whether the\nagent‚Äôs execution is successful or not.\nAs an evaluator, you will be presented with three primary components to assist you in your role:\n1. Web Task Instruction: This is a clear and specific directive provided in natural language, detailing\n32\n"
    },
    {
      "page_number": 33,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nthe online activity to be carried out.\n2. Result Response: This is a textual response obtained after the execution of the web task. It\nserves as textual result in response to the instruction.\n3. Result Screenshots: This is a visual representation of the screen showing the result or interme-\ndiate state of performing a web task. It serves as visual proof of the actions taken in response to\nthe instruction.\n‚Äì You SHOULD NOT make assumptions based on information not presented in the screenshot when\ncomparing it to the instructions.\n‚Äì Your primary responsibility is to conduct a thorough assessment of the web task instruction\nagainst the outcome depicted in the screenshot and in the response, evaluating whether the actions\ntaken align with the given instructions.\n‚Äì NOTE that the instruction may involve more than one task, for example, locating the garage and\nsummarizing the review. Failing to complete either task, such as not providing a summary, should\nbe considered unsuccessful.\n‚Äì NOTE that the screenshot is authentic, but the response provided by LLM is generated at the end\nof web browsing, and there may be discrepancies between the text and the screenshots.\n‚Äì Note that if the content in the Result response is not mentioned on or different from the screenshot,\nmark it as not success.\n‚Äì NOTE that the task may be impossible to complete, in which case the agent should indicate\nthis in the response. CAREFULLY VERIFY THE SCREENSHOT TO DETERMINE IF THE TASK IS\nIMPOSSIBLE TO COMPLETE. Be aware that the agent may fail because of its incorrect actions,\nplease do not mark it as impossible if the agent fails because of its incorrect actions.\nYou should explicit consider the following criterion:\n- Whether the claims in the response can be verified by the screenshot. E.g. if the response claims\nthe distance between two places, the screenshot should show the direction. YOU SHOULD EXPECT\nTHAT THERE IS A HIGH CHANCE THAT THE AGENT WILL MAKE UP AN ANSWER NOT VERIFIED\nBY THE SCREENSHOT.\n- Whether the agent completes EXACTLY what the task asks for. E.g. if the task asks to find a\nspecific place, the agent should not find a similar place.\nIn your responses:\nYou should first provide thoughts EXPLICITLY VERIFY ALL THREE CRITERION and then provide a\ndefinitive verdict on whether the task has been successfully accomplished, either as ‚ÄôSUCCESS‚Äô or\n‚ÄôNOT SUCCESS‚Äô.\nA task is ‚ÄôSUCCESS‚Äô only when all of the criteria are met. If any of the criteria are not met, the\ntask should be considered ‚ÄôNOT SUCCESS‚Äô.\nE.2. Agent Prompt\nWebVoayager\nImagine you are a robot browsing the web, just like humans. Now you need to complete a task. In\neach iteration, you will receive an observation that includes the accessibility tree of the webpage\nand a screenshot of the current viewpoint. The accessbility tree contains information about the\nweb elements and their properties. The screenshot will feature numerical labels placed in the TOP\n33\n"
    },
    {
      "page_number": 34,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nLEFT corner of web elements in the current viewpoint. Carefully analyze the webpage information\nto identify the numerical label corresponding to the web element that requires interaction, then\nfollow the guidelines and choose one of the following actions:\n1. Click a web element.\n2. Delete existing content in a textbox and then type content.\n3. Scroll up or down the whole window.\n4. Go back, returning to the previous webpage.\n5. Navigate to Bing‚Äôs homepage.\n6. Answer. This action should only be chosen when all questions in the task have been solved.\nCorrespondingly, action should STRICTLY follow the format specified by one of the following lines:\nClick [numerical_label]\nType [numerical_label] [content]\nScroll [up/down]\nGoBack\nBing\nANSWER [content]\nSome examples are:\nClick [8]\nType [22] [Boston]\nScroll [down]\nBing\nANSWER [06516]\nKey guidelines you MUST follow:\n* Action guidelines *\n1. The predicted action should be based on elements as long as it‚Äôs accessibility tree OR screenshot.\nSometimes, accessibility tree or screenshot captures more elements than the other, but it‚Äôs fine to\nuse either one.\n2. To input text for search bars, no need to click textbox first, directly type content. After typing,\nthe system automatically hits ‚ÄôENTER‚Äô key.\n3. When a complex task involves multiple questions or steps, select ‚ÄôANSWER‚Äô only at the very\nend, after addressing all of these questions or steps. Double check the formatting requirements in\nthe task when ANSWER. Always think twice before using ‚ÄôANSWER‚Äô action!!!\n4. When specifying the content for ‚ÄôType‚Äô and ‚ÄôANSWER‚Äô actions, be sure to wrap the content with\n‚Äô[]‚Äô.\n5. Use ‚ÄòGoBack‚Äò to return to the previous state, use it when you find the previous action incorrect.\n6. When you see a pop-up page, you should immediately ‚ÄòGoBack‚Äò to the previous page.\n7. Use ‚ÄòBing‚Äò when you need to navigate to a different website or search for new information.\nYour reply should strictly follow the format:\nThought: Your reasoning trace. A good practice is to follow this format:\n- Observation summary: where are you at now? list all elements that are related to the task goal.\ne.g. if you‚Äôre trying to filter something out, list all filters visible.\n- Planning: what sequence of actions do you need take to achieve the task goal? give a high-level\noverview of the steps you need to take.\n- Possible actions: to achieve that plan, what are potential actions you need to do immediately and\n34\n"
    },
    {
      "page_number": 35,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nwhat‚Äôs their effect? List at least 3 actions and analyze each of them.\nAction: Based on this reasoning, identify the single most optimal action. You should output it in\nthe format specified above (\"...STRICTLY follow the format...\").\nAfter you issue an action, the user will execute it and provide a new observation. Now solve the\nfollowing task.\nTask: {task_goal}\nCurrent URL: {url}\nScreenshot of current viewpoint: attached\nAccessibility tree of current viewpoint: {accessibility_tree}\nE.3. Experiment Details\nWe use the following hyperparameters to obtain the WebVoyager results.\n‚Ä¢ num_iteration: 12\n‚Ä¢ horizon_schedule: 10, 20, 20, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30\n‚Ä¢ actor_epochs: 1 # number of epochs to update the actor\n‚Ä¢ rollout_size: 512\n‚Ä¢ num_update_sample_per_iteration: 512\n‚Ä¢ lr: 4e-6\n‚Ä¢ optimizer: AdamW\n‚Ä¢ scheduler: WarmupCosineLR\n‚Ä¢ batch_size_per_gpu: 4\n‚Ä¢ grad_accum_steps: 2\n‚Ä¢ training_gpu_size: 6\n‚Ä¢ eval_horizon: 30 # note that train horizon is different for different methods, but evaluation horizon\nis kept the same\nIn Figure 6, the green area starts from iteration 6, because this is the first iteration where it‚Äôs possible to\nhave all trajectories sampled for training have a horizon of ùêª= 30.\n35\n"
    },
    {
      "page_number": 36,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nE.4. Case Studies: Strengths 1\nTask: Locate a recipe for an American apple pie on Allrecipes with a rating of at least 4 stars and more\nthan 50 reviews. Note the maximum temperature mentioned in the Directions.\nFully trained agent explores:\nStep 1-8\nStep 9-15\n36\n"
    },
    {
      "page_number": 37,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nEarly-stage agent prefers exploitation:\n37\n"
    },
    {
      "page_number": 38,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nE.5. Case Studies: Strengths 2\nTask: Identify the latest top-trending open-source project in the category of ‚ÄòMachine Learning‚Äô on GitHub,\nand check the number of stars it has received.\n38\n"
    },
    {
      "page_number": 39,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nE.6. Case Studies: Fail Modes 1\nTask: On Apple‚Äôs website, how many different types of keyboards are available when customizing your\n14-inch MacBook Pro?\nStep 1-8\nStep 9-16\n39\n"
    },
    {
      "page_number": 40,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nTask: Which university maintains and manages ArXiv. Accessing the university‚Äôs website from ArXiv, how\nmany undergraduate students are currently at the university.\n40\n"
    },
    {
      "page_number": 41,
      "text": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction\nE.7. Case Studies: Fail Modes 2\nTask: Identify a new open-source project on GitHub related to ‚ÄòAI agriculture‚Äô that created in 2022, and\nnote its main programming language and description.\n41\n"
    }
  ]
}