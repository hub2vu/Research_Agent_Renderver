{
  "filename": "117609_Path Gradients after Flow Matching.pdf",
  "total_pages": 31,
  "full_text": "Path Gradients after Flow Matching\nLorenz Vaitl\nlorenz.vaitl@outlook.com\nLeon Klein\nFreie Universität Berlin\nleon.klein@fu-berlin.de\nAbstract\nBoltzmann Generators have emerged as a promising machine learning tool for\ngenerating samples from equilibrium distributions of molecular systems using\nNormalizing Flows and importance weighting. Recently, Flow Matching has helped\nspeed up Continuous Normalizing Flows (CNFs), scale them to more complex\nmolecular systems, and minimize the length of the flow integration trajectories.\nWe investigate the benefits of using Path Gradients to fine-tune CNFs initially\ntrained by Flow Matching, in a setting where the target energy is known. Our\nexperiments show that this hybrid approach yields up to a threefold increase in\nsampling efficiency for molecular systems, all while using the same model, a similar\ncomputational budget and without the need for additional sampling. Furthermore,\nby measuring the length of the flow trajectories during fine-tuning, we show that\nPath Gradients largely preserve the learned structure of the flow.\n1\nIntroduction\nGenerative models, ranging from GANs (Goodfellow et al., 2014) and VAEs (Kingma & Welling,\n2014) to Normalizing Flows (Rezende & Mohamed, 2015; Papamakarios et al., 2019) and Diffusion\nModels (Ho et al., 2020; Song et al., 2021), have advanced rapidly in recent years, driving progress\nboth in media generation and in scientific applications such as simulation-based inference. While\nscientific workflows often incorporate domain-specific symmetries, they tend to under-exploit a\ncrucial resource: the unnormalized target density.\nBoltzmann Generators (Noé et al., 2019) are typically trained either via self-sampling (Boyda et al.,\n2021; Nicoli et al., 2020; Invernizzi et al., 2022; Midgley et al., 2023), leveraging gradients from the\ntarget distribution, or using samples from the target without incorporating gradient information (Nicoli\net al., 2023; Klein et al., 2023b; Draxler et al., 2024; Klein & Noé, 2024). However, these approaches\neach ignore complementary parts of the training signal: either the data or its local geometry. Notably,\nfirst-order information evaluated at target samples remains underused, despite its potential to improve\ntraining. In this work, we close this gap by fine-tuning Continuous Normalizing Flows, initially\ntrained with Flow Matching, using Path Gradients (Roeder et al., 2017; Vaitl, 2024) on samples from\nthe target distribution. Furthermore, our approach requires computing target gradients only once\nper training sample, avoiding the potentially high cost of repeatedly computing gradients on newly\ngenerated samples for self-sampling.\nFlow Matching, a method for training CNFs, is based on target samples. It is closely related to\ndiffusion model training and has recently gained traction for its simulation-free training and strong\nempirical performance, both in generative modeling benchmarks (Lipman et al., 2023; Esser et al.,\n2024; Jin et al., 2024) and in scientific domains (Stark et al., 2024; Jing et al., 2024; Klein & Noé,\n2024). Here, we investigate how incorporating Path Gradients on samples from the target distribution,\nenhances CNF performance post flow-matching.\nPath gradients are low variance gradient estimators, which have strong theoretical guarantees close to\nthe optimum (Roeder et al., 2017) and incorporate gradient information from both, the variational\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nand the target distribution (Vaitl et al., 2024). While they have been adopted for training Normalizing\nFlows in the field of Lattice Gauge Theory (Bacchio et al., 2023; Kanwar, 2024; Abbott et al., 2023)\nand also variational inference (Agrawal & Domke, 2024; Andrade, 2023), they remain underused\nin Biochemistry. In this work, we explore the potential of Path Gradient fine-tuning for Boltzmann\nGenerators with promising results. This indicates that even though training with Path Gradients is\norders of magnitude slower per iteration, its use of additional information and low variance allows it\nto outperform Flow Matching within the same computational constraints in fine-tuning.\nWe make the following main contributions:\n• We propose hybrid training using Flow Matching for pre-training and Path Gradients for\nfine-tuning. We do not require additional samples beyond the original force-labeled data,\nwhich were required already for generating the data. In order to ensure fast training, we are\nthe first to optimize CNFs using Path Gradients for the forward KL and making use of the\naugmented adjoint method (Vaitl et al., 2022b).\n• We show for many particle systems as well as alanine dipeptide that this fine-tuning approach\ncan triple importance-sampling efficiency within the same computational budget.\n• We investigate Path Gradients’ impact on properties trained during Flow Matching, namely\nflow trajectory length and the MSE Flow Matching loss, and show that these are mostly\nunaffected by the fine-tuning.\n0.0\n15.0\n30.0\n45.0\n60.0\nTime [s]\n0.0\n0.2\nKL(P|Q)\nFlow Matching\nPath Gradients\nHybrid\nStart Fine-tuning\nTarget\nFlow Matching  0.067\nPath gradients  0.021\nHybrid   0.001\nFigure 1: Training a CNF on a simple 2D Gaussian Mixture Model. Comparison between pure\ntraining with Flow Matching, pre-training with Flow Matching and fine-tuning with Path Gradients\nand pure training with Path Gradients. We see that given the same wall-time hybrid training performs\nbest in terms of forward KL divergence. The bottom row shows the target and the final model after\ntraining.\n2\nMethod\nGenerative models have recently garnered significant interest for biochemical applications (Noé et al.,\n2019; Jumper et al., 2021; Abramson et al., 2024). In this work, we focus on the application of\nenhancing or replacing molecular dynamics simulations.\n2.1\nBoltzmann Generators\nBoltzmann Generators (Noé et al., 2019) aim to asymptotically generate unbiased samples from the\nequilibrium Boltzmann distribution\np(x) = 1\nZ exp(−U(x))\n(1)\nwith an energy U(x) and an unknown normalization factor Z =\nR\nexp(−U(x))dx. The Boltzmann\ndistribution describes the probability of states in a physical system at thermal equilibrium. Traditional\n2\nsampling methods, such as Markov Chain Monte Carlo (MCMC) and Molecular Dynamics (MD)\nsimulations, generate samples sequentially. In contrast, Boltzmann Generators are designed to\nproduce independent samples directly from the target distribution. There are many different instances\nof Boltzmann Generators, many focus on molecular systems (Dibak et al., 2022; Köhler et al., 2021;\nMidgley et al., 2023; Ding & Zhang, 2021b,a; Kim et al., 2024; Rizzi et al., 2023; Tamagnone et al.,\n2024; Schönle et al., 2024; Klein & Noé, 2024; Tan et al., 2025), while others are trained to sample\nlattice and other many particle systems (Wirnsberger et al., 2020; Ahmad & Cai, 2022; Nicoli et al.,\n2020, 2021; Schebek et al., 2024; Abbott et al., 2023; Kanwar, 2024).\nBoltzmann Generators employ a Normalizing Flow to map samples from a simple base distribution\ninto a learned sampling distribution qθ, which is trained to approximate the target Boltzmann\ndistribution. We can use Boltzmann Generators to obtain asymptotically unbiased estimators for\nobservables over p(x) using the sampling distribution qθ and (self-normalized) importance sampling\n(Noé et al., 2019; Nicoli et al., 2020),\nEp(x) [O(x)] = Eqθ(x)\n\u0014 p(x)\nqθ(x)O(x)\n\u0015\n,\n(2)\nfor an observable O : Rd →R. For self-normalized importance sampling, we still have to estimate\nthe normalization constant\nZ =\nZ\nexp(−U(x))dx = Eqθ(x)\n\u0014 p(x)\nqθ(x)\n\u0015\n(3)\nwith an MC estimator. This separates Boltzmann Generators from related generative methods, which\nonly generate approximate samples from the Boltzmann distribution (Jing et al., 2022; Abdin & Kim,\n2023; Klein et al., 2023a; Schreiner et al., 2023; Jing et al., 2024; Lewis et al., 2024).\nWe can estimate the efficiency of importance sampling using the (relative) effective sampling size\n(ESS). The ESS\nESS :=\n \nEqθ\n\"\u0012 p\nqθ\n(x)\n\u00132#!−1\n=\n\u0012\nEp\n\u0014 p\nqθ\n(x)\n\u0015\u0013−1\n∈[0, 1]\n(4)\ncan be estimated either on samples from the model qθ or the target distribution p. Roughly speaking,\nthe ESS tells us how efficiently we are able to sample from the target distribution. The estimator\nESSq, based on samples from qθ, might fail to detect missing modes in the target distribution, while\nESSp requires samples from the target distribution. Both estimators exhibit high variance and might\noverestimate the performance if too few samples are being used. For more information confer (Nicoli\net al., 2023).\n2.2\nContinuous Normalizing Flows\nNeural ODEs, introduced by Chen et al. (2018), parameterize a continuous-time transformation\nthrough a neural-network-defined vector field vθ. By employing the adjoint sensitivity method\n(Pontryagin, 1987), gradients are computed in a backpropagation-like fashion with constant memory.\nBy computing the ODE of the divergence, we can compute the change in probability, which lets us\nuse a Neural ODE as a Normalizing Flow (Chen et al., 2018; Grathwohl et al., 2019).\nUsing a simple base distribution q0(x0) and the transformation\nTθ(x0) = x0 +\nZ 1\n0\nvθ(xt, t)dt ,\n(5)\nwe obtain a distribution qθ, which we can sample from and compute the probability\nlog qθ(Tθ(x0)) = log q0(x0) −log det\n\u0012∂Tθ(x0)\n∂x0\n\u0013\n= log q0(x0) −\nZ 1\n0\nTr\n\u0012∂vθ(xt, t)\n∂xt\n\u0013\ndt .\n(6)\nFor training samples from the target distribution, a straightforward way of approximating the target\ndensity is by maximum likelihood training, i.e. by minimizing\nLML(θ) = −Ep(x) [log qθ(x)] .\n(7)\n3\nMinimizing Equation (7) is equivalent to minimizing the forward KL divergence up to a constant\nKL(p|qθ) = Ep(x) [log p(x) −log qθ(x)]\nc= LML(θ) .\n(8)\nAs Köhler et al. (2020) showed, we can construct equivariant CNFs by using an equivariant network\nas the vector field.\n2.3\nFlow Matching\nInspired by advances in diffusion models, Flow Matching (Lipman et al., 2023; Albergo et al., 2023;\nLiu et al., 2022) has emerged as a promising competitor to diffusion models (Esser et al., 2024). Flow\nMatching enables us to train CNFs in a simulation free manner, i.e. training without the need of the\nexpensive integration of the ODEs (5) and (6).\nThe idea behind Flow Matching is to posit a base probability p0 and a vector field ut(x), which\ngenerates pt from the base density p0, such that the final probability p1 equals the target density p.\nGiven these components, Flow Matching aims to minimize the mean squared error between the target\nvector field ut(x) and the learned vector field vθ,t(x)\nLFM(θ) = Et∼U(0,1),x∼pt(x)\n\u0002\n||vθ(x, t) −ut(x)||2\u0003\n.\n(9)\nsince pt and ut are not known, it is intractable to compute the objective. Nevertheless, Lipman et al.\n(2023) showed that we can construct a conditional probability path p(x|z) and corresponding vector\nfield ut(·|z) conditioned on z = (x0, x1), which has identical gradients to Eq. (9). This yields the\nconditional Flow Matching objective\nLCFM(θ) = Et∼U(0,1),x∼pt(x|z)\n\u0002\n||vθ(x, t) −ut(x|z)||2\u0003\n.\n(10)\nIn this framework, there are different ways of constructing the conditional probability path. We here\nfocus on the parametrization introduced in Tong et al. (2023), which results in optimal integration\npaths\nz = (x0, x1)\nand\np(z) = π(x0, x1)\n(11)\nut(x|z) = x1 −x0\nand\npt(x|z) = N(x|t · x1 + (1 −t) · x0, σ2),\n(12)\nwhere π(x0, x1) denotes the optimal transport map between p0 and p1. As this map is again\nintractable, it is only evaluated per batch during training. We refer to this parametrization as OT\nFM. In Klein et al. (2023b); Song et al. (2023) the authors show that we can extend this to highly\nsymmetric systems, such as many particle systems and molecules, and still obtain optimal transport\npaths. The approach enforces the system’s symmetries in the OT map by computing the Wasserstein\ndistance over optimally aligned samples with respect to those symmetries. Hereafter, we denote this\nparameterization as EQ OT FM. For more details on optimal transport Flow Matching refer to Tong\net al. (2023) and Klein et al. (2023b); Song et al. (2023). Note, that we obtain the original formulation\nin Lipman et al. (2023) by using p(z) = p0(x0)p1(x1), we will refer to it as standard FM.\n2.4\nPath Gradients\nIntroduced in the context of variational auto-encoders (Roeder et al., 2017; Tucker et al., 2019),\nPath Gradient estimators have improved performance, especially when applied to normalizing flows\n(Agrawal et al., 2020; Vaitl et al., 2022a,b; Agrawal & Domke, 2024). In the field of simulating\nLattice Field Theories, they have become a go-to tool for training flows (Bacchio et al., 2023; Kanwar,\n2024; Abbott et al., 2023). In these applications, the reverse Kullback-Leibler divergence KL(qθ|p)\nis minimized by self-sampling, i.e. without existing samples from the target density, but only on\nsamples from qθ generated while training.\nPath gradients are unbiased gradient estimators which have low variance close to the optimum. We\nobtain Path Gradients by separating the total derivative of a reparametrized gradient for the reverse\nKL\nd\ndθKL(qθ|p) = Eq0(x0)\n\u0014 d\ndθ log qθ(Tθ(x0))\np(Tθ(x0))\n\u0015\n(13)\n4\ninto two partial derivatives\nEq0(x0)\n\"\n∂\n∂x1\nlog qθ(x1)\np(x1) · ∂Tθ(x0)\n∂θ\n|\n{z\n}\nPath Gradient of KL(qθ|p)\n+ ∂log qθ(x1)\n∂θ\n\f\f\f\nx1=Tθ(x0)\n|\n{z\n}\nScore term\n#\n,\n(14)\ncalled the Path Gradient and the score term 1.\nRoeder et al. (2017) observed, that the martingale score term vanishes in expectation, but has a\nnon-zero variance. Vaitl et al. (2022a) showed that its variance is\n1\nN Iθ for a batch of size N and\nthe Fisher Information Iθ of the variational distribution qθ. The low variance close to the optimum\nallows Path Gradient estimators to \"stick-the-landing\" (Roeder et al., 2017), i.e. having zero variance\nat the optimum, making it an ideal tool for fine-tuning. Vaitl et al. (2024) showed that Path Gradient\nestimators incorporate additional information about the derivative of both target and variational\ndistribution, opposed to the reparametrized gradient gradient estimator (Kingma & Welling, 2014;\nMohamed et al., 2020) (see Appendix A.11.1 for a detailed explanation). They hypothesized that\nthis leads to less overfitting and a generally better fit to the target density (see Vaitl (2024)). These\ntheoretical aspects make Path Gradients a promising suitor for fine-tuning Boltzmann Generators.\nPath Gradients on given samples\nRecently, Path Gradient estimators have been proposed for\nsamples from distributions different from the variational density qθ (Bauer & Mnih, 2021). By\nviewing the forward KL(p1|qθ) at t = 1 as a reverse KL(p0,θ|q0) at t = 0, we can straightforwardly\napply Path Gradients (Vaitl et al., 2024). Here, we assume p0,θ to be a (Continuous) Normalizing\nFlow from p1 as its base density and T −1\nθ\nto be the parametrized diffeomorphism.\nKL(p1|qθ) = Ep1(x1)[log p1(x1) −log qθ(x1)]\n= Ep1(x1)\nh\nlog p1(x1) −\n\u0012\nlog q0(T −1\nθ\n(x1)) −log det\n\f\f\f\f\n∂T −1\nθ\n(x1)\n∂x1\n\u0013\f\f\f\f\n|\n{z\n}\n=log qθ(x1)\ni\n(15)\n= Ep1(x1)\nh \u0012\nlog p1(x1) −log det\n\f\f\f\f\n∂T −1\nθ\n(x1)\n∂x1\n\f\f\f\f\n\u0013\n|\n{z\n}\n=log p0,θ(x0)\n−log q0(T −1\nθ\n(x1))\ni\n= Ep0,θ(x0)\nh\nlog p0,θ(x0) −log q0(x0)\ni\n= KL(p0,θ|q0) .\n(16)\nThis different view lets us compute Path Gradients with the same ease as for the reverse KL, but on\ngiven samples from the target distribution.\nWhile Vaitl et al. (2022a) and the present work both use Path Gradients, both minimize different\nlosses on different samples. Our work optimizes the Forward KL(p1|qθ) with path gradients on\nexisting samples from p, as proposed in Vaitl et al. (2024) using the algorithm proposed in Vaitl et al.\n(2022a). Vaitl et al. (2022a) minimize the reverse KL(qθ|p1) via self-sampling, i.e. on samples from\nthe model qθ. Training via self-sampling has many drawbacks, mainly: modes of the target p can be\nentirely missed, which invalidates all asymptotic guarantees and breaks importance sampling, see e.g.\nNicoli et al. (2023). This becomes increasingly likely in higher dimensions. Further, the forces for\nthe newly generated samples have to be evaluated, which can increase the cost of training or lead to\nunstable optimization behavior.\nSince the publication Vaitl et al. (2022b), Flow Matching has been established as the de facto training\nmethod for CNFs. Our work investigates and combines the performance of Flow Matching and Path\nGradients. Specifically, we investigate the effect of Path Gradients on the inner workings of the CNF,\nlike e.g. trajectory length, while Vaitl et al. (2022b) simply looked at the performance compared to\nstandard self-sampling losses.\nIn Appendix A.11, we zoom in on the properties of the different estimators. We show that while Path\nGradients exhibit zero variance at the optimum, generically, Flow Matching does not. We further\n1The score term ∂log qθ(x)\n∂θ\nis not to be confused with the force term ∂log qθ(x)\n∂x\n, which is often called the\nscore in the context of diffusion models (Song et al., 2021).\n5\n0.0\n15.0\n30.0\n45.0\n60.0\nTime [s]\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nMSE loss\nFlow Matching\nHybrid\nPath Gradients\nStart Fine-tuning\nFigure 2: FM loss objective 10 during training on 2D GMM averaged on three runs. Training with\nPath gradients leaves the MSE largely unchanged.\nexplicate the theoretical properties of the existing estimators for training via Maximum Likelihood,\nPath Gradients and Flow Matching.\nWhile the naive calculation of Path Gradients requires significantly more compute and memory than\nFlow Matching, we take several steps to obtain constant memory and speed ups in the next section.\nAs we show experimentally, fine-tuning with Path Gradient for a few epochs significantly improves\nthe performance for Continuous Normalizing Flows compared to only using Flow Matching.\n3\nPath Gradients and Flow Matching\nTo build an intuition for the behavior of Flow Matching (FM) versus Path Gradient (PG) estimators,\nwe start with a simple 2D Gaussian Mixture Model. We compare three training strategies: 1) FM\nonly, 2) PG only, and 3) a hybrid approach, using FM to quickly approximate the target distribution\np1, and subsequently applying PG for fine-tuning. Although slower per training step, PG has strong\ntheoretical guarantees near the optimum.\nFor Flow Matching, we use the standard formulation proposed in Lipman et al. (2023). The dynamics\nvθ is modeled using a four-layer fully connected neural network with 64 units per layer and ELU\nactivations. Given the simplicity of the model and task, we ignore memory usage in this setup. All\nexperiments are run on a CPU and complete in roughly one minute.\nAs shown in Figure 1, FM training rapidly improves initially but soon reaches a plateau with slow\nimprovements after. PG training in contrast, progresses more slowly but eventually reaches and\nsurpasses the FM plateau. The hybrid strategy, i.e. beginning with FM and switching to PG, results\nin the best performance, combining fast convergence with improved final accuracy.\nInterestingly, as shown in Figure 2, applying PG after FM has little effect on the MSE loss defined\nin Equation (10), suggesting that PG fine-tunes the variational distribution qθ without significantly\naltering the dynamics vθ. Nonetheless, this subtle adjustment leads to visibly better samples and\nimproved density matching (cf. Figure 1).\nIn the 2D case, we compute the divergence term in Equation (6) exactly by directly evaluating the\ntrace of the Jacobian. However, this approach scales quadratically with the number of dimensions\nand quickly becomes computationally prohibitive in higher-dimensional settings. A practical solution\nis Hutchinson’s trace estimator (Hutchinson, 1989), which provides us faster but noisy estimators for\nthe divergence.\nFor ODE integration, we use a fourth-order Runge–Kutta (RK4) scheme with 15 time steps, resulting\nin 60 function evaluations per integration trajectory. Under these settings, training on a single batch is\nempirically about 275 times faster with Flow Matching compared to Path Gradients. This discrepancy\nis expected, as the cost of function evaluations differs substantially between the two methods.\nWhile these results are promising, the 2D setup allows exact computation of the divergence and may\nnot generalize to higher-dimensional or more complex tasks. Moreover, memory usage becomes a\nsignificant bottleneck in these more realistic scenarios.\n6\nAugmented Adjoint Method\nIn order to have constant memory irrespective of the number of\nfunction evaluations, we adapt the augmented adjoint method (Vaitl et al., 2022b) for Path Gradient\nestimators for the forward KL divergence Equation (16)\nd\ndθKL(p0,θ|q0) = Ex1∼p1\n\u0014 ∂\n∂x0\n\u0012\nlog p0,θ\nq0\n(x0)\n\u0013 ∂T −1\nθ\n(x1)\n∂θ\n\u0015\n.\n(17)\nIn order to compute the force of the variational distribution\n∂log p0,θ(x0)\n∂x0\n(18)\nwe use the augmented adjoint method by solving the ODE\nd\ndt\n∂log pt,θ(xt)\n∂xt\n= −∂log pt,θ(xt)\n∂xt\nT ∂vθ(xt, t)\n∂xt\n−\n∂\n∂xt\nTr\n\u0012∂vθ(xt, t)\n∂xt\n\u0013\n(19)\nwith initial condition\n∂log p1,θ\n∂x1\n= ∂log p1(x1)\n∂x1\n(20)\nfrom t = 1 to 0 (Vaitl et al., 2022b).\nComputing Path Gradients requires computing the gradient of the divergence\nTr\n\u0012∂vθ(xt, t)\n∂xt\n\u0013\n(21)\nas well as the ODEs 5 and 19 per integration step. Even for training on a single time-step, this is\nmore resource intensive than Flow Matching, which only requires the derivative of the vector field\n∂vθ(xt,t)\n∂θ\n. Thus, although the memory demands do not increase with additional integration steps of\nthe ODE, the memory required for Path Gradients is greater than with Flow Matching. Since the\ncomputational load associated with calculating the terms varies across different architectures, we\ndiscuss them individually for every experiment.\nFor RK4 with 15 integration steps, the ODEs 5, 6 and 19 are evaluated in the backward direction\n(t : 1 →0) and the ODE 5 and the adjoint method in the forward direction, amounting in 300 function\nevaluations, compared to two (the forward and backward call) with Flow Matching. Thus we expect\nFlow Matching to be faster than Path Gradients by a factor of at least 150.\n4\nExperiments\nMotivated by the improvements on the toy model, we now turn to more challenging problems, namely\nthe experiments done in Klein et al. (2023b) and Klein & Noé (2024). To this end, we use the same\narchitecture, a CNF with an EGNN (E(n) Equivariant Graph Neural Network) (Satorras et al., 2021;\nGarcia Satorras et al., 2021) for the vector field. The architecture in Klein & Noé (2024) is mostly\nthe same as in Klein et al. (2023b), with the main difference of the encoding of the atoms. While\nthe model in Klein et al. (2023b) treats the same atom type as indistinguishable, the model in Klein\n& Noé (2024) encodes nearly all atoms differently. Only hydrogens bound to the same atom are\ntreated as indistinguishable. For more details on the model architecture see Appendix A.2. While\nwe maintain the architecture and the initial training process with Flow Matching, we change the\nlater training procedure to include Path Gradients. As in Klein et al. (2023b), we investigate optimal\ntransport Flow Matching (OT FM), – and for the second set of experiments on LJ13 – equivariant\noptimal transport Flow Matching (EQ OT FM), and naive Flow Matching (standard FM).\nWe first evaluate the models on two many particle system with pair-wise Lennard-Jones interactions\nwith 13 and 55 particles. In addition we investigate Alanine dipeptide (AD2) at T = 300K both with\na classical and a semi-empirical force field (XTB). In contrast to (Klein et al., 2023b; Klein & Noé,\n2024), we do not bias the training data towards the positive φ state. The bias was originally introduced\nto make learning problem simpler, as the unlikely state is more prominent (Klein et al., 2023b). This,\nhowever, skews the evaluation, since the metrics, namely ESS and Negative log likelihood (NLL), are\nbased on the target distribution, not the biased one. Moreover, it assumes system-specific knowledge\nof slow-varying degrees of freedom, information that is typically unavailable for unknown systems.\n7\n0\n2\n4\n6\n8\nEpoch\n0.6\n0.7\n0.8\nESS-Q\n0\n2\n4\n6\n8\nEpoch\n16.20\n16.15\n16.10\n16.05\n16.00\nNLL\nEQ OT\nOT\nStandard\n0\n2\n4\n6\n8\nEpoch\n2.5\n3.0\n3.5\nTrajectory Length\nFigure 3: Reverse ESS, NLL and trajectory length for Flows trained with standard FM, Optimal\nTransport and Equivariant Transport during fine-tuning with Path Gradients on LJ13. We can\nobserve that fine-tuning largely leaves the trajectory length unchanged, while substantially improving\nperformance. Mean ± sterr over three runs.\nFinally, we also investigate the 2AA dataset consisting of dipeptides simulated at T = 310K with a\nclassical force field (Klein et al., 2023a), to evaluate the influence of PG in a transferability. For more\ndetails on the datasets see Appendix A.3. We published code for replicating our experiments 2. In\ntotal, we investigate three scenarios:\n• First, fine-tuning with Path Gradients compared to fine-tuning with OT Flow Matching given\nthe same limited computational resources in Section 4.1.\n• Second, we apply Path Gradients on the FM-trained models using unlimited resources to\nmaximize performance after training with Standard FM, OT FM and EQ OT FM as done in\nKlein et al. (2023b), see Section 4.2. In this second case, we additionally examine the ODE\nintegration length to understand how PG fine-tuning influences the model.\n• Finally, we test if the path gradient fine-tuning improves results for transferability in the\nsetting by investigating transferable Boltzmann Generators (Klein & Noé, 2024) trained and\nevaluated on dipeptides, see Section 4.3.\n4.1\nFine-tuning using the same limited resources\nWe enforce comparable memory and wall-time constraints to those used for Flow Matching alone by\nadjusting batch size and employing gradient accumulation. Consequently, our fine-tuning runs with\nPath Gradients complete in less wall-time and occupy a similar memory footprint. For full training\nstatistics and implementation details, see Appendix A.4 and Appendix A.5.\nReplicating the experiments in Klein et al. (2023b), we evaluate how path-gradient fine-tuning impacts\nmodel performance. Table 1 presents a comparison between models trained with pure OT Flow\nMatching and those optimized using the hybrid approach of OT FM pre-training and path-gradient\nfine-tuning.\nWe observe that fine-tuning with Path Gradients improves the ESS metrics, i.e. ESSp and ESSq,\nacross most datasets and models, despite their high variance. The hybrid approach reliably improves\nthe NLL for all models and datasets, but one. The only exception is the standard Flow (Klein et al.,\n2023b) on AD2 with XTB, where fine-tuning with FM still performs better on average. Examining\nthe ESS, we find that both approaches fail to adequately fit the target density. This supports the\nhypothesis that Path Gradients are only effective for fine-tuning when the flow already provides a\nreasonably good approximation of the target. Notably, on LJ55, our hybrid approach nearly triples\nthe ESS, and on AD2 with XTB, it doubles it – in the same training time.\n4.2\nEffect of Path Gradient fine-tuning on flow trajectory length\nWe use the provided saved models from (Klein et al., 2023b) for LJ13 and investigate the performance\nand flowed trajectory length during fine-tuning with Path Gradients. We used a batch-size of 256\nand trained with Path Gradients for 10 epochs. In Figure 3, we can see that the different pre-trained\nmodels have similar NLLs and ESS, but differ in trajectory lengths. Fine-tuning reliably improves the\n2github.com/lenz3000/path-grads-after-fm\n8\nTable 1: Comparison between fine-tuning with Optimal Transport Flow Matching and Path Gradients.\nFirst all models were pre-trained with Optimal Transport Flow Matching like in (Klein et al., 2023b).\nWe compare fine-tuning with Flow Matching and Path Gradients. For all experiments, we limited the\nVRAM and runtime to be roughly equivalent. Mean ± sterr on three runs. bold: sterrs do not overlap.\nModel\nTraining type\nNLL (↓)\nESSq %(↑)\nESSp %(↑)\nLJ13\n(Klein et al., 2023b)\nOnly FM\n−16.09 ± 0.03\n54.36 ± 5.43\n58.18 ± 0.71\nHybrid (ours)\n−16.21 ± 0.00\n82.97 ± 0.40\n82.87 ± 0.35\nLJ55\n(Klein et al., 2023b)\nOnly FM\n−88.45 ± 0.04\n3.74 ± 1.06\n2.97 ± 0.08\nHybrid (ours)\n−89.19 ± 0.05\n11.04 ± 3.98\n13.71 ± 3.15\nAlanine dipeptide - XTB\n(Klein et al., 2023b)\nOnly FM\n−107.89 ± 0.07\n0.74 ± 0.33\n0.01 ± 0.01\nHybrid (ours)\n−107.77 ± 0.18\n0.25 ± 0.37\n0.01 ± 0.01\n(Klein & Noé, 2024)\nOnly FM\n−125.75 ± 0.01\n3.38 ± 0.50\n2.65 ± 0.50\nHybrid (ours)\n−125.82 ± 0.02\n7.30 ± 1.28\n6.57 ± 2.98\nAlanine dipeptide - Classical\n(Klein et al., 2023b)\nOnly FM\n−110.14 ± 0.01\n4.88 ± 0.42\n0.26 ± 0.19\nHybrid (ours)\n−110.30 ± 0.13\n2.86 ± 3.05\n0.04 ± 0.06\n(Klein & Noé, 2024)\nOnly FM\n−128.01 ± 0.01\n14.42 ± 2.44\n12.21 ± 1.06\nHybrid (ours)\n−128.26 ± 0.02\n24.39 ± 6.86\n13.89 ± 4.91\nFigure 4: Alanine dipeptide results for the TBG model and the classical force field with and without\nPath Gradient finetuning. Left: Ramachandran plots for the dihedral angel distribution of a reference\nMD simulation and non reweighted samples from the different TBG models. Right: Corresponding\nenergy distributions of generated samples.\nperformance of the flows with relative little change to the trajectory lengths. For the full statistics see\nAppendix A.7. We observe a similar pattern in the other systems investigated. For alanine dipeptide,\nfor instance, generated samples show significantly improved bond-length and energy distributions,\nyet the global conformational landscape, captured by the φ and ψ dihedral-angle distributions in the\nRamachandran plot, remains mostly unchanged (see Figure 4 and Figure 6).\nThese experiments show that our proposed hybrid approach is perfectly suited for maximizing\nperformance while keeping the properties of the model.\n4.3\nTransferability on dipeptides\nFinally, we also investigate whether fine-tuning with Path Gradients improves performance in the\ntransferable setting. To this end we fine-tune Transferable Boltzmann Generators (TBG) (Klein &\nNoé, 2024), which were trained on 200 different dipeptides and evaluate on 16 unseen ones, like in\nKlein & Noé (2024). The experiments show that fine-tuning with path gradients improves the NLL\nand energies for all evaluated unseen test dipeptides. On average, we observe a relative improvement\nof around 23% in the ESSq, reaching an efficiency of 9.79%. For more details see Appendix A.10.\n9\n5\nDiscussion\nWe have presented a hybrid training strategy for Boltzmann Generators that uses Flow Matching and\nPath Gradients. We have shown how to efficiently compute Path Gradients with constant memory and\nwithout the need for additional samples. While substantially slower per training step, Path Gradients\nare a powerful tool for fine-tuning, leveraging first-order information from the energy function – an\nunderexplored avenue in the scientific machine learning community. Our results demonstrate that\nPath Gradients can significantly improve sample quality on the same computational budget as Flow\nMatching, when the model is already reasonably close to the target distribution. Our experiments\nshow that Path Gradients only apply minor changes to the flow trajectory and to the variational\ndistribution qθ while still substantially increasing the sampling efficiency and performance.\n5.1\nLimitations\nPath Gradients come with several limitations. First, they require access to a well-defined and\ndifferentiable energy function, which restricts their use to domains like molecular modeling and\nexcludes standard tasks such as natural image generation. Second, they rely on unbiased training\nsamples. Finally, the method is computationally more expensive and tends to improve performance\nonly when the model is already close to the target distribution. While Path Gradients do not directly\nspeed up the expensive CNF sampling process, they increase the ESS, thereby reducing the total\nnumber of samples needed and, hence, at least partially alleviating the high sampling cost.\n5.2\nFuture Work\nGiven the similarities between Flow Matching and Diffusion models, extending Path Gradients to\ndiffusion-based frameworks presents an exciting and promising direction. The dynamics used in\nour experiments have also been used for molecular conformation generation using Diffusion models\n(Hoogeboom et al., 2022). Adoption to their applications would be an interesting and suitable\ncandidate for future work.\nFurthermore, in model distillation (e.g. (Salimans & Ho, 2022)) the gradient information of the larger\nmodel is available, even though there might be no first order information of the original data. Here\nour approach could help to improve the distillation process.\n5.3\nBroader Impact\nThis foundational research has no immediate societal impact, but if scalable, it could be used to\naccelerate drug and materials discovery by replacing MD simulations. Potential risks include misuse\nfor biothreat development and the lack of convergence guarantees, which may lead to incomplete\nsampling and misleading conclusions.\nAcknowledgements\nLV wants to thank Pan Kessel and Shinichi Nakajima for the helpful discussions. LK gratefully\nacknowledges support by BIFOLD - Berlin Institute for the Foundations of Learning and Data.\n10\nReferences\nAbbott, R., Albergo, M., Botev, A., Boyda, D., Cranmer, K., Hackett, D., Kanwar, G., Matthews, A.,\nRacaniere, S., Razavi, A., et al. Normalizing flows for lattice gauge theory in arbitrary space-time\ndimension. ArXiv preprint, abs/2305.12345, 2023.\nAbdin, O. and Kim, P. M. Pepflow: direct conformational sampling from peptide energy landscapes\nthrough hypernetwork-conditioned diffusion. bioRxiv, pp. 2023–06, 2023.\nAbramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., Ronneberger, O., Willmore, L.,\nBallard, A. J., Bambrick, J., et al. Accurate structure prediction of biomolecular interactions with\nalphafold 3. Nature, pp. 1–3, 2024.\nAgrawal, A. and Domke, J. Disentangling impact of capacity, objective, batchsize, estimators, and\nstep-size on flow vi. arXiv preprint arXiv:2412.08824, 2024.\nAgrawal, A., Sheldon, D. R., and Domke, J.\nAdvances in Black-Box VI: Normalizing\nFlows, Importance Weighting, and Optimization.\nIn Advances in Neural Information Pro-\ncessing Systems 33, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nc91e3483cf4f90057d02aa492d2b25b1-Abstract.html.\nAhmad, R. and Cai, W. Free energy calculation of crystalline solids using normalizing flows.\nModelling and Simulation in Materials Science and Engineering, 30(6):065007, sep 2022. ISSN\n0965-0393, 1361-651X. doi: 10.1088/1361-651X/ac7f4b. URL https://iopscience.iop.\norg/article/10.1088/1361-651X/ac7f4b.\nAlbergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: A unifying framework\nfor flows and diffusions. ArXiv preprint, abs/2303.08797, 2023. URL https://arxiv.org/\nabs/2303.08797.\nAndrade, D. LOFT-Stable Training of Normalizing Flows for Variational Inference. In 6th Workshop\non Tractable Probabilistic Modeling, 2023.\nBacchio, S., Kessel, P., Schaefer, S., and Vaitl, L. Learning trivializing gradient flows for lattice\ngauge theories. Physical Review D, 107:L051504, 2023. doi: 10.1103/PhysRevD.107.L051504.\nURL https://link.aps.org/doi/10.1103/PhysRevD.107.L051504.\nBannwarth, C., Ehlert, S., and Grimme, S. Gfn2-xtb—an accurate and broadly parametrized self-\nconsistent tight-binding quantum chemical method with multipole electrostatics and density-\ndependent dispersion contributions. Journal of Chemical Theory and Computation, 15(3):1652–\n1671, 2019. doi: 10.1021/acs.jctc.8b01176. URL https://doi.org/10.1021/acs.jctc.\n8b01176. PMID: 30741547.\nBauer, M. and Mnih, A. Generalized Doubly Reparameterized Gradient Estimators. In Proceedings\nof the 38th International Conference on Machine Learning, 2021. URL http://proceedings.\nmlr.press/v139/bauer21a.html.\nBoyda, D., Kanwar, G., Racanière, S., Rezende, D. J., Albergo, M. S., Cranmer, K., Hackett, D. C.,\nand Shanahan, P. E. Sampling using SU (N) gauge equivariant flows. Physical Review D, (7),\n2021.\nChen, T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural Ordinary Differential Equations.\nIn Advances in Neural Information Processing Systems 31, 2018.\nDibak, M., Klein, L., Krämer, A., and Noé, F. Temperature steerable flows and Boltzmann generators.\nPhys. Rev. Res., 4:L042005, Oct 2022. doi: 10.1103/PhysRevResearch.4.L042005.\nDing, X. and Zhang, B. Computing absolute free energy with deep generative models. Biophysical\nJournal, 120(3):195a, 2021a.\nDing, X. and Zhang, B. Deepbar: A fast and exact method for binding free energy computation.\nJournal of Physical Chemistry Letters, 12:2509–2515, 3 2021b. ISSN 19487185. doi: 10.1021/acs.\njpclett.1c00189.\n11\nDraxler, F., Sorrenson, P., Zimmermann, L., Rousselot, A., and Köthe, U. Free-form flows: Make\nany architecture a normalizing flow. In Dasgupta, S., Mandt, S., and Li, Y. (eds.), Proceedings\nof The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of\nProceedings of Machine Learning Research, pp. 2197–2205. PMLR, 02–04 May 2024. URL\nhttps://proceedings.mlr.press/v238/draxler24a.html.\nEsser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer,\nA., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In\nForty-first international conference on machine learning, 2024.\nGarcia Satorras, V., Hoogeboom, E., Fuchs, F., Posner, I., and Welling, M.\nE(n) equivariant\nnormalizing flows. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,\nJ. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 4181–4192.\nCurran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/\n21b5680d80f75a616096f2e791affac6-Paper.pdf.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C.,\nand Bengio, Y. Generative Adversarial Nets. In Advances in Neural Information Processing\nSystems 27, 2014.\nGrathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., and Duvenaud, D. FFJORD: Free-Form\nContinuous Dynamics for Scalable Reversible Generative Models. In Proceedings of the 7th\nInternational Conference on Learning Representations, 2019.\nHo, J., Jain, A., and Abbeel, P. Denoising Diffusion Probabilistic Models. In Advances in Neural\nInformation Processing Systems 33, 2020.\nHoogeboom, E., Satorras, V. G., Vignac, C., and Welling, M. Equivariant diffusion for molecule\ngeneration in 3d. In International conference on machine learning, pp. 8867–8887. PMLR, 2022.\nHutchinson, M. F. A stochastic estimator of the trace of the influence matrix for laplacian smoothing\nsplines. Communications in Statistics-Simulation and Computation, 18(3):1059–1076, 1989.\nInvernizzi, M., Krämer, A., Clementi, C., and Noé, F. Skipping the replica exchange ladder with\nnormalizing flows. The Journal of Physical Chemistry Letters, 13:11643–11649, 2022.\nJin, Y., Sun, Z., Li, N., Xu, K., Jiang, H., Zhuang, N., Huang, Q., Song, Y., Mu, Y., and Lin, Z.\nPyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954,\n2024.\nJing, B., Corso, G., Chang, J., Barzilay, R., and Jaakkola, T. Torsional diffusion for molecular\nconformer generation. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and\nOh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 24240–24253.\nCurran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper/2022/file/\n994545b2308bbbbc97e3e687ea9e464f-Paper-Conference.pdf.\nJing, B., Berger, B., and Jaakkola, T. Alphafold meets flow matching for generating protein ensembles.\narXiv preprint arXiv:2402.04845, 2024.\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool,\nK., Bates, R., Žídek, A., Potapenko, A., et al. Highly accurate protein structure prediction with\nalphafold. Nature, 596(7873):583–589, 2021.\nKanwar, G. Flow-based sampling for lattice field theories. arXiv preprint arXiv:2401.01297, 2024.\nKim, J. C., Bloore, D., Kapoor, K., Feng, J., Hao, M.-H., and Wang, M. Scalable normalizing\nflows enable boltzmann generators for macromolecules. In International Conference on Learning\nRepresentations (ICLR), 2024.\nKingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In Proceedings of the 2nd\nInternational Conference on Learning Representations, 2014. URL http://arxiv.org/abs/\n1312.6114.\nKlein, L. and Noé, F. Transferable boltzmann generators. arXiv preprint arXiv:2406.14426, 2024.\n12\nKlein, L., Foong, A. Y. K., Fjelde, T. E., Mlodozeniec, B. K., Brockschmidt, M., Nowozin, S., Noe,\nF., and Tomioka, R. Timewarp: Transferable acceleration of molecular dynamics by learning\ntime-coarsened dynamics. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023a. URL https://openreview.net/forum?id=EjMLpTgvKH.\nKlein, L., Krämer, A., and Noé, F. Equivariant flow matching. Advances in Neural Information\nProcessing Systems, 36:59886–59910, 2023b.\nKöhler, J., Klein, L., and Noé, F. Equivariant Flows: Exact Likelihood Generative Learning for\nSymmetric Densities. In Proceedings of the 37th International Conference on Machine Learning,\n2020. URL http://proceedings.mlr.press/v119/kohler20a.html.\nKöhler, J., Krämer, A., and Noé, F. Smooth Normalizing Flows. In Advances in Neural Information\nProcessing Systems 34, 2021.\nLarsen, A. H., Mortensen, J. J., Blomqvist, J., Castelli, I. E., Christensen, R., Dułak, M., Friis, J.,\nGroves, M. N., Hammer, B., Hargus, C., Hermes, E. D., Jennings, P. C., Jensen, P. B., Kermode, J.,\nKitchin, J. R., Kolsbjerg, E. L., Kubal, J., Kaasbjerg, K., Lysgaard, S., Maronsson, J. B., Maxson, T.,\nOlsen, T., Pastewka, L., Peterson, A., Rostgaard, C., Schiøtz, J., Schütt, O., Strange, M., Thygesen,\nK. S., Vegge, T., Vilhelmsen, L., Walter, M., Zeng, Z., and Jacobsen, K. W. The atomic simulation\nenvironment—a python library for working with atoms. Journal of Physics: Condensed Matter,\n29(27):273002, 2017. URL http://stacks.iop.org/0953-8984/29/i=27/a=273002.\nLewis, S., Hempel, T., Jiménez-Luna, J., Gastegger, M., Xie, Y., Foong, A. Y., Satorras, V. G., Abdin,\nO., Veeling, B. S., Zaporozhets, I., et al. Scalable emulation of protein equilibrium ensembles with\ngenerative deep learning. bioRxiv, pp. 2024–12, 2024.\nLipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative\nmodeling. In Proceedings of the 11th International Conference on Learning Representations,\n2023.\nLiu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003, 2022.\nMidgley, L. I., Stimper, V., Simm, G. N., Schölkopf, B., and Hernández-Lobato, J. M. Flow Annealed\nImportance Sampling Bootstrap. In Proceedings of the 11th International Conference on Learning\nRepresentations, 2023.\nMohamed, S., Rosca, M., Figurnov, M., and Mnih, A. Monte Carlo Gradient Estimation in Machine\nLearning. Journal of Machine Learning Research, 21:5183–5244, 2020. URL http://jmlr.\norg/papers/v21/19-346.html.\nNicoli, K. A., Nakajima, S., Strodthoff, N., Samek, W., Müller, K.-R., and Kessel, P. Asymptotically\nunbiased estimation of physical observables with neural samplers. Physical Review E, (2), 2020.\nNicoli, K. A., Anders, C. J., Funcke, L., Hartung, T., Jansen, K., Kessel, P., Nakajima, S., and Stornati,\nP. Estimation of Thermodynamic Observables in Lattice Field Theories with Deep Generative\nModels. Physical Review Letters, 126(3):032001, 2021.\nNicoli, K. A., Anders, C. J., Hartung, T., Jansen, K., Kessel, P., and Nakajima, S. Detecting and\nMitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories. Physical Review D,\n108:114501, 2023. doi: 10.1103/PhysRevD.108.114501. URL https://link.aps.org/doi/\n10.1103/PhysRevD.108.114501.\nNoé, F., Olsson, S., Köhler, J., and Wu, H. Boltzmann generators: Sampling equilibrium states of\nmany-body systems with deep learning. Science, 365(6457):eaaw1147, 2019.\nPapamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., and Lakshminarayanan, B. Normaliz-\ning flows for probabilistic modeling and inference. CoRR, 2019. URL http://arxiv.org/abs/\n1912.02762v1.\nPapamakarios, G., Nalisnick, E. T., Rezende, D. J., Mohamed, S., and Lakshminarayanan, B.\nNormalizing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning\nResearch, 22:57:1–57:64, 2021. URL http://jmlr.org/papers/v22/19-1028.html.\n13\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,\nGimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A.,\nChilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances in Neural Information Processing Systems\n32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nPoli, M., Massaroli, S., Yamashita, A., Asama, H., Park, J., and Ermon, S. Torchdyn: Implicit models\nand neural numerical methods in pytorch. In Neural Information Processing Systems, Workshop\non Physical Reasoning and Inductive Biases for the Real World, volume 2, 2021.\nPontryagin, L. S. Mathematical theory of optimal processes. Routledge, 1987.\nRezende, D. and Mohamed, S. Variational inference with normalizing flows. In International\nconference on machine learning, pp. 1530–1538. PMLR, 2015.\nRizzi, A., Carloni, P., and Parrinello, M. Multimap targeted free energy estimation, 2023.\nRoeder, G., Wu, Y., and Duvenaud, D.\nSticking the Landing:\nSimple, Lower-Variance\nGradient Estimators for Variational Inference.\nIn Advances in Neural Information Pro-\ncessing Systems 30, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\ne91068fff3d7fa1594dfdf3b4308433a-Abstract.html.\nSalimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. arXiv preprint\narXiv:2202.00512, 2022.\nSatorras, V. G., Hoogeboom, E., and Welling, M. E (n) equivariant graph neural networks. In\nInternational conference on machine learning, pp. 9323–9332. PMLR, 2021.\nSchebek, M., Invernizzi, M., Noé, F., and Rogal, J. Efficient mapping of phase diagrams with\nconditional boltzmann generators. Machine Learning: Science and Technology, 2024.\nSchönle, C., Gabrié, M., Lelièvre, T., and Stoltz, G. Sampling metastable systems using collective\nvariables and jarzynski-crooks paths. arXiv preprint arXiv:2405.18160, 2024.\nSchreiner, M., Winther, O., and Olsson, S. Implicit transfer operator learning: Multiple time-\nresolution models for molecular dynamics. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023. URL https://openreview.net/forum?id=1kZx7JiuA2.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-Based Gener-\native Modeling through Stochastic Differential Equations. In Proceedings of the 9th International\nConference on Learning Representations, 2021.\nSong, Y., Gong, J., Xu, M., Cao, Z., Lan, Y., Ermon, S., Zhou, H., and Ma, W.-Y. Equivariant\nflow matching with hybrid probability transport for 3d molecule generation. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.net/\nforum?id=hHUZ5V9XFu.\nStark, H., Jing, B., Barzilay, R., and Jaakkola, T. Harmonic self-conditioned flow matching for joint\nmulti-ligand docking and binding site design. In Forty-first International Conference on Machine\nLearning, 2024.\nTamagnone, S., Laio, A., and Gabrié, M. Coarse-grained molecular dynamics with normalizing flows.\nJournal of Chemical Theory and Computation, 20(18):7796–7805, 2024.\nTan, C. B., Bose, A. J., Lin, C., Klein, L., Bronstein, M. M., and Tong, A. Scalable equilibrium\nsampling with sequential boltzmann generators. arXiv preprint arXiv:2502.18462, 2025.\nTong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Ben-\ngio, Y. Conditional flow matching: Simulation-free dynamic optimal transport. arXiv preprint\narXiv:2302.00482, 2(3), 2023.\n14\nTucker, G., Lawson, D., Gu, S., and Maddison, C. J. Doubly Reparameterized Gradient Estimators\nfor Monte Carlo Objectives. In Proceedings of the 7th International Conference on Learning\nRepresentations, 2019.\nVaitl, L. Path Gradient Estimators for Normalizing Flows. PhD thesis, Technische Universität Berlin,\n2024.\nVaitl, L., Nicoli, K. A., Nakajima, S., and Kessel, P. Gradients should stay on path: better estimators\nof the reverse-and forward KL divergence for normalizing flows. Machine Learning: Science and\nTechnology, 3(4):045006, 2022a.\nVaitl, L., Nicoli, K. A., Nakajima, S., and Kessel, P. Path-Gradient Estimators for Continuous\nNormalizing Flows. In Proceedings of the 39th International Conference on Machine Learning,\n2022b. URL https://proceedings.mlr.press/v162/vaitl22a.html.\nVaitl, L., Winkler, L., Richter, L., and Kessel, P. Fast and unified path gradient estimators for nor-\nmalizing flows. In to be presented in 12th International Conference on Learning Representations,\n2024.\nWirnsberger, P., Ballard, A. J., Papamakarios, G., Abercrombie, S., Racanière, S., Pritzel, A.,\nJimenez Rezende, D., and Blundell, C. Targeted free energy estimation via learned mappings. The\nJournal of Chemical Physics, 153(14):144112, 2020.\n15\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: In Section 3 we present how to efficiently do fine-tuning in constant memory\nand Section 4, we show both, improved performance and minimal changes to the trajectory\nlength.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: In Section 5.1 we discuss the limitations of the presented approach.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n16\nAnswer: [Yes]\nJustification: The cross-references for the formulae for Path Gradients on samples in\nSection 2.4, as well as the adapted augmented adjoint state method Equation (19) are cross-\nreferenced and numbered. The same holds true for equations on EQ-OT-, OT-, and standard\nFM in Section 2.3\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide all the necessary information in Sections 4,A.5,A.2, A.4, A.7.\nFurther we recreate previously published experiments. Additionally we also provide code\nand checkpoints for the trained models.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\n17\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: We provide code and trained models in the supplementary material. We further\ndescribe where to download the datasets in Appendix A.3.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Appendix A.3 and Appendix A.7 specify data splits, hyperparameters, the\nfinding of hyperparameters and optimizer choice\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: We present standard errors on three repetitions for all our experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n18\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: In Section 4 and the Appendix we specify the main CPU or GPU, the compute\ntime as well as the VRAM usage, where it is relevant.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: Yes\nJustification: We conform to the Code of Ethics and have communicated the impact of the\nwork in Section 5.3\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We do this in Section 5.3.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n19\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: Does not apply\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We mention the licenses for the data, code, and models in Appendix A.3 and\nAppendix A.13.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n20\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: Code is documented.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: Does not apply.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: Does not apply\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n21\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: Only used for writing/editing\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n22\nFlow Matching \nHybrid Path Grads\n0.064\n0.048\n0.032\n0.016\n0.000\n0.016\n0.032\n0.048\n0.064\n0.080\np (log p\nlog q )\nFigure 5: Loss in space after training as done in Figure 1\nA\nAppendix\nA.1\n2D GMM experiment\nThe 2D GMM is a Gaussian Mixture model with 4 equally weighted Gaussians\nN\n\u0012\u0014\nai\nbi\n\u0015\n,\n\u0014\nci + 0.01\n0\n0\ndi + 0.01\n\u0015\u0013\n,\n(22)\nwith ai, bi, ci, di ∼N(0, 1), i ∈{1, 2, 3, 4}. The training set consists of 2000 samples, the KL(p|qθ)\nand MSE loss are evaluated on 2048 samples. We used Adam with default parameters and lr=1e-2 for\npure FM/PG and pre-training with FM and lr=5e-3 for finetuning with PG. All experiments are run\non an Intel i7-1165G7 CPU and ignoring the validation, training completes in roughly 45 seconds.\nFigure 5 shows the weighted difference\np(x) · log p(x)\nqθ(x)\n(23)\nafter training.\nA.2\nArchitecture\nWe use the same model architecture as introduced in Klein et al. (2023b); Klein & Noé (2024). We\nhere summarize it briefly, closely following the presentation in Klein & Noé (2024).\nThe underlying normalizing flow model for the Boltzmann Generator is a CNF. The corresponding\nvector field vθ(t, x) is parametrized by an O(D)- and S(N)-equivariant graph neural network\n(EGNN) Garcia Satorras et al. (2021); Satorras et al. (2021). The vector field vθ(x, t) consists of L\nconsecutive EGNN layers. The position of the i-th particle xi is updated according to the following\n23\nset of equations:\nh0\ni = (t, ai),\nml\nij = ϕe\n\u0000hl\ni, hl\nj, d2\nij\n\u0001\n,\n(24)\nxl+1\ni\n= xl\ni +\nX\nj̸=i\n\u0000xl\ni −xl\nj\n\u0001\ndij + 1 ϕd(ml\nij),\n(25)\nhl+1\ni\n= ϕh\n\u0000hl\ni, ml\ni\n\u0001\n,\nml\ni =\nX\nj̸=i\nϕm(ml\nij)ml\nij,\n(26)\nvθ(x0, t)i = xL\ni −x0\ni −1\nN\nN\nX\nj\n(xL\nj −x0\nj),\n(27)\nwhere the ϕα represent different neural networks, dij is the Euclidean distance between particle i and\nj, t is the time, ai is an embedding for each particle.\nThe proposed model architecture in Klein et al. (2023b) uses for alanine dipeptide distinct encodings\nai for all backbone atoms and the atom types for all other atoms. In contrast, the model architecture\nin Klein & Noé (2024) uses distinct encodings for all atoms, except for Hydrogens bond to the same\nCarbon atom. We refer to this model as TBG, which stands for transferable Boltzmann Generator,\neven though we do not deploy it in a transferable way in this work.\nFor more details see Klein et al. (2023b); Klein & Noé (2024).\nA.3\nDatasets\nHere we provide more details on the investigated datasets.\nWe use the same training and test splits as defined in Klein et al. (2023b); Klein & Noé (2024).\nLennard-Jones systems\nThe energy U(x) for the Lennard-Jones systems is given by\nU(x) = 1\n2\n\nX\ni,j\n \u0012 1\ndij\n\u001312\n−2\n\u0012 1\ndij\n\u00136!\n,\n(28)\nwhere dij is the distance between particle i and j.\nThe authors of Klein et al. (2023b)\n(CC BY 4.0) made the datasets available here:\nhttps://osf.io/srqg7/?view_only=\n28deeba0845546fb96d1b2f355db0da5.\nFor computing the metrics we use the following number of test samples:\nLJ13: ESSq: 5 × 105; ESSp, NLL: 5 × 105.\nLJ55: ESSq: 1 × 105; ESSp, NLL: 1 × 105.\nAlanine dipeptide\nThe classical alanine dipeptide dataset was generated with an MD simulation,\nusing the classical Amber ff99SBildn force-field at 300K for implicit solvent for a duration of 1 ms\nKöhler et al. (2021) with the openMM library. The datasets is available as part of the public bgmol\n(MIT licence) repository here: https://github.com/noegroup/bgmol.\nThe alanine dipeptide dataset with the semi empirical force-field, was generated by relaxing 105\nrandomly selected states from the classical MD simulation. The relaxation was performed with\nthe semi-empirical GFN2-xTB force-field for 100 fs each, using (Bannwarth et al., 2019) and the\nASE library (Larsen et al., 2017) with a friction constant of 0.5 a.u. The test set was created it\nin the same way. The authors of Klein et al. (2023b) made the relaxed alanine dipeptide with the\nsemi empirical force field available here (CC BY 4.0): https://osf.io/srqg7/?view_only=\n28deeba0845546fb96d1b2f355db0da5.\nFor AD2-XTB, we precompute the target forces of the samples, to re-use them during PG training.\nFor computing the metrics we use the following number of test samples:\nESSq: 2 × 105; ESSp, NLL: 1 × 105.\nDipetide dataset (2AA)\nThe dipeptide dataset was introduced by Klein et al. (2023a) and is\navailable at https://huggingface.co/datasets/microsoft/timewarp. The dataset consists\nof classical MD trajectories of dipeptides at 310K. There are 200 trajectories in the train set, simulated\nfor 50ns each and 100 each in the validation and test set, simulated for 1µs each.\n24\nFigure 6: Alanine dipeptide results for the TBG model and the classical force field with and without\nPath Gradient finetuning. Left and middle: Free energy projection for the φ and ψ dihedral angles,\nrespectively. Right: Bond-length distribution for a Carbon - Nitrogen bond.\nA.4\nFinetuning experiments\nWe used Adam with a learning rate of 1e-4 for PG and only the training set provided. For each of\nthe training points, we require access to the force of the target. We use Hutchinson’s estimator for\nestimating the trace of the Jacobian. Importantly, in order to keep the memory constant, we avoid\nsaving checkpoints. For our experiments we used A100 GPUs.\nFor LJ13, we used a batch-size of 64 instead of 256, which uses about 90% of the original memory\n(FM 1.52GB, PG 1.38GB). For fine-tuning we used 2 epochs for Path Gradients, which took 117\nmin, instead of 135 min for 1000 epochs with Flow Matching. For LJ55, we use the same batch-sizes\nresulting in 13.24GB for Flow Matching and 14.76GB for Path Gradients. We fine-tuned with Path\nGradients for 1 epoch, taking 394 minutes instead of 440 min for 400 epochs with Flow Matching.\nBecause we are measuring the ESS and neg-loglikelihood on the target density, we removed the\nreweighting from the Alanine Dipeptide data. If we changed the target density to its reweighted version\nand computed the gradients of the weighting w.r.t. the samples, we could also straightforwardly\napply Path Gradients to the reweighted distribution. Further, fine-tuning on AD2 with Path Gradients\nshowed some instabilities during training. To fight these, we employed gradient clipping to a norm of\n1 and gradient accumulation to a batch-size of around 1000 in batches of 50. The resulting fine-tuning\nwith Path Gradients uses 2.44GB VRAM and 131 min, compared to 200 minutes with 2.95GB with\nFlow Matching and batch-size 256.\nWe used the same number of samples like (Klein et al., 2023b) for the ESS, but note that a higher\nnumber might have been beneficial for more reliable estimates.\nFor the experiments, we did not run exhaustive hyperparameter tuning. The Batch-sizes were set to\nhave a similar memory footprint. The hybrid PG learning rate was set to the middle between the\ninitial and FM-fine-tuning one after some preliminary experiments.\nA.5\nAdditional results for alanine dipeptide\nHere we present additional alanine dipeptide results for the TBG model in Figure 6. We observe\nthat the φ and ψ dihedral-angle projections change little after path-gradient fine-tuning. Moreover,\nreweighting to the target distribution via Equation (2) succeeds in both cases, though it is more\nefficient post–fine-tuning, as reflected by a higher ESS (see Table 1). In contrast, all bond-length\ndistributions align much more closely with the target after fine-tuning, exemplified here by a Carbon -\nNitrogen bond in Figure 6.\nA.6\nAdditional results for alanine dipeptide - classical with more memory\nFor classical AD2, we further investigated the performance of fine-tuning with PGs when allowed\na larger memory footprint. We fine-tuned with batch-size 1024 for two epochs, which resulted in\n22GB VRAM usage and a runtime of 92 minutes. In Table 2 we can see that this yielded a further\nimprovement over Table 1.\n25\nTable 2: Additional experiment for fine-tuning TBG (Klein & Noé, 2024) with Path Gradients on\nclassical Alanine Dipeptide. First all models were pre-trained with Optimal Transport Flow Matching\nlike in (Klein et al., 2023b). We compare fine-tuning with Flow Matching and Path Gradients. Here\nwe did not limit the memory usage and trained PG with batch-size 1024 for 2 epochs. mean ± sterr\nover 3 runs.\nTraining type\nNLL (↓)\nESSq %(↑)\nESSp %(↑)\nOnly FM\n−128.01 ± 0.01\n14.42 ± 2.44\n12.21 ± 1.06\nHybrid (ours) unlimited\n−128.33 ± 0.04\n29.47 ± 2.24\n19.57 ± 4.79\nFigure 7: Integration trajectory lengths for the Lennard-Jones system with 13 particles. Compared\nare different Flow Matching models as well as the same models after fine-tuning with Path Gradients\n(Hybrid).\nA.7\nAdditional results for experiments on flow trajectory length\nFine-tuning on the 13-particle Lennard–Jones system used an A100 for 10 epochs (152 min/run)\nstarting from the Klein et al. (2023b) checkpoints, with training and evaluation on 5 × 105 samples.\nAs shown in Table 3, path-gradient fine-tuning boosts NLL and ESS across the board while leaving\nflow trajectory lengths nearly unchanged (see Figure 7). One of the nine runs diverged during training,\nwas discarded and repeated.\nTable 3: Performance metrics before and after fine-tuning for different methods for the LJ13 system.\nmean ± sterr over 3 runs.\nMethod\nFine-tuning\nTrajectory Length\nNLL\nESSq (%)\nESSp (%)\nStandard\nbefore\n3.76 ± 0.00\n−16.02 ± 0.01\n60.45 ± 0.89\n40.23 ± 20.11\nafter\n3.86 ± 0.00\n−16.22 ± 0.00\n85.78 ± 0.69\n85.93 ± 0.57\nOptimal Transport\nbefore\n2.80 ± 0.01\n−16.01 ± 0.01\n57.16 ± 0.88\n57.07 ± 0.54\nafter\n2.78 ± 0.04\n−16.22 ± 0.00\n85.91 ± 0.03\n85.78 ± 0.02\nEquivariant Op-\nbefore\n2.11 ± 0.00\n−16.04 ± 0.00\n58.11 ± 1.93\n19.77 ± 12.36\ntimal Transport\nafter\n2.19 ± 0.00\n−16.23 ± 0.00\n87.77 ± 0.24\n58.40 ± 29.20\nA.8\nComparison with maximum likelihood training\nWe here present comparisons with maximum likelihood training as a baseline on LJ13. To this end\nwe compare to Garcia Satorras et al. (2021), which is nearly the same architecture as in Klein et al.\n(2023b), but the model is trained via maximum likelihood training instead of flow matching. The\nresults are shown in Table 4.\nA.9\nAdditional energy histograms\nWe show additional energy histograms of the systems investigated in Section 4 in Figure 8. The\naverage energy for every investigated system is smaller and closer to the target energy distribution\nafter PG finetuning.\n26\nMethod\nNLL\nESSq in %\nML training (Garcia Satorras et al., 2021)\n−15.83 ± 0.07\n39.78 ± 6.19\nOnly FM (Klein et al., 2023b)\n−16.09 ± 0.03\n54.36 ± 5.43\nHybrid approach (Ours)\n−16.21 ± 0.00\n82.97 ± 0.40\nTable 4: Results for the Lennard-Jones system with 13 particles (LJ13).\nFigure 8: Energy histograms for the a) LJ13 systems, b) the LJ55 system, and c-i) Different\ndidpeptides from the testset for the transferable Boltzmann Generator (TBG).\nA.10\nResults for transferable Boltzmann Generators (TBG) evaluated on dipeptides\nWe applied the hybrid approach to transferable Boltzmann Generators (TBG) on dipeptides as\nintroduced in Klein & Noé (2024). We again fine-tune the pretrained Boltzmann Generator from\nKlein & Noé (2024) with path gradients. Training took 6 days on an A100 with learning rate 0.00001.\nThis transferable Boltzmann Generator is trained on a subset of all possible dipeptides and evaluated\non unseen ones (for more details see Appendix A.3). Due to the expensive evaluation, we evaluated\nthe model on 16 test dipeptides and chose the same subset as in Klein & Noé (2024). Our experiments\nshow that fine-tuning with path gradients improves the NLL and energies for all evaluated unseen\ntest dipeptides with an average improvement to -100.93 from -100.72 NLL and also shows average\nimprovements for the ESSq of around 23% to an efficiency of 9.79% as shown in Table 5. An\nexample energy histogram is shown in Figure 8c.\nA.11\nAbout the different estimators\nIn the following, we first discuss the Maximum Likelihood (ML) and Path Gradient (PG) estimators\nfor the forward KL and show benefits of PG over FM for a toy example.\nA.11.1\nThe Maximum Likelihood and Path Gradient Estimators\nIn short: both the ML and PG estimators are unbiased and consistent, but they differ in variance. For\nPG, we can give guarantees about their variance, once qθ equals to p, see e.g. (Vaitl, 2024; Roeder\net al., 2017; Tucker et al., 2019; Vaitl et al., 2022b).\n27\nDipeptide\nNLL (Before)\nNLL (After PG)\nESSq (Before) in %\nESSq (After PG) in %\nKS\n-100.82\n-100.99\n6.0\n4.6\nAT\n-75.49\n-75.61\n20.8\n21.5\nGN\n-65.45\n-65.55\n19.2\n25.6\nLW\n-148.91\n-149.22\n0.4\n3.6\nNY\n-118.24\n-118.47\n9.5\n10.5\nIM\n-106.80\n-107.01\n3.1\n4.6\nTD\n-81.09\n-81.19\n4.1\n11.6\nHT\n-103.06\n-103.27\n0.2\n5.5\nKG\n-88.60\n-88.79\n5.0\n7.4\nNF\n-116.45\n-116.67\n3.7\n12.2\nRL\n-135.71\n-136.12\n1.3\n1.5\nET\n-89.94\n-90.08\n6.3\n4.1\nAC\n-63.90\n-63.91\n31.8\n33.3\nGP\n-71.63\n-71.77\n10.5\n8.0\nKQ\n-119.36\n-119.68\n4.3\n2.0\nRV\n-126.28\n-126.62\n1.3\n0.6\nAverage\n-100.73\n-100.93\n7.96\n9.79\nTable 5: Comparison of NLL and ESS before and after PG for different dipeptides from the testset.\nBoth Maximum Likelihood and the Path Gradient estimators optimize the Forward KL Equation 8.\nLet’s look at both estimators in detail and compare them. A full derivation can be found in Appendix\nB.3.2 of Vaitl et al. (2024) & Chapter 4 of Vaitl (2024).\nTo obtain the estimator GML, we first observe that the first term in the KL divergence is constant\nw.r.t. θ.\nKL(p|qθ) = Ep(x1) [log p(x1) −log qθ(x1)] ,\n(29)\nwhich means that it does not enter in the gradient if we directly estimate the gradients via an MC\nestimator.\ndKL(p|qθ)\ndθ\n= −Ep(x1)\n\u0014 d\ndθ log qθ(x1)\n\u0015\n≈GML = −1\nN\nN\nX\ni=1\nd\ndθ log qθ(x(i)\n1 ) , x(i)\n1\n∼p .\n(30)\nFor the exact calculation of the estimator, we decompose it fully with\nqθ(x1) = q0(T −1\nθ\n(x1))\n\f\f\f\fdet ∂T −1\nθ\n(x1)\n∂x1\n\f\f\f\f .\n(31)\nThe actual calculation for the ML estimator then is\nGML = −1\nN\nN\nX\ni=1\nd\ndθ\n\u0012\nlog q0(T −1\nθ\n(x1)) + log\n\f\f\f\f\n∂T −1\nθ\n(x1)\n∂x1\n\f\f\f\f\n\u0013\n.\n(32)\nFor PG, we use Eq. 17 to directly obtain the MC estimator GP G\nd\ndθKL(p0,θ|q0) = Ex1∼p1\n\u0014 ∂\n∂x0\n\u0012\nlog p0,θ\nq0\n(x0)\n\u0013 ∂T −1\nθ\n(x1)\n∂θ\n\u0015\n≈GP G = 1\nN\nN\nX\ni=1\n∂\n∂x(i)\n0\n\u0010\nlog p0,θ(x(i)\n0 ) −log q0(x(i)\n0 )\n\u0011 ∂T −1\nθ\n(x(i)\n1 )\n∂θ\n, x(i)\n1\n∼p .\n(33)\nHere x(i)\n0 is a shorthand for T −1\nθ\n(x(i)\n1 ). If we again decompose the term we expose the full calculation\nGP G = 1\nN\nN\nX\ni=1\n∂\n∂x(i)\n0\n\u0012\nlog p(Tθ(x(i)\n0 )) + log\n\f\f\f\f\n∂Tθ(x0)\n∂x0\n\f\f\f\f −log q0(x(i)\n0 )\n\u0013 ∂T −1\nθ\n(x(i)\n1 )\n∂θ\n(34)\n28\nComparing (32) and (34), we see that the path gradient estimator incorporates GP G the gradient\ninformation of the target p – ∂log p(Tθ(x(i)\n0 )\n∂x(i)\n0\n– while GML – and by construction also GF M – does not.\nA.11.2\nVariance of the estimators\nOpposed to GML and GF M, we have nice guarantees about the variance of the PG estimator GP G at\nthe optimum and close to it.\nWe first recapitulate results from previous work and then, by using a simple example, show that GF M\ndoes not necessarily exhibit zero variance at the optimum.\nIn general, the variance of the Path Gradient estimators GP G is bounded by the squared Lipschitz\nconstant of the term\n(log p0,θ(x0) −log q0(x0)) = (log p(x1) −log qθ(x1)) ,\n(35)\nsee Mohamed et al. (2020) Section 5.3.2.\nThus, if the target density p is not well approximated by qθ, the variance of the gradient estimator\ncan be large and training with PG might not be beneficial. If log p and log qθ are close in the sense\nthat the Lipschitz constant of their difference is small, we can assume path gradient estimators to be\nhelpful.\nGradient estimators at the optimum\nIn the case of perfect approximation, i.e. qθ(x) = p(x)∀x ∈\nX, the following statements about the ML estimator and the Path Gradient estimators are known. The\ngradients for path gradient estimators are deterministically 0, i.e. E[GP G] = 0, Var[GP G] = 0, while\nfor ML the variance is generically nonzero E[GML] = 0, Var[GML] = 1\nN Iθ. Where Iθ is the Fisher\nInformation Matrix of p0,θ (Vaitl et al., 2022a).\nWhy is that?\nAlready the review by Papamakarios et al. (2021) notes in Section 2.3.3 that the\nduality of the KL divergence means that fitting the model qθ to the target p using ML is equivalent to\nfitting p0,θ to the base q0 under the reverse KL. This means that the results from Vaitl (2024) directly\nhold. We only adapted the notation.\nA.11.3\nVariance of GF M for a toy example\nWe do not have a term for the variance of GF M for general settings, but we can show that it does not\ndeterministically vanish like for GP G via a simple example. This means better behavior for PG than\nfor FM at the optimum, which we also verified empirically. Our assumptions aim to simplify the\nexample as much as possible.\n• First, assume the standard loss for Flow Matching.\n• Further, assume the two densities to be the same D-dimensional Normal distribution\nq0 = p = N(0, I).\n• Finally, assume the CNF is the identity parametrized by a single parameter θ, i.e.\nvθ = θ = 0.\nIn this example qθ(x1) = q0(x1) · I approximates the target density p perfectly and the optimal\ngradient estimator is 0. Yet, in this setting the variance of the estimator GF M is non-zero:\nVar[GF M] =\n8\nND ,\n(36)\npreventing the model from staying at the optimum during training.\nProof:\nThe gradient estimator for the FM loss Eq (10) is\nGF M = 1\nN\nX\ni\n∂\n∂θ\n\u0010\n||vθ −(x(i)\n1 −˜x(i)\n0 )||2\u0011\n,\n(37)\nwhere ˜x(i)\n0 , x(i)\n1\n∼N(0, I) 3.\n3Note that here ˜x(i)\n0\nand x(i)\n1\nare independently sampled. Before x(i)\n0\nwas the transformed sample\n29\nFirst, we break down the terms\nGF M = 1\nN\nN\nX\ni=1\n∂\n∂θ\n1\nD\nD\nX\nd=1\n\u0010\nvθ,d + ˜x(i)\n0,d −x(i)\n1,d\n\u00112\n=\n1\nND\nN\nX\ni=1\nD\nX\nd=1\n2\n\u0010\nvθ,d + ˜x(i)\n0,d −x(i)\n1,d\n\u0011 ∂vθ,d\n∂θ\n.\n(38)\nBecause vθ is parametrized by θ, we set in ∂vθ,d\n∂θ\n= 1 and vθ,d = 0 and separate the terms\nGF M =\n1\nND\nX\ni\nX\nd\n2(˜x(i)\n0,d −x(i)\n1,d) =\n2\nND\n X\nd\nX\ni\n˜x(i)\n0,d −\nX\nd\nX\ni\nx(i)\n1,d\n!\n.\n(39)\nWe can compute the distribution of GF M by using the property\nN\nX\ni=1\nai ∼N(0, Nσ2\na) for a ∼N(0, σ2\na) .\n(40)\nSo the sum over dimensions and samples follows the normal distribution with variance DN:\nX\nd\nX\ni\n˜x(i)\n0,d ∼N(0, DN)\n(41)\nand the difference has twice its variance\nX\nd\nX\ni\n˜x(i)\n0,d −\nX\nd\nX\ni\nx(i)\n1,d ∼N(0, 2DN) .\n(42)\nThe expectation of the estimator GF M then is 0 and the variance is simply\nVar[GF M] =\n4\nN 2D2 Var[(\nX\nd\nX\ni\n˜x(i)\n0,d −\nX\nd\nX\ni\nx(i)\n1,d)] =\n4\nN 2D2 2DN =\n8\nND .\n(43)\nInterestingly, the derivative GF M is invariant to re-ordering because the sums of x1 and ˜x0 are\ninvariant to permutation. The result thus also holds for OT-FM. This property is due to the simple\nassumption, the vector field vθ is independent of xt.\nA.12\nPseudocode for forward KL path gradients via augmented adjoint\nAlgorithm 1 Augmented Adjoint Dynamics\n1: function FORWARD(t, xt, ∇log q)\n2:\n˙x, ˙div ←black_box_dynamics(t, xt)\n3:\n˙\n∇log q ←gradientx\n\u0002\n−∇log q · ˙x −˙div\n\u0003\n4:\nreturn ˙x,\n˙\n∇log q, −˙div\n5: end function\nAlgorithm 2 Pathwise Gradient Estimator\n1: function PATHWISEGRADIENTESTIMATE(x1, prior, target, flow)\n2:\nlog p1 ←−target.energy(x1)\n3:\n∇log p1 ←gradientx1(log p1)\n▷Integrate using Augmented Adjoint state method\n4:\nx0, ∇log p0,θ, log | det J| ←flow.integrateAugAdjoint(x1, ∇log p1), inverse=True)\n5:\nlog q0 ←−prior.energy(x0)\n6:\n∇log q0 ←gradientxo(log q0)\n▷Compute gradient of loss w.r.t. sample x0\n7:\n∇x0L ←1\nN (∇log p0,θ −∇log q0) ▷Backpropagate using standard Adjoint state method\n8:\npath gradients ←gradientθ (x0 · detach(∇x0L)])\n9: end function\n30\nA.13\nCode libraries\nWe primarily use the following code libraries: PyTorch (BSD-3) (Paszke et al., 2019), bgflow (MIT\nlicense) (Noé et al., 2019; Köhler et al., 2020), torchdyn (Apache License 2.0) (Poli et al., 2021).\nAdditionally, we use the code from (Garcia Satorras et al., 2021) (MIT license) for EGNNs, as well as\nthe code from (Klein et al., 2023b) (MIT license) and (Klein & Noé, 2024) (MIT license) for models\nand dataset evaluations.\n31\n",
  "pages": [
    {
      "page_number": 1,
      "text": "Path Gradients after Flow Matching\nLorenz Vaitl\nlorenz.vaitl@outlook.com\nLeon Klein\nFreie Universität Berlin\nleon.klein@fu-berlin.de\nAbstract\nBoltzmann Generators have emerged as a promising machine learning tool for\ngenerating samples from equilibrium distributions of molecular systems using\nNormalizing Flows and importance weighting. Recently, Flow Matching has helped\nspeed up Continuous Normalizing Flows (CNFs), scale them to more complex\nmolecular systems, and minimize the length of the flow integration trajectories.\nWe investigate the benefits of using Path Gradients to fine-tune CNFs initially\ntrained by Flow Matching, in a setting where the target energy is known. Our\nexperiments show that this hybrid approach yields up to a threefold increase in\nsampling efficiency for molecular systems, all while using the same model, a similar\ncomputational budget and without the need for additional sampling. Furthermore,\nby measuring the length of the flow trajectories during fine-tuning, we show that\nPath Gradients largely preserve the learned structure of the flow.\n1\nIntroduction\nGenerative models, ranging from GANs (Goodfellow et al., 2014) and VAEs (Kingma & Welling,\n2014) to Normalizing Flows (Rezende & Mohamed, 2015; Papamakarios et al., 2019) and Diffusion\nModels (Ho et al., 2020; Song et al., 2021), have advanced rapidly in recent years, driving progress\nboth in media generation and in scientific applications such as simulation-based inference. While\nscientific workflows often incorporate domain-specific symmetries, they tend to under-exploit a\ncrucial resource: the unnormalized target density.\nBoltzmann Generators (Noé et al., 2019) are typically trained either via self-sampling (Boyda et al.,\n2021; Nicoli et al., 2020; Invernizzi et al., 2022; Midgley et al., 2023), leveraging gradients from the\ntarget distribution, or using samples from the target without incorporating gradient information (Nicoli\net al., 2023; Klein et al., 2023b; Draxler et al., 2024; Klein & Noé, 2024). However, these approaches\neach ignore complementary parts of the training signal: either the data or its local geometry. Notably,\nfirst-order information evaluated at target samples remains underused, despite its potential to improve\ntraining. In this work, we close this gap by fine-tuning Continuous Normalizing Flows, initially\ntrained with Flow Matching, using Path Gradients (Roeder et al., 2017; Vaitl, 2024) on samples from\nthe target distribution. Furthermore, our approach requires computing target gradients only once\nper training sample, avoiding the potentially high cost of repeatedly computing gradients on newly\ngenerated samples for self-sampling.\nFlow Matching, a method for training CNFs, is based on target samples. It is closely related to\ndiffusion model training and has recently gained traction for its simulation-free training and strong\nempirical performance, both in generative modeling benchmarks (Lipman et al., 2023; Esser et al.,\n2024; Jin et al., 2024) and in scientific domains (Stark et al., 2024; Jing et al., 2024; Klein & Noé,\n2024). Here, we investigate how incorporating Path Gradients on samples from the target distribution,\nenhances CNF performance post flow-matching.\nPath gradients are low variance gradient estimators, which have strong theoretical guarantees close to\nthe optimum (Roeder et al., 2017) and incorporate gradient information from both, the variational\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\n"
    },
    {
      "page_number": 2,
      "text": "and the target distribution (Vaitl et al., 2024). While they have been adopted for training Normalizing\nFlows in the field of Lattice Gauge Theory (Bacchio et al., 2023; Kanwar, 2024; Abbott et al., 2023)\nand also variational inference (Agrawal & Domke, 2024; Andrade, 2023), they remain underused\nin Biochemistry. In this work, we explore the potential of Path Gradient fine-tuning for Boltzmann\nGenerators with promising results. This indicates that even though training with Path Gradients is\norders of magnitude slower per iteration, its use of additional information and low variance allows it\nto outperform Flow Matching within the same computational constraints in fine-tuning.\nWe make the following main contributions:\n• We propose hybrid training using Flow Matching for pre-training and Path Gradients for\nfine-tuning. We do not require additional samples beyond the original force-labeled data,\nwhich were required already for generating the data. In order to ensure fast training, we are\nthe first to optimize CNFs using Path Gradients for the forward KL and making use of the\naugmented adjoint method (Vaitl et al., 2022b).\n• We show for many particle systems as well as alanine dipeptide that this fine-tuning approach\ncan triple importance-sampling efficiency within the same computational budget.\n• We investigate Path Gradients’ impact on properties trained during Flow Matching, namely\nflow trajectory length and the MSE Flow Matching loss, and show that these are mostly\nunaffected by the fine-tuning.\n0.0\n15.0\n30.0\n45.0\n60.0\nTime [s]\n0.0\n0.2\nKL(P|Q)\nFlow Matching\nPath Gradients\nHybrid\nStart Fine-tuning\nTarget\nFlow Matching  0.067\nPath gradients  0.021\nHybrid   0.001\nFigure 1: Training a CNF on a simple 2D Gaussian Mixture Model. Comparison between pure\ntraining with Flow Matching, pre-training with Flow Matching and fine-tuning with Path Gradients\nand pure training with Path Gradients. We see that given the same wall-time hybrid training performs\nbest in terms of forward KL divergence. The bottom row shows the target and the final model after\ntraining.\n2\nMethod\nGenerative models have recently garnered significant interest for biochemical applications (Noé et al.,\n2019; Jumper et al., 2021; Abramson et al., 2024). In this work, we focus on the application of\nenhancing or replacing molecular dynamics simulations.\n2.1\nBoltzmann Generators\nBoltzmann Generators (Noé et al., 2019) aim to asymptotically generate unbiased samples from the\nequilibrium Boltzmann distribution\np(x) = 1\nZ exp(−U(x))\n(1)\nwith an energy U(x) and an unknown normalization factor Z =\nR\nexp(−U(x))dx. The Boltzmann\ndistribution describes the probability of states in a physical system at thermal equilibrium. Traditional\n2\n"
    },
    {
      "page_number": 3,
      "text": "sampling methods, such as Markov Chain Monte Carlo (MCMC) and Molecular Dynamics (MD)\nsimulations, generate samples sequentially. In contrast, Boltzmann Generators are designed to\nproduce independent samples directly from the target distribution. There are many different instances\nof Boltzmann Generators, many focus on molecular systems (Dibak et al., 2022; Köhler et al., 2021;\nMidgley et al., 2023; Ding & Zhang, 2021b,a; Kim et al., 2024; Rizzi et al., 2023; Tamagnone et al.,\n2024; Schönle et al., 2024; Klein & Noé, 2024; Tan et al., 2025), while others are trained to sample\nlattice and other many particle systems (Wirnsberger et al., 2020; Ahmad & Cai, 2022; Nicoli et al.,\n2020, 2021; Schebek et al., 2024; Abbott et al., 2023; Kanwar, 2024).\nBoltzmann Generators employ a Normalizing Flow to map samples from a simple base distribution\ninto a learned sampling distribution qθ, which is trained to approximate the target Boltzmann\ndistribution. We can use Boltzmann Generators to obtain asymptotically unbiased estimators for\nobservables over p(x) using the sampling distribution qθ and (self-normalized) importance sampling\n(Noé et al., 2019; Nicoli et al., 2020),\nEp(x) [O(x)] = Eqθ(x)\n\u0014 p(x)\nqθ(x)O(x)\n\u0015\n,\n(2)\nfor an observable O : Rd →R. For self-normalized importance sampling, we still have to estimate\nthe normalization constant\nZ =\nZ\nexp(−U(x))dx = Eqθ(x)\n\u0014 p(x)\nqθ(x)\n\u0015\n(3)\nwith an MC estimator. This separates Boltzmann Generators from related generative methods, which\nonly generate approximate samples from the Boltzmann distribution (Jing et al., 2022; Abdin & Kim,\n2023; Klein et al., 2023a; Schreiner et al., 2023; Jing et al., 2024; Lewis et al., 2024).\nWe can estimate the efficiency of importance sampling using the (relative) effective sampling size\n(ESS). The ESS\nESS :=\n \nEqθ\n\"\u0012 p\nqθ\n(x)\n\u00132#!−1\n=\n\u0012\nEp\n\u0014 p\nqθ\n(x)\n\u0015\u0013−1\n∈[0, 1]\n(4)\ncan be estimated either on samples from the model qθ or the target distribution p. Roughly speaking,\nthe ESS tells us how efficiently we are able to sample from the target distribution. The estimator\nESSq, based on samples from qθ, might fail to detect missing modes in the target distribution, while\nESSp requires samples from the target distribution. Both estimators exhibit high variance and might\noverestimate the performance if too few samples are being used. For more information confer (Nicoli\net al., 2023).\n2.2\nContinuous Normalizing Flows\nNeural ODEs, introduced by Chen et al. (2018), parameterize a continuous-time transformation\nthrough a neural-network-defined vector field vθ. By employing the adjoint sensitivity method\n(Pontryagin, 1987), gradients are computed in a backpropagation-like fashion with constant memory.\nBy computing the ODE of the divergence, we can compute the change in probability, which lets us\nuse a Neural ODE as a Normalizing Flow (Chen et al., 2018; Grathwohl et al., 2019).\nUsing a simple base distribution q0(x0) and the transformation\nTθ(x0) = x0 +\nZ 1\n0\nvθ(xt, t)dt ,\n(5)\nwe obtain a distribution qθ, which we can sample from and compute the probability\nlog qθ(Tθ(x0)) = log q0(x0) −log det\n\u0012∂Tθ(x0)\n∂x0\n\u0013\n= log q0(x0) −\nZ 1\n0\nTr\n\u0012∂vθ(xt, t)\n∂xt\n\u0013\ndt .\n(6)\nFor training samples from the target distribution, a straightforward way of approximating the target\ndensity is by maximum likelihood training, i.e. by minimizing\nLML(θ) = −Ep(x) [log qθ(x)] .\n(7)\n3\n"
    },
    {
      "page_number": 4,
      "text": "Minimizing Equation (7) is equivalent to minimizing the forward KL divergence up to a constant\nKL(p|qθ) = Ep(x) [log p(x) −log qθ(x)]\nc= LML(θ) .\n(8)\nAs Köhler et al. (2020) showed, we can construct equivariant CNFs by using an equivariant network\nas the vector field.\n2.3\nFlow Matching\nInspired by advances in diffusion models, Flow Matching (Lipman et al., 2023; Albergo et al., 2023;\nLiu et al., 2022) has emerged as a promising competitor to diffusion models (Esser et al., 2024). Flow\nMatching enables us to train CNFs in a simulation free manner, i.e. training without the need of the\nexpensive integration of the ODEs (5) and (6).\nThe idea behind Flow Matching is to posit a base probability p0 and a vector field ut(x), which\ngenerates pt from the base density p0, such that the final probability p1 equals the target density p.\nGiven these components, Flow Matching aims to minimize the mean squared error between the target\nvector field ut(x) and the learned vector field vθ,t(x)\nLFM(θ) = Et∼U(0,1),x∼pt(x)\n\u0002\n||vθ(x, t) −ut(x)||2\u0003\n.\n(9)\nsince pt and ut are not known, it is intractable to compute the objective. Nevertheless, Lipman et al.\n(2023) showed that we can construct a conditional probability path p(x|z) and corresponding vector\nfield ut(·|z) conditioned on z = (x0, x1), which has identical gradients to Eq. (9). This yields the\nconditional Flow Matching objective\nLCFM(θ) = Et∼U(0,1),x∼pt(x|z)\n\u0002\n||vθ(x, t) −ut(x|z)||2\u0003\n.\n(10)\nIn this framework, there are different ways of constructing the conditional probability path. We here\nfocus on the parametrization introduced in Tong et al. (2023), which results in optimal integration\npaths\nz = (x0, x1)\nand\np(z) = π(x0, x1)\n(11)\nut(x|z) = x1 −x0\nand\npt(x|z) = N(x|t · x1 + (1 −t) · x0, σ2),\n(12)\nwhere π(x0, x1) denotes the optimal transport map between p0 and p1. As this map is again\nintractable, it is only evaluated per batch during training. We refer to this parametrization as OT\nFM. In Klein et al. (2023b); Song et al. (2023) the authors show that we can extend this to highly\nsymmetric systems, such as many particle systems and molecules, and still obtain optimal transport\npaths. The approach enforces the system’s symmetries in the OT map by computing the Wasserstein\ndistance over optimally aligned samples with respect to those symmetries. Hereafter, we denote this\nparameterization as EQ OT FM. For more details on optimal transport Flow Matching refer to Tong\net al. (2023) and Klein et al. (2023b); Song et al. (2023). Note, that we obtain the original formulation\nin Lipman et al. (2023) by using p(z) = p0(x0)p1(x1), we will refer to it as standard FM.\n2.4\nPath Gradients\nIntroduced in the context of variational auto-encoders (Roeder et al., 2017; Tucker et al., 2019),\nPath Gradient estimators have improved performance, especially when applied to normalizing flows\n(Agrawal et al., 2020; Vaitl et al., 2022a,b; Agrawal & Domke, 2024). In the field of simulating\nLattice Field Theories, they have become a go-to tool for training flows (Bacchio et al., 2023; Kanwar,\n2024; Abbott et al., 2023). In these applications, the reverse Kullback-Leibler divergence KL(qθ|p)\nis minimized by self-sampling, i.e. without existing samples from the target density, but only on\nsamples from qθ generated while training.\nPath gradients are unbiased gradient estimators which have low variance close to the optimum. We\nobtain Path Gradients by separating the total derivative of a reparametrized gradient for the reverse\nKL\nd\ndθKL(qθ|p) = Eq0(x0)\n\u0014 d\ndθ log qθ(Tθ(x0))\np(Tθ(x0))\n\u0015\n(13)\n4\n"
    },
    {
      "page_number": 5,
      "text": "into two partial derivatives\nEq0(x0)\n\"\n∂\n∂x1\nlog qθ(x1)\np(x1) · ∂Tθ(x0)\n∂θ\n|\n{z\n}\nPath Gradient of KL(qθ|p)\n+ ∂log qθ(x1)\n∂θ\n\f\f\f\nx1=Tθ(x0)\n|\n{z\n}\nScore term\n#\n,\n(14)\ncalled the Path Gradient and the score term 1.\nRoeder et al. (2017) observed, that the martingale score term vanishes in expectation, but has a\nnon-zero variance. Vaitl et al. (2022a) showed that its variance is\n1\nN Iθ for a batch of size N and\nthe Fisher Information Iθ of the variational distribution qθ. The low variance close to the optimum\nallows Path Gradient estimators to \"stick-the-landing\" (Roeder et al., 2017), i.e. having zero variance\nat the optimum, making it an ideal tool for fine-tuning. Vaitl et al. (2024) showed that Path Gradient\nestimators incorporate additional information about the derivative of both target and variational\ndistribution, opposed to the reparametrized gradient gradient estimator (Kingma & Welling, 2014;\nMohamed et al., 2020) (see Appendix A.11.1 for a detailed explanation). They hypothesized that\nthis leads to less overfitting and a generally better fit to the target density (see Vaitl (2024)). These\ntheoretical aspects make Path Gradients a promising suitor for fine-tuning Boltzmann Generators.\nPath Gradients on given samples\nRecently, Path Gradient estimators have been proposed for\nsamples from distributions different from the variational density qθ (Bauer & Mnih, 2021). By\nviewing the forward KL(p1|qθ) at t = 1 as a reverse KL(p0,θ|q0) at t = 0, we can straightforwardly\napply Path Gradients (Vaitl et al., 2024). Here, we assume p0,θ to be a (Continuous) Normalizing\nFlow from p1 as its base density and T −1\nθ\nto be the parametrized diffeomorphism.\nKL(p1|qθ) = Ep1(x1)[log p1(x1) −log qθ(x1)]\n= Ep1(x1)\nh\nlog p1(x1) −\n\u0012\nlog q0(T −1\nθ\n(x1)) −log det\n\f\f\f\f\n∂T −1\nθ\n(x1)\n∂x1\n\u0013\f\f\f\f\n|\n{z\n}\n=log qθ(x1)\ni\n(15)\n= Ep1(x1)\nh \u0012\nlog p1(x1) −log det\n\f\f\f\f\n∂T −1\nθ\n(x1)\n∂x1\n\f\f\f\f\n\u0013\n|\n{z\n}\n=log p0,θ(x0)\n−log q0(T −1\nθ\n(x1))\ni\n= Ep0,θ(x0)\nh\nlog p0,θ(x0) −log q0(x0)\ni\n= KL(p0,θ|q0) .\n(16)\nThis different view lets us compute Path Gradients with the same ease as for the reverse KL, but on\ngiven samples from the target distribution.\nWhile Vaitl et al. (2022a) and the present work both use Path Gradients, both minimize different\nlosses on different samples. Our work optimizes the Forward KL(p1|qθ) with path gradients on\nexisting samples from p, as proposed in Vaitl et al. (2024) using the algorithm proposed in Vaitl et al.\n(2022a). Vaitl et al. (2022a) minimize the reverse KL(qθ|p1) via self-sampling, i.e. on samples from\nthe model qθ. Training via self-sampling has many drawbacks, mainly: modes of the target p can be\nentirely missed, which invalidates all asymptotic guarantees and breaks importance sampling, see e.g.\nNicoli et al. (2023). This becomes increasingly likely in higher dimensions. Further, the forces for\nthe newly generated samples have to be evaluated, which can increase the cost of training or lead to\nunstable optimization behavior.\nSince the publication Vaitl et al. (2022b), Flow Matching has been established as the de facto training\nmethod for CNFs. Our work investigates and combines the performance of Flow Matching and Path\nGradients. Specifically, we investigate the effect of Path Gradients on the inner workings of the CNF,\nlike e.g. trajectory length, while Vaitl et al. (2022b) simply looked at the performance compared to\nstandard self-sampling losses.\nIn Appendix A.11, we zoom in on the properties of the different estimators. We show that while Path\nGradients exhibit zero variance at the optimum, generically, Flow Matching does not. We further\n1The score term ∂log qθ(x)\n∂θ\nis not to be confused with the force term ∂log qθ(x)\n∂x\n, which is often called the\nscore in the context of diffusion models (Song et al., 2021).\n5\n"
    },
    {
      "page_number": 6,
      "text": "0.0\n15.0\n30.0\n45.0\n60.0\nTime [s]\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nMSE loss\nFlow Matching\nHybrid\nPath Gradients\nStart Fine-tuning\nFigure 2: FM loss objective 10 during training on 2D GMM averaged on three runs. Training with\nPath gradients leaves the MSE largely unchanged.\nexplicate the theoretical properties of the existing estimators for training via Maximum Likelihood,\nPath Gradients and Flow Matching.\nWhile the naive calculation of Path Gradients requires significantly more compute and memory than\nFlow Matching, we take several steps to obtain constant memory and speed ups in the next section.\nAs we show experimentally, fine-tuning with Path Gradient for a few epochs significantly improves\nthe performance for Continuous Normalizing Flows compared to only using Flow Matching.\n3\nPath Gradients and Flow Matching\nTo build an intuition for the behavior of Flow Matching (FM) versus Path Gradient (PG) estimators,\nwe start with a simple 2D Gaussian Mixture Model. We compare three training strategies: 1) FM\nonly, 2) PG only, and 3) a hybrid approach, using FM to quickly approximate the target distribution\np1, and subsequently applying PG for fine-tuning. Although slower per training step, PG has strong\ntheoretical guarantees near the optimum.\nFor Flow Matching, we use the standard formulation proposed in Lipman et al. (2023). The dynamics\nvθ is modeled using a four-layer fully connected neural network with 64 units per layer and ELU\nactivations. Given the simplicity of the model and task, we ignore memory usage in this setup. All\nexperiments are run on a CPU and complete in roughly one minute.\nAs shown in Figure 1, FM training rapidly improves initially but soon reaches a plateau with slow\nimprovements after. PG training in contrast, progresses more slowly but eventually reaches and\nsurpasses the FM plateau. The hybrid strategy, i.e. beginning with FM and switching to PG, results\nin the best performance, combining fast convergence with improved final accuracy.\nInterestingly, as shown in Figure 2, applying PG after FM has little effect on the MSE loss defined\nin Equation (10), suggesting that PG fine-tunes the variational distribution qθ without significantly\naltering the dynamics vθ. Nonetheless, this subtle adjustment leads to visibly better samples and\nimproved density matching (cf. Figure 1).\nIn the 2D case, we compute the divergence term in Equation (6) exactly by directly evaluating the\ntrace of the Jacobian. However, this approach scales quadratically with the number of dimensions\nand quickly becomes computationally prohibitive in higher-dimensional settings. A practical solution\nis Hutchinson’s trace estimator (Hutchinson, 1989), which provides us faster but noisy estimators for\nthe divergence.\nFor ODE integration, we use a fourth-order Runge–Kutta (RK4) scheme with 15 time steps, resulting\nin 60 function evaluations per integration trajectory. Under these settings, training on a single batch is\nempirically about 275 times faster with Flow Matching compared to Path Gradients. This discrepancy\nis expected, as the cost of function evaluations differs substantially between the two methods.\nWhile these results are promising, the 2D setup allows exact computation of the divergence and may\nnot generalize to higher-dimensional or more complex tasks. Moreover, memory usage becomes a\nsignificant bottleneck in these more realistic scenarios.\n6\n"
    },
    {
      "page_number": 7,
      "text": "Augmented Adjoint Method\nIn order to have constant memory irrespective of the number of\nfunction evaluations, we adapt the augmented adjoint method (Vaitl et al., 2022b) for Path Gradient\nestimators for the forward KL divergence Equation (16)\nd\ndθKL(p0,θ|q0) = Ex1∼p1\n\u0014 ∂\n∂x0\n\u0012\nlog p0,θ\nq0\n(x0)\n\u0013 ∂T −1\nθ\n(x1)\n∂θ\n\u0015\n.\n(17)\nIn order to compute the force of the variational distribution\n∂log p0,θ(x0)\n∂x0\n(18)\nwe use the augmented adjoint method by solving the ODE\nd\ndt\n∂log pt,θ(xt)\n∂xt\n= −∂log pt,θ(xt)\n∂xt\nT ∂vθ(xt, t)\n∂xt\n−\n∂\n∂xt\nTr\n\u0012∂vθ(xt, t)\n∂xt\n\u0013\n(19)\nwith initial condition\n∂log p1,θ\n∂x1\n= ∂log p1(x1)\n∂x1\n(20)\nfrom t = 1 to 0 (Vaitl et al., 2022b).\nComputing Path Gradients requires computing the gradient of the divergence\nTr\n\u0012∂vθ(xt, t)\n∂xt\n\u0013\n(21)\nas well as the ODEs 5 and 19 per integration step. Even for training on a single time-step, this is\nmore resource intensive than Flow Matching, which only requires the derivative of the vector field\n∂vθ(xt,t)\n∂θ\n. Thus, although the memory demands do not increase with additional integration steps of\nthe ODE, the memory required for Path Gradients is greater than with Flow Matching. Since the\ncomputational load associated with calculating the terms varies across different architectures, we\ndiscuss them individually for every experiment.\nFor RK4 with 15 integration steps, the ODEs 5, 6 and 19 are evaluated in the backward direction\n(t : 1 →0) and the ODE 5 and the adjoint method in the forward direction, amounting in 300 function\nevaluations, compared to two (the forward and backward call) with Flow Matching. Thus we expect\nFlow Matching to be faster than Path Gradients by a factor of at least 150.\n4\nExperiments\nMotivated by the improvements on the toy model, we now turn to more challenging problems, namely\nthe experiments done in Klein et al. (2023b) and Klein & Noé (2024). To this end, we use the same\narchitecture, a CNF with an EGNN (E(n) Equivariant Graph Neural Network) (Satorras et al., 2021;\nGarcia Satorras et al., 2021) for the vector field. The architecture in Klein & Noé (2024) is mostly\nthe same as in Klein et al. (2023b), with the main difference of the encoding of the atoms. While\nthe model in Klein et al. (2023b) treats the same atom type as indistinguishable, the model in Klein\n& Noé (2024) encodes nearly all atoms differently. Only hydrogens bound to the same atom are\ntreated as indistinguishable. For more details on the model architecture see Appendix A.2. While\nwe maintain the architecture and the initial training process with Flow Matching, we change the\nlater training procedure to include Path Gradients. As in Klein et al. (2023b), we investigate optimal\ntransport Flow Matching (OT FM), – and for the second set of experiments on LJ13 – equivariant\noptimal transport Flow Matching (EQ OT FM), and naive Flow Matching (standard FM).\nWe first evaluate the models on two many particle system with pair-wise Lennard-Jones interactions\nwith 13 and 55 particles. In addition we investigate Alanine dipeptide (AD2) at T = 300K both with\na classical and a semi-empirical force field (XTB). In contrast to (Klein et al., 2023b; Klein & Noé,\n2024), we do not bias the training data towards the positive φ state. The bias was originally introduced\nto make learning problem simpler, as the unlikely state is more prominent (Klein et al., 2023b). This,\nhowever, skews the evaluation, since the metrics, namely ESS and Negative log likelihood (NLL), are\nbased on the target distribution, not the biased one. Moreover, it assumes system-specific knowledge\nof slow-varying degrees of freedom, information that is typically unavailable for unknown systems.\n7\n"
    },
    {
      "page_number": 8,
      "text": "0\n2\n4\n6\n8\nEpoch\n0.6\n0.7\n0.8\nESS-Q\n0\n2\n4\n6\n8\nEpoch\n16.20\n16.15\n16.10\n16.05\n16.00\nNLL\nEQ OT\nOT\nStandard\n0\n2\n4\n6\n8\nEpoch\n2.5\n3.0\n3.5\nTrajectory Length\nFigure 3: Reverse ESS, NLL and trajectory length for Flows trained with standard FM, Optimal\nTransport and Equivariant Transport during fine-tuning with Path Gradients on LJ13. We can\nobserve that fine-tuning largely leaves the trajectory length unchanged, while substantially improving\nperformance. Mean ± sterr over three runs.\nFinally, we also investigate the 2AA dataset consisting of dipeptides simulated at T = 310K with a\nclassical force field (Klein et al., 2023a), to evaluate the influence of PG in a transferability. For more\ndetails on the datasets see Appendix A.3. We published code for replicating our experiments 2. In\ntotal, we investigate three scenarios:\n• First, fine-tuning with Path Gradients compared to fine-tuning with OT Flow Matching given\nthe same limited computational resources in Section 4.1.\n• Second, we apply Path Gradients on the FM-trained models using unlimited resources to\nmaximize performance after training with Standard FM, OT FM and EQ OT FM as done in\nKlein et al. (2023b), see Section 4.2. In this second case, we additionally examine the ODE\nintegration length to understand how PG fine-tuning influences the model.\n• Finally, we test if the path gradient fine-tuning improves results for transferability in the\nsetting by investigating transferable Boltzmann Generators (Klein & Noé, 2024) trained and\nevaluated on dipeptides, see Section 4.3.\n4.1\nFine-tuning using the same limited resources\nWe enforce comparable memory and wall-time constraints to those used for Flow Matching alone by\nadjusting batch size and employing gradient accumulation. Consequently, our fine-tuning runs with\nPath Gradients complete in less wall-time and occupy a similar memory footprint. For full training\nstatistics and implementation details, see Appendix A.4 and Appendix A.5.\nReplicating the experiments in Klein et al. (2023b), we evaluate how path-gradient fine-tuning impacts\nmodel performance. Table 1 presents a comparison between models trained with pure OT Flow\nMatching and those optimized using the hybrid approach of OT FM pre-training and path-gradient\nfine-tuning.\nWe observe that fine-tuning with Path Gradients improves the ESS metrics, i.e. ESSp and ESSq,\nacross most datasets and models, despite their high variance. The hybrid approach reliably improves\nthe NLL for all models and datasets, but one. The only exception is the standard Flow (Klein et al.,\n2023b) on AD2 with XTB, where fine-tuning with FM still performs better on average. Examining\nthe ESS, we find that both approaches fail to adequately fit the target density. This supports the\nhypothesis that Path Gradients are only effective for fine-tuning when the flow already provides a\nreasonably good approximation of the target. Notably, on LJ55, our hybrid approach nearly triples\nthe ESS, and on AD2 with XTB, it doubles it – in the same training time.\n4.2\nEffect of Path Gradient fine-tuning on flow trajectory length\nWe use the provided saved models from (Klein et al., 2023b) for LJ13 and investigate the performance\nand flowed trajectory length during fine-tuning with Path Gradients. We used a batch-size of 256\nand trained with Path Gradients for 10 epochs. In Figure 3, we can see that the different pre-trained\nmodels have similar NLLs and ESS, but differ in trajectory lengths. Fine-tuning reliably improves the\n2github.com/lenz3000/path-grads-after-fm\n8\n"
    },
    {
      "page_number": 9,
      "text": "Table 1: Comparison between fine-tuning with Optimal Transport Flow Matching and Path Gradients.\nFirst all models were pre-trained with Optimal Transport Flow Matching like in (Klein et al., 2023b).\nWe compare fine-tuning with Flow Matching and Path Gradients. For all experiments, we limited the\nVRAM and runtime to be roughly equivalent. Mean ± sterr on three runs. bold: sterrs do not overlap.\nModel\nTraining type\nNLL (↓)\nESSq %(↑)\nESSp %(↑)\nLJ13\n(Klein et al., 2023b)\nOnly FM\n−16.09 ± 0.03\n54.36 ± 5.43\n58.18 ± 0.71\nHybrid (ours)\n−16.21 ± 0.00\n82.97 ± 0.40\n82.87 ± 0.35\nLJ55\n(Klein et al., 2023b)\nOnly FM\n−88.45 ± 0.04\n3.74 ± 1.06\n2.97 ± 0.08\nHybrid (ours)\n−89.19 ± 0.05\n11.04 ± 3.98\n13.71 ± 3.15\nAlanine dipeptide - XTB\n(Klein et al., 2023b)\nOnly FM\n−107.89 ± 0.07\n0.74 ± 0.33\n0.01 ± 0.01\nHybrid (ours)\n−107.77 ± 0.18\n0.25 ± 0.37\n0.01 ± 0.01\n(Klein & Noé, 2024)\nOnly FM\n−125.75 ± 0.01\n3.38 ± 0.50\n2.65 ± 0.50\nHybrid (ours)\n−125.82 ± 0.02\n7.30 ± 1.28\n6.57 ± 2.98\nAlanine dipeptide - Classical\n(Klein et al., 2023b)\nOnly FM\n−110.14 ± 0.01\n4.88 ± 0.42\n0.26 ± 0.19\nHybrid (ours)\n−110.30 ± 0.13\n2.86 ± 3.05\n0.04 ± 0.06\n(Klein & Noé, 2024)\nOnly FM\n−128.01 ± 0.01\n14.42 ± 2.44\n12.21 ± 1.06\nHybrid (ours)\n−128.26 ± 0.02\n24.39 ± 6.86\n13.89 ± 4.91\nFigure 4: Alanine dipeptide results for the TBG model and the classical force field with and without\nPath Gradient finetuning. Left: Ramachandran plots for the dihedral angel distribution of a reference\nMD simulation and non reweighted samples from the different TBG models. Right: Corresponding\nenergy distributions of generated samples.\nperformance of the flows with relative little change to the trajectory lengths. For the full statistics see\nAppendix A.7. We observe a similar pattern in the other systems investigated. For alanine dipeptide,\nfor instance, generated samples show significantly improved bond-length and energy distributions,\nyet the global conformational landscape, captured by the φ and ψ dihedral-angle distributions in the\nRamachandran plot, remains mostly unchanged (see Figure 4 and Figure 6).\nThese experiments show that our proposed hybrid approach is perfectly suited for maximizing\nperformance while keeping the properties of the model.\n4.3\nTransferability on dipeptides\nFinally, we also investigate whether fine-tuning with Path Gradients improves performance in the\ntransferable setting. To this end we fine-tune Transferable Boltzmann Generators (TBG) (Klein &\nNoé, 2024), which were trained on 200 different dipeptides and evaluate on 16 unseen ones, like in\nKlein & Noé (2024). The experiments show that fine-tuning with path gradients improves the NLL\nand energies for all evaluated unseen test dipeptides. On average, we observe a relative improvement\nof around 23% in the ESSq, reaching an efficiency of 9.79%. For more details see Appendix A.10.\n9\n"
    },
    {
      "page_number": 10,
      "text": "5\nDiscussion\nWe have presented a hybrid training strategy for Boltzmann Generators that uses Flow Matching and\nPath Gradients. We have shown how to efficiently compute Path Gradients with constant memory and\nwithout the need for additional samples. While substantially slower per training step, Path Gradients\nare a powerful tool for fine-tuning, leveraging first-order information from the energy function – an\nunderexplored avenue in the scientific machine learning community. Our results demonstrate that\nPath Gradients can significantly improve sample quality on the same computational budget as Flow\nMatching, when the model is already reasonably close to the target distribution. Our experiments\nshow that Path Gradients only apply minor changes to the flow trajectory and to the variational\ndistribution qθ while still substantially increasing the sampling efficiency and performance.\n5.1\nLimitations\nPath Gradients come with several limitations. First, they require access to a well-defined and\ndifferentiable energy function, which restricts their use to domains like molecular modeling and\nexcludes standard tasks such as natural image generation. Second, they rely on unbiased training\nsamples. Finally, the method is computationally more expensive and tends to improve performance\nonly when the model is already close to the target distribution. While Path Gradients do not directly\nspeed up the expensive CNF sampling process, they increase the ESS, thereby reducing the total\nnumber of samples needed and, hence, at least partially alleviating the high sampling cost.\n5.2\nFuture Work\nGiven the similarities between Flow Matching and Diffusion models, extending Path Gradients to\ndiffusion-based frameworks presents an exciting and promising direction. The dynamics used in\nour experiments have also been used for molecular conformation generation using Diffusion models\n(Hoogeboom et al., 2022). Adoption to their applications would be an interesting and suitable\ncandidate for future work.\nFurthermore, in model distillation (e.g. (Salimans & Ho, 2022)) the gradient information of the larger\nmodel is available, even though there might be no first order information of the original data. Here\nour approach could help to improve the distillation process.\n5.3\nBroader Impact\nThis foundational research has no immediate societal impact, but if scalable, it could be used to\naccelerate drug and materials discovery by replacing MD simulations. Potential risks include misuse\nfor biothreat development and the lack of convergence guarantees, which may lead to incomplete\nsampling and misleading conclusions.\nAcknowledgements\nLV wants to thank Pan Kessel and Shinichi Nakajima for the helpful discussions. LK gratefully\nacknowledges support by BIFOLD - Berlin Institute for the Foundations of Learning and Data.\n10\n"
    },
    {
      "page_number": 11,
      "text": "References\nAbbott, R., Albergo, M., Botev, A., Boyda, D., Cranmer, K., Hackett, D., Kanwar, G., Matthews, A.,\nRacaniere, S., Razavi, A., et al. Normalizing flows for lattice gauge theory in arbitrary space-time\ndimension. ArXiv preprint, abs/2305.12345, 2023.\nAbdin, O. and Kim, P. M. Pepflow: direct conformational sampling from peptide energy landscapes\nthrough hypernetwork-conditioned diffusion. bioRxiv, pp. 2023–06, 2023.\nAbramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., Ronneberger, O., Willmore, L.,\nBallard, A. J., Bambrick, J., et al. Accurate structure prediction of biomolecular interactions with\nalphafold 3. Nature, pp. 1–3, 2024.\nAgrawal, A. and Domke, J. Disentangling impact of capacity, objective, batchsize, estimators, and\nstep-size on flow vi. arXiv preprint arXiv:2412.08824, 2024.\nAgrawal, A., Sheldon, D. R., and Domke, J.\nAdvances in Black-Box VI: Normalizing\nFlows, Importance Weighting, and Optimization.\nIn Advances in Neural Information Pro-\ncessing Systems 33, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nc91e3483cf4f90057d02aa492d2b25b1-Abstract.html.\nAhmad, R. and Cai, W. Free energy calculation of crystalline solids using normalizing flows.\nModelling and Simulation in Materials Science and Engineering, 30(6):065007, sep 2022. ISSN\n0965-0393, 1361-651X. doi: 10.1088/1361-651X/ac7f4b. URL https://iopscience.iop.\norg/article/10.1088/1361-651X/ac7f4b.\nAlbergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: A unifying framework\nfor flows and diffusions. ArXiv preprint, abs/2303.08797, 2023. URL https://arxiv.org/\nabs/2303.08797.\nAndrade, D. LOFT-Stable Training of Normalizing Flows for Variational Inference. In 6th Workshop\non Tractable Probabilistic Modeling, 2023.\nBacchio, S., Kessel, P., Schaefer, S., and Vaitl, L. Learning trivializing gradient flows for lattice\ngauge theories. Physical Review D, 107:L051504, 2023. doi: 10.1103/PhysRevD.107.L051504.\nURL https://link.aps.org/doi/10.1103/PhysRevD.107.L051504.\nBannwarth, C., Ehlert, S., and Grimme, S. Gfn2-xtb—an accurate and broadly parametrized self-\nconsistent tight-binding quantum chemical method with multipole electrostatics and density-\ndependent dispersion contributions. Journal of Chemical Theory and Computation, 15(3):1652–\n1671, 2019. doi: 10.1021/acs.jctc.8b01176. URL https://doi.org/10.1021/acs.jctc.\n8b01176. PMID: 30741547.\nBauer, M. and Mnih, A. Generalized Doubly Reparameterized Gradient Estimators. In Proceedings\nof the 38th International Conference on Machine Learning, 2021. URL http://proceedings.\nmlr.press/v139/bauer21a.html.\nBoyda, D., Kanwar, G., Racanière, S., Rezende, D. J., Albergo, M. S., Cranmer, K., Hackett, D. C.,\nand Shanahan, P. E. Sampling using SU (N) gauge equivariant flows. Physical Review D, (7),\n2021.\nChen, T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural Ordinary Differential Equations.\nIn Advances in Neural Information Processing Systems 31, 2018.\nDibak, M., Klein, L., Krämer, A., and Noé, F. Temperature steerable flows and Boltzmann generators.\nPhys. Rev. Res., 4:L042005, Oct 2022. doi: 10.1103/PhysRevResearch.4.L042005.\nDing, X. and Zhang, B. Computing absolute free energy with deep generative models. Biophysical\nJournal, 120(3):195a, 2021a.\nDing, X. and Zhang, B. Deepbar: A fast and exact method for binding free energy computation.\nJournal of Physical Chemistry Letters, 12:2509–2515, 3 2021b. ISSN 19487185. doi: 10.1021/acs.\njpclett.1c00189.\n11\n"
    },
    {
      "page_number": 12,
      "text": "Draxler, F., Sorrenson, P., Zimmermann, L., Rousselot, A., and Köthe, U. Free-form flows: Make\nany architecture a normalizing flow. In Dasgupta, S., Mandt, S., and Li, Y. (eds.), Proceedings\nof The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of\nProceedings of Machine Learning Research, pp. 2197–2205. PMLR, 02–04 May 2024. URL\nhttps://proceedings.mlr.press/v238/draxler24a.html.\nEsser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer,\nA., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In\nForty-first international conference on machine learning, 2024.\nGarcia Satorras, V., Hoogeboom, E., Fuchs, F., Posner, I., and Welling, M.\nE(n) equivariant\nnormalizing flows. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,\nJ. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 4181–4192.\nCurran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/\n21b5680d80f75a616096f2e791affac6-Paper.pdf.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C.,\nand Bengio, Y. Generative Adversarial Nets. In Advances in Neural Information Processing\nSystems 27, 2014.\nGrathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., and Duvenaud, D. FFJORD: Free-Form\nContinuous Dynamics for Scalable Reversible Generative Models. In Proceedings of the 7th\nInternational Conference on Learning Representations, 2019.\nHo, J., Jain, A., and Abbeel, P. Denoising Diffusion Probabilistic Models. In Advances in Neural\nInformation Processing Systems 33, 2020.\nHoogeboom, E., Satorras, V. G., Vignac, C., and Welling, M. Equivariant diffusion for molecule\ngeneration in 3d. In International conference on machine learning, pp. 8867–8887. PMLR, 2022.\nHutchinson, M. F. A stochastic estimator of the trace of the influence matrix for laplacian smoothing\nsplines. Communications in Statistics-Simulation and Computation, 18(3):1059–1076, 1989.\nInvernizzi, M., Krämer, A., Clementi, C., and Noé, F. Skipping the replica exchange ladder with\nnormalizing flows. The Journal of Physical Chemistry Letters, 13:11643–11649, 2022.\nJin, Y., Sun, Z., Li, N., Xu, K., Jiang, H., Zhuang, N., Huang, Q., Song, Y., Mu, Y., and Lin, Z.\nPyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954,\n2024.\nJing, B., Corso, G., Chang, J., Barzilay, R., and Jaakkola, T. Torsional diffusion for molecular\nconformer generation. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and\nOh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 24240–24253.\nCurran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper/2022/file/\n994545b2308bbbbc97e3e687ea9e464f-Paper-Conference.pdf.\nJing, B., Berger, B., and Jaakkola, T. Alphafold meets flow matching for generating protein ensembles.\narXiv preprint arXiv:2402.04845, 2024.\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool,\nK., Bates, R., Žídek, A., Potapenko, A., et al. Highly accurate protein structure prediction with\nalphafold. Nature, 596(7873):583–589, 2021.\nKanwar, G. Flow-based sampling for lattice field theories. arXiv preprint arXiv:2401.01297, 2024.\nKim, J. C., Bloore, D., Kapoor, K., Feng, J., Hao, M.-H., and Wang, M. Scalable normalizing\nflows enable boltzmann generators for macromolecules. In International Conference on Learning\nRepresentations (ICLR), 2024.\nKingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In Proceedings of the 2nd\nInternational Conference on Learning Representations, 2014. URL http://arxiv.org/abs/\n1312.6114.\nKlein, L. and Noé, F. Transferable boltzmann generators. arXiv preprint arXiv:2406.14426, 2024.\n12\n"
    },
    {
      "page_number": 13,
      "text": "Klein, L., Foong, A. Y. K., Fjelde, T. E., Mlodozeniec, B. K., Brockschmidt, M., Nowozin, S., Noe,\nF., and Tomioka, R. Timewarp: Transferable acceleration of molecular dynamics by learning\ntime-coarsened dynamics. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023a. URL https://openreview.net/forum?id=EjMLpTgvKH.\nKlein, L., Krämer, A., and Noé, F. Equivariant flow matching. Advances in Neural Information\nProcessing Systems, 36:59886–59910, 2023b.\nKöhler, J., Klein, L., and Noé, F. Equivariant Flows: Exact Likelihood Generative Learning for\nSymmetric Densities. In Proceedings of the 37th International Conference on Machine Learning,\n2020. URL http://proceedings.mlr.press/v119/kohler20a.html.\nKöhler, J., Krämer, A., and Noé, F. Smooth Normalizing Flows. In Advances in Neural Information\nProcessing Systems 34, 2021.\nLarsen, A. H., Mortensen, J. J., Blomqvist, J., Castelli, I. E., Christensen, R., Dułak, M., Friis, J.,\nGroves, M. N., Hammer, B., Hargus, C., Hermes, E. D., Jennings, P. C., Jensen, P. B., Kermode, J.,\nKitchin, J. R., Kolsbjerg, E. L., Kubal, J., Kaasbjerg, K., Lysgaard, S., Maronsson, J. B., Maxson, T.,\nOlsen, T., Pastewka, L., Peterson, A., Rostgaard, C., Schiøtz, J., Schütt, O., Strange, M., Thygesen,\nK. S., Vegge, T., Vilhelmsen, L., Walter, M., Zeng, Z., and Jacobsen, K. W. The atomic simulation\nenvironment—a python library for working with atoms. Journal of Physics: Condensed Matter,\n29(27):273002, 2017. URL http://stacks.iop.org/0953-8984/29/i=27/a=273002.\nLewis, S., Hempel, T., Jiménez-Luna, J., Gastegger, M., Xie, Y., Foong, A. Y., Satorras, V. G., Abdin,\nO., Veeling, B. S., Zaporozhets, I., et al. Scalable emulation of protein equilibrium ensembles with\ngenerative deep learning. bioRxiv, pp. 2024–12, 2024.\nLipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative\nmodeling. In Proceedings of the 11th International Conference on Learning Representations,\n2023.\nLiu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003, 2022.\nMidgley, L. I., Stimper, V., Simm, G. N., Schölkopf, B., and Hernández-Lobato, J. M. Flow Annealed\nImportance Sampling Bootstrap. In Proceedings of the 11th International Conference on Learning\nRepresentations, 2023.\nMohamed, S., Rosca, M., Figurnov, M., and Mnih, A. Monte Carlo Gradient Estimation in Machine\nLearning. Journal of Machine Learning Research, 21:5183–5244, 2020. URL http://jmlr.\norg/papers/v21/19-346.html.\nNicoli, K. A., Nakajima, S., Strodthoff, N., Samek, W., Müller, K.-R., and Kessel, P. Asymptotically\nunbiased estimation of physical observables with neural samplers. Physical Review E, (2), 2020.\nNicoli, K. A., Anders, C. J., Funcke, L., Hartung, T., Jansen, K., Kessel, P., Nakajima, S., and Stornati,\nP. Estimation of Thermodynamic Observables in Lattice Field Theories with Deep Generative\nModels. Physical Review Letters, 126(3):032001, 2021.\nNicoli, K. A., Anders, C. J., Hartung, T., Jansen, K., Kessel, P., and Nakajima, S. Detecting and\nMitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories. Physical Review D,\n108:114501, 2023. doi: 10.1103/PhysRevD.108.114501. URL https://link.aps.org/doi/\n10.1103/PhysRevD.108.114501.\nNoé, F., Olsson, S., Köhler, J., and Wu, H. Boltzmann generators: Sampling equilibrium states of\nmany-body systems with deep learning. Science, 365(6457):eaaw1147, 2019.\nPapamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., and Lakshminarayanan, B. Normaliz-\ning flows for probabilistic modeling and inference. CoRR, 2019. URL http://arxiv.org/abs/\n1912.02762v1.\nPapamakarios, G., Nalisnick, E. T., Rezende, D. J., Mohamed, S., and Lakshminarayanan, B.\nNormalizing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning\nResearch, 22:57:1–57:64, 2021. URL http://jmlr.org/papers/v22/19-1028.html.\n13\n"
    },
    {
      "page_number": 14,
      "text": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,\nGimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A.,\nChilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances in Neural Information Processing Systems\n32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nPoli, M., Massaroli, S., Yamashita, A., Asama, H., Park, J., and Ermon, S. Torchdyn: Implicit models\nand neural numerical methods in pytorch. In Neural Information Processing Systems, Workshop\non Physical Reasoning and Inductive Biases for the Real World, volume 2, 2021.\nPontryagin, L. S. Mathematical theory of optimal processes. Routledge, 1987.\nRezende, D. and Mohamed, S. Variational inference with normalizing flows. In International\nconference on machine learning, pp. 1530–1538. PMLR, 2015.\nRizzi, A., Carloni, P., and Parrinello, M. Multimap targeted free energy estimation, 2023.\nRoeder, G., Wu, Y., and Duvenaud, D.\nSticking the Landing:\nSimple, Lower-Variance\nGradient Estimators for Variational Inference.\nIn Advances in Neural Information Pro-\ncessing Systems 30, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\ne91068fff3d7fa1594dfdf3b4308433a-Abstract.html.\nSalimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. arXiv preprint\narXiv:2202.00512, 2022.\nSatorras, V. G., Hoogeboom, E., and Welling, M. E (n) equivariant graph neural networks. In\nInternational conference on machine learning, pp. 9323–9332. PMLR, 2021.\nSchebek, M., Invernizzi, M., Noé, F., and Rogal, J. Efficient mapping of phase diagrams with\nconditional boltzmann generators. Machine Learning: Science and Technology, 2024.\nSchönle, C., Gabrié, M., Lelièvre, T., and Stoltz, G. Sampling metastable systems using collective\nvariables and jarzynski-crooks paths. arXiv preprint arXiv:2405.18160, 2024.\nSchreiner, M., Winther, O., and Olsson, S. Implicit transfer operator learning: Multiple time-\nresolution models for molecular dynamics. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023. URL https://openreview.net/forum?id=1kZx7JiuA2.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-Based Gener-\native Modeling through Stochastic Differential Equations. In Proceedings of the 9th International\nConference on Learning Representations, 2021.\nSong, Y., Gong, J., Xu, M., Cao, Z., Lan, Y., Ermon, S., Zhou, H., and Ma, W.-Y. Equivariant\nflow matching with hybrid probability transport for 3d molecule generation. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.net/\nforum?id=hHUZ5V9XFu.\nStark, H., Jing, B., Barzilay, R., and Jaakkola, T. Harmonic self-conditioned flow matching for joint\nmulti-ligand docking and binding site design. In Forty-first International Conference on Machine\nLearning, 2024.\nTamagnone, S., Laio, A., and Gabrié, M. Coarse-grained molecular dynamics with normalizing flows.\nJournal of Chemical Theory and Computation, 20(18):7796–7805, 2024.\nTan, C. B., Bose, A. J., Lin, C., Klein, L., Bronstein, M. M., and Tong, A. Scalable equilibrium\nsampling with sequential boltzmann generators. arXiv preprint arXiv:2502.18462, 2025.\nTong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Ben-\ngio, Y. Conditional flow matching: Simulation-free dynamic optimal transport. arXiv preprint\narXiv:2302.00482, 2(3), 2023.\n14\n"
    },
    {
      "page_number": 15,
      "text": "Tucker, G., Lawson, D., Gu, S., and Maddison, C. J. Doubly Reparameterized Gradient Estimators\nfor Monte Carlo Objectives. In Proceedings of the 7th International Conference on Learning\nRepresentations, 2019.\nVaitl, L. Path Gradient Estimators for Normalizing Flows. PhD thesis, Technische Universität Berlin,\n2024.\nVaitl, L., Nicoli, K. A., Nakajima, S., and Kessel, P. Gradients should stay on path: better estimators\nof the reverse-and forward KL divergence for normalizing flows. Machine Learning: Science and\nTechnology, 3(4):045006, 2022a.\nVaitl, L., Nicoli, K. A., Nakajima, S., and Kessel, P. Path-Gradient Estimators for Continuous\nNormalizing Flows. In Proceedings of the 39th International Conference on Machine Learning,\n2022b. URL https://proceedings.mlr.press/v162/vaitl22a.html.\nVaitl, L., Winkler, L., Richter, L., and Kessel, P. Fast and unified path gradient estimators for nor-\nmalizing flows. In to be presented in 12th International Conference on Learning Representations,\n2024.\nWirnsberger, P., Ballard, A. J., Papamakarios, G., Abercrombie, S., Racanière, S., Pritzel, A.,\nJimenez Rezende, D., and Blundell, C. Targeted free energy estimation via learned mappings. The\nJournal of Chemical Physics, 153(14):144112, 2020.\n15\n"
    },
    {
      "page_number": 16,
      "text": "NeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: In Section 3 we present how to efficiently do fine-tuning in constant memory\nand Section 4, we show both, improved performance and minimal changes to the trajectory\nlength.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: In Section 5.1 we discuss the limitations of the presented approach.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n16\n"
    },
    {
      "page_number": 17,
      "text": "Answer: [Yes]\nJustification: The cross-references for the formulae for Path Gradients on samples in\nSection 2.4, as well as the adapted augmented adjoint state method Equation (19) are cross-\nreferenced and numbered. The same holds true for equations on EQ-OT-, OT-, and standard\nFM in Section 2.3\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide all the necessary information in Sections 4,A.5,A.2, A.4, A.7.\nFurther we recreate previously published experiments. Additionally we also provide code\nand checkpoints for the trained models.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\n17\n"
    },
    {
      "page_number": 18,
      "text": "some way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: We provide code and trained models in the supplementary material. We further\ndescribe where to download the datasets in Appendix A.3.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Appendix A.3 and Appendix A.7 specify data splits, hyperparameters, the\nfinding of hyperparameters and optimizer choice\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: We present standard errors on three repetitions for all our experiments.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n18\n"
    },
    {
      "page_number": 19,
      "text": "• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: In Section 4 and the Appendix we specify the main CPU or GPU, the compute\ntime as well as the VRAM usage, where it is relevant.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: Yes\nJustification: We conform to the Code of Ethics and have communicated the impact of the\nwork in Section 5.3\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We do this in Section 5.3.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n19\n"
    },
    {
      "page_number": 20,
      "text": "• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: Does not apply\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We mention the licenses for the data, code, and models in Appendix A.3 and\nAppendix A.13.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n20\n"
    },
    {
      "page_number": 21,
      "text": "• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: Code is documented.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: Does not apply.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: Does not apply\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n21\n"
    },
    {
      "page_number": 22,
      "text": "• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA]\nJustification: Only used for writing/editing\nGuidelines:\n• The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n22\n"
    },
    {
      "page_number": 23,
      "text": "Flow Matching \nHybrid Path Grads\n0.064\n0.048\n0.032\n0.016\n0.000\n0.016\n0.032\n0.048\n0.064\n0.080\np (log p\nlog q )\nFigure 5: Loss in space after training as done in Figure 1\nA\nAppendix\nA.1\n2D GMM experiment\nThe 2D GMM is a Gaussian Mixture model with 4 equally weighted Gaussians\nN\n\u0012\u0014\nai\nbi\n\u0015\n,\n\u0014\nci + 0.01\n0\n0\ndi + 0.01\n\u0015\u0013\n,\n(22)\nwith ai, bi, ci, di ∼N(0, 1), i ∈{1, 2, 3, 4}. The training set consists of 2000 samples, the KL(p|qθ)\nand MSE loss are evaluated on 2048 samples. We used Adam with default parameters and lr=1e-2 for\npure FM/PG and pre-training with FM and lr=5e-3 for finetuning with PG. All experiments are run\non an Intel i7-1165G7 CPU and ignoring the validation, training completes in roughly 45 seconds.\nFigure 5 shows the weighted difference\np(x) · log p(x)\nqθ(x)\n(23)\nafter training.\nA.2\nArchitecture\nWe use the same model architecture as introduced in Klein et al. (2023b); Klein & Noé (2024). We\nhere summarize it briefly, closely following the presentation in Klein & Noé (2024).\nThe underlying normalizing flow model for the Boltzmann Generator is a CNF. The corresponding\nvector field vθ(t, x) is parametrized by an O(D)- and S(N)-equivariant graph neural network\n(EGNN) Garcia Satorras et al. (2021); Satorras et al. (2021). The vector field vθ(x, t) consists of L\nconsecutive EGNN layers. The position of the i-th particle xi is updated according to the following\n23\n"
    },
    {
      "page_number": 24,
      "text": "set of equations:\nh0\ni = (t, ai),\nml\nij = ϕe\n\u0000hl\ni, hl\nj, d2\nij\n\u0001\n,\n(24)\nxl+1\ni\n= xl\ni +\nX\nj̸=i\n\u0000xl\ni −xl\nj\n\u0001\ndij + 1 ϕd(ml\nij),\n(25)\nhl+1\ni\n= ϕh\n\u0000hl\ni, ml\ni\n\u0001\n,\nml\ni =\nX\nj̸=i\nϕm(ml\nij)ml\nij,\n(26)\nvθ(x0, t)i = xL\ni −x0\ni −1\nN\nN\nX\nj\n(xL\nj −x0\nj),\n(27)\nwhere the ϕα represent different neural networks, dij is the Euclidean distance between particle i and\nj, t is the time, ai is an embedding for each particle.\nThe proposed model architecture in Klein et al. (2023b) uses for alanine dipeptide distinct encodings\nai for all backbone atoms and the atom types for all other atoms. In contrast, the model architecture\nin Klein & Noé (2024) uses distinct encodings for all atoms, except for Hydrogens bond to the same\nCarbon atom. We refer to this model as TBG, which stands for transferable Boltzmann Generator,\neven though we do not deploy it in a transferable way in this work.\nFor more details see Klein et al. (2023b); Klein & Noé (2024).\nA.3\nDatasets\nHere we provide more details on the investigated datasets.\nWe use the same training and test splits as defined in Klein et al. (2023b); Klein & Noé (2024).\nLennard-Jones systems\nThe energy U(x) for the Lennard-Jones systems is given by\nU(x) = 1\n2\n\nX\ni,j\n \u0012 1\ndij\n\u001312\n−2\n\u0012 1\ndij\n\u00136!\n,\n(28)\nwhere dij is the distance between particle i and j.\nThe authors of Klein et al. (2023b)\n(CC BY 4.0) made the datasets available here:\nhttps://osf.io/srqg7/?view_only=\n28deeba0845546fb96d1b2f355db0da5.\nFor computing the metrics we use the following number of test samples:\nLJ13: ESSq: 5 × 105; ESSp, NLL: 5 × 105.\nLJ55: ESSq: 1 × 105; ESSp, NLL: 1 × 105.\nAlanine dipeptide\nThe classical alanine dipeptide dataset was generated with an MD simulation,\nusing the classical Amber ff99SBildn force-field at 300K for implicit solvent for a duration of 1 ms\nKöhler et al. (2021) with the openMM library. The datasets is available as part of the public bgmol\n(MIT licence) repository here: https://github.com/noegroup/bgmol.\nThe alanine dipeptide dataset with the semi empirical force-field, was generated by relaxing 105\nrandomly selected states from the classical MD simulation. The relaxation was performed with\nthe semi-empirical GFN2-xTB force-field for 100 fs each, using (Bannwarth et al., 2019) and the\nASE library (Larsen et al., 2017) with a friction constant of 0.5 a.u. The test set was created it\nin the same way. The authors of Klein et al. (2023b) made the relaxed alanine dipeptide with the\nsemi empirical force field available here (CC BY 4.0): https://osf.io/srqg7/?view_only=\n28deeba0845546fb96d1b2f355db0da5.\nFor AD2-XTB, we precompute the target forces of the samples, to re-use them during PG training.\nFor computing the metrics we use the following number of test samples:\nESSq: 2 × 105; ESSp, NLL: 1 × 105.\nDipetide dataset (2AA)\nThe dipeptide dataset was introduced by Klein et al. (2023a) and is\navailable at https://huggingface.co/datasets/microsoft/timewarp. The dataset consists\nof classical MD trajectories of dipeptides at 310K. There are 200 trajectories in the train set, simulated\nfor 50ns each and 100 each in the validation and test set, simulated for 1µs each.\n24\n"
    },
    {
      "page_number": 25,
      "text": "Figure 6: Alanine dipeptide results for the TBG model and the classical force field with and without\nPath Gradient finetuning. Left and middle: Free energy projection for the φ and ψ dihedral angles,\nrespectively. Right: Bond-length distribution for a Carbon - Nitrogen bond.\nA.4\nFinetuning experiments\nWe used Adam with a learning rate of 1e-4 for PG and only the training set provided. For each of\nthe training points, we require access to the force of the target. We use Hutchinson’s estimator for\nestimating the trace of the Jacobian. Importantly, in order to keep the memory constant, we avoid\nsaving checkpoints. For our experiments we used A100 GPUs.\nFor LJ13, we used a batch-size of 64 instead of 256, which uses about 90% of the original memory\n(FM 1.52GB, PG 1.38GB). For fine-tuning we used 2 epochs for Path Gradients, which took 117\nmin, instead of 135 min for 1000 epochs with Flow Matching. For LJ55, we use the same batch-sizes\nresulting in 13.24GB for Flow Matching and 14.76GB for Path Gradients. We fine-tuned with Path\nGradients for 1 epoch, taking 394 minutes instead of 440 min for 400 epochs with Flow Matching.\nBecause we are measuring the ESS and neg-loglikelihood on the target density, we removed the\nreweighting from the Alanine Dipeptide data. If we changed the target density to its reweighted version\nand computed the gradients of the weighting w.r.t. the samples, we could also straightforwardly\napply Path Gradients to the reweighted distribution. Further, fine-tuning on AD2 with Path Gradients\nshowed some instabilities during training. To fight these, we employed gradient clipping to a norm of\n1 and gradient accumulation to a batch-size of around 1000 in batches of 50. The resulting fine-tuning\nwith Path Gradients uses 2.44GB VRAM and 131 min, compared to 200 minutes with 2.95GB with\nFlow Matching and batch-size 256.\nWe used the same number of samples like (Klein et al., 2023b) for the ESS, but note that a higher\nnumber might have been beneficial for more reliable estimates.\nFor the experiments, we did not run exhaustive hyperparameter tuning. The Batch-sizes were set to\nhave a similar memory footprint. The hybrid PG learning rate was set to the middle between the\ninitial and FM-fine-tuning one after some preliminary experiments.\nA.5\nAdditional results for alanine dipeptide\nHere we present additional alanine dipeptide results for the TBG model in Figure 6. We observe\nthat the φ and ψ dihedral-angle projections change little after path-gradient fine-tuning. Moreover,\nreweighting to the target distribution via Equation (2) succeeds in both cases, though it is more\nefficient post–fine-tuning, as reflected by a higher ESS (see Table 1). In contrast, all bond-length\ndistributions align much more closely with the target after fine-tuning, exemplified here by a Carbon -\nNitrogen bond in Figure 6.\nA.6\nAdditional results for alanine dipeptide - classical with more memory\nFor classical AD2, we further investigated the performance of fine-tuning with PGs when allowed\na larger memory footprint. We fine-tuned with batch-size 1024 for two epochs, which resulted in\n22GB VRAM usage and a runtime of 92 minutes. In Table 2 we can see that this yielded a further\nimprovement over Table 1.\n25\n"
    },
    {
      "page_number": 26,
      "text": "Table 2: Additional experiment for fine-tuning TBG (Klein & Noé, 2024) with Path Gradients on\nclassical Alanine Dipeptide. First all models were pre-trained with Optimal Transport Flow Matching\nlike in (Klein et al., 2023b). We compare fine-tuning with Flow Matching and Path Gradients. Here\nwe did not limit the memory usage and trained PG with batch-size 1024 for 2 epochs. mean ± sterr\nover 3 runs.\nTraining type\nNLL (↓)\nESSq %(↑)\nESSp %(↑)\nOnly FM\n−128.01 ± 0.01\n14.42 ± 2.44\n12.21 ± 1.06\nHybrid (ours) unlimited\n−128.33 ± 0.04\n29.47 ± 2.24\n19.57 ± 4.79\nFigure 7: Integration trajectory lengths for the Lennard-Jones system with 13 particles. Compared\nare different Flow Matching models as well as the same models after fine-tuning with Path Gradients\n(Hybrid).\nA.7\nAdditional results for experiments on flow trajectory length\nFine-tuning on the 13-particle Lennard–Jones system used an A100 for 10 epochs (152 min/run)\nstarting from the Klein et al. (2023b) checkpoints, with training and evaluation on 5 × 105 samples.\nAs shown in Table 3, path-gradient fine-tuning boosts NLL and ESS across the board while leaving\nflow trajectory lengths nearly unchanged (see Figure 7). One of the nine runs diverged during training,\nwas discarded and repeated.\nTable 3: Performance metrics before and after fine-tuning for different methods for the LJ13 system.\nmean ± sterr over 3 runs.\nMethod\nFine-tuning\nTrajectory Length\nNLL\nESSq (%)\nESSp (%)\nStandard\nbefore\n3.76 ± 0.00\n−16.02 ± 0.01\n60.45 ± 0.89\n40.23 ± 20.11\nafter\n3.86 ± 0.00\n−16.22 ± 0.00\n85.78 ± 0.69\n85.93 ± 0.57\nOptimal Transport\nbefore\n2.80 ± 0.01\n−16.01 ± 0.01\n57.16 ± 0.88\n57.07 ± 0.54\nafter\n2.78 ± 0.04\n−16.22 ± 0.00\n85.91 ± 0.03\n85.78 ± 0.02\nEquivariant Op-\nbefore\n2.11 ± 0.00\n−16.04 ± 0.00\n58.11 ± 1.93\n19.77 ± 12.36\ntimal Transport\nafter\n2.19 ± 0.00\n−16.23 ± 0.00\n87.77 ± 0.24\n58.40 ± 29.20\nA.8\nComparison with maximum likelihood training\nWe here present comparisons with maximum likelihood training as a baseline on LJ13. To this end\nwe compare to Garcia Satorras et al. (2021), which is nearly the same architecture as in Klein et al.\n(2023b), but the model is trained via maximum likelihood training instead of flow matching. The\nresults are shown in Table 4.\nA.9\nAdditional energy histograms\nWe show additional energy histograms of the systems investigated in Section 4 in Figure 8. The\naverage energy for every investigated system is smaller and closer to the target energy distribution\nafter PG finetuning.\n26\n"
    },
    {
      "page_number": 27,
      "text": "Method\nNLL\nESSq in %\nML training (Garcia Satorras et al., 2021)\n−15.83 ± 0.07\n39.78 ± 6.19\nOnly FM (Klein et al., 2023b)\n−16.09 ± 0.03\n54.36 ± 5.43\nHybrid approach (Ours)\n−16.21 ± 0.00\n82.97 ± 0.40\nTable 4: Results for the Lennard-Jones system with 13 particles (LJ13).\nFigure 8: Energy histograms for the a) LJ13 systems, b) the LJ55 system, and c-i) Different\ndidpeptides from the testset for the transferable Boltzmann Generator (TBG).\nA.10\nResults for transferable Boltzmann Generators (TBG) evaluated on dipeptides\nWe applied the hybrid approach to transferable Boltzmann Generators (TBG) on dipeptides as\nintroduced in Klein & Noé (2024). We again fine-tune the pretrained Boltzmann Generator from\nKlein & Noé (2024) with path gradients. Training took 6 days on an A100 with learning rate 0.00001.\nThis transferable Boltzmann Generator is trained on a subset of all possible dipeptides and evaluated\non unseen ones (for more details see Appendix A.3). Due to the expensive evaluation, we evaluated\nthe model on 16 test dipeptides and chose the same subset as in Klein & Noé (2024). Our experiments\nshow that fine-tuning with path gradients improves the NLL and energies for all evaluated unseen\ntest dipeptides with an average improvement to -100.93 from -100.72 NLL and also shows average\nimprovements for the ESSq of around 23% to an efficiency of 9.79% as shown in Table 5. An\nexample energy histogram is shown in Figure 8c.\nA.11\nAbout the different estimators\nIn the following, we first discuss the Maximum Likelihood (ML) and Path Gradient (PG) estimators\nfor the forward KL and show benefits of PG over FM for a toy example.\nA.11.1\nThe Maximum Likelihood and Path Gradient Estimators\nIn short: both the ML and PG estimators are unbiased and consistent, but they differ in variance. For\nPG, we can give guarantees about their variance, once qθ equals to p, see e.g. (Vaitl, 2024; Roeder\net al., 2017; Tucker et al., 2019; Vaitl et al., 2022b).\n27\n"
    },
    {
      "page_number": 28,
      "text": "Dipeptide\nNLL (Before)\nNLL (After PG)\nESSq (Before) in %\nESSq (After PG) in %\nKS\n-100.82\n-100.99\n6.0\n4.6\nAT\n-75.49\n-75.61\n20.8\n21.5\nGN\n-65.45\n-65.55\n19.2\n25.6\nLW\n-148.91\n-149.22\n0.4\n3.6\nNY\n-118.24\n-118.47\n9.5\n10.5\nIM\n-106.80\n-107.01\n3.1\n4.6\nTD\n-81.09\n-81.19\n4.1\n11.6\nHT\n-103.06\n-103.27\n0.2\n5.5\nKG\n-88.60\n-88.79\n5.0\n7.4\nNF\n-116.45\n-116.67\n3.7\n12.2\nRL\n-135.71\n-136.12\n1.3\n1.5\nET\n-89.94\n-90.08\n6.3\n4.1\nAC\n-63.90\n-63.91\n31.8\n33.3\nGP\n-71.63\n-71.77\n10.5\n8.0\nKQ\n-119.36\n-119.68\n4.3\n2.0\nRV\n-126.28\n-126.62\n1.3\n0.6\nAverage\n-100.73\n-100.93\n7.96\n9.79\nTable 5: Comparison of NLL and ESS before and after PG for different dipeptides from the testset.\nBoth Maximum Likelihood and the Path Gradient estimators optimize the Forward KL Equation 8.\nLet’s look at both estimators in detail and compare them. A full derivation can be found in Appendix\nB.3.2 of Vaitl et al. (2024) & Chapter 4 of Vaitl (2024).\nTo obtain the estimator GML, we first observe that the first term in the KL divergence is constant\nw.r.t. θ.\nKL(p|qθ) = Ep(x1) [log p(x1) −log qθ(x1)] ,\n(29)\nwhich means that it does not enter in the gradient if we directly estimate the gradients via an MC\nestimator.\ndKL(p|qθ)\ndθ\n= −Ep(x1)\n\u0014 d\ndθ log qθ(x1)\n\u0015\n≈GML = −1\nN\nN\nX\ni=1\nd\ndθ log qθ(x(i)\n1 ) , x(i)\n1\n∼p .\n(30)\nFor the exact calculation of the estimator, we decompose it fully with\nqθ(x1) = q0(T −1\nθ\n(x1))\n\f\f\f\fdet ∂T −1\nθ\n(x1)\n∂x1\n\f\f\f\f .\n(31)\nThe actual calculation for the ML estimator then is\nGML = −1\nN\nN\nX\ni=1\nd\ndθ\n\u0012\nlog q0(T −1\nθ\n(x1)) + log\n\f\f\f\f\n∂T −1\nθ\n(x1)\n∂x1\n\f\f\f\f\n\u0013\n.\n(32)\nFor PG, we use Eq. 17 to directly obtain the MC estimator GP G\nd\ndθKL(p0,θ|q0) = Ex1∼p1\n\u0014 ∂\n∂x0\n\u0012\nlog p0,θ\nq0\n(x0)\n\u0013 ∂T −1\nθ\n(x1)\n∂θ\n\u0015\n≈GP G = 1\nN\nN\nX\ni=1\n∂\n∂x(i)\n0\n\u0010\nlog p0,θ(x(i)\n0 ) −log q0(x(i)\n0 )\n\u0011 ∂T −1\nθ\n(x(i)\n1 )\n∂θ\n, x(i)\n1\n∼p .\n(33)\nHere x(i)\n0 is a shorthand for T −1\nθ\n(x(i)\n1 ). If we again decompose the term we expose the full calculation\nGP G = 1\nN\nN\nX\ni=1\n∂\n∂x(i)\n0\n\u0012\nlog p(Tθ(x(i)\n0 )) + log\n\f\f\f\f\n∂Tθ(x0)\n∂x0\n\f\f\f\f −log q0(x(i)\n0 )\n\u0013 ∂T −1\nθ\n(x(i)\n1 )\n∂θ\n(34)\n28\n"
    },
    {
      "page_number": 29,
      "text": "Comparing (32) and (34), we see that the path gradient estimator incorporates GP G the gradient\ninformation of the target p – ∂log p(Tθ(x(i)\n0 )\n∂x(i)\n0\n– while GML – and by construction also GF M – does not.\nA.11.2\nVariance of the estimators\nOpposed to GML and GF M, we have nice guarantees about the variance of the PG estimator GP G at\nthe optimum and close to it.\nWe first recapitulate results from previous work and then, by using a simple example, show that GF M\ndoes not necessarily exhibit zero variance at the optimum.\nIn general, the variance of the Path Gradient estimators GP G is bounded by the squared Lipschitz\nconstant of the term\n(log p0,θ(x0) −log q0(x0)) = (log p(x1) −log qθ(x1)) ,\n(35)\nsee Mohamed et al. (2020) Section 5.3.2.\nThus, if the target density p is not well approximated by qθ, the variance of the gradient estimator\ncan be large and training with PG might not be beneficial. If log p and log qθ are close in the sense\nthat the Lipschitz constant of their difference is small, we can assume path gradient estimators to be\nhelpful.\nGradient estimators at the optimum\nIn the case of perfect approximation, i.e. qθ(x) = p(x)∀x ∈\nX, the following statements about the ML estimator and the Path Gradient estimators are known. The\ngradients for path gradient estimators are deterministically 0, i.e. E[GP G] = 0, Var[GP G] = 0, while\nfor ML the variance is generically nonzero E[GML] = 0, Var[GML] = 1\nN Iθ. Where Iθ is the Fisher\nInformation Matrix of p0,θ (Vaitl et al., 2022a).\nWhy is that?\nAlready the review by Papamakarios et al. (2021) notes in Section 2.3.3 that the\nduality of the KL divergence means that fitting the model qθ to the target p using ML is equivalent to\nfitting p0,θ to the base q0 under the reverse KL. This means that the results from Vaitl (2024) directly\nhold. We only adapted the notation.\nA.11.3\nVariance of GF M for a toy example\nWe do not have a term for the variance of GF M for general settings, but we can show that it does not\ndeterministically vanish like for GP G via a simple example. This means better behavior for PG than\nfor FM at the optimum, which we also verified empirically. Our assumptions aim to simplify the\nexample as much as possible.\n• First, assume the standard loss for Flow Matching.\n• Further, assume the two densities to be the same D-dimensional Normal distribution\nq0 = p = N(0, I).\n• Finally, assume the CNF is the identity parametrized by a single parameter θ, i.e.\nvθ = θ = 0.\nIn this example qθ(x1) = q0(x1) · I approximates the target density p perfectly and the optimal\ngradient estimator is 0. Yet, in this setting the variance of the estimator GF M is non-zero:\nVar[GF M] =\n8\nND ,\n(36)\npreventing the model from staying at the optimum during training.\nProof:\nThe gradient estimator for the FM loss Eq (10) is\nGF M = 1\nN\nX\ni\n∂\n∂θ\n\u0010\n||vθ −(x(i)\n1 −˜x(i)\n0 )||2\u0011\n,\n(37)\nwhere ˜x(i)\n0 , x(i)\n1\n∼N(0, I) 3.\n3Note that here ˜x(i)\n0\nand x(i)\n1\nare independently sampled. Before x(i)\n0\nwas the transformed sample\n29\n"
    },
    {
      "page_number": 30,
      "text": "First, we break down the terms\nGF M = 1\nN\nN\nX\ni=1\n∂\n∂θ\n1\nD\nD\nX\nd=1\n\u0010\nvθ,d + ˜x(i)\n0,d −x(i)\n1,d\n\u00112\n=\n1\nND\nN\nX\ni=1\nD\nX\nd=1\n2\n\u0010\nvθ,d + ˜x(i)\n0,d −x(i)\n1,d\n\u0011 ∂vθ,d\n∂θ\n.\n(38)\nBecause vθ is parametrized by θ, we set in ∂vθ,d\n∂θ\n= 1 and vθ,d = 0 and separate the terms\nGF M =\n1\nND\nX\ni\nX\nd\n2(˜x(i)\n0,d −x(i)\n1,d) =\n2\nND\n X\nd\nX\ni\n˜x(i)\n0,d −\nX\nd\nX\ni\nx(i)\n1,d\n!\n.\n(39)\nWe can compute the distribution of GF M by using the property\nN\nX\ni=1\nai ∼N(0, Nσ2\na) for a ∼N(0, σ2\na) .\n(40)\nSo the sum over dimensions and samples follows the normal distribution with variance DN:\nX\nd\nX\ni\n˜x(i)\n0,d ∼N(0, DN)\n(41)\nand the difference has twice its variance\nX\nd\nX\ni\n˜x(i)\n0,d −\nX\nd\nX\ni\nx(i)\n1,d ∼N(0, 2DN) .\n(42)\nThe expectation of the estimator GF M then is 0 and the variance is simply\nVar[GF M] =\n4\nN 2D2 Var[(\nX\nd\nX\ni\n˜x(i)\n0,d −\nX\nd\nX\ni\nx(i)\n1,d)] =\n4\nN 2D2 2DN =\n8\nND .\n(43)\nInterestingly, the derivative GF M is invariant to re-ordering because the sums of x1 and ˜x0 are\ninvariant to permutation. The result thus also holds for OT-FM. This property is due to the simple\nassumption, the vector field vθ is independent of xt.\nA.12\nPseudocode for forward KL path gradients via augmented adjoint\nAlgorithm 1 Augmented Adjoint Dynamics\n1: function FORWARD(t, xt, ∇log q)\n2:\n˙x, ˙div ←black_box_dynamics(t, xt)\n3:\n˙\n∇log q ←gradientx\n\u0002\n−∇log q · ˙x −˙div\n\u0003\n4:\nreturn ˙x,\n˙\n∇log q, −˙div\n5: end function\nAlgorithm 2 Pathwise Gradient Estimator\n1: function PATHWISEGRADIENTESTIMATE(x1, prior, target, flow)\n2:\nlog p1 ←−target.energy(x1)\n3:\n∇log p1 ←gradientx1(log p1)\n▷Integrate using Augmented Adjoint state method\n4:\nx0, ∇log p0,θ, log | det J| ←flow.integrateAugAdjoint(x1, ∇log p1), inverse=True)\n5:\nlog q0 ←−prior.energy(x0)\n6:\n∇log q0 ←gradientxo(log q0)\n▷Compute gradient of loss w.r.t. sample x0\n7:\n∇x0L ←1\nN (∇log p0,θ −∇log q0) ▷Backpropagate using standard Adjoint state method\n8:\npath gradients ←gradientθ (x0 · detach(∇x0L)])\n9: end function\n30\n"
    },
    {
      "page_number": 31,
      "text": "A.13\nCode libraries\nWe primarily use the following code libraries: PyTorch (BSD-3) (Paszke et al., 2019), bgflow (MIT\nlicense) (Noé et al., 2019; Köhler et al., 2020), torchdyn (Apache License 2.0) (Poli et al., 2021).\nAdditionally, we use the code from (Garcia Satorras et al., 2021) (MIT license) for EGNNs, as well as\nthe code from (Klein et al., 2023b) (MIT license) and (Klein & Noé, 2024) (MIT license) for models\nand dataset evaluations.\n31\n"
    }
  ]
}