import os
import logging
import json
import re
from typing import Any, Dict, List
from pathlib import Path
from openai import AsyncOpenAI

from ..base import MCPTool, ToolParameter, ExecutionError

# PDF ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("mcp.tools.page_analyzer")
logger.setLevel(logging.INFO)

# OpenAI í´ë¼ì´ì–¸íŠ¸
aclient = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))


async def interpret_paper_page(paper_id: str, page_num: int) -> Dict[str, Any]:
    """
    ë…¼ë¬¸ì˜ íŠ¹ì • í˜ì´ì§€ë¥¼ ì „ì²´ ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ 'í•´ì„¤'í•©ë‹ˆë‹¤.
    """
    try:
        logger.info(f"ğŸš€ [í•´ì„ ìš”ì²­] ID: {paper_id}, Page: {page_num}")

        base_path = Path(os.getenv("OUTPUT_DIR", "data/output"))
        paper_dir = find_paper_directory(base_path, paper_id)

        if not paper_dir:
            return {"error": f"Folder not found: {paper_id}"}

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        page_text = get_page_text_smart(paper_dir, page_num)

        if not page_text:
            return {
                "page": page_num,
                "original_text": "í…ìŠ¤íŠ¸ ì—†ìŒ",
                "interpretation": "í•´ë‹¹ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            }

        logger.info(f"âœ… í…ìŠ¤íŠ¸ í™•ë³´ ì™„ë£Œ ({len(page_text)}ì)")

        # ğŸ”¥ [í•µì‹¬ ë³€ê²½] í”„ë¡¬í”„íŠ¸ë¥¼ 'í•´ì„¤ê°€' ëª¨ë“œë¡œ ë³€ê²½
        prompt = f"""
        ë‹¹ì‹ ì€ ë…¸ë ¨í•œ AI ì—°êµ¬ì›ìœ¼ë¡œì„œ, ë™ë£Œì—ê²Œ ë…¼ë¬¸ ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ì—­í• ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤.
        ì•„ë˜ ì œê³µëœ ë…¼ë¬¸ì˜ í•œ í˜ì´ì§€ ë‚´ìš©ì„ ì½ê³ , í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ 'í•´ì„¤'í•´ ì£¼ì„¸ìš”.

        [ë¶„ì„í•  í˜ì´ì§€ ë‚´ìš© (Page {page_num})]:
        {page_text[:4000]}
        (ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦´ ìˆ˜ ìˆìŒ)

        **ğŸš¨ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì§€ì¹¨:**
        1. **ë‹¨ìˆœ ë²ˆì—­ ê¸ˆì§€**: ì˜ì–´ë¥¼ í•œêµ­ì–´ë¡œ ê·¸ëŒ€ë¡œ ì˜®ê¸°ì§€ ë§ˆì‹­ì‹œì˜¤. ë‚´ìš©ì„ ì™„ì „íˆ ì†Œí™”í•œ ë’¤, ë‹¹ì‹ ì˜ ì–¸ì–´ë¡œ ë‹¤ì‹œ ì„œìˆ í•˜ì„¸ìš”.
        2. **êµ¬ì¡°í™”ëœ ì¶œë ¥**: ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”.
           - **ğŸ’¡ 3ì¤„ í•µì‹¬ ìš”ì•½**: ì´ í˜ì´ì§€ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‚´ìš©ì„ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.
           - **ğŸ“– ìƒì„¸ í•´ì„¤**: ë¬¸ë‹¨ë³„ ë²ˆì—­ì´ ì•„ë‹ˆë¼, ë…¼ë¦¬ì  íë¦„ì— ë”°ë¼ ì´ì•¼ê¸°ë¥¼ í’€ì–´ì„œ ì„¤ëª…. (ì˜ˆ: "ì €ìë“¤ì€ ì—¬ê¸°ì„œ ~ë¼ëŠ” ë¬¸ì œë¥¼ ì§€ì í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ~ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.")
           - **ğŸ§  ì£¼ìš” ê°œë…/ìš©ì–´**: ë³¸ë¬¸ì— ë“±ì¥í•œ ì–´ë ¤ìš´ ì „ë¬¸ ìš©ì–´ë‚˜ ê°œë…ì´ ìˆë‹¤ë©´, ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í’€ì´.
        3. **í†¤ì•¤ë§¤ë„ˆ**: ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ, ì¹œì ˆí•œ ì–´ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.

        """

        # LLM í˜¸ì¶œ
        response = await aclient.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful and expert research assistant.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        interpretation = response.choices[0].message.content

        return {
            "page": page_num,
            "original_text": (
                page_text[:200] + "..." if len(page_text) > 200 else page_text
            ),
            "interpretation": interpretation,
        }

    except Exception as e:
        logger.error(f"ğŸ”¥ Server Error: {e}", exc_info=True)
        return {"error": str(e)}


def find_paper_directory(base_path: Path, paper_id: str) -> Path:
    target_dir = base_path / paper_id
    if target_dir.exists():
        return target_dir

    core_id = paper_id.split("arxiv.")[-1] if "arxiv." in paper_id else paper_id
    if not base_path.exists():
        return None

    for folder in base_path.iterdir():
        if folder.is_dir() and (core_id in folder.name or folder.name in paper_id):
            return folder
    return None


def get_page_text_smart(paper_dir: Path, page_num: int) -> str:
    """JSON ìš°ì„  í™•ì¸ í›„ PDF ì§ì ‘ ì½ê¸° (ë””ë²„ê¹… ê°•í™” ë²„ì „)"""
    json_path = paper_dir / "extracted_text.json"

    if json_path.exists():
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            pages_list = []
            if isinstance(data, dict) and "pages" in data:
                pages_list = data["pages"]
            elif isinstance(data, list):
                pages_list = data

            # í˜ì´ì§€ ì°¾ê¸° (ë¬¸ìì—´ ë³€í™˜ ë¹„êµ í•„ìˆ˜!)
            for p in pages_list:
                p_num = p.get("page") or p.get("page_number")
                if str(p_num) == str(page_num):
                    return p.get("text", "") or p.get("content", "")

        except Exception:
            pass  # JSON ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ PDFë¡œ ë„˜ì–´ê°

    # PDF Fallback
    pdf_files = list(paper_dir.glob("*.pdf"))
    if pdf_files and PdfReader:
        try:
            reader = PdfReader(pdf_files[0])
            idx = int(page_num) - 1
            if 0 <= idx < len(reader.pages):
                return reader.pages[idx].extract_text()
        except Exception:
            pass

    return None


# ==================== Section Analysis Tool ====================

OUTPUT_DIR = Path(os.getenv("OUTPUT_DIR", "/data/output"))


class AnalyzeSectionTool(MCPTool):
    """Analyze a specific section of a paper identified by its heading title."""

    @property
    def name(self) -> str:
        return "analyze_section"

    @property
    def description(self) -> str:
        return (
            "Analyze a specific section of a paper. Extracts text between the given "
            "section heading and the next heading from extracted_text.txt, then provides "
            "a detailed interpretation and explanation in Korean."
        )

    @property
    def parameters(self) -> List[ToolParameter]:
        return [
            ToolParameter(
                name="paper_id",
                type="string",
                description="Paper ID (e.g., '1809.04281')",
                required=True,
            ),
            ToolParameter(
                name="section_title",
                type="string",
                description="The section heading title to analyze",
                required=True,
            ),
            ToolParameter(
                name="next_section_title",
                type="string",
                description="The next section heading title (used as end boundary). Empty string means end of document.",
                required=False,
                default="",
            ),
        ]

    @property
    def category(self) -> str:
        return "analysis"

    def _extract_section_from_text(self, full_text: str, section_title: str, next_section_title: str) -> str:
        """Extract text between section_title and next_section_title from extracted text."""
        lines = full_text.split('\n')

        def normalize(s: str) -> str:
            return re.sub(r"\s+", " ", s).strip().lower()

        norm_target = normalize(section_title)
        norm_next = normalize(next_section_title) if next_section_title else ""

        # Find the start line
        start_idx = None
        for i, line in enumerate(lines):
            norm_line = normalize(line)
            if norm_target and (norm_target in norm_line or norm_line in norm_target):
                start_idx = i
                break

        if start_idx is None:
            target_words = set(norm_target.split())
            for i, line in enumerate(lines):
                norm_line = normalize(line)
                line_words = set(norm_line.split())
                if len(target_words) > 0 and len(target_words & line_words) >= len(target_words) * 0.7:
                    start_idx = i
                    break

        if start_idx is None:
            raise ExecutionError(
                f"Section '{section_title}' not found in extracted text.",
                tool_name=self.name,
            )

        # Find the end line
        end_idx = len(lines)
        if norm_next:
            for i in range(start_idx + 1, len(lines)):
                norm_line = normalize(lines[i])
                if norm_next in norm_line or norm_line in norm_next:
                    end_idx = i
                    break
            else:
                next_words = set(norm_next.split())
                for i in range(start_idx + 1, len(lines)):
                    norm_line = normalize(lines[i])
                    line_words = set(norm_line.split())
                    if len(next_words) > 0 and len(next_words & line_words) >= len(next_words) * 0.7:
                        end_idx = i
                        break

        section_lines = lines[start_idx + 1:end_idx]
        section_text = '\n'.join(section_lines).strip()

        if not section_text:
            raise ExecutionError(
                f"No text content found for section '{section_title}'.",
                tool_name=self.name,
            )

        return section_text

    async def execute(
        self,
        paper_id: str,
        section_title: str,
        next_section_title: str = "",
        **kwargs,
    ) -> Dict[str, Any]:
        # Read extracted_text.txt
        paper_dir = OUTPUT_DIR / paper_id
        text_file = paper_dir / "extracted_text.txt"

        if not text_file.exists():
            json_file = paper_dir / "extracted_text.json"
            if json_file.exists():
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    full_text = data.get("full_text", "")
            else:
                raise ExecutionError(
                    f"í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {text_file}",
                    tool_name=self.name,
                )
        else:
            with open(text_file, "r", encoding="utf-8") as f:
                full_text = f.read()

        if isinstance(full_text, str):
            full_text = full_text.encode("utf-8", "replace").decode("utf-8")

        # Extract section text
        section_text = self._extract_section_from_text(full_text, section_title, next_section_title)

        # Analyze the section using LLM
        prompt = f"""ë‹¹ì‹ ì€ ë…¸ë ¨í•œ AI ì—°êµ¬ì›ìœ¼ë¡œì„œ, ë™ë£Œì—ê²Œ ë…¼ë¬¸ ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ì—­í• ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ë…¼ë¬¸ì˜ '{section_title}' ì„¹ì…˜ ë‚´ìš©ì„ ì½ê³ , í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ 'í•´ì„¤'í•´ ì£¼ì„¸ìš”.

[ë¶„ì„í•  ì„¹ì…˜ ë‚´ìš©]:
{section_text[:6000]}

**ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì§€ì¹¨:**
1. **ë‹¨ìˆœ ë²ˆì—­ ê¸ˆì§€**: ì˜ì–´ë¥¼ í•œêµ­ì–´ë¡œ ê·¸ëŒ€ë¡œ ì˜®ê¸°ì§€ ë§ˆì‹­ì‹œì˜¤. ë‚´ìš©ì„ ì™„ì „íˆ ì†Œí™”í•œ ë’¤, ë‹¹ì‹ ì˜ ì–¸ì–´ë¡œ ë‹¤ì‹œ ì„œìˆ í•˜ì„¸ìš”.
2. **êµ¬ì¡°í™”ëœ ì¶œë ¥**: ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”.
   - **3ì¤„ í•µì‹¬ ìš”ì•½**: ì´ ì„¹ì…˜ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‚´ìš©ì„ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.
   - **ìƒì„¸ í•´ì„¤**: ë¬¸ë‹¨ë³„ ë²ˆì—­ì´ ì•„ë‹ˆë¼, ë…¼ë¦¬ì  íë¦„ì— ë”°ë¼ ì´ì•¼ê¸°ë¥¼ í’€ì–´ì„œ ì„¤ëª….
   - **ì£¼ìš” ê°œë…/ìš©ì–´**: ë³¸ë¬¸ì— ë“±ì¥í•œ ì–´ë ¤ìš´ ì „ë¬¸ ìš©ì–´ë‚˜ ê°œë…ì´ ìˆë‹¤ë©´, ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í’€ì´.
3. **í†¤ì•¤ë§¤ë„ˆ**: ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ, ì¹œì ˆí•œ ì–´ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

        response = await aclient.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful and expert research assistant."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        analysis_text = response.choices[0].message.content

        return {
            "status": "success",
            "paper_id": paper_id,
            "section_title": section_title,
            "analysis_text": analysis_text,
        }
