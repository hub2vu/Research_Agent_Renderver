import os
import logging
import json
import re
import time
from datetime import datetime
from typing import Any, Dict, List
from pathlib import Path
from openai import AsyncOpenAI

from ..base import MCPTool, ToolParameter, ExecutionError

# PDF ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("mcp.tools.page_analyzer")
logger.setLevel(logging.INFO)

# OpenAI í´ë¼ì´ì–¸íŠ¸
aclient = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

OUTPUT_DIR = Path(os.getenv("OUTPUT_DIR", "data/output"))


# =============================================================================
# í—¬í¼ í•¨ìˆ˜: ë…¼ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (Long Contextìš©)
# =============================================================================
def get_full_paper_text(paper_dir: Path) -> str:
    """JSON íŒŒì¼ì—ì„œ ë…¼ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        json_path = paper_dir / "extracted_text.json"
        if json_path.exists():
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            if isinstance(data, dict) and "full_text" in data:
                return data["full_text"][:150000]
            if isinstance(data, dict) and "pages" in data:
                return "\n".join([p.get("text", "") for p in data["pages"]])[:150000]
            if isinstance(data, list):
                return "\n".join([p.get("text", "") for p in data])[:150000]

    except Exception as e:
        logger.warning(f"ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    return ""  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜


def find_paper_directory(base_path: Path, paper_id: str) -> Path:
    target_dir = base_path / paper_id
    if target_dir.exists():
        return target_dir

    core_id = paper_id.split("arxiv.")[-1] if "arxiv." in paper_id else paper_id
    if not base_path.exists():
        return None

    for folder in base_path.iterdir():
        if folder.is_dir() and (core_id in folder.name or folder.name in paper_id):
            return folder
    return None


def get_page_text_smart(paper_dir: Path, page_num: int) -> str:
    """JSON ìš°ì„  í™•ì¸ í›„ PDF ì§ì ‘ ì½ê¸°"""
    json_path = paper_dir / "extracted_text.json"

    if json_path.exists():
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            pages_list = []
            if isinstance(data, dict) and "pages" in data:
                pages_list = data["pages"]
            elif isinstance(data, list):
                pages_list = data

            for p in pages_list:
                p_num = p.get("page") or p.get("page_number")
                if str(p_num) == str(page_num):
                    return p.get("text", "") or p.get("content", "")
        except Exception:
            pass

    pdf_files = list(paper_dir.glob("*.pdf"))
    if pdf_files and PdfReader:
        try:
            reader = PdfReader(pdf_files[0])
            idx = int(page_num) - 1
            if 0 <= idx < len(reader.pages):
                return reader.pages[idx].extract_text()
        except Exception:
            pass

    return None


# =============================================================================
# ë©”ì¸ í•¨ìˆ˜: í˜ì´ì§€ í•´ì„
# =============================================================================
async def interpret_paper_page(paper_id: str, page_num: int) -> Dict[str, Any]:
    try:
        logger.info(f"ğŸš€ [í•´ì„ ìš”ì²­] ID: {paper_id}, Page: {page_num}")

        base_path = OUTPUT_DIR
        paper_dir = find_paper_directory(base_path, paper_id)

        if not paper_dir:
            return {"error": f"Folder not found: {paper_id}"}

        page_text = get_page_text_smart(paper_dir, page_num)
        if not page_text:
            return {
                "page": page_num,
                "original_text": "í…ìŠ¤íŠ¸ ì—†ìŒ",
                "interpretation": "í•´ë‹¹ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            }

        # ğŸ”¥ ì „ì²´ ë§¥ë½ ê°€ì ¸ì˜¤ê¸°
        full_context = get_full_paper_text(paper_dir)

        prompt = f"""
        ë‹¹ì‹ ì€ ë…¸ë ¨í•œ AI ì—°êµ¬ì›ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ **[ë…¼ë¬¸ ì „ì²´ ë§¥ë½]**ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìê°€ ë³´ê³  ìˆëŠ” **[í˜„ì¬ í˜ì´ì§€]**ë¥¼ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ í•´ì„¤í•´ ì£¼ì„¸ìš”.

        [ì°¸ê³ : ë…¼ë¬¸ ì „ì²´ ë§¥ë½]
        {full_context[:10000]}... (ìƒëµ)

        [ë¶„ì„í•  í˜„ì¬ í˜ì´ì§€ (Page {page_num})]
        {page_text[:4000]}

        **ì§€ì¹¨:**
        1. **ë‹¨ìˆœ ë²ˆì—­ ê¸ˆì§€**: ë‚´ìš©ì„ ì™„ì „íˆ ì†Œí™”í•œ ë’¤, ë‹¹ì‹ ì˜ ì–¸ì–´ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
        2. **ì „ì²´ ë§¥ë½ ë°˜ì˜**: ì´ í˜ì´ì§€ê°€ ë…¼ë¬¸ ì „ì²´ íë¦„ì—ì„œ ì–´ë–¤ ìœ„ì¹˜ì¸ì§€, ì•ë’¤ ë‚´ìš©ê³¼ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ ì–¸ê¸‰í•˜ì„¸ìš”.
        3. **í˜•ì‹ ìœ ì§€**:
           - **ğŸ’¡ 3ì¤„ í•µì‹¬ ìš”ì•½**
           - **ğŸ“– ìƒì„¸ í•´ì„¤**
           - **ğŸ§  ì£¼ìš” ê°œë…/ìš©ì–´**
        """

        response = await aclient.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful and expert research assistant.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        interpretation = response.choices[0].message.content

        return {
            "page": page_num,
            "original_text": (
                page_text[:200] + "..." if len(page_text) > 200 else page_text
            ),
            "interpretation": interpretation,
        }

    except Exception as e:
        logger.error(f"ğŸ”¥ Server Error: {e}", exc_info=True)
        return {"error": str(e)}


# =============================================================================
# Tool: Section Analysis (ì „ì²´ ë§¥ë½ ë°˜ì˜ + í…ìŠ¤íŠ¸ ì¶œë ¥ ìœ ì§€)
# =============================================================================
class AnalyzeSectionTool(MCPTool):
    """Analyze a specific section using full paper context."""

    @property
    def name(self) -> str:
        return "analyze_section"

    @property
    def description(self) -> str:
        return "Analyze a specific section of a paper with full context awareness."

    @property
    def parameters(self) -> List[ToolParameter]:
        return [
            ToolParameter(
                name="paper_id", type="string", description="Paper ID", required=True
            ),
            ToolParameter(
                name="section_title",
                type="string",
                description="Section heading",
                required=True,
            ),
            ToolParameter(
                name="next_section_title",
                type="string",
                description="Next section heading",
                required=False,
                default="",
            ),
        ]

    # ì‚¬ìš©ìë‹˜ì˜ íŠ¼íŠ¼í•œ ì¶”ì¶œ ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    def _extract_section_from_text(
        self, full_text: str, section_title: str, next_section_title: str
    ) -> str:
        lines = full_text.split("\n")

        def normalize(s: str) -> str:
            return re.sub(r"\s+", " ", s).strip().lower()

        def strip_section_number(s: str) -> str:
            s = re.sub(r"^[\d]+\.[\d.]*\s*", "", s)
            s = re.sub(r"^[A-Za-z]\.[\d.]*\s*", "", s)
            s = re.sub(r"^\d+\s+", "", s)
            return s.strip()

        def normalize_for_match(s: str) -> str:
            s = normalize(s)
            s = strip_section_number(s)
            s = re.sub(r"[:\-â€“â€”]", " ", s)
            s = re.sub(r"\s+", " ", s).strip()
            return s

        def get_core_words(s: str) -> set:
            norm = normalize_for_match(s)
            return {w for w in norm.split() if len(w) > 2}

        def match_score(title: str, line: str) -> float:
            norm_title = normalize_for_match(title)
            norm_line = normalize_for_match(line)
            if norm_title == norm_line:
                return 1.0
            if norm_title in norm_line or norm_line in norm_title:
                return 0.9
            title_words = get_core_words(title)
            line_words = get_core_words(line)
            if not title_words:
                return 0.0
            overlap = len(title_words & line_words)
            return overlap / len(title_words)

        norm_target = normalize(section_title)
        norm_next = normalize(next_section_title) if next_section_title else ""

        start_idx = None
        best_score = 0.0

        for i, line in enumerate(lines):
            if len(line.strip()) < 3:
                continue
            score = match_score(section_title, line)
            if i + 1 < len(lines):
                combined = line + " " + lines[i + 1]
                combined_score = match_score(section_title, combined)
                score = max(score, combined_score)
            if score > best_score and score >= 0.5:
                best_score = score
                start_idx = i
            if score >= 0.9:
                break

        if start_idx is None:
            target_words = get_core_words(section_title)
            if target_words:
                longest_word = max(target_words, key=len)
                if len(longest_word) >= 4:
                    for i, line in enumerate(lines):
                        if longest_word in normalize(line):
                            start_idx = i
                            break

        if start_idx is None:
            logger.warning(
                f"ì„¹ì…˜ íƒ€ì´í‹€ '{section_title}'ì„ ì°¾ì§€ ëª»í•´ ì „ì²´ í…ìŠ¤íŠ¸ ì¼ë¶€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            return full_text[:4000]

        end_idx = len(lines)
        if norm_next:
            best_end_score = 0.0
            for i in range(start_idx + 1, len(lines)):
                line = lines[i]
                if len(line.strip()) < 3:
                    continue
                score = match_score(next_section_title, line)
                if i + 1 < len(lines):
                    combined = line + " " + lines[i + 1]
                    combined_score = match_score(next_section_title, combined)
                    score = max(score, combined_score)
                if score > best_end_score and score >= 0.5:
                    best_end_score = score
                    end_idx = i
                if score >= 0.9:
                    break

        section_lines = lines[start_idx + 1 : end_idx]
        section_text = "\n".join(section_lines).strip()

        if not section_text:
            return full_text[start_idx : start_idx + 4000]

        return section_text

    async def execute(
        self, paper_id: str, section_title: str, next_section_title: str = "", **kwargs
    ) -> Dict[str, Any]:
        paper_dir = OUTPUT_DIR / paper_id

        # 1. íŒŒì¼ ì½ê¸°
        text_file = paper_dir / "extracted_text.txt"
        full_text = ""

        if not text_file.exists():
            json_file = paper_dir / "extracted_text.json"
            if json_file.exists():
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    full_text = data.get("full_text", "")
        else:
            with open(text_file, "r", encoding="utf-8") as f:
                full_text = f.read()

        # Fallback
        if not full_text:
            full_text = get_full_paper_text(paper_dir)

        if isinstance(full_text, str):
            full_text = full_text.encode("utf-8", "replace").decode("utf-8")

        # 2. ì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ì¶”ì¶œ ë¡œì§ ì‚¬ìš©)
        section_text = self._extract_section_from_text(
            full_text, section_title, next_section_title
        )

        # 3. í”„ë¡¬í”„íŠ¸ ìˆ˜ì • (Full Context ì¶”ê°€)
        prompt = f"""ë‹¹ì‹ ì€ ë…¸ë ¨í•œ AI ì—°êµ¬ì›ìœ¼ë¡œì„œ, ë™ë£Œì—ê²Œ ë…¼ë¬¸ ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ì—­í• ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤.

ì•„ë˜ ì œê³µëœ **[ë…¼ë¬¸ ì „ì²´ ë‚´ìš©]**ì„ ì°¸ê³ í•˜ì—¬ ë§¥ë½ì„ ì´í•´í•˜ë˜, ì„¤ëª…ì€ ë°˜ë“œì‹œ **[ë¶„ì„í•  ì„¹ì…˜ ë‚´ìš©]**ì— ì§‘ì¤‘í•´ì„œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

[ì°¸ê³ : ë…¼ë¬¸ ì „ì²´ ë‚´ìš©]
{full_text}
(ì´ ë‚´ìš©ì€ ì „ì²´ íë¦„ ì´í•´ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.)

[ë¶„ì„í•  ì„¹ì…˜ ë‚´ìš©: {section_title}]
{section_text[:8000]}

**ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì§€ì¹¨:**
1. **ë²”ìœ„ ì œí•œ**: ì „ì²´ ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ì˜¤ì§ ìœ„ **[ë¶„ì„í•  ì„¹ì…˜ ë‚´ìš©]**ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ìƒì„¸íˆ í•´ì„¤í•˜ì„¸ìš”.
2. **ì‹¬ì¸µ í•´ì„¤**: ë‹¨ìˆœ ë²ˆì—­ì´ ì•„ë‹ˆë¼, ì „ì²´ ë§¥ë½ ì†ì—ì„œ ì´ ì„¹ì…˜ì´ ê°–ëŠ” ì˜ë¯¸ë¥¼ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
3. **êµ¬ì¡°í™”ëœ ì¶œë ¥**: ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”. (JSON ì•„ë‹˜, í…ìŠ¤íŠ¸ í˜•ì‹ ìœ ì§€)
   - **3ì¤„ í•µì‹¬ ìš”ì•½**: ì´ ì„¹ì…˜ì˜ í•µì‹¬ì„ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.
   - **ìƒì„¸ í•´ì„¤**: ë¬¸ë‹¨ë³„ ë‚´ìš©ì„ ë…¼ë¦¬ì  íë¦„ì— ë”°ë¼ ì„¤ëª….
   - **ì£¼ìš” ê°œë…/ìš©ì–´**: ë“±ì¥í•˜ëŠ” ì „ë¬¸ ìš©ì–´ í’€ì´.
4. **í†¤ì•¤ë§¤ë„ˆ**: ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

        response = await aclient.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful and expert research assistant.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        analysis_text = response.choices[0].message.content

        # Save to file system
        notes_dir = paper_dir / "notes"
        notes_dir.mkdir(parents=True, exist_ok=True)
        notes_file = notes_dir / "notes.json"
        
        # Load existing notes if available
        notes_data = {"notes": [], "updated_at": None}
        if notes_file.exists():
            try:
                with open(notes_file, "r", encoding="utf-8") as f:
                    notes_data = json.load(f)
            except:
                pass
        
        # Find or create note for this section
        notes_list = notes_data.get("notes", [])
        note_found = False
        for note in notes_list:
            if note.get("title") == section_title:
                note["content"] = analysis_text
                note["analyzed_at"] = datetime.now().isoformat()
                note_found = True
                break
        
        if not note_found:
            notes_list.append({
                "id": f"note_{int(time.time())}_{section_title[:20].replace(' ', '_')}",
                "title": section_title,
                "content": analysis_text,
                "isOpen": True,
                "analyzed_at": datetime.now().isoformat(),
            })
        
        notes_data["notes"] = notes_list
        notes_data["updated_at"] = datetime.now().isoformat()
        
        with open(notes_file, "w", encoding="utf-8") as f:
            json.dump(notes_data, f, ensure_ascii=False, indent=2)

        return {
            "status": "success",
            "paper_id": paper_id,
            "section_title": section_title,
            "analysis_text": analysis_text,
            "saved_to": str(notes_file),
        }


# =============================================================================
# í—¬í¼ í•¨ìˆ˜: ì´ˆë¡ ì¶”ì¶œ (translate.pyì˜ _extract_abstract_intro íŒ¨í„´ ì¬ì‚¬ìš©)
# =============================================================================
def _extract_abstract(text: str) -> str:
    """
    ë…¼ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ Abstract ì„¹ì…˜ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # Abstract ì„¹ì…˜ ì°¾ê¸°
    abstract_pattern = re.compile(
        r"(?i)^\s*(?:abstract|summary)\s*:?\s*\n(.*?)(?=\n\s*(?:1\.|introduction|keywords|index terms|ccs concepts|acm reference format|introduction:))",
        re.MULTILINE | re.DOTALL,
    )
    
    abstract_match = abstract_pattern.search(text)
    
    if abstract_match:
        abstract_text = abstract_match.group(1).strip()
        if len(abstract_text) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
            return abstract_text[:3000]  # í† í° íš¨ìœ¨ì„±ì„ ìœ„í•´ ìµœëŒ€ 3000ì
    
    # Fallback: í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ ì‚¬ìš©
    return text[:2000]


# =============================================================================
# Tool: Paper QA (ì´ˆë¡ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€)
# =============================================================================
class PaperQATool(MCPTool):
    """Answer questions about a paper using abstract as context."""

    @property
    def name(self) -> str:
        return "paper_qa"

    @property
    def description(self) -> str:
        return "Answer questions about a specific paper using the abstract as primary context. Optionally includes current section context."

    @property
    def parameters(self) -> List[ToolParameter]:
        return [
            ToolParameter(
                name="paper_id",
                type="string",
                description="Paper ID (folder name in output directory)",
                required=True,
            ),
            ToolParameter(
                name="question",
                type="string",
                description="User's question about the paper",
                required=True,
            ),
            ToolParameter(
                name="section_context",
                type="string",
                description="Optional: current section text the user is viewing",
                required=False,
                default="",
            ),
        ]

    @property
    def category(self) -> str:
        return "analysis"

    async def execute(
        self, paper_id: str, question: str, section_context: str = "", **kwargs
    ) -> Dict[str, Any]:
        # 1. ë…¼ë¬¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        paper_dir = find_paper_directory(OUTPUT_DIR, paper_id)
        if not paper_dir:
            paper_dir = OUTPUT_DIR / paper_id
        
        # 2. ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ
        full_text = get_full_paper_text(paper_dir)
        if not full_text:
            # Fallback: txt íŒŒì¼ ì‹œë„
            text_file = paper_dir / "extracted_text.txt"
            if text_file.exists():
                with open(text_file, "r", encoding="utf-8") as f:
                    full_text = f.read()
        
        if not full_text:
            return {
                "status": "error",
                "error": f"ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {paper_id}. ë¨¼ì € extract_allì„ ì‹¤í–‰í•˜ì„¸ìš”.",
            }
        
        # 3. ì´ˆë¡ ì¶”ì¶œ
        abstract = _extract_abstract(full_text)
        
        # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        section_part = ""
        if section_context and section_context.strip():
            section_part = f"""
[í˜„ì¬ ë³´ê³  ìˆëŠ” ì„¹ì…˜]
{section_context[:3000]}
"""
        
        prompt = f"""ë‹¹ì‹ ì€ ì´ ë…¼ë¬¸ì„ ì½ì€ AI ì—°êµ¬ ë™ë£Œì…ë‹ˆë‹¤. ì•„ë˜ ë…¼ë¬¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ë…¼ë¬¸ ì´ˆë¡]
{abstract}
{section_part}
[ì§ˆë¬¸]
{question}

**ì§€ì¹¨:**
1. ë…¼ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë…¼ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , "ë…¼ë¬¸ì—ì„œ ì§ì ‘ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
3. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. í•„ìš”ì‹œ ë…¼ë¬¸ì˜ ë§¥ë½ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""

        # 5. LLM í˜¸ì¶œ
        response = await aclient.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful research assistant who has read the paper thoroughly.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )

        answer = response.choices[0].message.content

        return {
            "status": "success",
            "paper_id": paper_id,
            "question": question,
            "answer": answer,
            "context_used": {
                "abstract_length": len(abstract),
                "section_context_provided": bool(section_context),
            },
        }