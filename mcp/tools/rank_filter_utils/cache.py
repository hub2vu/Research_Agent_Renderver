"""
Caching utilities for NeurIPS search results.

Provides caching functionality to avoid redundant searches and computations.
"""

import hashlib
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Optional

from .path_resolver import ensure_directory, resolve_path


def _generate_cache_key(query: str, profile: Dict[str, Any]) -> str:
    """
    Generate a cache key from query and profile.
    
    Args:
        query: Search query string
        profile: User profile dictionary
        
    Returns:
        Cache key string (MD5 hash)
    """
    # Normalize query (lowercase, strip whitespace)
    normalized_query = query.lower().strip()
    
    # Create profile string (sorted keys for consistency)
    profile_str = json.dumps({
        "interests": profile.get("interests", {}),
        "keywords": profile.get("keywords", {}),
        "constraints": profile.get("constraints", {})
    }, sort_keys=True)
    
    # Generate hash
    query_hash = hashlib.md5(normalized_query.encode()).hexdigest()[:8]
    profile_hash = hashlib.md5(profile_str.encode()).hexdigest()[:8]
    
    return f"{query_hash}_{profile_hash}"


def load_cache(
    cache_key: str,
    cache_dir: str = "cache/neurips_search",
    cache_ttl_hours: int = 24
) -> Optional[Dict[str, Any]]:
    """
    Load cached search results.
    
    Args:
        cache_key: Cache key generated by _generate_cache_key
        cache_dir: Cache directory path (relative to OUTPUT_DIR)
        cache_ttl_hours: Cache time-to-live in hours (default: 24)
        
    Returns:
        Cached result dictionary if found and not expired, None otherwise
    """
    cache_path = resolve_path(f"{cache_dir}/{cache_key}.json", path_type="output")
    
    if not cache_path.exists():
        return None
    
    try:
        with open(cache_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Check expiration
        timestamp_str = data.get("timestamp", "")
        if not timestamp_str:
            # Old cache format without timestamp, consider expired
            cache_path.unlink()
            return None
        
        timestamp = datetime.fromisoformat(timestamp_str)
        if datetime.now() - timestamp > timedelta(hours=cache_ttl_hours):
            # Cache expired, delete it
            cache_path.unlink()
            return None
        
        return data.get("result")
    except Exception:
        # If any error occurs, delete corrupted cache
        try:
            cache_path.unlink()
        except Exception:
            pass
        return None


def save_cache(
    cache_key: str,
    result: Dict[str, Any],
    cache_dir: str = "cache/neurips_search"
) -> None:
    """
    Save search results to cache.
    
    Args:
        cache_key: Cache key generated by _generate_cache_key
        result: Result dictionary to cache
        cache_dir: Cache directory path (relative to OUTPUT_DIR)
    """
    cache_path = resolve_path(f"{cache_dir}/{cache_key}.json", path_type="output")
    ensure_directory(cache_path.parent)
    
    data = {
        "timestamp": datetime.now().isoformat(),
        "result": result
    }
    
    try:
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        # Log error but don't fail the operation
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(f"Failed to save cache: {e}")


def clear_cache(
    cache_dir: str = "cache/neurips_search",
    older_than_hours: Optional[int] = None
) -> int:
    """
    Clear cache files.
    
    Args:
        cache_dir: Cache directory path (relative to OUTPUT_DIR)
        older_than_hours: If provided, only delete caches older than this many hours.
                         If None, delete all caches.
        
    Returns:
        Number of cache files deleted
    """
    cache_base_path = resolve_path(cache_dir, path_type="output")
    
    if not cache_base_path.exists():
        return 0
    
    deleted_count = 0
    cutoff_time = None
    if older_than_hours is not None:
        cutoff_time = datetime.now() - timedelta(hours=older_than_hours)
    
    try:
        for cache_file in cache_base_path.glob("*.json"):
            if older_than_hours is not None:
                # Check file modification time
                file_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if file_mtime > cutoff_time:
                    continue
            
            try:
                cache_file.unlink()
                deleted_count += 1
            except Exception:
                pass
    except Exception:
        pass
    
    return deleted_count
